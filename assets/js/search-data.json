{"0": {
    "doc": 404,
    "title": "OpenSearch cannot find that page.",
    "content": "Perhaps we moved something around, or you mistyped the URL? Try using search or go to the OpenSearch Documentation home page. If you need further help, see the OpenSearch community forum. ",
    "url": "https://vagimeli.github.io/404.html#opensearch-cannot-find-that-page",
    "relUrl": "/404.html#opensearch-cannot-find-that-page"
  },"1": {
    "doc": 404,
    "title": 404,
    "content": " ",
    "url": "https://vagimeli.github.io/404.html",
    "relUrl": "/404.html"
  },"2": {
    "doc": "About OpenSearch",
    "title": "Introduction to OpenSearch",
    "content": "OpenSearch is a distributed search and analytics engine based on Apache Lucene. After adding your data to OpenSearch, you can perform full-text searches on it with all of the features you might expect: search by field, search multiple indices, boost fields, rank results by score, sort results by field, and aggregate results. Unsurprisingly, people often use search engines like OpenSearch as the backend for a search application—think Wikipedia or an online store. It offers excellent performance and can scale up and down as the needs of the application grow or shrink. An equally popular, but less obvious use case is log analytics, in which you take the logs from an application, feed them into OpenSearch, and use the rich search and visualization functionality to identify issues. For example, a malfunctioning web server might throw a 500 error 0.5% of the time, which can be hard to notice unless you have a real-time graph of all HTTP status codes that the server has thrown in the past four hours. You can use OpenSearch Dashboards to build these sorts of visualizations from data in OpenSearch. ",
    "url": "https://vagimeli.github.io/about/#introduction-to-opensearch",
    "relUrl": "/about/#introduction-to-opensearch"
  },"3": {
    "doc": "About OpenSearch",
    "title": "Clusters and nodes",
    "content": "Its distributed design means that you interact with OpenSearch clusters. Each cluster is a collection of one or more nodes, servers that store your data and process search requests. You can run OpenSearch locally on a laptop—its system requirements are minimal—but you can also scale a single cluster to hundreds of powerful machines in a data center. In a single node cluster, such as a laptop, one machine has to do everything: manage the state of the cluster, index and search data, and perform any preprocessing of data prior to indexing it. As a cluster grows, however, you can subdivide responsibilities. Nodes with fast disks and plenty of RAM might be great at indexing and searching data, whereas a node with plenty of CPU power and a tiny disk could manage cluster state. For more information on setting node types, see Cluster formation. ",
    "url": "https://vagimeli.github.io/about/#clusters-and-nodes",
    "relUrl": "/about/#clusters-and-nodes"
  },"4": {
    "doc": "About OpenSearch",
    "title": "Indices and documents",
    "content": "OpenSearch organizes data into indices. Each index is a collection of JSON documents. If you have a set of raw encyclopedia articles or log lines that you want to add to OpenSearch, you must first convert them to JSON. A simple JSON document for a movie might look like this: . { \"title\": \"The Wind Rises\", \"release_date\": \"2013-07-20\" } . When you add the document to an index, OpenSearch adds some metadata, such as the unique document ID: . { \"_index\": \"&lt;index-name&gt;\", \"_type\": \"_doc\", \"_id\": \"&lt;document-id&gt;\", \"_version\": 1, \"_source\": { \"title\": \"The Wind Rises\", \"release_date\": \"2013-07-20\" } } . Indices also contain mappings and settings: . | A mapping is the collection of fields that documents in the index have. In this case, those fields are title and release_date. | Settings include data like the index name, creation date, and number of shards. | . ",
    "url": "https://vagimeli.github.io/about/#indices-and-documents",
    "relUrl": "/about/#indices-and-documents"
  },"5": {
    "doc": "About OpenSearch",
    "title": "Primary and replica shards",
    "content": "OpenSearch splits indices into shards for even distribution across nodes in a cluster. For example, a 400 GB index might be too large for any single node in your cluster to handle, but split into ten shards, each one 40 GB, OpenSearch can distribute the shards across ten nodes and work with each shard individually. By default, OpenSearch creates a replica shard for each primary shard. If you split your index into ten shards, for example, OpenSearch also creates ten replica shards. These replica shards act as backups in the event of a node failure—OpenSearch distributes replica shards to different nodes than their corresponding primary shards—but they also improve the speed and rate at which the cluster can process search requests. You might specify more than one replica per index for a search-heavy workload. Despite being a piece of an OpenSearch index, each shard is actually a full Lucene index—confusing, we know. This detail is important, though, because each instance of Lucene is a running process that consumes CPU and memory. More shards is not necessarily better. Splitting a 400 GB index into 1,000 shards, for example, would place needless strain on your cluster. A good rule of thumb is to keep shard size between 10–50 GB. ",
    "url": "https://vagimeli.github.io/about/#primary-and-replica-shards",
    "relUrl": "/about/#primary-and-replica-shards"
  },"6": {
    "doc": "About OpenSearch",
    "title": "REST API",
    "content": "You interact with OpenSearch clusters using the REST API, which offers a lot of flexibility. You can use clients like curl or any programming language that can send HTTP requests. To add a JSON document to an OpenSearch index (i.e. index a document), you send an HTTP request: . PUT https://&lt;host&gt;:&lt;port&gt;/&lt;index-name&gt;/_doc/&lt;document-id&gt; { \"title\": \"The Wind Rises\", \"release_date\": \"2013-07-20\" } . To run a search for the document: . GET https://&lt;host&gt;:&lt;port&gt;/&lt;index-name&gt;/_search?q=wind . To delete the document: . DELETE https://&lt;host&gt;:&lt;port&gt;/&lt;index-name&gt;/_doc/&lt;document-id&gt; . You can change most OpenSearch settings using the REST API, modify indices, check the health of the cluster, get statistics—almost everything. ",
    "url": "https://vagimeli.github.io/about/#rest-api",
    "relUrl": "/about/#rest-api"
  },"7": {
    "doc": "About OpenSearch",
    "title": "About OpenSearch",
    "content": " ",
    "url": "https://vagimeli.github.io/about/",
    "relUrl": "/about/"
  },"8": {
    "doc": "Breaking changes",
    "title": "2.0.0",
    "content": "Remove mapping types parameter . The type parameter has been removed from all OpenSearch API endpoints. Instead, indexes can be categorized by document type. For more details, see issue #1940. Deprecate non-inclusive terms . Non-inclusive terms are deprecated in version 2.x and will be permanently removed in OpenSearch 3.0. We are using the following replacements: . | “Whitelist” is now “Allow list” | “Blacklist” is now “Deny list” | “Master” is now “Cluster Manager” | . Add OpenSearch Notifications plugins . In OpenSearch 2.0, the Alerting plugin is now integrated with new plugins for Notifications. If you want to continue to use the notification action in the Alerting plugin, install the new backend plugins notifications-core and notifications. If you want to manage notifications in OpenSearch Dashboards, use the new notificationsDashboards plugin. For more information, see Questions about destinations on the Monitors page. Drop support for JDK 8 . A Lucene upgrade forced OpenSearch to drop support for JDK 8. As a consequence, the Java high-level REST client no longer supports JDK 8. Restoring JDK 8 support is currently an opensearch-java proposal #156 and will require removing OpenSearch core as a dependency from the Java client (issue #262). ",
    "url": "https://vagimeli.github.io/breaking-changes/#200",
    "relUrl": "/breaking-changes/#200"
  },"9": {
    "doc": "Breaking changes",
    "title": "Breaking changes",
    "content": " ",
    "url": "https://vagimeli.github.io/breaking-changes/",
    "relUrl": "/breaking-changes/"
  },"10": {
    "doc": "OpenSearch documentation",
    "title": "Getting started",
    "content": ". | About OpenSearch | Quickstart | Install OpenSearch | Install OpenSearch Dashboards | See the FAQ | . ",
    "url": "https://vagimeli.github.io/#getting-started",
    "relUrl": "/#getting-started"
  },"11": {
    "doc": "OpenSearch documentation",
    "title": "Why use OpenSearch?",
    "content": "With OpenSearch, you can perform the following use cases: . | | | | . | Fast, Scalable Full-text Search | Application and Infrastructure Monitoring | Security and Event Information Management | Operational Health Tracking | . | Help users find the right information within your application, website, or data lake catalog. | Easily store and analyze log data, and set automated alerts for underperformance. | Centralize logs to enable real-time security monitoring and forensic analysis. | Use observability logs, metrics, and traces to monitor your applications and business in real time. | . Additional features and plugins: . OpenSearch has several features and plugins to help index, secure, monitor, and analyze your data. Most OpenSearch plugins have corresponding OpenSearch Dashboards plugins that provide a convenient, unified user interface. | Anomaly detection - Identify atypical data and receive automatic notifications | KNN - Find “nearest neighbors” in your vector data | Performance Analyzer - Monitor and optimize your cluster | SQL - Use SQL or a piped processing language to query your data | Index State Management - Automate index operations | ML Commons plugin - Train and execute machine-learning models | Asynchronous search - Run search requests in the background | Cross-cluster replication - Replicate your data across multiple OpenSearch clusters | . ",
    "url": "https://vagimeli.github.io/#why-use-opensearch",
    "relUrl": "/#why-use-opensearch"
  },"12": {
    "doc": "OpenSearch documentation",
    "title": "The secure path forward",
    "content": "OpenSearch includes a demo configuration so that you can get up and running quickly, but before using OpenSearch in a production environment, you must configure the Security plugin manually with your own certificates, authentication method, users, and passwords. ",
    "url": "https://vagimeli.github.io/#the-secure-path-forward",
    "relUrl": "/#the-secure-path-forward"
  },"13": {
    "doc": "OpenSearch documentation",
    "title": "Looking for the Javadoc?",
    "content": "See opensearch.org/javadocs/. ",
    "url": "https://vagimeli.github.io/#looking-for-the-javadoc",
    "relUrl": "/#looking-for-the-javadoc"
  },"14": {
    "doc": "OpenSearch documentation",
    "title": "Get involved",
    "content": "OpenSearch is supported by Amazon Web Services. All components are available under the Apache License, Version 2.0 on GitHub. The project welcomes GitHub issues, bug fixes, features, plugins, documentation—anything at all. To get involved, see Contributing on the OpenSearch website. OpenSearch includes certain Apache-licensed Elasticsearch code from Elasticsearch B.V. and other source code. Elasticsearch B.V. is not the source of that other source code. ELASTICSEARCH is a registered trademark of Elasticsearch B.V. ",
    "url": "https://vagimeli.github.io/#get-involved",
    "relUrl": "/#get-involved"
  },"15": {
    "doc": "OpenSearch documentation",
    "title": "OpenSearch documentation",
    "content": "OpenSearch Documentation Learn to use OpenSearch, the highly scalable and extensible open-source software suite for search, analytics, observability, and other data-intensive applications. Contribute . ",
    "url": "https://vagimeli.github.io/",
    "relUrl": "/"
  },"16": {
    "doc": "Quickstart",
    "title": "Quickstart",
    "content": "Get started using OpenSearch and OpenSearch Dashboards by deploying your containers with Docker. Before proceeding, you need to get Docker and Docker Compose installed on your local machine. The Docker Compose commands used in this guide are written with a hyphen (for example, docker-compose). If you installed Docker Desktop on your machine, which automatically installs a bundled version of Docker Compose, then you should remove the hyphen. For example, change docker-compose to docker compose. ",
    "url": "https://vagimeli.github.io/quickstart/",
    "relUrl": "/quickstart/"
  },"17": {
    "doc": "Quickstart",
    "title": "Starting your cluster",
    "content": "You’ll need a special file, called a Compose file, that Docker Compose uses to define and create the containers in your cluster. The OpenSearch Project provides a sample Compose file that you can use to get started. Learn more about working with Compose files by reviewing the official Compose specification. | Before running OpenSearch on your machine, you should disable memory paging and swapping performance on the host to improve performance and increase the number of memory maps available to OpenSearch. See important system settings for more information. # Disable memory paging and swapping. sudo swapoff -a # Edit the sysctl config file that defines the host's max map count. sudo vi /etc/sysctl.conf # Set max map count to the recommended value of 262144. vm.max_map_count=262144 # Reload the kernel parameters. sudo sysctl -p . | Download the sample Compose file to your host. You can download the file with command line utilities like curl and wget, or you can manually copy docker-compose.yml from the OpenSearch Project documentation-website repository using a web browser. # Using cURL: curl -O https://raw.githubusercontent.com/opensearch-project/documentation-website/2.7/assets/examples/docker-compose.yml # Using wget: wget https://raw.githubusercontent.com/opensearch-project/documentation-website/2.7/assets/examples/docker-compose.yml . | In your terminal application, navigate to the directory containing the docker-compose.yml file you just downloaded, and run the following command to create and start the cluster as a background process. docker-compose up -d . | Confirm that the containers are running with the command docker-compose ps. You should see an output like the following: $ docker-compose ps NAME COMMAND SERVICE STATUS PORTS opensearch-dashboards \"./opensearch-dashbo…\" opensearch-dashboards running 0.0.0.0:5601-&gt;5601/tcp opensearch-node1 \"./opensearch-docker…\" opensearch-node1 running 0.0.0.0:9200-&gt;9200/tcp, 9300/tcp, 0.0.0.0:9600-&gt;9600/tcp, 9650/tcp opensearch-node2 \"./opensearch-docker…\" opensearch-node2 running 9200/tcp, 9300/tcp, 9600/tcp, 9650/tcp . | Query the OpenSearch REST API to verify that the service is running. You should use -k (also written as --insecure) to disable host name checking because the default security configuration uses demo certificates. Use -u to pass the default username and password (admin:admin). curl https://localhost:9200 -ku admin:admin . Sample response: . { \"name\" : \"opensearch-node1\", \"cluster_name\" : \"opensearch-cluster\", \"cluster_uuid\" : \"W0B8gPotTAajhMPbC9D4ww\", \"version\" : { \"distribution\" : \"opensearch\", \"number\" : \"2.6.0\", \"build_type\" : \"tar\", \"build_hash\" : \"7203a5af21a8a009aece1474446b437a3c674db6\", \"build_date\" : \"2023-02-24T18:58:37.352296474Z\", \"build_snapshot\" : false, \"lucene_version\" : \"9.5.0\", \"minimum_wire_compatibility_version\" : \"7.10.0\", \"minimum_index_compatibility_version\" : \"7.0.0\" }, \"tagline\" : \"The OpenSearch Project: https://opensearch.org/\" } . | Explore OpenSearch Dashboards by opening http://localhost:5601/ in a web browser on the same host that is running your OpenSearch cluster. The default username is admin and the default password is admin. | . ",
    "url": "https://vagimeli.github.io/quickstart/#starting-your-cluster",
    "relUrl": "/quickstart/#starting-your-cluster"
  },"18": {
    "doc": "Quickstart",
    "title": "Create an index and field mappings using sample data",
    "content": "Create an index and define field mappings using a dataset provided by the OpenSearch Project. The same fictitious e-commerce data is also used for sample visualizations in OpenSearch Dashboards. To learn more, see Getting started with OpenSearch Dashboards. | Download ecommerce-field_mappings.json. This file defines a mapping for the sample data you will use. # Using cURL: curl -O https://raw.githubusercontent.com/opensearch-project/documentation-website/2.7/assets/examples/ecommerce-field_mappings.json # Using wget: wget https://raw.githubusercontent.com/opensearch-project/documentation-website/2.7/assets/examples/ecommerce-field_mappings.json . | Download ecommerce.json. This file contains the index data formatted so that it can be ingested by the bulk API. To learn more, see index data and Bulk. # Using cURL: curl -O https://raw.githubusercontent.com/opensearch-project/documentation-website/2.7/assets/examples/ecommerce.json # Using wget: wget https://raw.githubusercontent.com/opensearch-project/documentation-website/2.7/assets/examples/ecommerce.json . | Define the field mappings with the mapping file. curl -H \"Content-Type: application/x-ndjson\" -X PUT \"https://localhost:9200/ecommerce\" -ku admin:admin --data-binary \"@ecommerce-field_mappings.json\" . | Upload the index to the bulk API. curl -H \"Content-Type: application/x-ndjson\" -X PUT \"https://localhost:9200/ecommerce/_bulk\" -ku admin:admin --data-binary \"@ecommerce.json\" . | Query the data using the search API. The following command submits a query that will return documents where customer_first_name is Sonya. curl -H 'Content-Type: application/json' -X GET \"https://localhost:9200/ecommerce/_search?pretty=true\" -ku admin:admin -d' {\"query\":{\"match\":{\"customer_first_name\":\"Sonya\"}}}' . Queries submitted to the OpenSearch REST API will generally return a flat JSON by default. For a human readable response body, use the query parameter pretty=true. For more information about pretty and other useful query parameters, see Common REST parameters. | Access OpenSearch Dashboards by opening http://localhost:5601/ in a web browser on the same host that is running your OpenSearch cluster. The default username is admin and the default password is admin. | On the top menu bar, go to Management &gt; Dev Tools. | In the left pane of the console, enter the following: GET ecommerce/_search { \"query\": { \"match\": { \"customer_first_name\": \"Sonya\" } } } . | Choose the triangle icon at the top right of the request to submit the query. You can also submit the request by pressing Ctrl+Enter (or Cmd+Enter for Mac users). To learn more about using the OpenSearch Dashboards console for submitting queries, see Running queries in the console. | . ",
    "url": "https://vagimeli.github.io/quickstart/#create-an-index-and-field-mappings-using-sample-data",
    "relUrl": "/quickstart/#create-an-index-and-field-mappings-using-sample-data"
  },"19": {
    "doc": "Quickstart",
    "title": "Next steps",
    "content": "You successfully deployed your own OpenSearch cluster with OpenSearch Dashboards and added some sample data. Now you’re ready to learn about configuration and functionality in more detail. Here are a few recommendations on where to begin: . | About the Security plugin | OpenSearch configuration | OpenSearch plugin installation | Getting started with OpenSearch Dashboards | OpenSearch tools | Index APIs | . ",
    "url": "https://vagimeli.github.io/quickstart/#next-steps",
    "relUrl": "/quickstart/#next-steps"
  },"20": {
    "doc": "Quickstart",
    "title": "Common issues",
    "content": "Review these common issues and suggested solutions if your containers fail to start or exit unexpectedly. Docker commands require elevated permissions . Eliminate the need for running your Docker commands with sudo by adding your user to the docker user group. See Docker’s Post-installation steps for Linux for more information. sudo usermod -aG docker $USER . Error message: “-bash: docker-compose: command not found” . If you installed Docker Desktop, then Docker Compose is already installed on your machine. Try docker compose (without the hyphen) instead of docker-compose. See Use Docker Compose. Error message: “docker: ‘compose’ is not a docker command.” . If you installed Docker Engine, then you must install Docker Compose separately, and you will use the command docker-compose (with a hyphen). See Docker Compose. Error message: “max virtual memory areas vm.max_map_count [65530] is too low” . OpenSearch will fail to start if your host’s vm.max_map_count is too low. Review the important system settings if you see the following errors in the service log, and set vm.max_map_count appropriately. opensearch-node1 | ERROR: [1] bootstrap checks failed opensearch-node1 | [1]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144] opensearch-node1 | ERROR: OpenSearch did not exit normally - check the logs at /usr/share/opensearch/logs/opensearch-cluster.log . ",
    "url": "https://vagimeli.github.io/quickstart/#common-issues",
    "relUrl": "/quickstart/#common-issues"
  },"21": {
    "doc": "Version history",
    "title": "Version history",
    "content": "| OpenSearch version | Release highlights | Release date | . | 2.7.0 | Includes searchable snapshots and segment replication, which are now generally available. Adds multiple data sources, observability features, dynamic tenant management, component templates, and shape-based map filters in OpenSearch Dashboards. Includes the flat object field type, hot shard identification, and a new automatic reloading mechanism for ML models. For a full list of release highlights, see the Release Notes. | 02 May 2023 | . | 2.6.0 | Includes simple schema for observability, index management UI enhancements, Security Analytics enhancements, search backpressure at the coordinator node level, and the ability to add maps to dashboards. Experimental features include a new ML model health dashboard, new text embedding models in ML, and SigV4 authentication in Dashboards. For a full list of release highlights, see the Release Notes. | 28 February 2023 | . | 2.5.0 | Includes index management UI enhancements, multi-layer maps, Jaeger support for observability, Debian distributions, returning cluster health by awareness attribute, cluster manager task throttling, weighted zonal search request routing policy, and query string support in index rollups. Experimental features include request-level durability in remote-backed storage and GPU acceleration for ML nodes. For a full list of release highlights, see the Release Notes. | 24 January 2023 | . | 2.4.1 | Includes maintenance changes and bug fixes for gradle check and indexing pressure tests. Adds support for skipping changelog. | 13 December 2022 | . | 2.4.0 | Includes Windows support, Point-in-time search, custom k-NN filtering, xy_point and xy_shape field types for Cartesian coordinates, GeoHex grid aggregation, and resilience enhancements, including search backpressure. In OpenSearch Dashboards, this release adds snapshot restore functionality, multiple authentication, and aggregate view of saved objects. This release includes the following experimental features: searchable snapshots, Compare Search Results, multiple data sources in OpenSearch Dashboards, a new Model Serving Framework in ML Commons, a new Neural Search plugin that supports semantic search, and a new Security Analytics plugin to analyze security logs. For a full list of release highlights, see the Release Notes. | 15 November 2022 | . | 2.3.0 | This release includes the following experimental features: segment replication, remote-backed storage, and drag and drop for OpenSearch Dashboards. Experimental features allow you to test new functionality in OpenSearch. Because these features are still being developed, your testing and feedback can help shape the development of the feature before it’s official released. We do not recommend use of experimental features in production. Additionally, this release adds maketime and makedate datetime functions for the SQL plugin. Creates a new OpenSearch Playground demo site for OpenSearch Dashboards. For a full list of release highlights, see the Release Notes. | 14 September 2022 | . | 2.2.1 | Includes gradle updates and bug fixes for gradle check. | 01 September 2022 | . | 2.2.0 | Includes support for Logistic Regression and RCF Summarize machine learning algorithms in ML Commons, Lucene or C-based Nmslib and Faiss libraries for approximate k-NN search, search by relevance using SQL and PPL queries, custom region maps for visualizations, and rollup enhancements. For a full list of release highlights, see the Release Notes. | 11 August 2022 | . | 2.1.0 | Includes support for dedicated ML node in the ML Commons plugin, relevance search and other features in SQL, multi-terms aggregation, and Snapshot Management. For a full list of release highlights, see the Release Notes. | 07 July 2022 | . | 2.0.1 | Includes bug fixes and maintenance updates for Alerting and Anomaly Detection. | 16 June 2022 | . | 2.0.0 | Includes document-level monitors for alerting, OpenSearch Notifications plugins, and Geo Map Tiles in OpenSearch Dashboards. Also adds support for Lucene 9 and bug fixes for all OpenSearch plugins. For a full list of release highlights, see the Release Notes. | 26 May 2022 | . | 2.0.0-rc1 | The Release Candidate for 2.0.0. This version allows you to preview the upcoming 2.0.0 release before the GA release. The preview release adds document-level alerting, support for Lucene 9, and the ability to use term lookup queries in document level security. | 03 May 2022 | . | 1.3.9 | Adds Debian support. Includes upgrades, enhancements, and maintenance updates for OpenSearch core, k-NN, and OpenSearch security. | 16 March 2023 | . | 1.3.8 | Adds OpenSearch security enhancements. Updates tool scripts to run on Windows. Includes maintenance updates and bug fixes for Anomaly Detection and OpenSearch security. | 02 February 2023 | . | 1.3.7 | Adds Windows support. Includes maintenance updates and bug fixes for error handling. | 13 December 2022 | . | 1.3.6 | Includes maintenance updates and bug fixes for tenancy in the OpenSearch Security Dashboards plugin. | 06 October 2022 | . | 1.3.5 | Includes maintenance updates and bug fixes for gradle check and OpenSearch security. | 01 September 2022 | . | 1.3.4 | Includes maintenance updates and bug fixes for OpenSearch and OpenSearch Dashboards. | 14 July 2022 | . | 1.3.3 | Adds enhancements to Anomaly Detection and ML Commons. Bug fixes for Anomaly Detection, Observability, and k-NN. | 09 June 2022 | . | 1.3.2 | Bug fixes for Anomaly Detection and the Security Dashboards Plugin, adds the option to install OpenSearch using RPM, as well as enhancements to the ML Commons execute task, and the removal of the job-scheduler zip in Anomaly Detection. | 05 May 2022 | . | 1.3.1 | Bug fixes when using document-level security, and adjusted ML Commons to use the latest RCF jar and protostuff to RCF model serialization. | 30 March 2022 | . | 1.3.0 | Adds Model Type Validation to Validate Detector API, continuous transforms, custom actions, applied policy parameter to Explain API, default action retries, and new rollover and transition conditions to Index Management, new ML Commons plugin, parse command to SQL, Application Analytics, Live Tail, Correlation, and Events Flyout to Observability, and auto backport and support for OPENSEARCH_JAVA_HOME to Performance Analyzer. Bug fixes. | 17 March 2022 | . | 1.2.4 | Updates Performance Analyzer, SQL, and Security plugins to Log4j 2.17.1, Alerting and Job Scheduler to cron-utils 9.1.6, and gson in Anomaly Detection and SQL. | 18 January 2022 | . | 1.2.3 | Updates the version of Log4j used in OpenSearch to Log4j 2.17.0 as recommended by the advisory in CVE-2021-45105. | 22 December 2021 | . | 1.2.0 | Adds observability, new validation API for Anomaly Detection, shard-level indexing back-pressure, new “match” query type for SQL and PPL, support for Faiss libraries in k-NN, and custom Dashboards branding. | 23 November 2021 | . | 1.1.0 | Adds cross-cluster replication, security for Index Management, bucket-level alerting, a CLI to help with upgrading from Elasticsearch OSS to OpenSearch, and enhancements to high cardinality data in the anomaly detection plugin. | 05 October 2021 | . | 1.0.1 | Bug fixes. | 01 September 2021 | . | 1.0.0 | General availability release. Adds compatibility setting for clients that require a version check before connecting. | 12 July 2021 | . | 1.0.0-rc1 | First release candidate. | 07 June 2021 | . | 1.0.0-beta1 | Initial beta release. Refactors plugins to work with OpenSearch. | 13 May 2021 | . ",
    "url": "https://vagimeli.github.io/version-history/",
    "relUrl": "/version-history/"
  },"22": {
    "doc": "Configuring OpenSearch",
    "title": "Configuring OpenSearch",
    "content": "Most OpenSearch configuration can take place in the cluster settings API. Certain operations require you to modify opensearch.yml and restart the cluster. Whenever possible, use the cluster settings API instead; opensearch.yml is local to each node, whereas the API applies the setting to all nodes in the cluster. Certain settings, however, require opensearch.yml. In general, these settings relate to networking, cluster formation, and the local file system. To learn more, see Cluster formation. ",
    "url": "https://vagimeli.github.io/install-and-configure/configuration/",
    "relUrl": "/install-and-configure/configuration/"
  },"23": {
    "doc": "Configuring OpenSearch",
    "title": "Specify settings as environment variables",
    "content": "You can specify environment variables as arguments using -E when launching OpenSearch: ./opensearch -Ecluster.name=opensearch-cluster -Enode.name=opensearch-node1 -Ehttp.host=0.0.0.0 -Ediscovery.type=single-node . ",
    "url": "https://vagimeli.github.io/install-and-configure/configuration/#specify-settings-as-environment-variables",
    "relUrl": "/install-and-configure/configuration/#specify-settings-as-environment-variables"
  },"24": {
    "doc": "Configuring OpenSearch",
    "title": "Update cluster settings using the API",
    "content": "The first step in changing a setting is to view the current settings: . GET _cluster/settings?include_defaults=true . For a more concise summary of non-default settings: . GET _cluster/settings . Three categories of setting exist in the cluster settings API: persistent, transient, and default. Persistent settings, well, persist after a cluster restart. After a restart, OpenSearch clears transient settings. If you specify the same setting in multiple places, OpenSearch uses the following precedence: . | Transient settings | Persistent settings | Settings from opensearch.yml | Default settings | . To change a setting, just specify the new one as either persistent or transient. This example shows the flat settings form: . PUT _cluster/settings { \"persistent\" : { \"action.auto_create_index\" : false } } . You can also use the expanded form, which lets you copy and paste from the GET response and change existing values: . PUT _cluster/settings { \"persistent\": { \"action\": { \"auto_create_index\": false } } } . For more information about the Cluster Settings API, see Cluster settings. ",
    "url": "https://vagimeli.github.io/install-and-configure/configuration/#update-cluster-settings-using-the-api",
    "relUrl": "/install-and-configure/configuration/#update-cluster-settings-using-the-api"
  },"25": {
    "doc": "Configuring OpenSearch",
    "title": "Configuration file",
    "content": "You can find opensearch.yml in /usr/share/opensearch/config/opensearch.yml (Docker) or /etc/opensearch/opensearch.yml (most Linux distributions) on each node. You can edit the OPENSEARCH_PATH_CONF=/etc/opensearch to change the config directory location. This variable is sourced from /etc/default/opensearch(Debian package) and /etc/sysconfig/opensearch(RPM package). If you set your customized OPENSEARCH_PATH_CONF variable, be aware that other default environment variables will not be loaded. You don’t mark settings in opensearch.yml as persistent or transient, and settings use the flat form: . cluster.name: my-application action.auto_create_index: true compatibility.override_main_response_version: true . The demo configuration includes a number of settings for the Security plugin that you should modify before using OpenSearch for a production workload. To learn more, see Security. (Optional) CORS header configuration . If you are working on a client application running against an OpenSearch cluster on a different domain, you can configure headers in opensearch.yml to allow for developing a local application on the same machine. Use Cross Origin Resource Sharing so your application can make calls to the OpenSearch API running locally. Add the following lines in your custom-opensearch.yml file (note that the “-“ must be the first character in each line). - http.host:0.0.0.0 - http.port:9200 - http.cors.allow-origin:\"http://localhost\" - http.cors.enabled:true - http.cors.allow-headers:X-Requested-With,X-Auth-Token,Content-Type,Content-Length,Authorization - http.cors.allow-credentials:true . ",
    "url": "https://vagimeli.github.io/install-and-configure/configuration/#configuration-file",
    "relUrl": "/install-and-configure/configuration/#configuration-file"
  },"26": {
    "doc": "Install and upgrade OpenSearch",
    "title": "Install and upgrade OpenSearch",
    "content": "OpenSearch and OpenSearch Dashboards are available on any compatible host that supports Docker (such as Linux, MacOS, or Windows). Additionally, you can install both products on various Linux distributions and on Windows. Download OpenSearch for your preferred platform and then choose one of the following installation guides. | OpenSearch | OpenSearch Dashboards | . | Docker | Docker | . | Helm | Helm | . | Tarball | Tarball | . | RPM | RPM | . | Debian | Debian | . | Ansible playbook |   | . | Windows | Windows | . After you’ve installed OpenSearch, learn about configuring it for your deployment. For more information about upgrading your OpenSearch cluster, see the upgrade guide. For plugin installation, see Installing plugins. ",
    "url": "https://vagimeli.github.io/install-and-configure/index/",
    "relUrl": "/install-and-configure/index/"
  },"27": {
    "doc": "Debian",
    "title": "Installing OpenSearch Dashboards (Debian)",
    "content": "Installing OpenSearch Dashboards using the Advanced Packaging Tool (APT) package manager simplifies the process considerably compared to the Tarball method. For example, the package manager handles several technical considerations, such as the installation path, location of configuration files, and creation of a service managed by systemd. Before installing OpenSearch Dashboards you must configure an OpenSearch cluster. Refer to the OpenSearch Debian installation guide for steps. This guide assumes that you are comfortable working from the Linux command line interface (CLI). You should understand how to input commands, navigate between directories, and edit text files. Some example commands reference the vi text editor, but you may use any text editor available. ",
    "url": "https://vagimeli.github.io/install-and-configure/install-dashboards/debian/#installing-opensearch-dashboards-debian",
    "relUrl": "/install-and-configure/install-dashboards/debian/#installing-opensearch-dashboards-debian"
  },"28": {
    "doc": "Debian",
    "title": "Installing OpenSearch Dashboards from a package",
    "content": ". | Download the Debian package for the desired version directly from the OpenSearch downloads page. The Debian package can be downloaded for both x64 and arm64 architectures. | From the CLI, install using dpkg. # x64 sudo dpkg -i opensearch-dashboards-2.7.0-linux-x64.deb # arm64 sudo dpkg -i opensearch-dashboards-2.7.0-linux-arm64.deb . | After the installation completes, reload the systemd manager configuration. sudo systemctl daemon-reload . | Enable OpenSearch as a service. sudo systemctl enable opensearch-dashboards . | Start the OpenSearch service. sudo systemctl start opensearch-dashboards . | Verify that OpenSearch launched correctly. sudo systemctl status opensearch-dashboards . | . Fingerprint verification . The Debian package is not signed. If you would like to verify the fingerprint, the OpenSearch Project provides a .sig file as well as the .deb package for use with GNU Privacy Guard (GPG). | Download the desired Debian package. curl -SLO https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.7.0/opensearch-dashboards-2.7.0-linux-x64.deb . | Download the corresponding signature file. curl -SLO https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.7.0/opensearch-dashboards-2.7.0-linux-x64.deb.sig . | Download and import the GPG key. curl -o- https://artifacts.opensearch.org/publickeys/opensearch.pgp | gpg --import - . | Verify the signature. gpg --verify opensearch-dashboards-2.7.0-linux-x64.deb.sig opensearch-dashboards-2.7.0-linux-x64.deb . | . ",
    "url": "https://vagimeli.github.io/install-and-configure/install-dashboards/debian/#installing-opensearch-dashboards-from-a-package",
    "relUrl": "/install-and-configure/install-dashboards/debian/#installing-opensearch-dashboards-from-a-package"
  },"29": {
    "doc": "Debian",
    "title": "Installing OpenSearch Dashboards from an APT repository",
    "content": "APT, the primary package management tool for Debian–based operating systems, allows you to download and install the Debian package from the APT repository. | Import the public GPG key. This key is used to verify that the APT repository is signed. curl -o- https://artifacts.opensearch.org/publickeys/opensearch.pgp | sudo apt-key add - . | Create an APT repository for OpenSearch. echo \"deb https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.x/apt stable main\" | sudo tee -a /etc/apt/sources.list.d/opensearch-dashboards-2.x.list . | Verify that the repository was created successfully. sudo apt-get update . | With the repository information added, list all available versions of OpenSearch: sudo apt list -a opensearch-dashboards . | Choose the version of OpenSearch you want to install: . | Unless otherwise indicated, the latest available version of OpenSearch is installed. sudo apt-get install opensearch-dashboards . | To install a specific version of OpenSearch Dashboards, pass a version number after the package name. # Specify the version manually using opensearch=&lt;version&gt; sudo apt-get install opensearch-dashboards=2.7.0 . | . | Once complete, enable OpenSearch. sudo systemctl enable opensearch-dashboards . | Start OpenSearch. sudo systemctl start opensearch-dashboards . | Verify that OpenSearch launched correctly. sudo systemctl status opensearch-dashboards . | . ",
    "url": "https://vagimeli.github.io/install-and-configure/install-dashboards/debian/#installing-opensearch-dashboards-from-an-apt-repository",
    "relUrl": "/install-and-configure/install-dashboards/debian/#installing-opensearch-dashboards-from-an-apt-repository"
  },"30": {
    "doc": "Debian",
    "title": "Exploring OpenSearch Dashboards",
    "content": "By default, OpenSearch Dashboards, like OpenSearch, binds to localhost when you initially install it. As a result, OpenSearch Dashboards is not reachable from a remote host unless the configuration is updated. | Open opensearch_dashboards.yml. sudo vi /etc/opensearch-dashboards/opensearch_dashboards.yml . | Specify a network interface that OpenSearch Dashboards should bind to. # Use 0.0.0.0 to bind to any available interface. server.host: 0.0.0.0 . | Save and quit. | Restart OpenSearch Dashboards to apply the configuration change. sudo systemctl restart opensearch-dashboards . | From a web browser, navigate to OpenSearch Dashboards. The default port is 5601. | Log in with the default username admin and the default password admin. | Visit Getting started with OpenSearch Dashboards to learn more. | . ",
    "url": "https://vagimeli.github.io/install-and-configure/install-dashboards/debian/#exploring-opensearch-dashboards",
    "relUrl": "/install-and-configure/install-dashboards/debian/#exploring-opensearch-dashboards"
  },"31": {
    "doc": "Debian",
    "title": "Debian",
    "content": " ",
    "url": "https://vagimeli.github.io/install-and-configure/install-dashboards/debian/",
    "relUrl": "/install-and-configure/install-dashboards/debian/"
  },"32": {
    "doc": "Docker",
    "title": "Run OpenSearch Dashboards using Docker",
    "content": "You can start OpenSearch Dashboards using docker run after creating a Docker network and starting OpenSearch, but the process of connecting OpenSearch Dashboards to OpenSearch is significantly easier with a Docker Compose file. | Run docker pull opensearchproject/opensearch-dashboards:2.7.0. | Create a docker-compose.yml file appropriate for your environment. A sample file that includes OpenSearch Dashboards is available on the OpenSearch Docker installation page. Just like opensearch.yml, you can pass a custom opensearch_dashboards.yml to the container in the Docker Compose file. | Run docker-compose up. Wait for the containers to start. Then see the OpenSearch Dashboards documentation. | When finished, run docker-compose down. | . ",
    "url": "https://vagimeli.github.io/install-and-configure/install-dashboards/docker/#run-opensearch-dashboards-using-docker",
    "relUrl": "/install-and-configure/install-dashboards/docker/#run-opensearch-dashboards-using-docker"
  },"33": {
    "doc": "Docker",
    "title": "Docker",
    "content": " ",
    "url": "https://vagimeli.github.io/install-and-configure/install-dashboards/docker/",
    "relUrl": "/install-and-configure/install-dashboards/docker/"
  },"34": {
    "doc": "Helm",
    "title": "Run OpenSearch Dashboards using Helm",
    "content": "Helm is a package manager that allows you to easily install and manage OpenSearch Dashboards in a Kubernetes cluster. You can define your OpenSearch configurations in a YAML file and use Helm to deploy your applications in a version-controlled and reproducible way. The Helm chart contains the resources described in the following table. | Resource | Description | . | Chart.yaml | Information about the chart. | . | values.yaml | Default configuration values for the chart. | . | templates | Templates that combine with values to generate the Kubernetes manifest files. | . The specification in the default Helm chart supports many standard use cases and setups. You can modify the default chart to configure your desired specifications and set Transport Layer Security (TLS) and role-based access control (RBAC). For information about the default configuration, steps to configure security, and configurable parameters, see the README. The instructions here assume you have a Kubernetes cluster with Helm preinstalled. See the Kubernetes documentation for steps to configure a Kubernetes cluster and the Helm documentation to install Helm. ",
    "url": "https://vagimeli.github.io/install-and-configure/install-dashboards/helm/#run-opensearch-dashboards-using-helm",
    "relUrl": "/install-and-configure/install-dashboards/helm/#run-opensearch-dashboards-using-helm"
  },"35": {
    "doc": "Helm",
    "title": "Prerequisites",
    "content": "Before you get started, you must first use Helm to install OpenSearch. Make sure that you can send requests to your OpenSearch pod: . $ curl -XGET https://localhost:9200 -u 'admin:admin' --insecure { \"name\" : \"opensearch-cluster-master-1\", \"cluster_name\" : \"opensearch-cluster\", \"cluster_uuid\" : \"hP2gq5bPS3SLp8Z7wXm8YQ\", \"version\" : { \"distribution\" : \"opensearch\", \"number\" : \"1.0.0\", \"build_type\" : \"tar\", \"build_hash\" : \"34550c5b17124ddc59458ef774f6b43a086522e3\", \"build_date\" : \"2021-07-02T23:22:21.383695Z\", \"build_snapshot\" : false, \"lucene_version\" : \"8.8.2\", \"minimum_wire_compatibility_version\" : \"6.8.0\", \"minimum_index_compatibility_version\" : \"6.0.0-beta1\" }, \"tagline\" : \"The OpenSearch Project: https://opensearch.org/\" } . ",
    "url": "https://vagimeli.github.io/install-and-configure/install-dashboards/helm/#prerequisites",
    "relUrl": "/install-and-configure/install-dashboards/helm/#prerequisites"
  },"36": {
    "doc": "Helm",
    "title": "Install OpenSearch Dashboards using Helm",
    "content": ". | Change to the opensearch-dashboards directory: . cd opensearch-dashboards . | Package the Helm chart: . helm package . | Deploy OpenSearch Dashboards: . helm install --generate-name opensearch-dashboards-1.0.0.tgz . The output shows you the specifications instantiated from the install. To customize the deployment, pass in the values that you want to override with a custom YAML file: . helm install --values=customvalues.yaml opensearch-dashboards-1.0.0.tgz . | . Sample output . NAME: opensearch-dashboards-1-1629223356 LAST DEPLOYED: Tue Aug 17 18:02:37 2021 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: 1. Get the application URL by running these commands: export POD_NAME=$(kubectl get pods --namespace default -l \"app.kubernetes.io/name=opensearch-dashboards,app.kubernetes.io/instance=op ensearch-dashboards-1-1629223356\" -o jsonpath=\"{.items[0].metadata.name}\") export CONTAINER_PORT=$(kubectl get pod --namespace default $POD_NAME -o jsonpath=\"{.spec.containers[0].ports[0].containerPort}\") echo \"Visit http://127.0.0.1:8080 to use your application\" kubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT . To make sure your OpenSearch Dashboards pod is up and running, run the following command: . $ kubectl get pods NAME READY STATUS RESTARTS AGE opensearch-cluster-master-0 1/1 Running 0 4m35s opensearch-cluster-master-1 1/1 Running 0 4m35s opensearch-cluster-master-2 1/1 Running 0 4m35s opensearch-dashboards-1-1629223356-758bd8747f-8www5 1/1 Running 0 66s . To set up port forwarding to access OpenSearch Dashboards, exit the OpenSearch shell and run the following command: . $ kubectl port-forward deployment/opensearch-dashboards-1-1629223356 5601 . You can now access OpenSearch Dashboards from your browser at: http://localhost:5601. ",
    "url": "https://vagimeli.github.io/install-and-configure/install-dashboards/helm/#install-opensearch-dashboards-using-helm",
    "relUrl": "/install-and-configure/install-dashboards/helm/#install-opensearch-dashboards-using-helm"
  },"37": {
    "doc": "Helm",
    "title": "Uninstall using Helm",
    "content": "To identify the OpenSearch Dashboards deployment that you want to delete: . $ helm list NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION opensearch-1-1629223146 default 1 2021-08-17 17:59:07.664498239 +0000 UTCdeployedopensearch-1.0.0 1.0.0 opensearch-dashboards-1-1629223356 default 1 2021-08-17 18:02:37.600796946 +0000 UTCdepl oyedopensearch-dashboards-1.0.0 1.0.0 . To delete or uninstall a deployment, run the following command: . helm delete opensearch-dashboards-1-1629223356 . ",
    "url": "https://vagimeli.github.io/install-and-configure/install-dashboards/helm/#uninstall-using-helm",
    "relUrl": "/install-and-configure/install-dashboards/helm/#uninstall-using-helm"
  },"38": {
    "doc": "Helm",
    "title": "Helm",
    "content": " ",
    "url": "https://vagimeli.github.io/install-and-configure/install-dashboards/helm/",
    "relUrl": "/install-and-configure/install-dashboards/helm/"
  },"39": {
    "doc": "Installing OpenSearch Dashboards",
    "title": "Installing OpenSearch Dashboards",
    "content": "OpenSearch Dashboards provides a fully integrated solution for visually exploring, discovering, and querying your observability data. You can install OpenSearch Dashboards with any of the following options: . | Docker | Tarball | RPM | Debian | Helm | Windows | . ",
    "url": "https://vagimeli.github.io/install-and-configure/install-dashboards/index/",
    "relUrl": "/install-and-configure/install-dashboards/index/"
  },"40": {
    "doc": "Installing OpenSearch Dashboards",
    "title": "Browser compatibility",
    "content": "OpenSearch Dashboards supports the following web browsers: . | Chrome | Firefox | Safari | Edge (Chromium) | . Other Chromium-based browsers might work, as well. Internet Explorer and Microsoft Edge Legacy are not supported. ",
    "url": "https://vagimeli.github.io/install-and-configure/install-dashboards/index/#browser-compatibility",
    "relUrl": "/install-and-configure/install-dashboards/index/#browser-compatibility"
  },"41": {
    "doc": "Installing OpenSearch Dashboards",
    "title": "Configuration",
    "content": "To learn how to configure TLS for OpenSearch Dashboards, see Configure TLS. ",
    "url": "https://vagimeli.github.io/install-and-configure/install-dashboards/index/#configuration",
    "relUrl": "/install-and-configure/install-dashboards/index/#configuration"
  },"42": {
    "doc": "Managing OpenSearch Dashboards plugins",
    "title": "Managing OpenSearch Dashboards plugins",
    "content": "OpenSearch Dashboards provides a command line tool called opensearch-plugin for managing plugins. This tool allows you to: . | List installed plugins. | Install plugins. | Remove an installed plugin. | . ",
    "url": "https://vagimeli.github.io/install-and-configure/install-dashboards/plugins/",
    "relUrl": "/install-and-configure/install-dashboards/plugins/"
  },"43": {
    "doc": "Managing OpenSearch Dashboards plugins",
    "title": "Plugin compatibility",
    "content": "Major, minor, and patch plugin versions must match OpenSearch major, minor, and patch versions in order to be compatible. For example, plugins versions 2.3.0.x work only with OpenSearch 2.3.0. ",
    "url": "https://vagimeli.github.io/install-and-configure/install-dashboards/plugins/#plugin-compatibility",
    "relUrl": "/install-and-configure/install-dashboards/plugins/#plugin-compatibility"
  },"44": {
    "doc": "Managing OpenSearch Dashboards plugins",
    "title": "Prerequisites",
    "content": ". | A compatible OpenSearch cluster | The corresponding OpenSearch plugins installed on that cluster | The corresponding version of OpenSearch Dashboards (for example, OpenSearch Dashboards 2.3.0 works with OpenSearch 2.3.0) | . ",
    "url": "https://vagimeli.github.io/install-and-configure/install-dashboards/plugins/#prerequisites",
    "relUrl": "/install-and-configure/install-dashboards/plugins/#prerequisites"
  },"45": {
    "doc": "Managing OpenSearch Dashboards plugins",
    "title": "Available plugins",
    "content": "The following table lists available OpenSearch Dashboards plugins. | Plugin Name | Repository | Earliest Available Version | . | Alerting Dashboards | alerting-dashboards-plugin | 1.0.0 | . | Anomaly Detection Dashboards | anomaly-detection-dashboards-plugin | 1.0.0 | . | Custom Import Maps Dashboards | dashboards-maps | 2.2.0 | . | Search Relevance Dashboards | dashboards-search-relevance | 2.4.0 | . | Gantt Chart Dashboards | gantt-chart | 1.0.0 | . | Index Management Dashboards | index-management-dashboards-plugin | 1.0.0 | . | Notebooks Dashboards | dashboards-notebooks | 1.0.0 | . | Notifications Dashboards | notifications | 2.0.0 | . | Observability Dashboards | dashboards-observability | 2.0.0 | . | Query Workbench Dashboards | query-workbench | 1.0.0 | . | Reports Dashboards | dashboards-reporting | 1.0.0 | . | Security Analytics Dashboards | security-analytics-dashboards-plugin | 2.4.0 | . | Security Dashboards | security-dashboards-plugin | 1.0.0 | . ",
    "url": "https://vagimeli.github.io/install-and-configure/install-dashboards/plugins/#available-plugins",
    "relUrl": "/install-and-configure/install-dashboards/plugins/#available-plugins"
  },"46": {
    "doc": "Managing OpenSearch Dashboards plugins",
    "title": "Install",
    "content": "Navigate to the OpenSearch Dashboards home directory (for example, /usr/share/opensearch-dashboards) and run the install command for each plugin. ",
    "url": "https://vagimeli.github.io/install-and-configure/install-dashboards/plugins/#install",
    "relUrl": "/install-and-configure/install-dashboards/plugins/#install"
  },"47": {
    "doc": "Managing OpenSearch Dashboards plugins",
    "title": "Viewing a list of installed plugins",
    "content": "To view the list of installed plugins from the command line, use the following command: . sudo bin/opensearch-dashboards-plugin list . ",
    "url": "https://vagimeli.github.io/install-and-configure/install-dashboards/plugins/#viewing-a-list-of-installed-plugins",
    "relUrl": "/install-and-configure/install-dashboards/plugins/#viewing-a-list-of-installed-plugins"
  },"48": {
    "doc": "Managing OpenSearch Dashboards plugins",
    "title": "Remove plugins",
    "content": "To remove a plugin: . sudo bin/opensearch-dashboards-plugin remove &lt;plugin-name&gt; . Then remove all associated entries from opensearch_dashboards.yml. For certain plugins, you must also remove the “optimze” bundle. This is a sample command for the Anomaly Detection plugin: . sudo rm /usr/share/opensearch-dashboards/optimize/bundles/opensearch-anomaly-detection-opensearch-dashboards.* . Then restart OpenSearch Dashboards. After you remove any plugin, OpenSearch Dashboards performs an optimize operation the next time you start it. This operation takes several minutes even on fast machines, so be patient. ",
    "url": "https://vagimeli.github.io/install-and-configure/install-dashboards/plugins/#remove-plugins",
    "relUrl": "/install-and-configure/install-dashboards/plugins/#remove-plugins"
  },"49": {
    "doc": "Managing OpenSearch Dashboards plugins",
    "title": "Updating plugins",
    "content": "OpenSearch Dashboards doesn’t update plugins. Instead, you have to remove the old version and its optimized bundle, reinstall them, and restart OpenSearch Dashboards: . | Remove the old version: . sudo bin/opensearch-dashboards-plugin remove &lt;plugin-name&gt; . | Remove the optimized bundle: . sudo rm /usr/share/opensearch-dashboards/optimize/bundles/&lt;bundle-name&gt; . | Reinstall the new version: . sudo bin/opensearch-dashboards-plugin install &lt;plugin-name&gt; . | Restart OpenSearch Dashboards. | . For example, to remove and reinstall the anomaly detection plugin: . sudo bin/opensearch-plugin remove opensearch-anomaly-detection sudo rm /usr/share/opensearch-dashboards/optimize/bundles/opensearch-anomaly-detection-opensearch-dashboards.* sudo bin/opensearch-dashboards-plugin install &lt;AD OpenSearch Dashboards plugin artifact URL&gt; . ",
    "url": "https://vagimeli.github.io/install-and-configure/install-dashboards/plugins/#updating-plugins",
    "relUrl": "/install-and-configure/install-dashboards/plugins/#updating-plugins"
  },"50": {
    "doc": "RPM",
    "title": "Run OpenSearch Dashboards using RPM Package Manager (RPM)",
    "content": "OpenSearch Dashboards is the default visualization tool for data in OpenSearch. It also serves as a user interface for many of the OpenSearch plugins, including security, alerting, Index State Management, SQL, and more. ",
    "url": "https://vagimeli.github.io/install-and-configure/install-dashboards/rpm/#run-opensearch-dashboards-using-rpm-package-manager-rpm",
    "relUrl": "/install-and-configure/install-dashboards/rpm/#run-opensearch-dashboards-using-rpm-package-manager-rpm"
  },"51": {
    "doc": "RPM",
    "title": "Install OpenSearch Dashboards from a package",
    "content": ". | Download the RPM package for the desired version directly from the OpenSearch downloads page. The RPM package can be download for both x64 and arm64 architectures. | Import the public GPG key. This key verifies that your OpenSearch instance is signed. sudo rpm --import https://artifacts.opensearch.org/publickeys/opensearch.pgp . | From the command line interface (CLI), you can install the package with rpm or yum. x64 # Install the x64 package using yum. sudo yum install opensearch-dashboards-2.7.0-linux-x64.rpm # Install the x64 package using rpm. sudo rpm -ivh opensearch-dashboards-2.7.0-linux-x64.rpm . arm64 . # Install the arm64 package using yum. sudo yum install opensearch-dashboards-2.7.0-linux-arm64.rpm # Install the arm64 package using rpm. sudo rpm -ivh opensearch-dashboards-2.7.0-linux-arm64.rpm . | After the installation succeeds, enable OpenSearch Dashboards as a service. sudo systemctl enable opensearch-dashboards . | Start OpenSearch Dashboards. sudo systemctl start opensearch-dashboards . | Verify that OpenSearch Dashboards launched correctly. sudo systemctl status opensearch-dashboards . | . ",
    "url": "https://vagimeli.github.io/install-and-configure/install-dashboards/rpm/#install-opensearch-dashboards-from-a-package",
    "relUrl": "/install-and-configure/install-dashboards/rpm/#install-opensearch-dashboards-from-a-package"
  },"52": {
    "doc": "RPM",
    "title": "Install OpenSearch Dashboards from a local YUM repository",
    "content": "YUM, the primary package management tool for Red Hat-based operating systems, allows you to download and install the RPM package from the YUM repository library. | Create a local repository file for OpenSearch Dashboards: sudo curl -SL https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.x/opensearch-dashboards-2.x.repo -o /etc/yum.repos.d/opensearch-dashboards-2.x.repo . | Verify that the repository was created successfully. sudo yum repolist . | Clean your YUM cache, to ensure a smooth installation: sudo yum clean all . | With the repository file downloaded, list all available versions of OpenSearch-Dashboards: sudo yum list opensearch-dashboards --showduplicates . | Choose the version of OpenSearch Dashboards you want to install: . | Unless otherwise indicated, the highest minor version of OpenSearch installs. sudo yum install opensearch-dashboards . | To install a specific version of OpenSearch Dashboards: sudo yum install 'opensearch-dashboards-2.7.0' . | . | During installation, the installer will present you with the GPG key fingerprint. Verify that the information matches the following: Fingerprint: c5b7 4989 65ef d1c2 924b a9d5 39d3 1987 9310 d3fc . | If correct, enter yes or y. The OpenSearch installation continues. | . | Once complete, you can run OpenSearch Dashboards. sudo systemctl start opensearch-dashboards . | . ",
    "url": "https://vagimeli.github.io/install-and-configure/install-dashboards/rpm/#install-opensearch-dashboards-from-a-local-yum-repository",
    "relUrl": "/install-and-configure/install-dashboards/rpm/#install-opensearch-dashboards-from-a-local-yum-repository"
  },"53": {
    "doc": "RPM",
    "title": "RPM",
    "content": " ",
    "url": "https://vagimeli.github.io/install-and-configure/install-dashboards/rpm/",
    "relUrl": "/install-and-configure/install-dashboards/rpm/"
  },"54": {
    "doc": "Tarball",
    "title": "Run OpenSearch Dashboards using the tarball",
    "content": ". | Download the tarball from the OpenSearch downloads page. | Extract the TAR file to a directory and change to that directory: . # x64 tar -zxf opensearch-dashboards-2.7.0-linux-x64.tar.gz cd opensearch-dashboards # ARM64 tar -zxf opensearch-dashboards-2.7.0-linux-arm64.tar.gz cd opensearch-dashboards . | If desired, modify config/opensearch_dashboards.yml. | Run OpenSearch Dashboards: ./bin/opensearch-dashboards . | . ",
    "url": "https://vagimeli.github.io/install-and-configure/install-dashboards/tar/#run-opensearch-dashboards-using-the-tarball",
    "relUrl": "/install-and-configure/install-dashboards/tar/#run-opensearch-dashboards-using-the-tarball"
  },"55": {
    "doc": "Tarball",
    "title": "Tarball",
    "content": " ",
    "url": "https://vagimeli.github.io/install-and-configure/install-dashboards/tar/",
    "relUrl": "/install-and-configure/install-dashboards/tar/"
  },"56": {
    "doc": "Configure TLS",
    "title": "Configure TLS for OpenSearch Dashboards",
    "content": "By default, for ease of testing and getting started, OpenSearch Dashboards runs over HTTP. To enable TLS for HTTPS, update the following settings in opensearch_dashboards.yml. | Setting | Description | . | opensearch.ssl.verificationMode | This setting is for communications between OpenSearch and OpenSearch Dashboards. Valid values are full, certificate, or none. We recommend full if you enable TLS, which enables hostname verification. certificate just checks the certificate, not the hostname, and none performs no checks (suitable for HTTP). Default is full. | . | opensearch.ssl.certificateAuthorities | If opensearch.ssl.verificationMode is full or certificate, specify the full path to one or more CA certificates that comprise a trusted chain for your OpenSearch cluster. For example, you might need to include a root CA and an intermediate CA if you used the intermediate CA to issue your admin, client, and node certificates. | . | server.ssl.enabled | This setting is for communications between OpenSearch Dashboards and the web browser. Set to true for HTTPS, false for HTTP. | . | server.ssl.certificate | If server.ssl.enabled is true, specify the full path to a valid client certificate for your OpenSearch cluster. You can generate your own or get one from a certificate authority. | . | server.ssl.key | If server.ssl.enabled is true, specify the full path (e.g. /usr/share/opensearch-dashboards-1.0.0/config/my-client-cert-key.pem to the key for your client certificate. You can generate your own or get one from a certificate authority. | . | opensearch_security.cookie.secure | If you enable TLS for OpenSearch Dashboards, change this setting to true. For HTTP, set it to false. | . This opensearch_dashboards.yml configuration shows OpenSearch and OpenSearch Dashboards running on the same machine with the demo configuration: . opensearch.hosts: [\"https://localhost:9200\"] opensearch.ssl.verificationMode: full opensearch.username: \"kibanaserver\" opensearch.password: \"kibanaserver\" opensearch.requestHeadersAllowlist: [ authorization,securitytenant ] server.ssl.enabled: true server.ssl.certificate: /usr/share/opensearch-dashboards/config/client-cert.pem server.ssl.key: /usr/share/opensearch-dashboards/config/client-cert-key.pem opensearch.ssl.certificateAuthorities: [ \"/usr/share/opensearch-dashboards/config/root-ca.pem\", \"/usr/share/opensearch-dashboards/config/intermediate-ca.pem\" ] opensearch_security.multitenancy.enabled: true opensearch_security.multitenancy.tenants.preferred: [\"Private\", \"Global\"] opensearch_security.readonly_mode.roles: [\"kibana_read_only\"] opensearch_security.cookie.secure: true . If you use the Docker install, you can pass a custom opensearch_dashboards.yml to the container. To learn more, see the Docker installation page. After enabling these settings and starting OpenSearch Dashboards, you can connect to it at https://localhost:5601. You might have to acknowledge a browser warning if your certificates are self-signed. To avoid this sort of warning (or outright browser incompatibility), best practice is to use certificates from trusted certificate authority. ",
    "url": "https://vagimeli.github.io/install-and-configure/install-dashboards/tls/#configure-tls-for-opensearch-dashboards",
    "relUrl": "/install-and-configure/install-dashboards/tls/#configure-tls-for-opensearch-dashboards"
  },"57": {
    "doc": "Configure TLS",
    "title": "Configure TLS",
    "content": " ",
    "url": "https://vagimeli.github.io/install-and-configure/install-dashboards/tls/",
    "relUrl": "/install-and-configure/install-dashboards/tls/"
  },"58": {
    "doc": "Windows",
    "title": "Run OpenSearch Dashboards on Windows",
    "content": "Perform the following steps to install OpenSearch Dashboards on Windows. Make sure you have a zip utility installed. | Download the opensearch-dashboards-2.7.0-windows-x64.zip archive. | To extract the archive contents, right-click to select Extract All. Note: Some versions of the Windows operating system limit the file path length. If you encounter a path-length-related error when unzipping the archive, perform the following steps to enable long path support: . | Open Powershell by entering powershell in the search box next to Start on the taskbar. | Run the following command in Powershell: Set-ItemProperty -Path HKLM:\\SYSTEM\\CurrentControlSet\\Control\\FileSystem LongPathsEnabled -Type DWORD -Value 1 -Force . | Restart your computer. | . | Run OpenSearch Dashboards. There are two ways of running OpenSearch Dashboards: . | Run the batch script using the Windows UI: . | Navigate to the top directory of your OpenSearch Dashboards installation and open the opensearch-dashboards-2.7.0 folder. | If desired, modify opensearch_dashboards.yml located in the config folder, to change the default OpenSearch Dashboards settings. | Open the bin folder and run the batch script by double-clicking the opensearch-dashboards.bat file. This opens a command prompt with an OpenSearch Dashboards instance running. | . | Run the batch script from Command Prompt or Powershell: . | Open Command Prompt by entering cmd, or Powershell by entering powershell, in the search box next to Start on the taskbar. | Change to the top directory of your OpenSearch Dashboards installation. cd \\path\\to\\opensearch-dashboards-2.7.0 . | If desired, modify config\\opensearch_dashboards.yml. | Run the batch script to start OpenSearch Dashboards.\\bin\\opensearch-dashboards.bat . | . | . | . To stop OpenSearch Dashboards, press Ctrl+C in Command Prompt or Powershell, or simply close the Command Prompt or Powershell window. ",
    "url": "https://vagimeli.github.io/install-and-configure/install-dashboards/windows/#run-opensearch-dashboards-on-windows",
    "relUrl": "/install-and-configure/install-dashboards/windows/#run-opensearch-dashboards-on-windows"
  },"59": {
    "doc": "Windows",
    "title": "Windows",
    "content": " ",
    "url": "https://vagimeli.github.io/install-and-configure/install-dashboards/windows/",
    "relUrl": "/install-and-configure/install-dashboards/windows/"
  },"60": {
    "doc": "Ansible playbook",
    "title": "Ansible playbook",
    "content": "You can use an Ansible playbook to install and configure a production-ready OpenSearch cluster along with OpenSearch Dashboards. The Ansible playbook only supports deployment of OpenSearch and OpenSearch Dashboards to CentOS7 hosts. ",
    "url": "https://vagimeli.github.io/install-and-configure/install-opensearch/ansible/",
    "relUrl": "/install-and-configure/install-opensearch/ansible/"
  },"61": {
    "doc": "Ansible playbook",
    "title": "Prerequisites",
    "content": "Make sure you have Ansible and Java 8 installed. ",
    "url": "https://vagimeli.github.io/install-and-configure/install-opensearch/ansible/#prerequisites",
    "relUrl": "/install-and-configure/install-opensearch/ansible/#prerequisites"
  },"62": {
    "doc": "Ansible playbook",
    "title": "Configuration",
    "content": ". | Clone the OpenSearch ansible-playbook repository: . git clone https://github.com/opensearch-project/ansible-playbook . copy . | Configure the node properties in the inventories/opensearch/hosts file: . ansible_host=&lt;Public IP address&gt; ansible_user=root ip=&lt;Private IP address / 0.0.0.0&gt; . copy . where: . | ansible_host is the IP address of the target node that you want the Ansible playbook to install OpenSearch and OpenSearch DashBoards on. | ip is the IP address that you want OpenSearch and OpenSearch DashBoards to bind to. You can specify the private IP of the target node, or localhost, or 0.0.0.0. | . | You can modify the default configuration values in the inventories/opensearch/group_vars/all/all.yml file. For example, you can increase the Java memory heap size: . xms_value: 8 xmx_value: 8 . copy . | . Make sure you have direct SSH access into the root user of the target node. ",
    "url": "https://vagimeli.github.io/install-and-configure/install-opensearch/ansible/#configuration",
    "relUrl": "/install-and-configure/install-opensearch/ansible/#configuration"
  },"63": {
    "doc": "Ansible playbook",
    "title": "Run OpenSearch and OpenSearch Dashboards using Ansible playbook",
    "content": ". | Run the Ansible playbook with root privileges: . ansible-playbook -i inventories/opensearch/hosts opensearch.yml --extra-vars \"admin_password=Test@123 kibanaserver_password=Test@6789\" . copy . You can set the passwords for reserved users (admin and kibanaserver) using the admin_password and kibanaserver_password variables. | After the deployment process is complete, you can access OpenSearch and OpenSearch Dashboards with the username admin and the password that you set for the admin_password variable. If you bind ip to a private IP or localhost, make sure you’re logged into the server that deployed the playbook to access OpenSearch and OpenSearch Dashboards: . curl https://localhost:9200 -u 'admin:Test@123' --insecure . copy . If you bind ip to 0.0.0.0, then replace localhost with the public IP or the private IP (if it’s in the same network). | . ",
    "url": "https://vagimeli.github.io/install-and-configure/install-opensearch/ansible/#run-opensearch-and-opensearch-dashboards-using-ansible-playbook",
    "relUrl": "/install-and-configure/install-opensearch/ansible/#run-opensearch-and-opensearch-dashboards-using-ansible-playbook"
  },"64": {
    "doc": "Debian",
    "title": "Debian",
    "content": "Installing OpenSearch using the Advanced Packaging Tool (APT) package manager simplifies the process considerably compared to the Tarball method. Several technical considerations, such as the installation path, location of configuration files, and creation of a service managed by systemd, as examples, are handled automatically by the package manager. Generally speaking, installing OpenSearch from the Debian distribution can be broken down into a few steps: . | Download and install OpenSearch. | Install manually from a Debian package or from an APT repository. | . | (Optional) Test OpenSearch. | Confirm that OpenSearch is able to run before you apply any custom configuration. | This can be done without any security (no password, no certificates) or with a demo security configuration that can be applied by a packaged script. | . | Configure OpenSearch for your environment. | Apply basic settings to OpenSearch and start using it in your environment. | . | . The Debian distribution provides everything you need to run OpenSearch inside Debian-based Linux Distributions, such as Ubuntu. This guide assumes that you are comfortable working from the Linux command line interface (CLI). You should understand how to input commands, navigate between directories, and edit text files. Some example commands reference the vi text editor, but you may use any text editor available. ",
    "url": "https://vagimeli.github.io/install-and-configure/install-opensearch/debian/#debian",
    "relUrl": "/install-and-configure/install-opensearch/debian/#debian"
  },"65": {
    "doc": "Debian",
    "title": "Step 1: Download and install OpenSearch",
    "content": "Install OpenSearch from a package . | Download the Debian package for the desired version directly from the OpenSearch downloads page. The Debian package can be downloaded for both x64 and arm64 architectures. | From the CLI, install using dpkg. # x64 sudo dpkg -i opensearch-2.7.0-linux-x64.deb # arm64 sudo dpkg -i opensearch-2.7.0-linux-arm64.deb . | After the installation succeeds, enable OpenSearch as a service. sudo systemctl enable opensearch . copy . | Start the OpenSearch service. sudo systemctl start opensearch . copy . | Verify that OpenSearch launched correctly. sudo systemctl status opensearch . copy . | . Fingerprint verification . The Debian package is not signed. If you would like to verify the fingerprint, the OpenSearch Project provides a .sig file as well as the .deb package for use with GNU Privacy Guard (GPG). | Download the desired Debian package. curl -SLO https://artifacts.opensearch.org/releases/bundle/opensearch/2.7.0/opensearch-2.7.0-linux-x64.deb . copy . | Download the corresponding signature file. curl -SLO https://artifacts.opensearch.org/releases/bundle/opensearch/2.7.0/opensearch-2.7.0-linux-x64.deb.sig . copy . | Download and import the GPG key. curl -o- https://artifacts.opensearch.org/publickeys/opensearch.pgp | gpg --import - . copy . | Verify the signature. gpg --verify opensearch-2.7.0-linux-x64.deb.sig opensearch-2.7.0-linux-x64.deb . copy . | . Install OpenSearch from an APT repository . APT, the primary package management tool for Debian–based operating systems, allows you to download and install the Debian package from the APT repository. | Import the public GPG key. This key is used to verify that the APT repository is signed. curl -o- https://artifacts.opensearch.org/publickeys/opensearch.pgp | sudo apt-key add - . copy . | Create an APT repository for OpenSearch: echo \"deb https://artifacts.opensearch.org/releases/bundle/opensearch/2.x/apt stable main\" | sudo tee -a /etc/apt/sources.list.d/opensearch-2.x.list . copy . | Verify that the repository was created successfully. sudo apt-get update . copy . | With the repository information added, list all available versions of OpenSearch: sudo apt list -a opensearch . copy . | Choose the version of OpenSearch you want to install: . | Unless otherwise indicated, the latest available version of OpenSearch is installed. sudo apt-get install opensearch . copy . | To install a specific version of OpenSearch: # Specify the version manually using opensearch=&lt;version&gt; sudo apt-get install opensearch=2.7.0 . | . | During installation, the installer will present you with the GPG key fingerprint. Verify that the information matches the following: Fingerprint: c5b7 4989 65ef d1c2 924b a9d5 39d3 1987 9310 d3fc . copy . | Once complete, enable OpenSearch. sudo systemctl enable opensearch . copy . | Start OpenSearch. sudo systemctl start opensearch . copy . | Verify that OpenSearch launched correctly. sudo systemctl status opensearch . copy . | . ",
    "url": "https://vagimeli.github.io/install-and-configure/install-opensearch/debian/#step-1-download-and-install-opensearch",
    "relUrl": "/install-and-configure/install-opensearch/debian/#step-1-download-and-install-opensearch"
  },"66": {
    "doc": "Debian",
    "title": "Step 2: (Optional) Test OpenSearch",
    "content": "Before proceeding with any configuration, you should test your installation of OpenSearch. Otherwise, it can be difficult to determine whether future problems are due to installation issues or custom settings you applied after installation. When OpenSearch is installed using the Debian package, some demo security settings are automatically applied. This includes self-signed TLS certificates and several users and roles. If you would like to configure these yourself, see Set up OpenSearch in your environment. An OpenSearch node in its default configuration (with demo certificates and users with default passwords) is not suitable for a production environment. If you plan to use the node in a production environment, you should, at a minimum, replace the demo TLS certificates with your own TLS certificates and update the list of internal users and passwords. See Security configuration for additional guidance to ensure that your nodes are configured according to your security requirements. | Send requests to the server to verify that OpenSearch is running. Note the use of the --insecure flag, which is required because the TLS certificates are self-signed. | Send a request to port 9200: curl -X GET https://localhost:9200 -u 'admin:admin' --insecure . copy . You should get a response that looks like this: . { \"name\":\"hostname\", \"cluster_name\":\"opensearch\", \"cluster_uuid\":\"QqgpHCbnSRKcPAizqjvoOw\", \"version\":{ \"distribution\":\"opensearch\", \"number\":&lt;version&gt;, \"build_type\":&lt;build-type&gt;, \"build_hash\":&lt;build-hash&gt;, \"build_date\":&lt;build-date&gt;, \"build_snapshot\":false, \"lucene_version\":&lt;lucene-version&gt;, \"minimum_wire_compatibility_version\":\"7.10.0\", \"minimum_index_compatibility_version\":\"7.0.0\" }, \"tagline\":\"The OpenSearch Project: https://opensearch.org/\" } . | Query the plugins endpoint: curl -X GET https://localhost:9200/_cat/plugins?v -u 'admin:admin' --insecure . copy . | . The response should look like this: . name component version hostname opensearch-alerting 2.7.0 hostname opensearch-anomaly-detection 2.7.0 hostname opensearch-asynchronous-search 2.7.0 hostname opensearch-cross-cluster-replication 2.7.0 hostname opensearch-geospatial 2.7.0 hostname opensearch-index-management 2.7.0 hostname opensearch-job-scheduler 2.7.0 hostname opensearch-knn 2.7.0 hostname opensearch-ml 2.7.0 hostname opensearch-neural-search 2.7.0 hostname opensearch-notifications 2.7.0 hostname opensearch-notifications-core 2.7.0 hostname opensearch-observability 2.7.0 hostname opensearch-performance-analyzer 2.7.0 hostname opensearch-reports-scheduler 2.7.0 hostname opensearch-security 2.7.0 hostname opensearch-security-analytics 2.7.0 hostname opensearch-sql 2.7.0 . | . ",
    "url": "https://vagimeli.github.io/install-and-configure/install-opensearch/debian/#step-2-optional-test-opensearch",
    "relUrl": "/install-and-configure/install-opensearch/debian/#step-2-optional-test-opensearch"
  },"67": {
    "doc": "Debian",
    "title": "Step 3: Set up OpenSearch in your environment",
    "content": "Users who do not have prior experience with OpenSearch may want a list of recommended settings in order to get started with the service. By default, OpenSearch is not bound to a network interface and cannot be reached by external hosts. Additionally, security settings are populated by default user names and passwords. The following recommendations will enable a user to bind OpenSearch to a network interface, create and sign TLS certificates, and configure basic authentication. The following recommended settings will allow you to: . | Bind OpenSearch to an IP or network interface on the host. | Set initial and maximum JVM heap sizes. | Define an environment variable that points to the bundled JDK. | Configure your own TLS certificates—no third-party certificate authority (CA) is required. | Create an admin user with a custom password. | . If you ran the security demo script, then you will need to manually reconfigure settings that were modified. Refer to Security configuration for guidance before proceeding. Before modifying any configuration files, it’s always a good idea to save a backup copy before making changes. The backup file can be used to mitigate any issues caused by a bad configuration. | Open opensearch.yml. sudo vi /etc/opensearch/opensearch.yml . copy . | Add the following lines: # Bind OpenSearch to the correct network interface. Use 0.0.0.0 # to include all available interfaces or specify an IP address # assigned to a specific interface. network.host: 0.0.0.0 # Unless you have already configured a cluster, you should set # discovery.type to single-node, or the bootstrap checks will # fail when you try to start the service. discovery.type: single-node # If you previously disabled the Security plugin in opensearch.yml, # be sure to re-enable it. Otherwise you can skip this setting. plugins.security.disabled: false . copy . | Save your changes and close the file. | Specify initial and maximum JVM heap sizes. | Open jvm.options. vi /etc/opensearch/jvm.options . copy . | Modify the values for initial and maximum heap sizes. As a starting point, you should set these values to half of the available system memory. For dedicated hosts this value can be increased based on your workflow requirements. | As an example, if the host machine has 8 GB of memory, then you might want to set the initial and maximum heap sizes to 4 GB: -Xms4g -Xmx4g . copy . | . | Save your changes and close the file. | . | . Configure TLS . TLS certificates provide additional security for your cluster by allowing clients to confirm the identity of hosts and encrypt traffic between the client and host. For more information, refer to Configure TLS Certificates and Generate Certificates, which are included in the Security plugin documentation. For work performed in a development environment, self-signed certificates are usually adequate. This section will guide you through the basic steps required to generate your own TLS certificates and apply them to your OpenSearch host. | Navigate to the directory where the certificates will be stored. cd /etc/opensearch . copy . | Delete the demo certificates. sudo rm -f *pem . copy . | Generate a root certificate. This is what you will use to sign your other certificates. # Create a private key for the root certificate sudo openssl genrsa -out root-ca-key.pem 2048 # Use the private key to create a self-signed root certificate. Be sure to # replace the arguments passed to -subj so they reflect your specific host. sudo openssl req -new -x509 -sha256 -key root-ca-key.pem -subj \"/C=CA/ST=ONTARIO/L=TORONTO/O=ORG/OU=UNIT/CN=ROOT\" -out root-ca.pem -days 730 . | Next, create the admin certificate. This certificate is used to gain elevated rights for performing administrative tasks relating to the Security plugin. # Create a private key for the admin certificate. sudo openssl genrsa -out admin-key-temp.pem 2048 # Convert the private key to PKCS#8. sudo openssl pkcs8 -inform PEM -outform PEM -in admin-key-temp.pem -topk8 -nocrypt -v1 PBE-SHA1-3DES -out admin-key.pem # Create the certficiate signing request (CSR). A common name (CN) of \"A\" is acceptable because this certificate is # used for authenticating elevated access and is not tied to a host. sudo openssl req -new -key admin-key.pem -subj \"/C=CA/ST=ONTARIO/L=TORONTO/O=ORG/OU=UNIT/CN=A\" -out admin.csr # Sign the admin certificate with the root certificate and private key you created earlier. sudo openssl x509 -req -in admin.csr -CA root-ca.pem -CAkey root-ca-key.pem -CAcreateserial -sha256 -out admin.pem -days 730 . | Create a certificate for the node being configured. # Create a private key for the node certificate. sudo openssl genrsa -out node1-key-temp.pem 2048 # Convert the private key to PKCS#8. sudo openssl pkcs8 -inform PEM -outform PEM -in node1-key-temp.pem -topk8 -nocrypt -v1 PBE-SHA1-3DES -out node1-key.pem # Create the CSR and replace the arguments passed to -subj so they reflect your specific host. # The CN should match a DNS A record for the host-do not use the hostname. sudo openssl req -new -key node1-key.pem -subj \"/C=CA/ST=ONTARIO/L=TORONTO/O=ORG/OU=UNIT/CN=node1.dns.a-record\" -out node1.csr # Create an extension file that defines a SAN DNS name for the host. This # should match the DNS A record of the host. sudo sh -c 'echo subjectAltName=DNS:node1.dns.a-record &gt; node1.ext' # Sign the node certificate with the root certificate and private key that you created earlier. sudo openssl x509 -req -in node1.csr -CA root-ca.pem -CAkey root-ca-key.pem -CAcreateserial -sha256 -out node1.pem -days 730 -extfile node1.ext . | Remove temporary files that are no longer required. sudo rm -f *temp.pem *csr *ext . copy . | Make sure the remaining certificates are owned by the opensearch user. sudo chown opensearch:opensearch admin-key.pem admin.pem node1-key.pem node1.pem root-ca-key.pem root-ca.pem root-ca.srl . copy . | Add these certificates to opensearch.yml as described in Generate Certificates. Advanced users might also choose to append the settings using a script: #! /bin/bash # Before running this script, make sure to replace the CN in the # node's distinguished name with a real DNS A record. echo \"plugins.security.ssl.transport.pemcert_filepath: /etc/opensearch/node1.pem\" | sudo tee -a /etc/opensearch/opensearch.yml echo \"plugins.security.ssl.transport.pemkey_filepath: /etc/opensearch/node1-key.pem\" | sudo tee -a /etc/opensearch/opensearch.yml echo \"plugins.security.ssl.transport.pemtrustedcas_filepath: /etc/opensearch/root-ca.pem\" | sudo tee -a /etc/opensearch/opensearch.yml echo \"plugins.security.ssl.http.enabled: true\" | sudo tee -a /etc/opensearch/opensearch.yml echo \"plugins.security.ssl.http.pemcert_filepath: /etc/opensearch/node1.pem\" | sudo tee -a /etc/opensearch/opensearch.yml echo \"plugins.security.ssl.http.pemkey_filepath: /etc/opensearch/node1-key.pem\" | sudo tee -a /etc/opensearch/opensearch.yml echo \"plugins.security.ssl.http.pemtrustedcas_filepath: /etc/opensearch/root-ca.pem\" | sudo tee -a /etc/opensearch/opensearch.yml echo \"plugins.security.allow_default_init_securityindex: true\" | sudo tee -a /etc/opensearch/opensearch.yml echo \"plugins.security.authcz.admin_dn:\" | sudo tee -a /etc/opensearch/opensearch.yml echo \" - 'CN=A,OU=UNIT,O=ORG,L=TORONTO,ST=ONTARIO,C=CA'\" | sudo tee -a /etc/opensearch/opensearch.yml echo \"plugins.security.nodes_dn:\" | sudo tee -a /etc/opensearch/opensearch.yml echo \" - 'CN=node1.dns.a-record,OU=UNIT,O=ORG,L=TORONTO,ST=ONTARIO,C=CA'\" | sudo tee -a /etc/opensearch/opensearch.yml echo \"plugins.security.audit.type: internal_opensearch\" | sudo tee -a /etc/opensearch/opensearch.yml echo \"plugins.security.enable_snapshot_restore_privilege: true\" | sudo tee -a /etc/opensearch/opensearch.yml echo \"plugins.security.check_snapshot_restore_write_privileges: true\" | sudo tee -a /etc/opensearch/opensearch.yml echo \"plugins.security.restapi.roles_enabled: [\\\"all_access\\\", \\\"security_rest_api_access\\\"]\" | sudo tee -a /etc/opensearch/opensearch.yml . copy . | (Optional) Add trust for the self-signed root certificate. # Copy the root certificate to the correct directory sudo cp /etc/opensearch/root-ca.pem /etc/pki/ca-trust/source/anchors/ # Add trust sudo update-ca-trust . | . Configure a user . Users are defined and authenticated by OpenSearch in a variety of ways. One method that does not require additional backend infrastructure is to manually configure users in internal_users.yml. See YAML files for more information about configuring users. The following steps explain how to remove all demo users except for the admin user and how to replace the admin default password using a script. | Navigate to the Security plugins tools directory. cd /usr/share/opensearch/plugins/opensearch-security/tools . copy . | Run hash.sh to generate a new password. | This script will fail if a path to the JDK has not been defined. # Example output if a JDK isn't found... $ ./hash.sh ************************************************************************** ** This tool will be deprecated in the next major release of OpenSearch ** ** https://github.com/opensearch-project/security/issues/1755 ** ************************************************************************** which: no java in (/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/user/.local/bin:/home/user/bin) WARNING: nor OPENSEARCH_JAVA_HOME nor JAVA_HOME is set, will use ./hash.sh: line 35: java: command not found . copy . | Declare an environment variable when you invoke the script in order to avoid issues: OPENSEARCH_JAVA_HOME=/usr/share/opensearch/jdk ./hash.sh . copy . | Enter the desired password at the prompt and make a note of the output hash. | . | Open internal_users.yml. sudo vi /etc/opensearch/opensearch-security/internal_users.yml . copy . | Remove all demo users except for admin and replace the hash with the output provided by hash.sh in a previous step. The file should look similar to the following example: --- # This is the internal user database # The hash value is a bcrypt hash and can be generated with plugin/tools/hash.sh _meta: type: \"internalusers\" config_version: 2 # Define your internal users here admin: hash: \"$2y$1EXAMPLEQqwS8TUcoEXAMPLEeZ3lEHvkEXAMPLERqjyh1icEXAMPLE.\" reserved: true backend_roles: - \"admin\" description: \"Admin user\" . copy . | . Apply changes . Now that TLS certificates are installed and demo users were removed or assigned new passwords, the last step is to apply the configuration changes. This last configuration step requires invoking securityadmin.sh while OpenSearch is running on the host. | OpenSearch must be running for securityadmin.sh to apply changes. If you made changes to opensearch.yml, restart OpenSearch. sudo systemctl restart opensearch . copy . | Open a separate terminal session with the host and navigate to the directory containing securityadmin.sh. # Change to the correct directory cd /usr/share/opensearch/plugins/opensearch-security/tools . | Invoke the script. See Apply changes using securityadmin.sh for definitions of the arguments you must pass. # You can omit the environment variable if you declared this in your $PATH. OPENSEARCH_JAVA_HOME=/usr/share/opensearch/jdk ./securityadmin.sh -cd /etc/opensearch/opensearch-security/ -cacert /etc/opensearch/root-ca.pem -cert /etc/opensearch/admin.pem -key /etc/opensearch/admin-key.pem -icl -nhnv . copy . | . Verify that the service is running . OpenSearch is now running on your host with custom TLS certificates and a secure user for basic authentication. You can verify external connectivity by sending an API request to your OpenSearch node from another host. During the previous test you directed requests to localhost. Now that TLS certificates have been applied and the new certificates reference your host’s actual DNS record, requests to localhost will fail the CN check and the certificate will be considered invalid. Instead, requests should be sent to the address you specified while generating the certificate. You should add trust for the root certificate to your client before sending requests. If you do not add trust, then you must use the -k option so that cURL ignores CN and root certificate validation. $ curl https://your.host.address:9200 -u admin:yournewpassword -k { \"name\":\"hostname\", \"cluster_name\":\"opensearch\", \"cluster_uuid\":\"QqgpHCbnSRKcPAizqjvoOw\", \"version\":{ \"distribution\":\"opensearch\", \"number\":&lt;version&gt;, \"build_type\":&lt;build-type&gt;, \"build_hash\":&lt;build-hash&gt;, \"build_date\":&lt;build-date&gt;, \"build_snapshot\":false, \"lucene_version\":&lt;lucene-version&gt;, \"minimum_wire_compatibility_version\":\"7.10.0\", \"minimum_index_compatibility_version\":\"7.0.0\" }, \"tagline\":\"The OpenSearch Project: https://opensearch.org/\" } . ",
    "url": "https://vagimeli.github.io/install-and-configure/install-opensearch/debian/#step-3-set-up-opensearch-in-your-environment",
    "relUrl": "/install-and-configure/install-opensearch/debian/#step-3-set-up-opensearch-in-your-environment"
  },"68": {
    "doc": "Debian",
    "title": "Upgrade to a newer version",
    "content": "OpenSearch instances installed using dpkg or apt-get can be easily upgraded to a newer version. Manual upgrade with DPKG . Download the Debian package for the desired upgrade version directly from the OpenSearch downloads page. Navigate to the directory containing the distribution and run the following command: . sudo dpkg -i opensearch-2.7.0-linux-x64.deb . copy . APT-GET . To upgrade to the latest version of OpenSearch using apt-get: . sudo apt-get upgrade opensearch . copy . You can also upgrade to a specific OpenSearch version: . sudo apt-get upgrade opensearch=&lt;version&gt; . copy . ",
    "url": "https://vagimeli.github.io/install-and-configure/install-opensearch/debian/#upgrade-to-a-newer-version",
    "relUrl": "/install-and-configure/install-opensearch/debian/#upgrade-to-a-newer-version"
  },"69": {
    "doc": "Debian",
    "title": "Related links",
    "content": ". | OpenSearch configuration | Install and configure OpenSearch Dashboards | OpenSearch plugin installation | About the Security plugin | . ",
    "url": "https://vagimeli.github.io/install-and-configure/install-opensearch/debian/#related-links",
    "relUrl": "/install-and-configure/install-opensearch/debian/#related-links"
  },"70": {
    "doc": "Debian",
    "title": "Debian",
    "content": " ",
    "url": "https://vagimeli.github.io/install-and-configure/install-opensearch/debian/",
    "relUrl": "/install-and-configure/install-opensearch/debian/"
  },"71": {
    "doc": "Docker",
    "title": "Docker",
    "content": "Docker greatly simplifies the process of configuring and managing your OpenSearch clusters. You can pull official images from Docker Hub or Amazon Elastic Container Registry (Amazon ECR) and quickly deploy a cluster using Docker Compose and any of the sample Docker Compose files included in this guide. Experienced OpenSearch users can further customize their deployment by creating a custom Docker Compose file. Docker containers are portable and will run on any compatible host that supports Docker (such as Linux, MacOS, or Windows). The portability of a Docker container offers flexibility over other installations methods, like RPM or a manual Tarball installation, which both require additional configuration after downloading and unpacking. This guide assumes that you are comfortable working from the Linux command line interface (CLI). You should understand how to input commands, navigate between directories, and edit text files. For help with Docker or Docker Compose, refer to the official documentation on their websites. ",
    "url": "https://vagimeli.github.io/install-and-configure/install-opensearch/docker/",
    "relUrl": "/install-and-configure/install-opensearch/docker/"
  },"72": {
    "doc": "Docker",
    "title": "Install Docker and Docker Compose",
    "content": "Visit Get Docker for guidance on installing and configuring Docker for your environment. If you are installing Docker Engine using the CLI, then Docker, by default, will not have any constraints on available host resources. Depending on your environment, you may wish to configure resource limits in Docker. See Runtime options with Memory, CPUs, and GPUs for information. Docker Desktop users should set host memory utilization to a minimum of 4 GB by opening Docker Desktop and selecting Settings → Resources. Docker Compose is a utility that allows users to launch multiple containers with a single command. You pass a file to Docker Compose when you invoke it. Docker Compose reads those settings and starts the requested containers. Docker Compose is installed automatically with Docker Desktop, but users operating in a command line environment must install Docker Compose manually. You can find information about installing Docker Compose on the official Docker Compose GitHub page. If you need to install Docker Compose manually and your host supports Python, you can use pip to install the Docker Compose package automatically. ",
    "url": "https://vagimeli.github.io/install-and-configure/install-opensearch/docker/#install-docker-and-docker-compose",
    "relUrl": "/install-and-configure/install-opensearch/docker/#install-docker-and-docker-compose"
  },"73": {
    "doc": "Docker",
    "title": "Important host settings",
    "content": "Before launching OpenSearch you should review some important system settings that can impact the performance of your services. | Disable memory paging and swapping performance on the host to improve performance. sudo swapoff -a . | Increase the number of memory maps available to OpenSearch. # Edit the sysctl config file sudo vi /etc/sysctl.conf # Add a line to define the desired value # or change the value if the key exists, # and then save your changes. vm.max_map_count=262144 # Reload the kernel parameters using sysctl sudo sysctl -p # Verify that the change was applied by checking the value cat /proc/sys/vm/max_map_count . | . ",
    "url": "https://vagimeli.github.io/install-and-configure/install-opensearch/docker/#important-host-settings",
    "relUrl": "/install-and-configure/install-opensearch/docker/#important-host-settings"
  },"74": {
    "doc": "Docker",
    "title": "Run OpenSearch in a Docker container",
    "content": "Official OpenSearch images are hosted on Docker Hub and Amazon ECR. If you want to inspect the images you can pull them individually using docker pull, such as in the following examples. Docker Hub: . docker pull opensearchproject/opensearch:latest docker pull opensearchproject/opensearch-dashboards:latest . Amazon ECR: . docker pull public.ecr.aws/opensearchproject/opensearch:latest docker pull public.ecr.aws/opensearchproject/opensearch-dashboards:latest . To download a specific version of OpenSearch or OpenSearch Dashboards other than the latest available version, modify the image tag where it is referenced (either in the command line or in a Docker Compose file). For example, opensearchproject/opensearch:2.7.0 will pull OpenSearch version 2.7.0. Refer to the official image repositories for available versions. Before continuing, you should verify that Docker is working correctly by deploying OpenSearch in a single container. | Run the following command: # This command maps ports 9200 and 9600, sets the discovery type to \"single-node\" and requests the newest image of OpenSearch docker run -d -p 9200:9200 -p 9600:9600 -e \"discovery.type=single-node\" opensearchproject/opensearch:latest . | Send a request to port 9200. The default username and password are admin. curl https://localhost:9200 -ku 'admin:admin' . copy . | You should get a response that looks like this: { \"name\" : \"a937e018cee5\", \"cluster_name\" : \"docker-cluster\", \"cluster_uuid\" : \"GLAjAG6bTeWErFUy_d-CLw\", \"version\" : { \"distribution\" : \"opensearch\", \"number\" : &lt;version&gt;, \"build_type\" : &lt;build-type&gt;, \"build_hash\" : &lt;build-hash&gt;, \"build_date\" : &lt;build-date&gt;, \"build_snapshot\" : false, \"lucene_version\" : &lt;lucene-version&gt;, \"minimum_wire_compatibility_version\" : \"7.10.0\", \"minimum_index_compatibility_version\" : \"7.0.0\" }, \"tagline\" : \"The OpenSearch Project: https://opensearch.org/\" } . | . | Before stopping the running container, display a list of all running containers and copy the container ID for the OpenSearch node you are testing. In the following example, the container ID is a937e018cee5: $ docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a937e018cee5 opensearchproject/opensearch:latest \"./opensearch-docker…\" 19 minutes ago Up 19 minutes 0.0.0.0:9200-&gt;9200/tcp, 9300/tcp, 0.0.0.0:9600-&gt;9600/tcp, 9650/tcp wonderful_boyd . | Stop the running container by passing the container ID to docker stop. docker stop &lt;containerId&gt; . copy . | . Remember that docker container ls does not list stopped containers. If you would like to review stopped containers, use docker container ls -a. You can remove unneeded containers manually with docker container rm &lt;containerId_1&gt; &lt;containerId_2&gt; &lt;containerId_3&gt; [...] (pass all container IDs you wish to stop, separated by spaces), or if you want to remove all stopped containers, you can use the shorter command docker prune. ",
    "url": "https://vagimeli.github.io/install-and-configure/install-opensearch/docker/#run-opensearch-in-a-docker-container",
    "relUrl": "/install-and-configure/install-opensearch/docker/#run-opensearch-in-a-docker-container"
  },"75": {
    "doc": "Docker",
    "title": "Deploy an OpenSearch cluster using Docker Compose",
    "content": "Although it is technically possible to build an OpenSearch cluster by creating containers one command at a time, it is far easier to define your environment in a YAML file and let Docker Compose manage the cluster. The following section contains example YAML files that you can use to launch a predefined cluster with OpenSearch and OpenSearch Dashboards. These examples are useful for testing and development, but are not suitable for a production environment. If you don’t have prior experience using Docker Compose, you may wish to review the Docker Compose specification for guidance on syntax and formatting before making any changes to the dictionary structures in the examples. The YAML file that defines the environment is referred to as a Docker Compose file. By default, docker-compose commands will first check your current directory for a file that matches any of the following names: . | docker-compose.yml | docker-compose.yaml | compose.yml | compose.yaml | . If none of those files exist in your current directory, the docker-compose command fails. You can specify a custom file location and name when invoking docker-compose with the -f flag: . # Use a relative or absolute path to the file. docker-compose -f /path/to/your-file.yml up . If this is your first time launching an OpenSearch cluster using Docker Compose, use the following example docker-compose.yml file. Save it in the home directory of your host and name it docker-compose.yml. This file will create a cluster that contains three containers: two containers running the OpenSearch service and a single container running OpenSearch Dashboards. These containers will communicate over a bridge network called opensearch-net and use two volumes, one for each OpenSearch node. Because this file does not explicitly disable the demo security configuration, self-signed TLS certificates are installed and internal users with default names and passwords are created. Sample docker-compose.yml . version: '3' services: opensearch-node1: # This is also the hostname of the container within the Docker network (i.e. https://opensearch-node1/) image: opensearchproject/opensearch:latest # Specifying the latest available image - modify if you want a specific version container_name: opensearch-node1 environment: - cluster.name=opensearch-cluster # Name the cluster - node.name=opensearch-node1 # Name the node that will run in this container - discovery.seed_hosts=opensearch-node1,opensearch-node2 # Nodes to look for when discovering the cluster - cluster.initial_cluster_manager_nodes=opensearch-node1,opensearch-node2 # Nodes eligible to serve as cluster manager - bootstrap.memory_lock=true # Disable JVM heap memory swapping - \"OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m\" # Set min and max JVM heap sizes to at least 50% of system RAM ulimits: memlock: soft: -1 # Set memlock to unlimited (no soft or hard limit) hard: -1 nofile: soft: 65536 # Maximum number of open files for the opensearch user - set to at least 65536 hard: 65536 volumes: - opensearch-data1:/usr/share/opensearch/data # Creates volume called opensearch-data1 and mounts it to the container ports: - 9200:9200 # REST API - 9600:9600 # Performance Analyzer networks: - opensearch-net # All of the containers will join the same Docker bridge network opensearch-node2: image: opensearchproject/opensearch:latest # This should be the same image used for opensearch-node1 to avoid issues container_name: opensearch-node2 environment: - cluster.name=opensearch-cluster - node.name=opensearch-node2 - discovery.seed_hosts=opensearch-node1,opensearch-node2 - cluster.initial_cluster_manager_nodes=opensearch-node1,opensearch-node2 - bootstrap.memory_lock=true - \"OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m\" ulimits: memlock: soft: -1 hard: -1 nofile: soft: 65536 hard: 65536 volumes: - opensearch-data2:/usr/share/opensearch/data networks: - opensearch-net opensearch-dashboards: image: opensearchproject/opensearch-dashboards:latest # Make sure the version of opensearch-dashboards matches the version of opensearch installed on other nodes container_name: opensearch-dashboards ports: - 5601:5601 # Map host port 5601 to container port 5601 expose: - \"5601\" # Expose port 5601 for web access to OpenSearch Dashboards environment: OPENSEARCH_HOSTS: '[\"https://opensearch-node1:9200\",\"https://opensearch-node2:9200\"]' # Define the OpenSearch nodes that OpenSearch Dashboards will query networks: - opensearch-net volumes: opensearch-data1: opensearch-data2: networks: opensearch-net: . copy . If you override opensearch_dashboards.yml settings using environment variables in your compose file, use all uppercase letters and replace periods with underscores (for example, for opensearch.hosts, use OPENSEARCH_HOSTS). This behavior is inconsistent with overriding opensearch.yml settings, where the conversion is just a change to the assignment operator (for example, discovery.type: single-node in opensearch.yml is defined as discovery.type=single-node in docker-compose.yml). From the home directory of your host (containing docker-compose.yml), create and start the containers in detached mode: . docker-compose up -d . copy . Verify that the service containers started correctly: . docker-compose ps . copy . If a container failed to start, you can review the service logs: . # If you don't pass a service name, docker-compose will show you logs from all of the nodes docker-compose logs &lt;serviceName&gt; . copy . Verify access to OpenSearch Dashboards by connecting to http://localhost:5601 from a browser. The default username and password are admin. We do not recommend using this configuration on hosts that are accessible from the public internet until you have customized the security configuration of your deployment. Remember that localhost cannot be accessed remotely. If you are deploying these containers to a remote host, then you will need to establish a network connection and replace localhost with the IP or DNS record corresponding to the host. Stop the running containers in your cluster: . docker-compose down . copy . docker-compose down will stop the running containers, but it will not remove the Docker volumes that exist on the host. If you don’t care about the contents of these volumes, use the -v option to delete all volumes, for example, docker-compose down -v. ",
    "url": "https://vagimeli.github.io/install-and-configure/install-opensearch/docker/#deploy-an-opensearch-cluster-using-docker-compose",
    "relUrl": "/install-and-configure/install-opensearch/docker/#deploy-an-opensearch-cluster-using-docker-compose"
  },"76": {
    "doc": "Docker",
    "title": "Configure OpenSearch",
    "content": "Unlike the RPM distribution of OpenSearch, which requires a large amount of post-installation configuration, running OpenSearch clusters with Docker allows you to define the environment before the containers are even created. This is possible whether you use Docker or Docker Compose. For example, take a look at the following command: . docker run \\ -p 9200:9200 -p 9600:9600 \\ -e \"discovery.type=single-node\" \\ -v /path/to/custom-opensearch.yml:/usr/share/opensearch/config/opensearch.yml \\ opensearchproject/opensearch:latest . copy . By reviewing each part of the command, you can see that it: . | Maps ports 9200 and 9600 (HOST_PORT:CONTAINER_PORT). | Sets discovery.type to single-node so that bootstrap checks don’t fail for this single-node deployment. | Uses the -v flag to pass a local file called custom-opensearch.yml to the container, replacing the opensearch.yml file included with the image. | Requests the opensearchproject/opensearch:latest image from Docker Hub. | Runs the container. | . If you compare this command to the Sample docker-compose.yml file, you might notice some common settings, such as the port mappings and the image reference. The command, however, is only deploying a single container running OpenSearch and will not create a container for OpenSearch Dashboards. Furthermore, if you want to use custom TLS certificates, users, or roles, or define additional volumes and networks, then this “one-line” command rapidly grows to an impractical size. That is where the utility of Docker Compose becomes useful. When you build your OpenSearch cluster with Docker Compose you might find it easier to pass custom configuration files from your host to the container, as opposed to enumerating every individual setting in docker-compose.yml. Similar to how the example docker run command mounted a volume from the host to the container using the -v flag, compose files can specify volumes to mount as a sub-option to the corresponding service. The following truncated YAML file demonstrates how to mount a file or directory to the container. Refer to the official Docker documentation on volumes for comprehensive information about volume usage and syntax. services: opensearch-node1: volumes: - opensearch-data1:/usr/share/opensearch/data - ./custom-opensearch.yml:/usr/share/opensearch/config/opensearch.yml opensearch-node2: volumes: - opensearch-data2:/usr/share/opensearch/data - ./custom-opensearch.yml:/usr/share/opensearch/config/opensearch.yml opensearch-dashboards: volumes: - ./custom-opensearch_dashboards.yml:/usr/share/opensearch-dashboards/config/opensearch_dashboards.yml . copy . Sample Docker Compose file for development . If you want to build your own compose file from an example, review the following sample docker-compose.yml file. This sample file creates two OpenSearch nodes and one OpenSearch Dashboards node with the Security plugin disabled. You can use this sample file as a starting point while reviewing Configuring basic security settings. version: '3' services: opensearch-node1: image: opensearchproject/opensearch:latest container_name: opensearch-node1 environment: - cluster.name=opensearch-cluster # Name the cluster - node.name=opensearch-node1 # Name the node that will run in this container - discovery.seed_hosts=opensearch-node1,opensearch-node2 # Nodes to look for when discovering the cluster - cluster.initial_cluster_manager_nodes=opensearch-node1,opensearch-node2 # Nodes eligibile to serve as cluster manager - bootstrap.memory_lock=true # Disable JVM heap memory swapping - \"OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m\" # Set min and max JVM heap sizes to at least 50% of system RAM - \"DISABLE_INSTALL_DEMO_CONFIG=true\" # Prevents execution of bundled demo script which installs demo certificates and security configurations to OpenSearch - \"DISABLE_SECURITY_PLUGIN=true\" # Disables Security plugin ulimits: memlock: soft: -1 # Set memlock to unlimited (no soft or hard limit) hard: -1 nofile: soft: 65536 # Maximum number of open files for the opensearch user - set to at least 65536 hard: 65536 volumes: - opensearch-data1:/usr/share/opensearch/data # Creates volume called opensearch-data1 and mounts it to the container ports: - 9200:9200 # REST API - 9600:9600 # Performance Analyzer networks: - opensearch-net # All of the containers will join the same Docker bridge network opensearch-node2: image: opensearchproject/opensearch:latest container_name: opensearch-node2 environment: - cluster.name=opensearch-cluster # Name the cluster - node.name=opensearch-node2 # Name the node that will run in this container - discovery.seed_hosts=opensearch-node1,opensearch-node2 # Nodes to look for when discovering the cluster - cluster.initial_cluster_manager_nodes=opensearch-node1,opensearch-node2 # Nodes eligibile to serve as cluster manager - bootstrap.memory_lock=true # Disable JVM heap memory swapping - \"OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m\" # Set min and max JVM heap sizes to at least 50% of system RAM - \"DISABLE_INSTALL_DEMO_CONFIG=true\" # Prevents execution of bundled demo script which installs demo certificates and security configurations to OpenSearch - \"DISABLE_SECURITY_PLUGIN=true\" # Disables Security plugin ulimits: memlock: soft: -1 # Set memlock to unlimited (no soft or hard limit) hard: -1 nofile: soft: 65536 # Maximum number of open files for the opensearch user - set to at least 65536 hard: 65536 volumes: - opensearch-data2:/usr/share/opensearch/data # Creates volume called opensearch-data2 and mounts it to the container networks: - opensearch-net # All of the containers will join the same Docker bridge network opensearch-dashboards: image: opensearchproject/opensearch-dashboards:latest container_name: opensearch-dashboards ports: - 5601:5601 # Map host port 5601 to container port 5601 expose: - \"5601\" # Expose port 5601 for web access to OpenSearch Dashboards environment: - 'OPENSEARCH_HOSTS=[\"http://opensearch-node1:9200\",\"http://opensearch-node2:9200\"]' - \"DISABLE_SECURITY_DASHBOARDS_PLUGIN=true\" # disables security dashboards plugin in OpenSearch Dashboards networks: - opensearch-net volumes: opensearch-data1: opensearch-data2: networks: opensearch-net: . copy . Configuring basic security settings . Before making your OpenSearch cluster available to external hosts, it’s a good idea to review the deployment’s security configuration. You may recall from the first Sample docker-compose.yml file that, unless disabled by setting DISABLE_SECURITY_PLUGIN=true, a bundled script will apply a default demo security configuration to the nodes in the cluster. Because this configuration is used for demo purposes, the default usernames and passwords are known. For that reason, we recommend that you create your own security configuration files and use volumes to pass these files to the containers. For specific guidance on OpenSearch security settings, see Security configuration. To use your own certificates in your configuration, add all of the necessary certificates to the volumes section of the compose file: . volumes: - ./root-ca.pem:/usr/share/opensearch/config/root-ca.pem - ./admin.pem:/usr/share/opensearch/config/admin.pem - ./admin-key.pem:/usr/share/opensearch/config/admin-key.pem - ./node1.pem:/usr/share/opensearch/config/node1.pem - ./node1-key.pem:/usr/share/opensearch/config/node1-key.pem . copy . When you add TLS certificates to your OpenSearch nodes with Docker Compose volumes, you should also include a custom opensearch.yml file that defines those certificates. For example: . volumes: - ./root-ca.pem:/usr/share/opensearch/config/root-ca.pem - ./admin.pem:/usr/share/opensearch/config/admin.pem - ./admin-key.pem:/usr/share/opensearch/config/admin-key.pem - ./node1.pem:/usr/share/opensearch/config/node1.pem - ./node1-key.pem:/usr/share/opensearch/config/node1-key.pem - ./custom-opensearch.yml:/usr/share/opensearch/config/opensearch.yml . copy . Remember that the certificates you specify in your compose file must be the same as the certificates defined in your custom opensearch.yml file. You should replace the root, admin, and node certificates with your own. For more information see Configure TLS certificates. plugins.security.ssl.transport.pemcert_filepath: node1.pem plugins.security.ssl.transport.pemkey_filepath: node1-key.pem plugins.security.ssl.transport.pemtrustedcas_filepath: root-ca.pem plugins.security.ssl.http.pemcert_filepath: node1.pem plugins.security.ssl.http.pemkey_filepath: node1-key.pem plugins.security.ssl.http.pemtrustedcas_filepath: root-ca.pem plugins.security.authcz.admin_dn: - CN=admin,OU=SSL,O=Test,L=Test,C=DE . copy . After configuring security settings, your custom opensearch.yml file might look something like the following example, which adds TLS certificates and the distinguished name (DN) of the admin certificate, defines a few permissions, and enables verbose audit logging: . plugins.security.ssl.transport.pemcert_filepath: node1.pem plugins.security.ssl.transport.pemkey_filepath: node1-key.pem plugins.security.ssl.transport.pemtrustedcas_filepath: root-ca.pem plugins.security.ssl.transport.enforce_hostname_verification: false plugins.security.ssl.http.enabled: true plugins.security.ssl.http.pemcert_filepath: node1.pem plugins.security.ssl.http.pemkey_filepath: node1-key.pem plugins.security.ssl.http.pemtrustedcas_filepath: root-ca.pem plugins.security.allow_default_init_securityindex: true plugins.security.authcz.admin_dn: - CN=A,OU=UNIT,O=ORG,L=TORONTO,ST=ONTARIO,C=CA plugins.security.nodes_dn: - 'CN=N,OU=UNIT,O=ORG,L=TORONTO,ST=ONTARIO,C=CA' plugins.security.audit.type: internal_opensearch plugins.security.enable_snapshot_restore_privilege: true plugins.security.check_snapshot_restore_write_privileges: true plugins.security.restapi.roles_enabled: [\"all_access\", \"security_rest_api_access\"] cluster.routing.allocation.disk.threshold_enabled: false opendistro_security.audit.config.disabled_rest_categories: NONE opendistro_security.audit.config.disabled_transport_categories: NONE . copy . For a full list of settings, see Security. Use the same process to specify a Backend configuration in /usr/share/opensearch/config/opensearch-security/config.yml as well as new internal users, roles, mappings, action groups, and tenants in their respective YAML files. After replacing the certificates and creating your own internal users, roles, mappings, action groups, and tenants, use Docker Compose to start the cluster: . docker-compose up -d . copy . Working with plugins . To use the OpenSearch image with a custom plugin, you must first create a Dockerfile. Review the official Docker documentation for information about creating a Dockerfile. FROM opensearchproject/opensearch:latest RUN /usr/share/opensearch/bin/opensearch-plugin install --batch &lt;pluginId&gt; . Then run the following commands: . # Build an image from a Dockerfile docker build --tag=opensearch-custom-plugin . # Start the container from the custom image docker run -p 9200:9200 -p 9600:9600 -v /usr/share/opensearch/data opensearch-custom-plugin . Alternatively, you might want to remove a plugin from an image before deploying it. This example Dockerfile removes the Security plugin: . FROM opensearchproject/opensearch:latest RUN /usr/share/opensearch/bin/opensearch-plugin remove opensearch-security . copy . You can also use a Dockerfile to pass your own certificates for use with the Security plugin: . FROM opensearchproject/opensearch:latest COPY --chown=opensearch:opensearch opensearch.yml /usr/share/opensearch/config/ COPY --chown=opensearch:opensearch my-key-file.pem /usr/share/opensearch/config/ COPY --chown=opensearch:opensearch my-certificate-chain.pem /usr/share/opensearch/config/ COPY --chown=opensearch:opensearch my-root-cas.pem /usr/share/opensearch/config/ . copy . ",
    "url": "https://vagimeli.github.io/install-and-configure/install-opensearch/docker/#configure-opensearch",
    "relUrl": "/install-and-configure/install-opensearch/docker/#configure-opensearch"
  },"77": {
    "doc": "Docker",
    "title": "Related links",
    "content": ". | OpenSearch configuration | Performance analyzer | Install and configure OpenSearch Dashboards | About Security in OpenSearch | . ",
    "url": "https://vagimeli.github.io/install-and-configure/install-opensearch/docker/#related-links",
    "relUrl": "/install-and-configure/install-opensearch/docker/#related-links"
  },"78": {
    "doc": "Helm",
    "title": "Helm",
    "content": "Helm is a package manager that allows you to easily install and manage OpenSearch in a Kubernetes cluster. You can define your OpenSearch configurations in a YAML file and use Helm to deploy your applications in a version-controlled and reproducible way. The Helm chart contains the resources described in the following table. | Resource | Description | . | Chart.yaml | Information about the chart. | . | values.yaml | Default configuration values for the chart. | . | templates | Templates that combine with values to generate the Kubernetes manifest files. | . The specification in the default Helm chart supports many standard use cases and setups. You can modify the default chart to configure your desired specifications and set Transport Layer Security (TLS) and role-based access control (RBAC). For information about the default configuration, steps to configure security, and configurable parameters, see the README. The instructions here assume you have a Kubernetes cluster with Helm preinstalled. See the Kubernetes documentation for steps to configure a Kubernetes cluster and the Helm documentation to install Helm. ",
    "url": "https://vagimeli.github.io/install-and-configure/install-opensearch/helm/",
    "relUrl": "/install-and-configure/install-opensearch/helm/"
  },"79": {
    "doc": "Helm",
    "title": "Prerequisites",
    "content": "The default Helm chart deploys a three-node cluster. We recommend that you have at least 8 GiB of memory available for this deployment. You can expect the deployment to fail if, say, you have less than 4 GiB of memory available. ",
    "url": "https://vagimeli.github.io/install-and-configure/install-opensearch/helm/#prerequisites",
    "relUrl": "/install-and-configure/install-opensearch/helm/#prerequisites"
  },"80": {
    "doc": "Helm",
    "title": "Install OpenSearch using Helm",
    "content": ". | Add opensearch helm-charts repository to Helm: . helm repo add opensearch https://opensearch-project.github.io/helm-charts/ . copy . | Update the available charts locally from charts repositories: . helm repo update . copy . | To search for the OpenSearch-related Helm charts: . helm search repo opensearch . copy . NAME CHART VERSION APP VERSION DESCRIPTION opensearch/opensearch 1.0.7 1.0.0 A Helm chart for OpenSearch opensearch/opensearch-dashboards 1.0.4 1.0.0 A Helm chart for OpenSearch Dashboards . | Deploy OpenSearch: . helm install my-deployment opensearch/opensearch . copy . | . You can also build the opensearch-1.0.0.tgz file manually: . | Change to the opensearch directory: . cd charts/opensearch . copy . | Package the Helm chart: . helm package . copy . | Deploy OpenSearch: . helm install --generate-name opensearch-1.0.0.tgz . copy . The output shows you the specifications instantiated from the install. To customize the deployment, pass in the values that you want to override with a custom YAML file: . helm install --values=customvalues.yaml opensearch-1.0.0.tgz . copy . | . Sample output . NAME: opensearch-1-1629223146 LAST DEPLOYED: Tue Aug 17 17:59:07 2021 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Watch all cluster members come up. $ kubectl get pods --namespace=default -l app=opensearch-cluster-master -w . To make sure your OpenSearch pod is up and running, run the following command: . $ kubectl get pods NAME READY STATUS RESTARTS AGE opensearch-cluster-master-0 1/1 Running 0 3m56s opensearch-cluster-master-1 1/1 Running 0 3m56s opensearch-cluster-master-2 1/1 Running 0 3m56s . To access the OpenSearch shell: . $ kubectl exec -it opensearch-cluster-master-0 -- /bin/bash . copy . You can send requests to the pod to verify that OpenSearch is up and running: . $ curl -XGET https://localhost:9200 -u 'admin:admin' --insecure { \"name\" : \"opensearch-cluster-master-1\", \"cluster_name\" : \"opensearch-cluster\", \"cluster_uuid\" : \"hP2gq5bPS3SLp8Z7wXm8YQ\", \"version\" : { \"distribution\" : \"opensearch\", \"number\" : &lt;version&gt;, \"build_type\" : &lt;build-type&gt;, \"build_hash\" : &lt;build-hash&gt;, \"build_date\" : &lt;build-date&gt;, \"build_snapshot\" : false, \"lucene_version\" : &lt;lucene-version&gt;, \"minimum_wire_compatibility_version\" : \"6.8.0\", \"minimum_index_compatibility_version\" : \"6.0.0-beta1\" }, \"tagline\" : \"The OpenSearch Project: https://opensearch.org/\" } . ",
    "url": "https://vagimeli.github.io/install-and-configure/install-opensearch/helm/#install-opensearch-using-helm",
    "relUrl": "/install-and-configure/install-opensearch/helm/#install-opensearch-using-helm"
  },"81": {
    "doc": "Helm",
    "title": "Uninstall using Helm",
    "content": "To identify the OpenSearch deployment that you want to delete: . $ helm list NAME NAMESPACEREVISIONUPDATED STATUS CHART APP VERSION opensearch-1-1629223146 default 1 2021-08-17 17:59:07.664498239 +0000 UTCdeployedopensearch-1.0.0 1.0.0 . To delete or uninstall a deployment, run the following command: . helm delete opensearch-1-1629223146 . copy . For steps to install OpenSearch Dashboards, see Helm to install OpenSearch Dashboards. ",
    "url": "https://vagimeli.github.io/install-and-configure/install-opensearch/helm/#uninstall-using-helm",
    "relUrl": "/install-and-configure/install-opensearch/helm/#uninstall-using-helm"
  },"82": {
    "doc": "Installing OpenSearch",
    "title": "Installing OpenSearch",
    "content": "This section details how to install OpenSearch on your host, including which operating systems are compatible with OpenSearch, which ports to open, and which important settings to configure on your host. ",
    "url": "https://vagimeli.github.io/install-and-configure/install-opensearch/index/",
    "relUrl": "/install-and-configure/install-opensearch/index/"
  },"83": {
    "doc": "Installing OpenSearch",
    "title": "Operating system compatibility",
    "content": "We recommend installing OpenSearch on Red Hat Enterprise Linux (RHEL) or Debian-based Linux distributions that use systemd, such as CentOS, Amazon Linux 2, or Ubuntu Long-Term Support (LTS). OpenSearch should work on most Linux distributions, but we only test a handful. We recommend RHEL 7 or 8, CentOS 7 or 8, Amazon Linux 2, or Ubuntu 16.04, 18.04, or 20.04 for any version of OpenSearch. OpenSearch also supports Windows Server 2019. ",
    "url": "https://vagimeli.github.io/install-and-configure/install-opensearch/index/#operating-system-compatibility",
    "relUrl": "/install-and-configure/install-opensearch/index/#operating-system-compatibility"
  },"84": {
    "doc": "Installing OpenSearch",
    "title": "File system recommendations",
    "content": "Avoid using a network file system for node storage in a production workflow. Using a network file system for node storage can cause performance issues in your cluster due to factors such as network conditions (like latency or limited throughput) or read/write speeds. You should use solid-state drives (SSDs) installed on the host for node storage where possible. ",
    "url": "https://vagimeli.github.io/install-and-configure/install-opensearch/index/#file-system-recommendations",
    "relUrl": "/install-and-configure/install-opensearch/index/#file-system-recommendations"
  },"85": {
    "doc": "Installing OpenSearch",
    "title": "Java compatibility",
    "content": "The OpenSearch distribution for Linux ships with a compatible Adoptium JDK version of Java in the jdk directory. To find the JDK version, run ./jdk/bin/java -version. For example, the OpenSearch 1.0.0 tarball ships with Java 15.0.1+9 (non-LTS), OpenSearch 1.3.0 ships with Java 11.0.14.1+1 (LTS), and OpenSearch 2.0.0 ships with Java 17.0.2+8 (LTS). OpenSearch is tested with all compatible Java versions. | OpenSearch Version | Compatible Java Versions | Bundled Java Version | . | 1.0 - 1.2.x | 11, 15 | 15.0.1+9 | . | 1.3.x | 8, 11, 14 | 11.0.14.1+1 | . | 2.0.0 | 11, 17 | 17.0.2+8 | . To use a different Java installation, set the OPENSEARCH_JAVA_HOME or JAVA_HOME environment variable to the Java install location. For example: . export OPENSEARCH_JAVA_HOME=/path/to/opensearch-2.7.0/jdk . ",
    "url": "https://vagimeli.github.io/install-and-configure/install-opensearch/index/#java-compatibility",
    "relUrl": "/install-and-configure/install-opensearch/index/#java-compatibility"
  },"86": {
    "doc": "Installing OpenSearch",
    "title": "Network requirements",
    "content": "The following ports need to be open for OpenSearch components. | Port number | OpenSearch component | . | 443 | OpenSearch Dashboards in AWS OpenSearch Service with encryption in transit (TLS) | . | 5601 | OpenSearch Dashboards | . | 9200 | OpenSearch REST API | . | 9250 | Cross-cluster search | . | 9300 | Node communication and transport | . | 9600 | Performance Analyzer | . ",
    "url": "https://vagimeli.github.io/install-and-configure/install-opensearch/index/#network-requirements",
    "relUrl": "/install-and-configure/install-opensearch/index/#network-requirements"
  },"87": {
    "doc": "Installing OpenSearch",
    "title": "Important settings",
    "content": "For production workloads, make sure the Linux setting vm.max_map_count is set to at least 262144. Even if you use the Docker image, set this value on the host machine. To check the current value, run this command: . cat /proc/sys/vm/max_map_count . To increase the value, add the following line to /etc/sysctl.conf: . vm.max_map_count=262144 . Then run sudo sysctl -p to reload. The sample docker-compose.yml file also contains several key settings: . | bootstrap.memory_lock=true . Disables swapping (along with memlock). Swapping can dramatically decrease performance and stability, so you should ensure it is disabled on production clusters. Enabling the bootstrap.memory_lock setting will cause the JVM to reserve any memory it needs. The Java SE Hotspot VM Garbage Collection Tuning Guide documents a default 1 gigabyte (GB) Class Metadata native memory reservation. Combined with Java heap, this may result in an error due to the lack of native memory on VMs with less memory than these requirements. To prevent errors, limit the reserved memory size using -XX:CompressedClassSpaceSize or -XX:MaxMetaspaceSize and set the size of the Java heap to make sure you have enough system memory. | OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m . Sets the size of the Java heap (we recommend half of system RAM). | nofile 65536 . Sets a limit of 65536 open files for the OpenSearch user. | port 9600 . Allows you to access Performance Analyzer on port 9600. | . Do not declare the same JVM options in multiple locations because it can result in unexpected behavior or a failure of the OpenSearch service to start. If you declare JVM options using an environment variable, such as OPENSEARCH_JAVA_OPTS=-Xms3g -Xmx3g, then you should comment out any references to that JVM option in config/jvm.options. Conversely, if you define JVM options in config/jvm.options, then you should not define those JVM options using environment variables. ",
    "url": "https://vagimeli.github.io/install-and-configure/install-opensearch/index/#important-settings",
    "relUrl": "/install-and-configure/install-opensearch/index/#important-settings"
  },"88": {
    "doc": "RPM",
    "title": "RPM",
    "content": "Installing OpenSearch using RPM Package Manager (RPM) simplifies the process considerably compared to the Tarball method. Several technical considerations, such as the installation path, location of configuration files, and creation of a service managed by systemd, as examples, are handled automatically by the package manager. Generally speaking, installing OpenSearch from the RPM distribution can be broken down into a few steps: . | Download and install OpenSearch. | Install manually from an RPM package or from a YUM repository. | . | (Optional) Test OpenSearch. | Confirm that OpenSearch is able to run before you apply any custom configuration. | This can be done without any security (no password, no certificates) or with a demo security configuration that can be applied by a packaged script. | . | Configure OpenSearch for your environment. | Apply basic settings to OpenSearch and start using it in your environment. | . | . The RPM distribution provides everything you need to run OpenSearch inside Red Hat or Red Hat–based Linux Distributions, such as supported CentOS and RHEL versions, and Amazon Linux 2. If you have your own Java installation and set JAVA_HOME in your terminal application, macOS works, as well. This guide assumes that you are comfortable working from the Linux command line interface (CLI). You should understand how to input commands, navigate between directories, and edit text files. Some example commands reference the vi text editor, but you may use any text editor available. ",
    "url": "https://vagimeli.github.io/install-and-configure/install-opensearch/rpm/#rpm",
    "relUrl": "/install-and-configure/install-opensearch/rpm/#rpm"
  },"89": {
    "doc": "RPM",
    "title": "Step 1: Download and install OpenSearch",
    "content": "Install OpenSearch from a package . | Download the RPM package for the desired version directly from the OpenSearch downloads page. The RPM package can be downloaded for both x64 and arm64 architectures. | Import the public GNU Privacy Guard (GPG) key. This key verifies that your OpenSearch instance is signed. sudo rpm --import https://artifacts.opensearch.org/publickeys/opensearch.pgp . copy . | From the CLI, you can install the package with rpm or yum. # Install the x64 package using yum. sudo yum install opensearch-2.7.0-linux-x64.rpm # Install the x64 package using rpm. sudo rpm -ivh opensearch-2.7.0-linux-x64.rpm # Install the arm64 package using yum. sudo yum install opensearch-2.7.0-linux-x64.rpm # Install the arm64 package using rpm. sudo rpm -ivh opensearch-2.7.0-linux-x64.rpm . | After the installation succeeds, enable OpenSearch as a service. sudo systemctl enable opensearch . copy . | Start OpenSearch. sudo systemctl start opensearch . copy . | Verify that OpenSearch launched correctly. sudo systemctl status opensearch . copy . | . Install OpenSearch from a YUM repository . YUM, the primary package management tool for Red Hat–based operating systems, allows you to download and install the RPM package from the YUM repository. | Create a local repository file for OpenSearch: sudo curl -SL https://artifacts.opensearch.org/releases/bundle/opensearch/2.x/opensearch-2.x.repo -o /etc/yum.repos.d/opensearch-2.x.repo . copy . | Clean your YUM cache to ensure a smooth installation: sudo yum clean all . copy . | Verify that the repository was created successfully. sudo yum repolist . copy . | With the repository file downloaded, list all available versions of OpenSearch: sudo yum list opensearch --showduplicates . copy . | Choose the version of OpenSearch you want to install: . | Unless otherwise indicated, the latest available version of OpenSearch is installed. sudo yum install opensearch . copy . | To install a specific version of OpenSearch: sudo yum install 'opensearch-2.7.0' . copy . | . | During installation, the installer will present you with the GPG key fingerprint. Verify that the information matches the following: Fingerprint: c5b7 4989 65ef d1c2 924b a9d5 39d3 1987 9310 d3fc . copy . | If correct, enter yes or y. The OpenSearch installation continues. | . | Once complete, you can run OpenSearch. sudo systemctl start opensearch . copy . | Verify that OpenSearch launched correctly. sudo systemctl status opensearch . copy . | . ",
    "url": "https://vagimeli.github.io/install-and-configure/install-opensearch/rpm/#step-1-download-and-install-opensearch",
    "relUrl": "/install-and-configure/install-opensearch/rpm/#step-1-download-and-install-opensearch"
  },"90": {
    "doc": "RPM",
    "title": "Step 2: (Optional) Test OpenSearch",
    "content": "Before proceeding with any configuration, you should test your installation of OpenSearch. Otherwise, it can be difficult to determine whether future problems are due to installation issues or custom settings you applied after installation. When OpenSearch is installed using the RPM package, some demo security settings are automatically applied. This includes self-signed TLS certificates and several users and roles. If you would like to configure these yourself, see Set up OpenSearch in your environment. An OpenSearch node in its default configuration (with demo certificates and users with default passwords) is not suitable for a production environment. If you plan to use the node in a production environment, you should, at a minimum, replace the demo TLS certificates with your own TLS certificates and update the list of internal users and passwords. See Security configuration for additional guidance to ensure that your nodes are configured according to your security requirements. | Send requests to the server to verify that OpenSearch is running. Note the use of the --insecure flag, which is required because the TLS certificates are self-signed. | Send a request to port 9200: curl -X GET https://localhost:9200 -u 'admin:admin' --insecure . copy . You should get a response that looks like this: . { \"name\" : \"hostname\", \"cluster_name\" : \"opensearch\", \"cluster_uuid\" : \"6XNc9m2gTUSIoKDqJit0PA\", \"version\" : { \"distribution\" : \"opensearch\", \"number\" : &lt;version&gt;, \"build_type\" : &lt;build-type&gt;, \"build_hash\" : &lt;build-hash&gt;, \"build_date\" : &lt;build-date&gt;, \"build_snapshot\" : false, \"lucene_version\" : &lt;lucene-version&gt;, \"minimum_wire_compatibility_version\" : \"7.10.0\", \"minimum_index_compatibility_version\" : \"7.0.0\" }, \"tagline\" : \"The OpenSearch Project: https://opensearch.org/\" } . | Query the plugins endpoint: curl -X GET https://localhost:9200/_cat/plugins?v -u 'admin:admin' --insecure . copy . The response should look like this: . name component version hostname opensearch-alerting 2.7.0 hostname opensearch-anomaly-detection 2.7.0 hostname opensearch-asynchronous-search 2.7.0 hostname opensearch-cross-cluster-replication 2.7.0 hostname opensearch-index-management 2.7.0 hostname opensearch-job-scheduler 2.7.0 hostname opensearch-knn 2.7.0 hostname opensearch-ml 2.7.0 hostname opensearch-notifications 2.7.0 hostname opensearch-notifications-core 2.7.0 hostname opensearch-observability 2.7.0 hostname opensearch-performance-analyzer 2.7.0 hostname opensearch-reports-scheduler 2.7.0 hostname opensearch-security 2.7.0 hostname opensearch-sql 2.7.0 . | . | . ",
    "url": "https://vagimeli.github.io/install-and-configure/install-opensearch/rpm/#step-2-optional-test-opensearch",
    "relUrl": "/install-and-configure/install-opensearch/rpm/#step-2-optional-test-opensearch"
  },"91": {
    "doc": "RPM",
    "title": "Step 3: Set up OpenSearch in your environment",
    "content": "Users who do not have prior experience with OpenSearch may want a list of recommended settings in order to get started with the service. By default, OpenSearch is not bound to a network interface and cannot be reached by external hosts. Additionally, security settings are populated by default user names and passwords. The following recommendations will enable a user to bind OpenSearch to a network interface, create and sign TLS certificates, and configure basic authentication. The following recommended settings will allow you to: . | Bind OpenSearch to an IP or network interface on the host. | Set initial and maximum JVM heap sizes. | Define an environment variable that points to the bundled JDK. | Configure your own TLS certificates—no third-party certificate authority (CA) is required. | Create an admin user with a custom password. | . If you ran the security demo script, then you will need to manually reconfigure settings that were modified. Refer to Security configuration for guidance before proceeding. Before modifying any configuration files, it’s always a good idea to save a backup copy before making changes. The backup file can be used to mitigate any issues caused by a bad configuration. | Open opensearch.yml. sudo vi /etc/opensearch/opensearch.yml . copy . | Add the following lines: # Bind OpenSearch to the correct network interface. Use 0.0.0.0 # to include all available interfaces or specify an IP address # assigned to a specific interface. network.host: 0.0.0.0 # Unless you have already configured a cluster, you should set # discovery.type to single-node, or the bootstrap checks will # fail when you try to start the service. discovery.type: single-node # If you previously disabled the Security plugin in opensearch.yml, # be sure to re-enable it. Otherwise you can skip this setting. plugins.security.disabled: false . copy . | Save your changes and close the file. | Specify initial and maximum JVM heap sizes. | Open jvm.options. vi /etc/opensearch/jvm.options . copy . | Modify the values for initial and maximum heap sizes. As a starting point, you should set these values to half of the available system memory. For dedicated hosts this value can be increased based on your workflow requirements. | As an example, if the host machine has 8 GB of memory, then you might want to set the initial and maximum heap sizes to 4 GB: -Xms4g -Xmx4g . copy . | . | Save your changes and close the file. | . | . Configure TLS . TLS certificates provide additional security for your cluster by allowing clients to confirm the identity of hosts and encrypt traffic between the client and host. For more information, refer to Configure TLS Certificates and Generate Certificates, which are included in the Security plugin documentation. For work performed in a development environment, self-signed certificates are usually adequate. This section will guide you through the basic steps required to generate your own TLS certificates and apply them to your OpenSearch host. | Navigate to the directory where the certificates will be stored. cd /etc/opensearch . copy . | Delete the demo certificates. sudo rm -f *pem . copy . | Generate a root certificate. This is what you will use to sign your other certificates. # Create a private key for the root certificate sudo openssl genrsa -out root-ca-key.pem 2048 # Use the private key to create a self-signed root certificate. Be sure to # replace the arguments passed to -subj so they reflect your specific host. sudo openssl req -new -x509 -sha256 -key root-ca-key.pem -subj \"/C=CA/ST=ONTARIO/L=TORONTO/O=ORG/OU=UNIT/CN=ROOT\" -out root-ca.pem -days 730 . | Next, create the admin certificate. This certificate is used to gain elevated rights for performing administrative tasks relating to the Security plugin. # Create a private key for the admin certificate. sudo openssl genrsa -out admin-key-temp.pem 2048 # Convert the private key to PKCS#8. sudo openssl pkcs8 -inform PEM -outform PEM -in admin-key-temp.pem -topk8 -nocrypt -v1 PBE-SHA1-3DES -out admin-key.pem # Create the certficiate signing request (CSR). A common name (CN) of \"A\" is acceptable because this certificate is # used for authenticating elevated access and is not tied to a host. sudo openssl req -new -key admin-key.pem -subj \"/C=CA/ST=ONTARIO/L=TORONTO/O=ORG/OU=UNIT/CN=A\" -out admin.csr # Sign the admin certificate with the root certificate and private key you created earlier. sudo openssl x509 -req -in admin.csr -CA root-ca.pem -CAkey root-ca-key.pem -CAcreateserial -sha256 -out admin.pem -days 730 . | Create a certificate for the node being configured. # Create a private key for the node certificate. sudo openssl genrsa -out node1-key-temp.pem 2048 # Convert the private key to PKCS#8. sudo openssl pkcs8 -inform PEM -outform PEM -in node1-key-temp.pem -topk8 -nocrypt -v1 PBE-SHA1-3DES -out node1-key.pem # Create the CSR and replace the arguments passed to -subj so they reflect your specific host. # The CN should match a DNS A record for the host-do not use the hostname. sudo openssl req -new -key node1-key.pem -subj \"/C=CA/ST=ONTARIO/L=TORONTO/O=ORG/OU=UNIT/CN=node1.dns.a-record\" -out node1.csr # Create an extension file that defines a SAN DNS name for the host. This # should match the DNS A record of the host. sudo sh -c 'echo subjectAltName=DNS:node1.dns.a-record &gt; node1.ext' # Sign the node certificate with the root certificate and private key that you created earlier. sudo openssl x509 -req -in node1.csr -CA root-ca.pem -CAkey root-ca-key.pem -CAcreateserial -sha256 -out node1.pem -days 730 -extfile node1.ext . | Remove temporary files that are no longer required. sudo rm -f *temp.pem *csr *ext . copy . | Make sure the remaining certificates are owned by the opensearch user. sudo chown opensearch:opensearch admin-key.pem admin.pem node1-key.pem node1.pem root-ca-key.pem root-ca.pem root-ca.srl . copy . | Add these certificates to opensearch.yml as described in Generate Certificates. Advanced users might also choose to append the settings using a script: #! /bin/bash # Before running this script, make sure to replace the CN in the # node's distinguished name with a real DNS A record. echo \"plugins.security.ssl.transport.pemcert_filepath: /etc/opensearch/node1.pem\" | sudo tee -a /etc/opensearch/opensearch.yml echo \"plugins.security.ssl.transport.pemkey_filepath: /etc/opensearch/node1-key.pem\" | sudo tee -a /etc/opensearch/opensearch.yml echo \"plugins.security.ssl.transport.pemtrustedcas_filepath: /etc/opensearch/root-ca.pem\" | sudo tee -a /etc/opensearch/opensearch.yml echo \"plugins.security.ssl.http.enabled: true\" | sudo tee -a /etc/opensearch/opensearch.yml echo \"plugins.security.ssl.http.pemcert_filepath: /etc/opensearch/node1.pem\" | sudo tee -a /etc/opensearch/opensearch.yml echo \"plugins.security.ssl.http.pemkey_filepath: /etc/opensearch/node1-key.pem\" | sudo tee -a /etc/opensearch/opensearch.yml echo \"plugins.security.ssl.http.pemtrustedcas_filepath: /etc/opensearch/root-ca.pem\" | sudo tee -a /etc/opensearch/opensearch.yml echo \"plugins.security.allow_default_init_securityindex: true\" | sudo tee -a /etc/opensearch/opensearch.yml echo \"plugins.security.authcz.admin_dn:\" | sudo tee -a /etc/opensearch/opensearch.yml echo \" - 'CN=A,OU=UNIT,O=ORG,L=TORONTO,ST=ONTARIO,C=CA'\" | sudo tee -a /etc/opensearch/opensearch.yml echo \"plugins.security.nodes_dn:\" | sudo tee -a /etc/opensearch/opensearch.yml echo \" - 'CN=node1.dns.a-record,OU=UNIT,O=ORG,L=TORONTO,ST=ONTARIO,C=CA'\" | sudo tee -a /etc/opensearch/opensearch.yml echo \"plugins.security.audit.type: internal_opensearch\" | sudo tee -a /etc/opensearch/opensearch.yml echo \"plugins.security.enable_snapshot_restore_privilege: true\" | sudo tee -a /etc/opensearch/opensearch.yml echo \"plugins.security.check_snapshot_restore_write_privileges: true\" | sudo tee -a /etc/opensearch/opensearch.yml echo \"plugins.security.restapi.roles_enabled: [\\\"all_access\\\", \\\"security_rest_api_access\\\"]\" | sudo tee -a /etc/opensearch/opensearch.yml . copy . | (Optional) Add trust for the self-signed root certificate. # Copy the root certificate to the correct directory sudo cp /etc/opensearch/root-ca.pem /etc/pki/ca-trust/source/anchors/ # Add trust sudo update-ca-trust . | . Configure a user . Users are defined and authenticated by OpenSearch in a variety of ways. One method that does not require additional backend infrastructure is to manually configure users in internal_users.yml. See YAML files for more information about configuring users. The following steps explain how to remove all demo users except for the admin user and how to replace the admin default password using a script. | Navigate to the Security plugins tools directory. cd /usr/share/opensearch/plugins/opensearch-security/tools . copy . | Run hash.sh to generate a new password. | This script will fail if a path to the JDK has not been defined. # Example output if a JDK isn't found... $ ./hash.sh ************************************************************************** ** This tool will be deprecated in the next major release of OpenSearch ** ** https://github.com/opensearch-project/security/issues/1755 ** ************************************************************************** which: no java in (/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/user/.local/bin:/home/user/bin) WARNING: nor OPENSEARCH_JAVA_HOME nor JAVA_HOME is set, will use ./hash.sh: line 35: java: command not found . | Declare an environment variable when you invoke the script in order to avoid issues: OPENSEARCH_JAVA_HOME=/usr/share/opensearch/jdk ./hash.sh . copy . | Enter the desired password at the prompt and make a note of the output hash. | . | Open internal_users.yml. sudo vi /etc/opensearch/opensearch-security/internal_users.yml . copy . | Remove all demo users except for admin and replace the hash with the output provided by hash.sh in a previous step. The file should look similar to the following example: --- # This is the internal user database # The hash value is a bcrypt hash and can be generated with plugin/tools/hash.sh _meta: type: \"internalusers\" config_version: 2 # Define your internal users here admin: hash: \"$2y$1EXAMPLEQqwS8TUcoEXAMPLEeZ3lEHvkEXAMPLERqjyh1icEXAMPLE.\" reserved: true backend_roles: - \"admin\" description: \"Admin user\" . copy . | . Apply changes . Now that TLS certificates are installed and demo users were removed or assigned new passwords, the last step is to apply the configuration changes. This last configuration step requires invoking securityadmin.sh while OpenSearch is running on the host. | OpenSearch must be running for securityadmin.sh to apply changes. If you made changes to opensearch.yml, restart OpenSearch. sudo systemctl restart opensearch . | Open a separate terminal session with the host and navigate to the directory containing securityadmin.sh. # Change to the correct directory cd /usr/share/opensearch/plugins/opensearch-security/tools . | Invoke the script. See Apply changes using securityadmin.sh for definitions of the arguments you must pass. # You can omit the environment variable if you declared this in your $PATH. OPENSEARCH_JAVA_HOME=/usr/share/opensearch/jdk ./securityadmin.sh -cd /etc/opensearch/opensearch-security/ -cacert /etc/opensearch/root-ca.pem -cert /etc/opensearch/admin.pem -key /etc/opensearch/admin-key.pem -icl -nhnv . | . Verify that the service is running . OpenSearch is now running on your host with custom TLS certificates and a secure user for basic authentication. You can verify external connectivity by sending an API request to your OpenSearch node from another host. During the previous test you directed requests to localhost. Now that TLS certificates have been applied and the new certificates reference your host’s actual DNS record, requests to localhost will fail the CN check and the certificate will be considered invalid. Instead, requests should be sent to the address you specified while generating the certificate. You should add trust for the root certificate to your client before sending requests. If you do not add trust, then you must use the -k option so that cURL ignores CN and root certificate validation. $ curl https://your.host.address:9200 -u admin:yournewpassword -k { \"name\" : \"hostname-here\", \"cluster_name\" : \"opensearch\", \"cluster_uuid\" : \"efC0ANNMQlGQ5TbhNflVPg\", \"version\" : { \"distribution\" : \"opensearch\", \"number\" : &lt;version&gt;, \"build_type\" : &lt;build-type&gt;, \"build_hash\" : &lt;build-hash&gt;, \"build_date\" : &lt;build-date&gt;, \"build_snapshot\" : false, \"lucene_version\" : &lt;lucene-version&gt;, \"minimum_wire_compatibility_version\" : \"7.10.0\", \"minimum_index_compatibility_version\" : \"7.0.0\" }, \"tagline\" : \"The OpenSearch Project: https://opensearch.org/\" } . ",
    "url": "https://vagimeli.github.io/install-and-configure/install-opensearch/rpm/#step-3-set-up-opensearch-in-your-environment",
    "relUrl": "/install-and-configure/install-opensearch/rpm/#step-3-set-up-opensearch-in-your-environment"
  },"92": {
    "doc": "RPM",
    "title": "Upgrade to a newer version",
    "content": "OpenSearch instances installed using RPM or YUM can be easily upgraded to a newer version. We recommend updating with YUM, but you can also upgrade using RPM. Manual upgrade with RPM . Download the RPM package for the desired upgrade version directly from the OpenSearch downloads page. Navigate to the directory containing the distribution and run the following command: . rpm -Uvh opensearch-2.7.0-linux-x64.rpm . copy . YUM . To upgrade to the latest version of OpenSearch using YUM: . sudo yum update . copy . You can also upgrade to a specific OpenSearch version: . sudo yum update opensearch-&lt;version-number&gt; . copy . ",
    "url": "https://vagimeli.github.io/install-and-configure/install-opensearch/rpm/#upgrade-to-a-newer-version",
    "relUrl": "/install-and-configure/install-opensearch/rpm/#upgrade-to-a-newer-version"
  },"93": {
    "doc": "RPM",
    "title": "Related links",
    "content": ". | OpenSearch configuration | Install and configure OpenSearch Dashboards | OpenSearch plugin installation | About the Security plugin | . ",
    "url": "https://vagimeli.github.io/install-and-configure/install-opensearch/rpm/#related-links",
    "relUrl": "/install-and-configure/install-opensearch/rpm/#related-links"
  },"94": {
    "doc": "RPM",
    "title": "RPM",
    "content": " ",
    "url": "https://vagimeli.github.io/install-and-configure/install-opensearch/rpm/",
    "relUrl": "/install-and-configure/install-opensearch/rpm/"
  },"95": {
    "doc": "Tarball",
    "title": "Tarball",
    "content": "Installing OpenSearch from a tarball, also known as a tar archive, may appeal to users who want granular control over installation details like file permissions and installation paths. Generally speaking, the installation of OpenSearch from a tarball can be broken down into a few steps: . | Download and unpack OpenSearch. | Configure important system settings. | These settings are applied to the host before modifying any OpenSearch files. | . | (Optional) Test OpenSearch. | Confirm that OpenSearch is able to run before you apply any custom configuration. | This can be done without any security (no password, no certificates) or with a demo security configuration that can be applied by a packaged script. | . | Configure OpenSearch for your environment. | Apply basic settings to OpenSearch and start using it in your environment. | . | . The tarball is a self-contained directory with everything needed to run OpenSearch, including an integrated Java Development Kit (JDK). This installation method is compatible with most Linux distributions, including CentOS 7, Amazon Linux 2, and Ubuntu 18.04. If you have your own Java installation and set the environment variable JAVA_HOME in the terminal, macOS works as well. This guide assumes that you are comfortable working from the Linux command line interface (CLI). You should understand how to input commands, navigate between directories, and edit text files. Some example commands reference the vi text editor, but you may use any text editor available. ",
    "url": "https://vagimeli.github.io/install-and-configure/install-opensearch/tar/",
    "relUrl": "/install-and-configure/install-opensearch/tar/"
  },"96": {
    "doc": "Tarball",
    "title": "Step 1: Download and unpack OpenSearch",
    "content": ". | Download the appropriate tar.gz archive from the OpenSearch downloads page or by using the command line (such as with wget). # x64 wget https://artifacts.opensearch.org/releases/bundle/opensearch/2.7.0/opensearch-2.7.0-linux-x64.tar.gz # ARM64 wget https://artifacts.opensearch.org/releases/bundle/opensearch/2.7.0/opensearch-2.7.0-linux-arm64.tar.gz . | Extract the contents of the tarball. # x64 tar -xvf opensearch-2.7.0-linux-x64.tar.gz # ARM64 tar -xvf opensearch-2.7.0-linux-arm64.tar.gz . | . ",
    "url": "https://vagimeli.github.io/install-and-configure/install-opensearch/tar/#step-1-download-and-unpack-opensearch",
    "relUrl": "/install-and-configure/install-opensearch/tar/#step-1-download-and-unpack-opensearch"
  },"97": {
    "doc": "Tarball",
    "title": "Step 2: Configure important system settings",
    "content": "Before launching OpenSearch you should review some important system settings. | Disable memory paging and swapping performance on the host to improve performance. sudo swapoff -a . copy . | Increase the number of memory maps available to OpenSearch. # Edit the sysctl config file sudo vi /etc/sysctl.conf # Add a line to define the desired value # or change the value if the key exists, # and then save your changes. vm.max_map_count=262144 # Reload the kernel parameters using sysctl sudo sysctl -p # Verify that the change was applied by checking the value cat /proc/sys/vm/max_map_count . | . ",
    "url": "https://vagimeli.github.io/install-and-configure/install-opensearch/tar/#step-2-configure-important-system-settings",
    "relUrl": "/install-and-configure/install-opensearch/tar/#step-2-configure-important-system-settings"
  },"98": {
    "doc": "Tarball",
    "title": "Step 3: (Optional) Test OpenSearch",
    "content": "Before proceeding you should test your installation of OpenSearch. Otherwise, it can be difficult to determine whether future problems are due to installation issues or custom settings you applied after installation. There are two quick methods for testing OpenSearch at this stage: . | (Security enabled) Apply a generic configuration using the demo security script included in the tar archive. | (Security disabled) Manually disable the Security plugin and test the instance before applying your own custom security settings. | . The demo security script will apply a generic configuration to your instance of OpenSearch. This configuration defines some environment variables and also applies self-signed TLS certificates. If you would like to configure these yourself, see Step 4: Set up OpenSearch in your environment. If you only want to verify that the service is properly configured and you intend to configure security settings yourself, then you may want to disable the Security plugin and launch the service without encryption or authentication. An OpenSearch node configured by the demo security script is not suitable for a production environment. If you plan to use the node in a production environment after running opensearch-tar-install.sh, you should, at a minimum, replace the demo TLS certificates with your own TLS certificates and update the list of internal users and passwords. See Security configuration for additional guidance to ensure that your nodes are configured according to your security requirements. Option 1: Test your Opensearch settings with security enabled . | Change to the top directory of your OpenSearch installation. cd /path/to/opensearch-2.7.0 . copy . | Run the demo security script./opensearch-tar-install.sh . copy . | Open another terminal session and send requests to the server to verify that OpenSearch is running. Note the use of the --insecure flag, which is required because the TLS certificates are self-signed. | Send a request to port 9200: curl -X GET https://localhost:9200 -u 'admin:admin' --insecure . copy . You should get a response that looks like this: . { \"name\" : \"hostname\", \"cluster_name\" : \"opensearch\", \"cluster_uuid\" : \"6XNc9m2gTUSIoKDqJit0PA\", \"version\" : { \"distribution\" : \"opensearch\", \"number\" : &lt;version&gt;, \"build_type\" : &lt;build-type&gt;, \"build_hash\" : &lt;build-hash&gt;, \"build_date\" : &lt;build-date&gt;, \"build_snapshot\" : false, \"lucene_version\" : &lt;lucene-version&gt;, \"minimum_wire_compatibility_version\" : \"7.10.0\", \"minimum_index_compatibility_version\" : \"7.0.0\" }, \"tagline\" : \"The OpenSearch Project: https://opensearch.org/\" } . | Query the plugins endpoint: curl -X GET https://localhost:9200/_cat/plugins?v -u 'admin:admin' --insecure . copy . The response should look like this: . name component version hostname opensearch-alerting 2.7.0 hostname opensearch-anomaly-detection 2.7.0 hostname opensearch-asynchronous-search 2.7.0 hostname opensearch-cross-cluster-replication 2.7.0 hostname opensearch-index-management 2.7.0 hostname opensearch-job-scheduler 2.7.0 hostname opensearch-knn 2.7.0 hostname opensearch-ml 2.7.0 hostname opensearch-notifications 2.7.0 hostname opensearch-notifications-core 2.7.0 hostname opensearch-observability 2.7.0 hostname opensearch-performance-analyzer 2.7.0 hostname opensearch-reports-scheduler 2.7.0 hostname opensearch-security 2.7.0 hostname opensearch-sql 2.7.0 . | . | Return to the original terminal session and stop the process by pressing CTRL + C. | . Option 2: Test your OpenSearch settings with security disabled . | Open the configuration file. vi /path/to/opensearch-2.7.0/config/opensearch.yml . copy . | Add the following line to disable the Security plugin: plugins.security.disabled: true . copy . | Save the change and close the file. | Open another terminal session and send requests to the server to verify that OpenSearch is running. Because the Security plugin has been disabled, you will be sending commands using HTTP rather than HTTPS. | Send a request to port 9200. curl -X GET http://localhost:9200 . copy . You should get a response that looks like this: . { \"name\" : \"hostname\", \"cluster_name\" : \"opensearch\", \"cluster_uuid\" : \"6XNc9m2gTUSIoKDqJit0PA\", \"version\" : { \"distribution\" : \"opensearch\", \"number\" : &lt;version&gt;, \"build_type\" : &lt;build-type&gt;, \"build_hash\" : &lt;build-hash&gt;, \"build_date\" : &lt;build-date&gt;, \"build_snapshot\" : false, \"lucene_version\" : &lt;lucene-version&gt;, \"minimum_wire_compatibility_version\" : \"7.10.0\", \"minimum_index_compatibility_version\" : \"7.0.0\" }, \"tagline\" : \"The OpenSearch Project: https://opensearch.org/\" } . | Query the plugins endpoint. curl -X GET http://localhost:9200/_cat/plugins?v . copy . The response should look like this: . name component version hostname opensearch-alerting 2.7.0 hostname opensearch-anomaly-detection 2.7.0 hostname opensearch-asynchronous-search 2.7.0 hostname opensearch-cross-cluster-replication 2.7.0 hostname opensearch-index-management 2.7.0 hostname opensearch-job-scheduler 2.7.0 hostname opensearch-knn 2.7.0 hostname opensearch-ml 2.7.0 hostname opensearch-notifications 2.7.0 hostname opensearch-notifications-core 2.7.0 hostname opensearch-observability 2.7.0 hostname opensearch-performance-analyzer 2.7.0 hostname opensearch-reports-scheduler 2.7.0 hostname opensearch-security 2.7.0 hostname opensearch-sql 2.7.0 . | . | . ",
    "url": "https://vagimeli.github.io/install-and-configure/install-opensearch/tar/#step-3-optional-test-opensearch",
    "relUrl": "/install-and-configure/install-opensearch/tar/#step-3-optional-test-opensearch"
  },"99": {
    "doc": "Tarball",
    "title": "Step 4: Set up OpenSearch in your environment",
    "content": "Users who do not have prior experience with OpenSearch may want a list of recommended settings in order to get started with the service. By default, OpenSearch is not bound to a network interface and cannot be reached by external hosts. Additionally, security settings are either undefined (greenfield install) or populated by default usernames and passwords if you ran the security demo script by invoking opensearch-tar-install.sh. The following recommendations will enable a user to bind OpenSearch to a network interface, create and sign TLS certificates, and configure basic authentication. The following recommended settings will allow you to: . | Bind OpenSearch to an IP or network interface on the host. | Set initial and maximum JVM heap sizes. | Define an environment variable that points to the bundled JDK. | Configure your own TLS certificates - no third-party certificate authority (CA) is required. | Create an admin user with a custom password. | . If you ran the security demo script, then you will need to manually reconfigure settings that were modified. Refer to Security configuration for guidance before proceeding. Before modifying any configuration files, it’s always a good idea to save a backup copy before making changes. The backup file can be used to revert any issues caused by a bad configuration. | Open opensearch.yml. vi /path/to/opensearch-2.7.0/config/opensearch.yml . copy . | Add the following lines. # Bind OpenSearch to the correct network interface. Use 0.0.0.0 # to include all available interfaces or specify an IP address # assigned to a specific interface. network.host: 0.0.0.0 # Unless you have already configured a cluster, you should set # discovery.type to single-node, or the bootstrap checks will # fail when you try to start the service. discovery.type: single-node # If you previously disabled the Security plugin in opensearch.yml, # be sure to re-enable it. Otherwise you can skip this setting. plugins.security.disabled: false . copy . | Save your changes and close the file. | Specify initial and maximum JVM heap sizes. | Open jvm.options. vi /path/to/opensearch-2.7.0/config/jvm.options . copy . | Modify the values for initial and maximum heap sizes. As a starting point, you should set these values to half of the available system memory. For dedicated hosts this value can be increased based on your workflow requirements. | As an example, if the host machine has 8 GB of memory then you might want to set the initial and maximum heap sizes to 4 GB: -Xms4g -Xmx4g . copy . | . | Save your changes and close the file. | . | Specify the location of the included JDK. export OPENSEARCH_JAVA_HOME=/path/to/opensearch-2.7.0/jdk . copy . | . Configure TLS . TLS certificates provide additional security for your cluster by allowing clients to confirm the identity of hosts and encrypt traffic between the client and host. For more information, refer to Configure TLS Certificates and Generate Certificates, which are included in the Security plugin documentation. For work performed in a development environment, self-signed certificates are usually adequate. This section will guide you through the basic steps required to generate your own TLS certificates and apply them to your OpenSearch host. | Navigate to the OpenSearch config directory. This is where the certificates will be stored. cd /path/to/opensearch-2.7.0/config/ . copy . | Generate a root certificate. This is what you will use to sign your other certificates. # Create a private key for the root certificate openssl genrsa -out root-ca-key.pem 2048 # Use the private key to create a self-signed root certificate. Be sure to # replace the arguments passed to -subj so they reflect your specific host. openssl req -new -x509 -sha256 -key root-ca-key.pem -subj \"/C=CA/ST=ONTARIO/L=TORONTO/O=ORG/OU=UNIT/CN=ROOT\" -out root-ca.pem -days 730 . | Next, create the admin certificate. This certificate is used to gain elevated rights for performing administrative tasks relating to the Security plugin. # Create a private key for the admin certificate. openssl genrsa -out admin-key-temp.pem 2048 # Convert the private key to PKCS#8. openssl pkcs8 -inform PEM -outform PEM -in admin-key-temp.pem -topk8 -nocrypt -v1 PBE-SHA1-3DES -out admin-key.pem # Create the CSR. A common name (CN) of \"A\" is acceptable because this certificate is # used for authenticating elevated access and is not tied to a host. openssl req -new -key admin-key.pem -subj \"/C=CA/ST=ONTARIO/L=TORONTO/O=ORG/OU=UNIT/CN=A\" -out admin.csr # Sign the admin certificate with the root certificate and private key you created earlier. openssl x509 -req -in admin.csr -CA root-ca.pem -CAkey root-ca-key.pem -CAcreateserial -sha256 -out admin.pem -days 730 . | Create a certificate for the node being configured. # Create a private key for the node certificate. openssl genrsa -out node1-key-temp.pem 2048 # Convert the private key to PKCS#8. openssl pkcs8 -inform PEM -outform PEM -in node1-key-temp.pem -topk8 -nocrypt -v1 PBE-SHA1-3DES -out node1-key.pem # Create the CSR and replace the arguments passed to -subj so they reflect your specific host. # The CN should match a DNS A record for the host--do not use the hostname. openssl req -new -key node1-key.pem -subj \"/C=CA/ST=ONTARIO/L=TORONTO/O=ORG/OU=UNIT/CN=node1.dns.a-record\" -out node1.csr # Create an extension file that defines a SAN DNS name for the host. This # should match the DNS A record of the host. echo 'subjectAltName=DNS:node1.dns.a-record' &gt; node1.ext # Sign the node certificate with the root certificate and private key that you created earlier. openssl x509 -req -in node1.csr -CA root-ca.pem -CAkey root-ca-key.pem -CAcreateserial -sha256 -out node1.pem -days 730 -extfile node1.ext . | Remove temporary files that are no longer required. rm *temp.pem *csr *ext . copy . | Add these certificates to opensearch.yml as described in Generate Certificates. Advanced users might also choose to append the settings using a script: #! /bin/bash # Before running this script, make sure to replace the /path/to your OpenSearch directory, # and remember to replace the CN in the node's distinguished name with a real # DNS A record. echo \"plugins.security.ssl.transport.pemcert_filepath: /path/to/opensearch-2.7.0/config/node1.pem\" | sudo tee -a /path/to/opensearch-2.7.0/config/opensearch.yml echo \"plugins.security.ssl.transport.pemkey_filepath: /path/to/opensearch-2.7.0/config/node1-key.pem\" | sudo tee -a /path/to/opensearch-2.7.0/config/opensearch.yml echo \"plugins.security.ssl.transport.pemtrustedcas_filepath: /path/to/opensearch-2.7.0/config/root-ca.pem\" | sudo tee -a /path/to/opensearch-2.7.0/config/opensearch.yml echo \"plugins.security.ssl.http.enabled: true\" | sudo tee -a /path/to/opensearch-2.7.0/config/opensearch.yml echo \"plugins.security.ssl.http.pemcert_filepath: /path/to/opensearch-2.7.0/config/node1.pem\" | sudo tee -a /path/to/opensearch-2.7.0/config/opensearch.yml echo \"plugins.security.ssl.http.pemkey_filepath: /path/to/opensearch-2.7.0/config/node1-key.pem\" | sudo tee -a /path/to/opensearch-2.7.0/config/opensearch.yml echo \"plugins.security.ssl.http.pemtrustedcas_filepath: /path/to/opensearch-2.7.0/config/root-ca.pem\" | sudo tee -a /path/to/opensearch-2.7.0/config/opensearch.yml echo \"plugins.security.allow_default_init_securityindex: true\" | sudo tee -a /path/to/opensearch-2.7.0/config/opensearch.yml echo \"plugins.security.authcz.admin_dn:\" | sudo tee -a /path/to/opensearch-2.7.0/config/opensearch.yml echo \" - 'CN=A,OU=UNIT,O=ORG,L=TORONTO,ST=ONTARIO,C=CA'\" | sudo tee -a /path/to/opensearch-2.7.0/config/opensearch.yml echo \"plugins.security.nodes_dn:\" | sudo tee -a /path/to/opensearch-2.7.0/config/opensearch.yml echo \" - 'CN=node1.dns.a-record,OU=UNIT,O=ORG,L=TORONTO,ST=ONTARIO,C=CA'\" | sudo tee -a /path/to/opensearch-2.7.0/config/opensearch.yml echo \"plugins.security.audit.type: internal_opensearch\" | sudo tee -a /path/to/opensearch-2.7.0/config/opensearch.yml echo \"plugins.security.enable_snapshot_restore_privilege: true\" | sudo tee -a /path/to/opensearch-2.7.0/config/opensearch.yml echo \"plugins.security.check_snapshot_restore_write_privileges: true\" | sudo tee -a /path/to/opensearch-2.7.0/config/opensearch.yml echo \"plugins.security.restapi.roles_enabled: [\\\"all_access\\\", \\\"security_rest_api_access\\\"]\" | sudo tee -a /path/to/opensearch-2.7.0/config/opensearch.yml . copy . | (Optional) Add trust for the self-signed root certificate. # Copy the root certificate to the correct directory sudo cp /path/to/opensearch-2.7.0/config/root-ca.pem /etc/pki/ca-trust/source/anchors/ # Add trust sudo update-ca-trust . | . Configure a user . Users are defined and authenticated by OpenSearch in a variety of ways. One method, which does not require additional backend infrastructure, is to manually configure users in internal_users.yml. See YAML files for more information about configuring users. The following steps explain how to remove all demo users except for the admin user and how to replace the admin default password using a script. | Make the Security plugin scripts executable. chmod 755 /path/to/opensearch-2.7.0/plugins/opensearch-security/tools/*.sh . copy . | Run hash.sh to generate a new password. | This script will fail if a path to the JDK has not been defined. # Example output if a JDK isn't found... $ ./hash.sh ************************************************************************** ** This tool will be deprecated in the next major release of OpenSearch ** ** https://github.com/opensearch-project/security/issues/1755 ** ************************************************************************** which: no java in (/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/user/.local/bin:/home/user/bin) WARNING: nor OPENSEARCH_JAVA_HOME nor JAVA_HOME is set, will use ./hash.sh: line 35: java: command not found . copy . | Declare an environment variable when you invoke the script in order to avoid issues: OPENSEARCH_JAVA_HOME=/path/to/opensearch-2.7.0/jdk ./hash.sh . copy . | Enter the desired password at the prompt and make a note of the output hash. | . | Open internal_users.yml. vi /path/to/opensearch-2.7.0/config/opensearch-security/internal_users.yml . copy . | Remove all demo users except for admin and replace the hash with the output provided by hash.sh in a previous step. The file should look similar to the following example: --- # This is the internal user database # The hash value is a bcrypt hash and can be generated with plugin/tools/hash.sh _meta: type: \"internalusers\" config_version: 2 # Define your internal users here admin: hash: \"$2y$1EXAMPLEQqwS8TUcoEXAMPLEeZ3lEHvkEXAMPLERqjyh1icEXAMPLE.\" reserved: true backend_roles: - \"admin\" description: \"Admin user\" . copy . | . Apply changes . Now that TLS certificates are installed and demo users were removed or assigned new passwords, the last step is to apply the configuration changes. This last configuration step requires invoking securityadmin.sh while OpenSearch is running on the host. | Start OpenSearch. It must be running for securityadmin.sh to apply changes. # Change directories cd /path/to/opensearch-2.7.0/bin # Run the service in the foreground ./opensearch . | Open a separate terminal session with the host and navigate to the directory containing securityadmin.sh. # Change to the correct directory cd /path/to/opensearch-2.7.0/plugins/opensearch-security/tools . | Invoke the script. See Apply changes using securityadmin.sh for definitions of the arguments you must pass. # You can omit the environment variable if you declared this in your $PATH. OPENSEARCH_JAVA_HOME=/path/to/opensearch-2.7.0/jdk ./securityadmin.sh -cd /path/to/opensearch-2.7.0/config/opensearch-security/ -cacert /path/to/opensearch-2.7.0/config/root-ca.pem -cert /path/to/opensearch-2.7.0/config/admin.pem -key /path/to/opensearch-2.7.0/config/admin-key.pem -icl -nhnv . | Stop and restart the running OpenSearch process to apply the changes. | . Verify that the service is running . OpenSearch is now running on your host with custom TLS certificates and a secure user for basic authentication. You can verify external connectivity by sending an API request to your OpenSearch node from another host. During previous tests you directed requests to localhost. Now that TLS certificates have been applied and the new certificates reference your host’s actual DNS record, requests to localhost will fail the CN check and the certificate will be considered invalid. Instead, requests should be sent to the address you specified while generating the certificate. You should add trust for the root certificate to your client before sending requests. If you do not add trust, then you must use the -k option so that cURL ignores CN and root certificate validation. $ curl https://your.host.address:9200 -u admin:yournewpassword -k { \"name\" : \"hostname-here\", \"cluster_name\" : \"opensearch\", \"cluster_uuid\" : \"efC0ANNMQlGQ5TbhNflVPg\", \"version\" : { \"distribution\" : \"opensearch\", \"number\" : \"2.1.0\", \"build_type\" : \"tar\", \"build_hash\" : \"388c80ad94529b1d9aad0a735c4740dce2932a32\", \"build_date\" : \"2022-06-30T21:31:04.823801692Z\", \"build_snapshot\" : false, \"lucene_version\" : \"9.2.0\", \"minimum_wire_compatibility_version\" : \"7.10.0\", \"minimum_index_compatibility_version\" : \"7.0.0\" }, \"tagline\" : \"The OpenSearch Project: https://opensearch.org/\" } . Run OpenSearch as a service with systemd . This section will guide you through creating a service for OpenSearch and registering it with systemd. After the service has been defined, you can enable, start, and stop the OpenSearch service using systemctl commands. The commands in this section reflect an environment where OpenSearch has been installed to /opt/opensearch and should be changed depending on your installation path. The following configuration is only suitable for testing in a non-production environment. We do not recommend using the following configuration in a production environment. You should install OpenSearch with the RPM distribution if you want to run OpenSearch as a systemd-managed service on your host. The tarball installation does not define a specific installation path, users, roles, or permissions. Failure to properly secure your host environment can result in unexpected behavior. | Create a user for the OpenSearch service. sudo adduser --system --shell /bin/bash -U --no-create-home opensearch . copy . | Add your user to the opensearch user group. sudo usermod -aG opensearch $USER . copy . | Change the file owner to opensearch. Make sure to change the path if your OpenSearch files are in a different directory. sudo chown -R opensearch /opt/opensearch/ . copy . | Create the service file and open it for editing. sudo vi /etc/systemd/system/opensearch.service . copy . | Enter the following example service configuration. Make sure to change references to the path if your OpenSearch files are in a different directory. [Unit] Description=OpenSearch Wants=network-online.target After=network-online.target [Service] Type=forking RuntimeDirectory=data WorkingDirectory=/opt/opensearch ExecStart=/opt/opensearch/bin/opensearch -d User=opensearch Group=opensearch StandardOutput=journal StandardError=inherit LimitNOFILE=65535 LimitNPROC=4096 LimitAS=infinity LimitFSIZE=infinity TimeoutStopSec=0 KillSignal=SIGTERM KillMode=process SendSIGKILL=no SuccessExitStatus=143 TimeoutStartSec=75 [Install] WantedBy=multi-user.target . copy . | Reload systemd manager configuration. sudo systemctl daemon-reload . copy . | Enable the OpenSearch service. sudo systemctl enable opensearch.service . copy . | Start the OpenSearch service. sudo systemctl start opensearch . copy . | Verify that the service is running. sudo systemctl status opensearch . copy . | . ",
    "url": "https://vagimeli.github.io/install-and-configure/install-opensearch/tar/#step-4-set-up-opensearch-in-your-environment",
    "relUrl": "/install-and-configure/install-opensearch/tar/#step-4-set-up-opensearch-in-your-environment"
  },"100": {
    "doc": "Tarball",
    "title": "Related links",
    "content": ". | OpenSearch configuration | Configure Performance Analyzer for Tarball Installation | Install and configure OpenSearch Dashboards | OpenSearch plugin installation | About the Security plugin | . ",
    "url": "https://vagimeli.github.io/install-and-configure/install-opensearch/tar/#related-links",
    "relUrl": "/install-and-configure/install-opensearch/tar/#related-links"
  },"101": {
    "doc": "Windows",
    "title": "Windows",
    "content": "The following sections describe installing OpenSearch on Windows from a zip archive. Generally speaking, the installation of OpenSearch from a zip archive can be broken down into a few steps: . | Download and unpack OpenSearch. | (Optional) Test OpenSearch. | Confirm that OpenSearch is able to run before you apply any custom configuration. | This can be done without any security (no password, no certificates) or with a demo security configuration that can be applied by a packaged script. | . | Configure OpenSearch for your environment. | Apply basic settings to OpenSearch and start using it in your environment. | . | . The Windows OpenSearch archive is a self-contained directory with everything needed to run OpenSearch, including an integrated Java Development Kit (JDK). If you have your own Java installation and set the environment variable JAVA_HOME, OpenSearch will use that installation if the OPENSEARCH_JAVA_HOME environment variable is not set. To learn how to set the OPENSEARCH_JAVA_HOME environment variable, see Step 3: Set up OpenSearch in your environment. ",
    "url": "https://vagimeli.github.io/install-and-configure/install-opensearch/windows/",
    "relUrl": "/install-and-configure/install-opensearch/windows/"
  },"102": {
    "doc": "Windows",
    "title": "Prerequisites",
    "content": "Make sure you have a zip utility installed. ",
    "url": "https://vagimeli.github.io/install-and-configure/install-opensearch/windows/#prerequisites",
    "relUrl": "/install-and-configure/install-opensearch/windows/#prerequisites"
  },"103": {
    "doc": "Windows",
    "title": "Step 1: Download and unpack OpenSearch",
    "content": "Perform the following steps to install OpenSearch on Windows. | Download the opensearch-2.7.0-windows-x64.zip archive. | To extract the archive contents, right-click to select Extract All. | . ",
    "url": "https://vagimeli.github.io/install-and-configure/install-opensearch/windows/#step-1-download-and-unpack-opensearch",
    "relUrl": "/install-and-configure/install-opensearch/windows/#step-1-download-and-unpack-opensearch"
  },"104": {
    "doc": "Windows",
    "title": "Step 2: (Optional) Test OpenSearch",
    "content": "Before proceeding with any configuration, you should test your installation of OpenSearch. Otherwise, it can be difficult to determine whether future problems are due to installation issues or custom settings you applied after installation. There are two quick methods for testing OpenSearch at this stage: . | (Security enabled) Apply a generic configuration using the batch script included in the Windows archive. | (Security disabled) Manually disable the Security plugin and test the instance before applying your own custom security settings. | . The batch script will apply a generic configuration to your instance of OpenSearch. This configuration defines some environment variables and also applies self-signed TLS certificates. Alternatively, you can choose to configure these yourself. If you only want to verify that the service is properly configured and you intend to configure security settings yourself, then you may want to disable the Security plugin and launch the service without encryption or authentication. An OpenSearch node in its default configuration (with demo certificates and users with default passwords) is not suitable for a production environment. If you plan to use the node in a production environment, you should, at a minimum, replace the demo TLS certificates with your own TLS certificates and update the list of internal users and passwords. See Security configuration for additional guidance to ensure that your nodes are configured according to your security requirements. Option 1: Test your OpenSearch settings with security enabled . | Run the demo batch script. There are two ways of running the batch script: . | Run the batch script using the Windows UI: . | Navigate to the top directory of your OpenSearch installation and open the opensearch-2.7.0 folder. | Run the batch script by double-clicking the opensearch-windows-install.bat file. This opens a command prompt with an OpenSearch instance running. | . | Run the batch script from Command prompt or Powershell: . | Open Command Prompt by entering cmd, or Powershell by entering powershell, in the search box next to Start on the taskbar. | Change to the top directory of your OpenSearch installation. cd \\path\\to\\opensearch-2.7.0 . copy . | Run the batch script.\\opensearch-windows-install.bat . copy . | . | . | Open a new command prompt and send requests to the server to verify that OpenSearch is running. Note the use of the --insecure flag, which is required because the TLS certificates are self-signed. | Send a request to port 9200: curl.exe -X GET https://localhost:9200 -u \"admin:admin\" --insecure . copy . You should get a response that looks like this: . { \"name\" : \"hostname-here\", \"cluster_name\" : \"opensearch\", \"cluster_uuid\" : \"7Nqtr0LrQTOveFcBb7Kufw\", \"version\" : { \"distribution\" : \"opensearch\", \"number\" : &lt;version&gt;, \"build_type\" : &lt;build-type&gt;, \"build_hash\" : &lt;build-hash&gt;, \"build_date\" : &lt;build-date&gt;, \"build_snapshot\" : false, \"lucene_version\" : &lt;lucene-version&gt;, \"minimum_wire_compatibility_version\" : \"7.10.0\", \"minimum_index_compatibility_version\" : \"7.0.0\" }, \"tagline\" : \"The OpenSearch Project: https://opensearch.org/\" } . | Query the plugins endpoint: curl.exe -X GET https://localhost:9200/_cat/plugins?v -u \"admin:admin\" --insecure . copy . The response should look like this: . hostname opensearch-alerting 2.7.0 hostname opensearch-anomaly-detection 2.7.0 hostname opensearch-asynchronous-search 2.7.0 hostname opensearch-cross-cluster-replication 2.7.0 hostname opensearch-geospatial 2.7.0 hostname opensearch-index-management 2.7.0 hostname opensearch-job-scheduler 2.7.0 hostname opensearch-knn 2.7.0 hostname opensearch-ml 2.7.0 hostname opensearch-neural-search 2.7.0 hostname opensearch-notifications 2.7.0 hostname opensearch-notifications-core 2.7.0 hostname opensearch-observability 2.7.0 hostname opensearch-reports-scheduler 2.7.0 hostname opensearch-security 2.7.0 hostname opensearch-security-analytics 2.7.0 hostname opensearch-sql 2.7.0 . | . | . Option 2: Test your OpenSearch settings with security disabled . | Open the opensearch-2.7.0\\config folder. | Open the opensearch.yml file with a text editor. | Add the following line to disable the Security plugin: plugins.security.disabled: true . copy . | Save the change and close the file. | Navigate to the top directory of your OpenSearch installation and open the opensearch-2.7.0 folder. | Run the default by double-clicking the opensearch-windows-install.bat file. This opens a command prompt with an OpenSearch instance running. | Open a new command prompt and send requests to the server to verify that OpenSearch is running. Because the Security plugin has been disabled, you will be sending commands using HTTP rather than HTTPS. | Send a request to port 9200: curl.exe -X GET http://localhost:9200 . copy . You should get a response that looks like this: . { \"name\" : \"hostname-here\", \"cluster_name\" : \"opensearch\", \"cluster_uuid\" : \"7Nqtr0LrQTOveFcBb7Kufw\", \"version\" : { \"distribution\" : \"opensearch\", \"number\" : \"2.4.0\", \"build_type\" : \"zip\", \"build_hash\" : \"77ef9e304dd6ee95a600720a387a9735bbcf7bc9\", \"build_date\" : \"2022-11-05T05:50:15.404072800Z\", \"build_snapshot\" : false, \"lucene_version\" : \"9.4.1\", \"minimum_wire_compatibility_version\" : \"7.10.0\", \"minimum_index_compatibility_version\" : \"7.0.0\" }, \"tagline\" : \"The OpenSearch Project: https://opensearch.org/\" } . | Query the plugins endpoint: curl.exe -X GET http://localhost:9200/_cat/plugins?v . copy . The response should look like this: . hostname opensearch-alerting 2.7.0 hostname opensearch-anomaly-detection 2.7.0 hostname opensearch-asynchronous-search 2.7.0 hostname opensearch-cross-cluster-replication 2.7.0 hostname opensearch-geospatial 2.7.0 hostname opensearch-index-management 2.7.0 hostname opensearch-job-scheduler 2.7.0 hostname opensearch-knn 2.7.0 hostname opensearch-ml 2.7.0 hostname opensearch-neural-search 2.7.0 hostname opensearch-notifications 2.7.0 hostname opensearch-notifications-core 2.7.0 hostname opensearch-observability 2.7.0 hostname opensearch-reports-scheduler 2.7.0 hostname opensearch-security 2.7.0 hostname opensearch-security-analytics 2.7.0 hostname opensearch-sql 2.7.0 . | . | . To stop OpenSearch, press Ctrl+C in Command Prompt or Powershell, or simply close the Command Prompt or Powershell window. ",
    "url": "https://vagimeli.github.io/install-and-configure/install-opensearch/windows/#step-2-optional-test-opensearch",
    "relUrl": "/install-and-configure/install-opensearch/windows/#step-2-optional-test-opensearch"
  },"105": {
    "doc": "Windows",
    "title": "Step 3: Set up OpenSearch in your environment",
    "content": "Users who do not have prior experience with OpenSearch may want a list of recommended settings in order to get started with the service. By default, OpenSearch is not bound to a network interface and cannot be reached by external hosts. Additionally, security settings are either undefined (greenfield install) or populated by default usernames and passwords if you ran the security demo script by invoking opensearch-windows-install.bat. The following recommendations will enable a user to bind OpenSearch to a network interface. The following recommended settings will allow you to: . | Bind OpenSearch to an IP or network interface on the host. | Set initial and maximum JVM heap sizes. | Define an environment variable that points to the bundled JDK. | . If you ran the security demo script, then you will need to manually reconfigure settings that were modified. Refer to Security configuration for guidance before proceeding. Before modifying any configuration files, it’s always a good idea to save a backup copy before making changes. The backup file can be used to revert any issues caused by a bad configuration. | Open the opensearch-2.7.0\\config folder. | Open the opensearch.yml file with a text editor. | Add the following lines: # Bind OpenSearch to the correct network interface. Use 0.0.0.0 # to include all available interfaces or specify an IP address # assigned to a specific interface. network.host: 0.0.0.0 # Unless you have already configured a cluster, you should set # discovery.type to single-node, or the bootstrap checks will # fail when you try to start the service. discovery.type: single-node # If you previously disabled the Security plugin in opensearch.yml, # be sure to re-enable it. Otherwise you can skip this setting. plugins.security.disabled: false . copy . | Save your changes and close the file. | Specify initial and maximum JVM heap sizes. | Open the opensearch-2.7.0\\config folder. | Open the jvm.options file with a text editor. | Modify the values for initial and maximum heap sizes. As a starting point, you should set these values to half of the available system memory. For dedicated hosts this value can be increased based on your workflow requirements. As an example, if the host machine has 8 GB of memory, then you might want to set the initial and maximum heap sizes to 4 GB: -Xms4g -Xmx4g . copy . | Save your changes and close the file. | . | Specify the location of the included JDK. | In the search box next to Start on the taskbar, enter edit environment variables for your account or edit the system environment variables. To edit the system environment variables, you need admin rights. User environment variables take precedence over system environment variables. | Select Edit environment variables for your account or Edit the system environment variables. | If the System Properties dialog opens, in the Advanced tab, select Environment Variables. | Under User variables or System variables, select New. | In Variable name, enter OPENSEARCH_JAVA_HOME. | In Variable value, enter \\path\\to\\opensearch-2.7.0\\jdk. | Select OK to close all dialogs. | . | . ",
    "url": "https://vagimeli.github.io/install-and-configure/install-opensearch/windows/#step-3-set-up-opensearch-in-your-environment",
    "relUrl": "/install-and-configure/install-opensearch/windows/#step-3-set-up-opensearch-in-your-environment"
  },"106": {
    "doc": "Windows",
    "title": "Plugin compatibility",
    "content": "The Performance Analyzer plugin is not available on Windows. All other OpenSearch plugins, including the k-NN plugin, are available. For a complete list of plugins, see Available plugins. ",
    "url": "https://vagimeli.github.io/install-and-configure/install-opensearch/windows/#plugin-compatibility",
    "relUrl": "/install-and-configure/install-opensearch/windows/#plugin-compatibility"
  },"107": {
    "doc": "Windows",
    "title": "Related links",
    "content": ". | OpenSearch configuration | OpenSearch plugin installation | About the Security plugin | . ",
    "url": "https://vagimeli.github.io/install-and-configure/install-opensearch/windows/#related-links",
    "relUrl": "/install-and-configure/install-opensearch/windows/#related-links"
  },"108": {
    "doc": "Installing plugins",
    "title": "Installing plugins",
    "content": "You can install individual plugins for OpenSearch based on your needs. For information about available plugins, see Available plugins. ",
    "url": "https://vagimeli.github.io/install-and-configure/plugins/",
    "relUrl": "/install-and-configure/plugins/"
  },"109": {
    "doc": "Installing plugins",
    "title": "Managing plugins",
    "content": "OpenSearch uses a command line tool called opensearch-plugin for managing plugins. This tool allows you to: . | List installed plugins. | Install plugins. | Remove an installed plugin. | . Print help text by passing -h or --help. Depending on your host configuration, you might also need to run the command with sudo privileges. If you are running OpenSearch in a Docker container, plugins must be installed, removed, and configured by modifying the Docker image. For information, see Working with plugins . ",
    "url": "https://vagimeli.github.io/install-and-configure/plugins/#managing-plugins",
    "relUrl": "/install-and-configure/plugins/#managing-plugins"
  },"110": {
    "doc": "Installing plugins",
    "title": "List",
    "content": "Use list to see a list of plugins that have already been installed. Usage: . bin/opensearch-plugin list . Example: . $ ./opensearch-plugin list opensearch-alerting opensearch-anomaly-detection opensearch-asynchronous-search opensearch-cross-cluster-replication opensearch-geospatial opensearch-index-management opensearch-job-scheduler opensearch-knn opensearch-ml opensearch-notifications opensearch-notifications-core opensearch-observability opensearch-performance-analyzer opensearch-reports-scheduler opensearch-security opensearch-sql . You can also list installed plugins by using the CAT API. Path and HTTP method . GET _cat/plugins . Sample response . opensearch-node1 opensearch-alerting 2.0.1.0 opensearch-node1 opensearch-anomaly-detection 2.0.1.0 opensearch-node1 opensearch-asynchronous-search 2.0.1.0 opensearch-node1 opensearch-cross-cluster-replication 2.0.1.0 opensearch-node1 opensearch-index-management 2.0.1.0 opensearch-node1 opensearch-job-scheduler 2.0.1.0 opensearch-node1 opensearch-knn 2.0.1.0 opensearch-node1 opensearch-ml 2.0.1.0 opensearch-node1 opensearch-notifications 2.0.1.0 opensearch-node1 opensearch-notifications-core 2.0.1.0 . ",
    "url": "https://vagimeli.github.io/install-and-configure/plugins/#list",
    "relUrl": "/install-and-configure/plugins/#list"
  },"111": {
    "doc": "Installing plugins",
    "title": "Install",
    "content": "There are three ways to install plugins using the opensearch-plugin: . | Install a plugin by name | Install a plugin by from a zip file | Install a plugin using Maven coordinates | . Install a plugin by name: . For a list of plugins that can be installed by name, see Additional plugins. Usage: . bin/opensearch-plugin install &lt;plugin-name&gt; . Example: . $ sudo ./opensearch-plugin install analysis-icu -&gt; Installing analysis-icu -&gt; Downloading analysis-icu from opensearch [=================================================] 100% -&gt; Installed analysis-icu with folder name analysis-icu . Install a plugin from a zip file: . Remote zip files can be installed by replacing &lt;zip-file&gt; with the URL of the hosted file. The tool only supports downloading over HTTP/HTTPS protocols. For local zip files, replace &lt;zip-file&gt; with file: followed by the absolute or relative path to the plugin zip file as in the second example below. Usage: . bin/opensearch-plugin install &lt;zip-file&gt; . Example: . # Zip file is hosted on a remote server - in this case, Maven central repository. $ sudo ./opensearch-plugin install https://repo1.maven.org/maven2/org/opensearch/plugin/opensearch-anomaly-detection/2.2.0.0/opensearch-anomaly-detection-2.2.0.0.zip -&gt; Installing https://repo1.maven.org/maven2/org/opensearch/plugin/opensearch-anomaly-detection/2.2.0.0/opensearch-anomaly-detection-2.2.0.0.zip -&gt; Downloading https://repo1.maven.org/maven2/org/opensearch/plugin/opensearch-anomaly-detection/2.2.0.0/opensearch-anomaly-detection-2.2.0.0.zip [=================================================] 100% @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ @ WARNING: plugin requires additional permissions @ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ * java.lang.RuntimePermission accessClassInPackage.sun.misc * java.lang.RuntimePermission accessDeclaredMembers * java.lang.RuntimePermission getClassLoader * java.lang.RuntimePermission setContextClassLoader * java.lang.reflect.ReflectPermission suppressAccessChecks * java.net.SocketPermission * connect,resolve * javax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] registerMBean * javax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] unregisterMBean * javax.management.MBeanServerPermission createMBeanServer * javax.management.MBeanTrustPermission register See http://docs.oracle.com/javase/8/docs/technotes/guides/security/permissions.html for descriptions of what these permissions allow and the associated risks. Continue with installation? [y/N]y -&gt; Installed opensearch-anomaly-detection with folder name opensearch-anomaly-detection # Zip file in a local directory. $ sudo ./opensearch-plugin install file:/home/user/opensearch-anomaly-detection-2.2.0.0.zip -&gt; Installing file:/home/user/opensearch-anomaly-detection-2.2.0.0.zip -&gt; Downloading file:/home/user/opensearch-anomaly-detection-2.2.0.0.zip [=================================================] 100% @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ @ WARNING: plugin requires additional permissions @ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ * java.lang.RuntimePermission accessClassInPackage.sun.misc * java.lang.RuntimePermission accessDeclaredMembers * java.lang.RuntimePermission getClassLoader * java.lang.RuntimePermission setContextClassLoader * java.lang.reflect.ReflectPermission suppressAccessChecks * java.net.SocketPermission * connect,resolve * javax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] registerMBean * javax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] unregisterMBean * javax.management.MBeanServerPermission createMBeanServer * javax.management.MBeanTrustPermission register See http://docs.oracle.com/javase/8/docs/technotes/guides/security/permissions.html for descriptions of what these permissions allow and the associated risks. Continue with installation? [y/N]y -&gt; Installed opensearch-anomaly-detection with folder name opensearch-anomaly-detection . Install a plugin using Maven coordinates: . The opensearch-plugin install tool also accepts Maven coordinates for available artifacts and versions hosted on Maven Central. opensearch-plugin will parse the Maven coordinates you provide and construct a URL. As a result, the host must be able to connect directly to Maven Central. The plugin installation will fail if you pass coordinates to a proxy or local repository. Usage: . bin/opensearch-plugin install &lt;groupId&gt;:&lt;artifactId&gt;:&lt;version&gt; . Example: . $ sudo ./opensearch-plugin install org.opensearch.plugin:opensearch-anomaly-detection:2.2.0.0 -&gt; Installing org.opensearch.plugin:opensearch-anomaly-detection:2.2.0.0 -&gt; Downloading org.opensearch.plugin:opensearch-anomaly-detection:2.2.0.0 from maven central [=================================================] 100% @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ @ WARNING: plugin requires additional permissions @ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ * java.lang.RuntimePermission accessClassInPackage.sun.misc * java.lang.RuntimePermission accessDeclaredMembers * java.lang.RuntimePermission getClassLoader * java.lang.RuntimePermission setContextClassLoader * java.lang.reflect.ReflectPermission suppressAccessChecks * java.net.SocketPermission * connect,resolve * javax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] registerMBean * javax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name=pool,type=GenericObjectPool] unregisterMBean * javax.management.MBeanServerPermission createMBeanServer * javax.management.MBeanTrustPermission register See http://docs.oracle.com/javase/8/docs/technotes/guides/security/permissions.html for descriptions of what these permissions allow and the associated risks. Continue with installation? [y/N]y -&gt; Installed opensearch-anomaly-detection with folder name opensearch-anomaly-detection . Restart your OpenSearch node after installing a plugin. ",
    "url": "https://vagimeli.github.io/install-and-configure/plugins/#install",
    "relUrl": "/install-and-configure/plugins/#install"
  },"112": {
    "doc": "Installing plugins",
    "title": "Remove",
    "content": "You can remove a plugin that has already been installed with the remove option. Usage: . bin/opensearch-plugin remove &lt;plugin-name&gt; . Example: . $ sudo $ ./opensearch-plugin remove opensearch-anomaly-detection -&gt; removing [opensearch-anomaly-detection]... Restart your OpenSearch node after removing a plugin. ",
    "url": "https://vagimeli.github.io/install-and-configure/plugins/#remove",
    "relUrl": "/install-and-configure/plugins/#remove"
  },"113": {
    "doc": "Installing plugins",
    "title": "Batch mode",
    "content": "When installing plugins that require additional privileges not included by default, the plugins will prompt the user for confirmation of the required privileges. To grant all requested privileges, use batch mode to skip the confirmation prompt. To force batch mode when installing plugins, add the -b or --batch option: . bin/opensearch-plugin install --batch &lt;plugin-name&gt; . ",
    "url": "https://vagimeli.github.io/install-and-configure/plugins/#batch-mode",
    "relUrl": "/install-and-configure/plugins/#batch-mode"
  },"114": {
    "doc": "Installing plugins",
    "title": "Available plugins",
    "content": "Major, minor, and patch plugin versions must match OpenSearch major, minor, and patch versions in order to be compatible. For example, plugins versions 2.3.0.x work only with OpenSearch 2.3.0. Bundled Plugins . The following plugins are bundled with all OpenSearch distributions except for minimum distribution packages. | Plugin Name | Repository | Earliest Available Version | . | Alerting | opensearch-alerting | 1.0.0 | . | Anomaly Detection | opensearch-anomaly-detection | 1.0.0 | . | Asynchronous Search | opensearch-asynchronous-search | 1.0.0 | . | Cross Cluster Replication | opensearch-cross-cluster-replication | 1.1.0 | . | Notebooks1 | opensearch-notebooks | 1.0.0 to 1.1.0 | . | Notifications | notifications | 2.0.0 | . | Reports Scheduler | opensearch-reports-scheduler | 1.0.0 | . | Geospatial | opensearch-geospatial | 2.2.0 | . | Index Management | opensearch-index-management | 1.0.0 | . | Job Scheduler | opensearch-job-scheduler | 1.0.0 | . | k-NN | opensearch-knn | 1.0.0 | . | ML Commons | opensearch-ml | 1.3.0 | . | Neural Search | neural-search | 2.4.0 | . | Observability | opensearch-observability | 1.2.0 | . | Performance Analyzer2 | opensearch-performance-analyzer | 1.0.0 | . | Security | opensearch-security | 1.0.0 | . | Security Analytics | opensearch-security-analytics | 2.4.0 | . | SQL | opensearch-sql | 1.0.0 | . 1Dashboard Notebooks was merged in to the Observability plugin with the release of OpenSearch 1.2.0. 2Performance Analyzer is not available on Windows. Additional plugins . Members of the OpenSearch community have built countless plugins for the service. Although it isn’t possible to build an exhaustive list of every plugin, since many plugins are not maintained within the OpenSearch GitHub repository, the following list of plugins are available to be installed by name using bin/opensearch-plugin install &lt;plugin-name&gt;. | Plugin Name | Earliest Available Version | . | analysis-icu | 1.0.0 | . | analysis-kuromoji | 1.0.0 | . | analysis-nori | 1.0.0 | . | analysis-phonetic | 1.0.0 | . | analysis-smartcn | 1.0.0 | . | analysis-stempel | 1.0.0 | . | analysis-ukrainian | 1.0.0 | . | discovery-azure-classic | 1.0.0 | . | discovery-ec2 | 1.0.0 | . | discovery-gce | 1.0.0 | . | ingest-attachment | 1.0.0 | . | mapper-annotated-text | 1.0.0 | . | mapper-murmur3 | 1.0.0 | . | mapper-size | 1.0.0 | . | repository-azure | 1.0.0 | . | repository-gcs | 1.0.0 | . | repository-hdfs | 1.0.0 | . | repository-s3 | 1.0.0 | . | store-smb | 1.0.0 | . | transport-nio | 1.0.0 | . ",
    "url": "https://vagimeli.github.io/install-and-configure/plugins/#available-plugins",
    "relUrl": "/install-and-configure/plugins/#available-plugins"
  },"115": {
    "doc": "Installing plugins",
    "title": "Related links",
    "content": ". | About Observability | About security analytics | About the Security plugin | Alerting | Anomaly detection | Asynchronous search | Cross-cluster replication | Index State Management | k-NN | ML Commons plugin | Neural Search | Notifications | OpenSearch Dashboards | Performance Analyzer | SQL | . ",
    "url": "https://vagimeli.github.io/install-and-configure/plugins/#related-links",
    "relUrl": "/install-and-configure/plugins/#related-links"
  },"116": {
    "doc": "Upgrades appendix",
    "title": "Upgrades appendix",
    "content": "Use the upgrades appendix to find additional supporting documentation, such as labs that include example API requests and configuration files to supplement the related process documentation. Specific procedures outlined in the appendix section can be used in a variety of ways: . | New OpenSearch users can use the steps and example resources we provide to learn about configuring and using OpenSearch and OpenSearch Dashboards. | System administrators who work with OpenSearch clusters can use the examples we provide to simulate cluster maintenance in a test environment before applying any changes to a production workload. | . If you would like to request a specific topic, please comment on issue #2830 in the OpenSearch Project on GitHub. The specific commands included in this appendix serve as examples of interacting with the OpenSearch API, and the underlying host, in order to demonstrate the steps described in the related upgrade process documents. The intention is not to be overly prescriptive but instead to add context for users who are new to OpenSearch and want to see practical examples. ",
    "url": "https://vagimeli.github.io/install-and-configure/upgrade-opensearch/appendix/index/",
    "relUrl": "/install-and-configure/upgrade-opensearch/appendix/index/"
  },"117": {
    "doc": "Rolling upgrade lab",
    "title": "Rolling upgrade lab",
    "content": "You can follow these steps on your own compatible host to recreate the same cluster state the OpenSearch Project used for testing rolling upgrades. This exercise is useful if you want to test the upgrade process in a development environment. The steps used in this lab were validated on an arbitrarily chosen Amazon Elastic Compute Cloud (Amazon EC2) t2.large instance using Amazon Linux 2 kernel version Linux 5.10.162-141.675.amzn2.x86_64 and Docker version 20.10.17, build 100c701. The instance was provisioned with an attached 20 GiB gp2 Amazon EBS root volume. These specifications are included for informational purposes and do not represent hardware requirements for OpenSearch or OpenSearch Dashboards. References in this procedure to the $HOME path on the host machine in this procedure are represented by the tilde character (“~”) to make the instructions more portable. If you would prefer to specify an absolute path, modify the volume paths defined in upgrade-demo-cluster.sh and used throughout relevant commands in this document to reflect your environment. ",
    "url": "https://vagimeli.github.io/install-and-configure/upgrade-opensearch/appendix/rolling-upgrade-lab/#rolling-upgrade-lab",
    "relUrl": "/install-and-configure/upgrade-opensearch/appendix/rolling-upgrade-lab/#rolling-upgrade-lab"
  },"118": {
    "doc": "Rolling upgrade lab",
    "title": "Setting up the environment",
    "content": "As you follow the steps in this document, you will define several Docker resources, including containers, volumes, and a dedicated Docker network, using a script we provide. You can clean up your environment with the following command if you want to restart the process: . docker container stop $(docker container ls -aqf name=os-); \\ docker container rm $(docker container ls -aqf name=os-); \\ docker volume rm -f $(docker volume ls -q | egrep 'data-0|repo-0'); \\ docker network rm opensearch-dev-net . copy . The command removes container names matching the regular expression os-*, data volumes matching data-0* and repo-0*, and the Docker network named opensearch-dev-net. If you have other Docker resources running on your host, then you should review and modify the command to avoid removing other resources unintentionally. This command does not revert host configuration changes, like memory swapping behavior. After selecting a host, you can begin the lab: . | Install the appropriate version of Docker Engine for your Linux distribution and system architecture. | Configure important system settings on your host: . | Disable memory paging and swapping on the host to improve performance: sudo swapoff -a . copy . | Increase the number of memory maps available to OpenSearch. Open the sysctl configuration file for editing. This example command uses the vim text editor, but you can use any available text editor: sudo vim /etc/sysctl.conf . copy . | Add the following line to /etc/sysctl.conf: vm.max_map_count=262144 . copy . | Save and quit. If you use the vi or vim text editors, you save and quit by switching to command mode, and entering :wq! or ZZ. | Apply the configuration change: sudo sysctl -p . copy . | . | Create a new directory called deploy in your home directory, then navigate to it. You will use ~/deploy for paths in the deployment script, configuration files, and TLS certificates: mkdir ~/deploy &amp;&amp; cd ~/deploy . copy . | Download upgrade-demo-cluster.sh from the OpenSearch Project documentation-website repository: wget https://raw.githubusercontent.com/opensearch-project/documentation-website/main/assets/examples/upgrade-demo-cluster.sh . copy . | Run the script without any modifications in order to deploy four containers running OpenSearch and one container running OpenSearch Dashboards, with custom, self-signed TLS certificates and a pre-defined set of internal users: sh upgrade-demo-cluster.sh . copy . | Confirm that the containers were launched successfully: docker container ls . copy . Example response . CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 6e5218c8397d opensearchproject/opensearch-dashboards:1.3.7 \"./opensearch-dashbo…\" 24 seconds ago Up 22 seconds 0.0.0.0:5601-&gt;5601/tcp, :::5601-&gt;5601/tcp os-dashboards-01 cb5188308b21 opensearchproject/opensearch:1.3.7 \"./opensearch-docker…\" 25 seconds ago Up 24 seconds 9300/tcp, 9650/tcp, 0.0.0.0:9204-&gt;9200/tcp, :::9204-&gt;9200/tcp, 0.0.0.0:9604-&gt;9600/tcp, :::9604-&gt;9600/tcp os-node-04 71b682aa6671 opensearchproject/opensearch:1.3.7 \"./opensearch-docker…\" 26 seconds ago Up 25 seconds 9300/tcp, 9650/tcp, 0.0.0.0:9203-&gt;9200/tcp, :::9203-&gt;9200/tcp, 0.0.0.0:9603-&gt;9600/tcp, :::9603-&gt;9600/tcp os-node-03 f894054a9378 opensearchproject/opensearch:1.3.7 \"./opensearch-docker…\" 27 seconds ago Up 26 seconds 9300/tcp, 9650/tcp, 0.0.0.0:9202-&gt;9200/tcp, :::9202-&gt;9200/tcp, 0.0.0.0:9602-&gt;9600/tcp, :::9602-&gt;9600/tcp os-node-02 2e9c91c959cd opensearchproject/opensearch:1.3.7 \"./opensearch-docker…\" 28 seconds ago Up 27 seconds 9300/tcp, 9650/tcp, 0.0.0.0:9201-&gt;9200/tcp, :::9201-&gt;9200/tcp, 0.0.0.0:9601-&gt;9600/tcp, :::9601-&gt;9600/tcp os-node-01 . | The amount of time OpenSearch needs to initialize the cluster varies depending on the performance capabilities of the underlying host. You can follow container logs to see what OpenSearch is doing during the bootstrap process: . | Enter the following command to display logs for container os-node-01 in the terminal window: docker logs -f os-node-01 . copy . | You will see a log entry resembling the following example when the node is ready: Example . [INFO ][o.o.s.c.ConfigurationRepository] [os-node-01] Node 'os-node-01' initialized . | Press Ctrl+C to stop following container logs and return to the command prompt. | . | Use cURL to query the OpenSearch REST API. In the following command, os-node-01 is queried by sending the request to host port 9201, which is mapped to port 9200 on the container: curl -s \"https://localhost:9201\" -ku admin:admin . copy . Example response . { \"name\" : \"os-node-01\", \"cluster_name\" : \"opensearch-dev-cluster\", \"cluster_uuid\" : \"g1MMknuDRuuD9IaaNt56KA\", \"version\" : { \"distribution\" : \"opensearch\", \"number\" : \"1.3.7\", \"build_type\" : \"tar\", \"build_hash\" : \"db18a0d5a08b669fb900c00d81462e221f4438ee\", \"build_date\" : \"2022-12-07T22:59:20.186520Z\", \"build_snapshot\" : false, \"lucene_version\" : \"8.10.1\", \"minimum_wire_compatibility_version\" : \"6.8.0\", \"minimum_index_compatibility_version\" : \"6.0.0-beta1\" }, \"tagline\" : \"The OpenSearch Project: https://opensearch.org/\" } . | . Tip: Use the -s option with curl to hide the progress meter and error messages. ",
    "url": "https://vagimeli.github.io/install-and-configure/upgrade-opensearch/appendix/rolling-upgrade-lab/#setting-up-the-environment",
    "relUrl": "/install-and-configure/upgrade-opensearch/appendix/rolling-upgrade-lab/#setting-up-the-environment"
  },"119": {
    "doc": "Rolling upgrade lab",
    "title": "Adding data and configuring OpenSearch Security",
    "content": "Now that the OpenSearch cluster is running, it’s time to add data and configure some OpenSearch Security settings. The data you add and settings you configure will be validated again after the version upgrade is complete. This section can be broken down into two parts: . | Indexing data with the REST API | Adding data using OpenSearch Dashboards | . Indexing data with the REST API . | Download the sample field mappings file: wget https://raw.githubusercontent.com/opensearch-project/documentation-website/main/assets/examples/ecommerce-field_mappings.json . copy . | Next, download the bulk data that you will ingest into this index: wget https://raw.githubusercontent.com/opensearch-project/documentation-website/main/assets/examples/ecommerce.json . copy . | Use the Create index API to create an index using the mappings defined in ecommerce-field_mappings.json: curl -H \"Content-Type: application/x-ndjson\" \\ -X PUT \"https://localhost:9201/ecommerce?pretty\" \\ --data-binary \"@ecommerce-field_mappings.json\" \\ -ku admin:admin . copy . Example response . { \"acknowledged\" : true, \"shards_acknowledged\" : true, \"index\" : \"ecommerce\" } . | Use the Bulk API to add data to the new ecommerce index from ecommerce.json: curl -H \"Content-Type: application/x-ndjson\" \\ -X PUT \"https://localhost:9201/ecommerce/_bulk?pretty\" \\ --data-binary \"@ecommerce.json\" \\ -ku admin:admin . copy . Example response (truncated) . { \"took\" : 3323, \"errors\" : false, \"items\" : [ ... \"index\" : { \"_index\" : \"ecommerce\", \"_type\" : \"_doc\", \"_id\" : \"4674\", \"_version\" : 1, \"result\" : \"created\", \"_shards\" : { \"total\" : 2, \"successful\" : 2, \"failed\" : 0 }, \"_seq_no\" : 4674, \"_primary_term\" : 1, \"status\" : 201 } ] } . | A search query can also confirm that the data was indexed successfully. The following query returns the number of documents in which keyword `customer_first_name` equals `Sonya`: . curl -H 'Content-Type: application/json' \\ -X GET \"https://localhost:9201/ecommerce/_search?pretty=true&amp;filter_path=hits.total\" \\ -d'{\"query\":{\"match\":{\"customer_first_name\":\"Sonya\"}}}' \\ -ku admin:admin . copy . Example response . { \"hits\" : { \"total\" : { \"value\" : 106, \"relation\" : \"eq\" } } } . | . Adding data using OpenSearch Dashboards . | Open a web browser and navigate to port 5601 on your Docker host (for example, https://HOST_ADDRESS:5601). If OpenSearch Dashboards is running and you have network access to the host from your browser client, then you will be redirected to a login page. | If the web browser throws an error because the TLS certificates are self-signed, then you might need to bypass certificate checks in your browser. Refer to the browser’s documentation for information about bypassing certificate checks. The common name (CN) for each certificate is generated according to the container and node names for intracluster communication, so connecting to the host from a browser will still result in an “invalid CN” warning. | . | Enter the default username (admin) and password (admin). | On the OpenSearch Dashboards Home page, select Add sample data. | Under Sample web logs, select Add data. | Optional: Select View data to review the [Logs] Web Traffic dashboard. | . | Select the Menu button to open the Navigation pane, then go to Security &gt; Internal users. | Select Create internal user. | Provide a Username and Password. | In the Backend role field, enter admin. | Select Create. | . ",
    "url": "https://vagimeli.github.io/install-and-configure/upgrade-opensearch/appendix/rolling-upgrade-lab/#adding-data-and-configuring-opensearch-security",
    "relUrl": "/install-and-configure/upgrade-opensearch/appendix/rolling-upgrade-lab/#adding-data-and-configuring-opensearch-security"
  },"120": {
    "doc": "Rolling upgrade lab",
    "title": "Backing up important files",
    "content": "Always create backups before making changes to your cluster, especially if the cluster is running in a production environment. In this section you will be: . | Registering a snapshot repository. | Creating a snapshot. | Backing up security settings. | . Registering a snapshot repository . | Register a repository using the volume that was mapped by upgrade-demo-cluster.sh: curl -H 'Content-Type: application/json' \\ -X PUT \"https://localhost:9201/_snapshot/snapshot-repo?pretty\" \\ -d '{\"type\":\"fs\",\"settings\":{\"location\":\"/usr/share/opensearch/snapshots\"}}' \\ -ku admin:admin . copy . Example response . { \"acknowledged\" : true } . | Optional: Perform an additional check to verify that the repository was created successfully: curl -H 'Content-Type: application/json' \\ -X POST \"https://localhost:9201/_snapshot/snapshot-repo/_verify?timeout=0s&amp;master_timeout=50s&amp;pretty\" \\ -ku admin:admin . copy . Example response . { \"nodes\" : { \"UODBXfAlRnueJ67grDxqgw\" : { \"name\" : \"os-node-03\" }, \"14I_OyBQQXio8nmk0xsVcQ\" : { \"name\" : \"os-node-04\" }, \"tQp3knPRRUqHvFNKpuD2vQ\" : { \"name\" : \"os-node-02\" }, \"rPe8D6ssRgO5twIP00wbCQ\" : { \"name\" : \"os-node-01\" } } } . | . Creating a snapshot . Snapshots are backups of a cluster’s indexes and state. See Snapshots to learn more. | Create a snapshot that includes all indexes and the cluster state: curl -H 'Content-Type: application/json' \\ -X PUT \"https://localhost:9201/_snapshot/snapshot-repo/cluster-snapshot-v137?wait_for_completion=true&amp;pretty\" \\ -ku admin:admin . copy . Example response . { \"snapshot\" : { \"snapshot\" : \"cluster-snapshot-v137\", \"uuid\" : \"-IYB8QNPShGOTnTtMjBjNg\", \"version_id\" : 135248527, \"version\" : \"1.3.7\", \"indices\" : [ \"opensearch_dashboards_sample_data_logs\", \".opendistro_security\", \"security-auditlog-2023.02.27\", \".kibana_1\", \".kibana_92668751_admin_1\", \"ecommerce\", \"security-auditlog-2023.03.06\", \"security-auditlog-2023.02.28\", \"security-auditlog-2023.03.07\" ], \"data_streams\" : [ ], \"include_global_state\" : true, \"state\" : \"SUCCESS\", \"start_time\" : \"2023-03-07T18:33:00.656Z\", \"start_time_in_millis\" : 1678213980656, \"end_time\" : \"2023-03-07T18:33:01.471Z\", \"end_time_in_millis\" : 1678213981471, \"duration_in_millis\" : 815, \"failures\" : [ ], \"shards\" : { \"total\" : 9, \"failed\" : 0, \"successful\" : 9 } } } . | . Backing up security settings . Cluster administrators can modify OpenSearch Security settings by using any of the following methods: . | Modifying YAML files and running securityadmin.sh | Making REST API requests using the admin certificate | Making changes with OpenSearch Dashboards | . Regardless of the method you choose, OpenSearch Security writes your configuration to a special system index called .opendistro_security. This system index is preserved through the upgrade process, and it is also saved in the snapshot you created. However, restoring system indexes requires elevated access granted by the admin certificate. To learn more, see System indexes and Configuring TLS certificates. You can also export your OpenSearch Security settings as YAML files by running securityadmin.sh with the -backup option on any of your OpenSearch nodes. These YAML files can be used to reinitialize the .opendistro_security index with your existing configuration. The following steps will guide you through generating these backup files and copying them to your host for storage: . | Open an interactive pseudo-TTY session with os-node-01: docker exec -it os-node-01 bash . copy . | Create a directory called backups and navigate to it: mkdir /usr/share/opensearch/backups &amp;&amp; cd /usr/share/opensearch/backups . copy . | Use securityadmin.sh to create backups of your OpenSearch Security settings in /usr/share/opensearch/backups/: /usr/share/opensearch/plugins/opensearch-security/tools/securityadmin.sh \\ -backup /usr/share/opensearch/backups \\ -icl \\ -nhnv \\ -cacert /usr/share/opensearch/config/root-ca.pem \\ -cert /usr/share/opensearch/config/admin.pem \\ -key /usr/share/opensearch/config/admin-key.pem . copy . Example response . Security Admin v7 Will connect to localhost:9300 ... done Connected as CN=A,OU=DOCS,O=OPENSEARCH,L=PORTLAND,ST=OREGON,C=US OpenSearch Version: 1.3.7 OpenSearch Security Version: 1.3.7.0 Contacting opensearch cluster 'opensearch' and wait for YELLOW clusterstate ... Clustername: opensearch-dev-cluster Clusterstate: GREEN Number of nodes: 4 Number of data nodes: 4 .opendistro_security index already exists, so we do not need to create one. Will retrieve '/config' into /usr/share/opensearch/backups/config.yml SUCC: Configuration for 'config' stored in /usr/share/opensearch/backups/config.yml Will retrieve '/roles' into /usr/share/opensearch/backups/roles.yml SUCC: Configuration for 'roles' stored in /usr/share/opensearch/backups/roles.yml Will retrieve '/rolesmapping' into /usr/share/opensearch/backups/roles_mapping.yml SUCC: Configuration for 'rolesmapping' stored in /usr/share/opensearch/backups/roles_mapping.yml Will retrieve '/internalusers' into /usr/share/opensearch/backups/internal_users.yml SUCC: Configuration for 'internalusers' stored in /usr/share/opensearch/backups/internal_users.yml Will retrieve '/actiongroups' into /usr/share/opensearch/backups/action_groups.yml SUCC: Configuration for 'actiongroups' stored in /usr/share/opensearch/backups/action_groups.yml Will retrieve '/tenants' into /usr/share/opensearch/backups/tenants.yml SUCC: Configuration for 'tenants' stored in /usr/share/opensearch/backups/tenants.yml Will retrieve '/nodesdn' into /usr/share/opensearch/backups/nodes_dn.yml SUCC: Configuration for 'nodesdn' stored in /usr/share/opensearch/backups/nodes_dn.yml Will retrieve '/whitelist' into /usr/share/opensearch/backups/whitelist.yml SUCC: Configuration for 'whitelist' stored in /usr/share/opensearch/backups/whitelist.yml Will retrieve '/audit' into /usr/share/opensearch/backups/audit.yml SUCC: Configuration for 'audit' stored in /usr/share/opensearch/backups/audit.yml . | Optional: Create a backup directory for TLS certificates and store copies of the certificates. Repeat this for each node if you use unique TLS certificates: mkdir /usr/share/opensearch/backups/certs &amp;&amp; cp /usr/share/opensearch/config/*pem /usr/share/opensearch/backups/certs/ . copy . | Terminate the pseudo-TTY session: exit . copy . | Copy the files to your host: docker cp os-node-01:/usr/share/opensearch/backups ~/deploy/ . copy . | . ",
    "url": "https://vagimeli.github.io/install-and-configure/upgrade-opensearch/appendix/rolling-upgrade-lab/#backing-up-important-files",
    "relUrl": "/install-and-configure/upgrade-opensearch/appendix/rolling-upgrade-lab/#backing-up-important-files"
  },"121": {
    "doc": "Rolling upgrade lab",
    "title": "Performing the upgrade",
    "content": "Now that the cluster is configured and you have made backups of important files and settings, it’s time to begin the version upgrade. Some steps included in this section, like disabling shard replication and flushing the transaction log, will not impact the performance of your cluster. These steps are included as best practices and can significantly improve cluster performance in situations where clients continue interacting with the OpenSearch cluster throughout the upgrade, such as by querying existing data or indexing documents. | Disable shard replication to stop the movement of Lucene index segments within your cluster: curl -H 'Content-type: application/json' \\ -X PUT \"https://localhost:9201/_cluster/settings?pretty\" \\ -d'{\"persistent\":{\"cluster.routing.allocation.enable\":\"primaries\"}}' \\ -ku admin:admin . copy . Example response . { \"acknowledged\" : true, \"persistent\" : { \"cluster\" : { \"routing\" : { \"allocation\" : { \"enable\" : \"primaries\" } } } }, \"transient\" : { } } . | Perform a flush operation on the cluster to commit transaction log entries to the Lucene index: curl -X POST \"https://localhost:9201/_flush?pretty\" -ku admin:admin . copy . Example response . { \"_shards\" : { \"total\" : 20, \"successful\" : 20, \"failed\" : 0 } } . | Select a node to upgrade. You can upgrade nodes in any order because all of the nodes in this demo cluster are eligible cluster managers. The following command will stop and remove container os-node-01 without removing the mounted data volume: docker stop os-node-01 &amp;&amp; docker container rm os-node-01 . copy . | Start a new container named os-node-01 with the opensearchproject/opensearch:2.5.0 image and using the same mapped volumes as the original container: docker run -d \\ -p 9201:9200 -p 9601:9600 \\ -e \"OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m\" \\ --ulimit nofile=65536:65536 --ulimit memlock=-1:-1 \\ -v data-01:/usr/share/opensearch/data \\ -v repo-01:/usr/share/opensearch/snapshots \\ -v ~/deploy/opensearch-01.yml:/usr/share/opensearch/config/opensearch.yml \\ -v ~/deploy/root-ca.pem:/usr/share/opensearch/config/root-ca.pem \\ -v ~/deploy/admin.pem:/usr/share/opensearch/config/admin.pem \\ -v ~/deploy/admin-key.pem:/usr/share/opensearch/config/admin-key.pem \\ -v ~/deploy/os-node-01.pem:/usr/share/opensearch/config/os-node-01.pem \\ -v ~/deploy/os-node-01-key.pem:/usr/share/opensearch/config/os-node-01-key.pem \\ --network opensearch-dev-net \\ --ip 172.20.0.11 \\ --name os-node-01 \\ opensearchproject/opensearch:2.5.0 . copy . Example response . d26d0cb2e1e93e9c01bb00f19307525ef89c3c3e306d75913860e6542f729ea4 . | Optional: Query the cluster to determine which node is acting as the cluster manager. You can run this command at any time during the process to see when a new cluster manager is elected: curl -s \"https://localhost:9201/_cat/nodes?v&amp;h=name,version,node.role,master\" \\ -ku admin:admin | column -t . copy . Example response . name version node.role master os-node-01 2.5.0 dimr - os-node-04 1.3.7 dimr * os-node-02 1.3.7 dimr - os-node-03 1.3.7 dimr - . | Optional: Query the cluster to see how shard allocation changes as nodes are removed and replaced. You can run this command at any time during the process to see how shard statuses change: curl -s \"https://localhost:9201/_cat/shards\" \\ -ku admin:admin . copy . Example response . security-auditlog-2023.03.06 0 p STARTED 53 214.5kb 172.20.0.13 os-node-03 security-auditlog-2023.03.06 0 r UNASSIGNED .kibana_1 0 p STARTED 3 14.5kb 172.20.0.12 os-node-02 .kibana_1 0 r STARTED 3 14.5kb 172.20.0.13 os-node-03 ecommerce 0 p STARTED 4675 3.9mb 172.20.0.12 os-node-02 ecommerce 0 r STARTED 4675 3.9mb 172.20.0.14 os-node-04 security-auditlog-2023.03.07 0 p STARTED 37 175.7kb 172.20.0.14 os-node-04 security-auditlog-2023.03.07 0 r UNASSIGNED .opendistro_security 0 p STARTED 10 67.9kb 172.20.0.12 os-node-02 .opendistro_security 0 r STARTED 10 67.9kb 172.20.0.13 os-node-03 .opendistro_security 0 r STARTED 10 64.5kb 172.20.0.14 os-node-04 .opendistro_security 0 r UNASSIGNED security-auditlog-2023.02.27 0 p STARTED 4 80.5kb 172.20.0.12 os-node-02 security-auditlog-2023.02.27 0 r UNASSIGNED security-auditlog-2023.02.28 0 p STARTED 6 104.1kb 172.20.0.14 os-node-04 security-auditlog-2023.02.28 0 r UNASSIGNED opensearch_dashboards_sample_data_logs 0 p STARTED 14074 9.1mb 172.20.0.12 os-node-02 opensearch_dashboards_sample_data_logs 0 r STARTED 14074 8.9mb 172.20.0.13 os-node-03 .kibana_92668751_admin_1 0 r STARTED 33 37.3kb 172.20.0.13 os-node-03 .kibana_92668751_admin_1 0 p STARTED 33 37.3kb 172.20.0.14 os-node-04 . | Stop os-node-02: docker stop os-node-02 &amp;&amp; docker container rm os-node-02 . copy . | Start a new container named os-node-02 with the opensearchproject/opensearch:2.5.0 image and using the same mapped volumes as the original container: docker run -d \\ -p 9202:9200 -p 9602:9600 \\ -e \"OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m\" \\ --ulimit nofile=65536:65536 --ulimit memlock=-1:-1 \\ -v data-02:/usr/share/opensearch/data \\ -v repo-01:/usr/share/opensearch/snapshots \\ -v ~/deploy/opensearch-02.yml:/usr/share/opensearch/config/opensearch.yml \\ -v ~/deploy/root-ca.pem:/usr/share/opensearch/config/root-ca.pem \\ -v ~/deploy/admin.pem:/usr/share/opensearch/config/admin.pem \\ -v ~/deploy/admin-key.pem:/usr/share/opensearch/config/admin-key.pem \\ -v ~/deploy/os-node-02.pem:/usr/share/opensearch/config/os-node-02.pem \\ -v ~/deploy/os-node-02-key.pem:/usr/share/opensearch/config/os-node-02-key.pem \\ --network opensearch-dev-net \\ --ip 172.20.0.12 \\ --name os-node-02 \\ opensearchproject/opensearch:2.5.0 . copy . Example response . 7b802865bd6eb420a106406a54fc388ed8e5e04f6cbd908c2a214ea5ce72ac00 . | Stop os-node-03: docker stop os-node-03 &amp;&amp; docker container rm os-node-03 . copy . | Start a new container named os-node-03 with the opensearchproject/opensearch:2.5.0 image and using the same mapped volumes as the original container: docker run -d \\ -p 9203:9200 -p 9603:9600 \\ -e \"OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m\" \\ --ulimit nofile=65536:65536 --ulimit memlock=-1:-1 \\ -v data-03:/usr/share/opensearch/data \\ -v repo-01:/usr/share/opensearch/snapshots \\ -v ~/deploy/opensearch-03.yml:/usr/share/opensearch/config/opensearch.yml \\ -v ~/deploy/root-ca.pem:/usr/share/opensearch/config/root-ca.pem \\ -v ~/deploy/admin.pem:/usr/share/opensearch/config/admin.pem \\ -v ~/deploy/admin-key.pem:/usr/share/opensearch/config/admin-key.pem \\ -v ~/deploy/os-node-03.pem:/usr/share/opensearch/config/os-node-03.pem \\ -v ~/deploy/os-node-03-key.pem:/usr/share/opensearch/config/os-node-03-key.pem \\ --network opensearch-dev-net \\ --ip 172.20.0.13 \\ --name os-node-03 \\ opensearchproject/opensearch:2.5.0 . copy . Example response . d7f11726841a89eb88ff57a8cbecab392399f661a5205f0c81b60a995fc6c99d . | Stop os-node-04: docker stop os-node-04 &amp;&amp; docker container rm os-node-04 . copy . | Start a new container named os-node-04 with the opensearchproject/opensearch:2.5.0 image and using the same mapped volumes as the original container: docker run -d \\ -p 9204:9200 -p 9604:9600 \\ -e \"OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m\" \\ --ulimit nofile=65536:65536 --ulimit memlock=-1:-1 \\ -v data-04:/usr/share/opensearch/data \\ -v repo-01:/usr/share/opensearch/snapshots \\ -v ~/deploy/opensearch-04.yml:/usr/share/opensearch/config/opensearch.yml \\ -v ~/deploy/root-ca.pem:/usr/share/opensearch/config/root-ca.pem \\ -v ~/deploy/admin.pem:/usr/share/opensearch/config/admin.pem \\ -v ~/deploy/admin-key.pem:/usr/share/opensearch/config/admin-key.pem \\ -v ~/deploy/os-node-04.pem:/usr/share/opensearch/config/os-node-04.pem \\ -v ~/deploy/os-node-04-key.pem:/usr/share/opensearch/config/os-node-04-key.pem \\ --network opensearch-dev-net \\ --ip 172.20.0.14 \\ --name os-node-04 \\ opensearchproject/opensearch:2.5.0 . copy . Example response . 26f8286ab11e6f8dcdf6a83c95f265172f9557578a1b292af84c6f5ef8738e1d . | Confirm that your cluster is running the new version: curl -s \"https://localhost:9201/_cat/nodes?v&amp;h=name,version,node.role,master\" \\ -ku admin:admin | column -t . copy . Example response . name version node.role master os-node-01 2.5.0 dimr * os-node-02 2.5.0 dimr - os-node-04 2.5.0 dimr - os-node-03 2.5.0 dimr - . | The last component you should upgrade is the OpenSearch Dashboards node. First, stop and remove the old container: docker stop os-dashboards-01 &amp;&amp; docker rm os-dashboards-01 . copy . | Create a new container running the target version of OpenSearch Dashboards: docker run -d \\ -p 5601:5601 --expose 5601 \\ -v ~/deploy/opensearch_dashboards.yml:/usr/share/opensearch-dashboards/config/opensearch_dashboards.yml \\ -v ~/deploy/root-ca.pem:/usr/share/opensearch-dashboards/config/root-ca.pem \\ -v ~/deploy/os-dashboards-01.pem:/usr/share/opensearch-dashboards/config/os-dashboards-01.pem \\ -v ~/deploy/os-dashboards-01-key.pem:/usr/share/opensearch-dashboards/config/os-dashboards-01-key.pem \\ --network opensearch-dev-net \\ --ip 172.20.0.10 \\ --name os-dashboards-01 \\ opensearchproject/opensearch-dashboards:2.5.0 . copy . Example response . 310de7a24cf599ca0b39b241db07fa8865592ebe15b6f5fda26ad19d8e1c1e09 . | Make sure the OpenSearch Dashboards container started properly. A command like the following can be used to confirm that requests to https://HOST_ADDRESS:5601 are redirected (HTTP status code 302) to /app/login?: curl https://localhost:5601 -kI . copy . Example response . HTTP/1.1 302 Found location: /app/login? osd-name: opensearch-dashboards-dev cache-control: private, no-cache, no-store, must-revalidate set-cookie: security_authentication=; Max-Age=0; Expires=Thu, 01 Jan 1970 00:00:00 GMT; Secure; HttpOnly; Path=/ content-length: 0 Date: Wed, 08 Mar 2023 15:36:53 GMT Connection: keep-alive Keep-Alive: timeout=120 . | Re-enable allocation of replica shards: curl -H 'Content-type: application/json' \\ -X PUT \"https://localhost:9201/_cluster/settings?pretty\" \\ -d'{\"persistent\":{\"cluster.routing.allocation.enable\":\"all\"}}' \\ -ku admin:admin . copy . Example response . { \"acknowledged\" : true, \"persistent\" : { \"cluster\" : { \"routing\" : { \"allocation\" : { \"enable\" : \"all\" } } } }, \"transient\" : { } } . | . ",
    "url": "https://vagimeli.github.io/install-and-configure/upgrade-opensearch/appendix/rolling-upgrade-lab/#performing-the-upgrade",
    "relUrl": "/install-and-configure/upgrade-opensearch/appendix/rolling-upgrade-lab/#performing-the-upgrade"
  },"122": {
    "doc": "Rolling upgrade lab",
    "title": "Validating the upgrade",
    "content": "You successfully deployed a secure OpenSearch cluster, indexed data, created a dashboard populated with sample data, created a new internal user, backed up your important files, and upgraded the cluster from version 1.3.7 to 2.5.0. Before you continue exploring and experimenting with OpenSearch and OpenSearch Dashboards, you should validate the outcome of the upgrade. For this cluster, post-upgrade validation steps can include verifying the following: . | Running version | Health and shard allocation | Data consistency | . Verifying the new running version . | Verify the current running version of your OpenSearch nodes: curl -s \"https://localhost:9201/_cat/nodes?v&amp;h=name,version,node.role,master\" \\ -ku admin:admin | column -t . copy . Example response . name version node.role master os-node-01 2.5.0 dimr * os-node-02 2.5.0 dimr - os-node-04 2.5.0 dimr - os-node-03 2.5.0 dimr - . | Verify the current running version of OpenSearch Dashboards: . | Option 1: Verify the OpenSearch Dashboards version from the web interface. | Open a web browser and navigate to port 5601 on your Docker host (for example, https://HOST_ADDRESS:5601). | Log in with the default username (admin) and default password (admin). | Select the Help button in the upper-right corner. The version is displayed in a pop-up window. | Select the Help button again to close the pop-up window. | . | Option 2: Verify the OpenSearch Dashboards version by inspecting manifest.yml. | From the command line, open an interactive pseudo-TTY session with the OpenSearch Dashboards container: docker exec -it os-dashboards-01 bash . copy . | Check manifest.yml for the version: head -n 5 manifest.yml . copy . Example response . --- schema-version: '1.1' build: name: OpenSearch Dashboards version: 2.5.0 . | Terminate the pseudo-TTY session: exit . copy . | . | . | . Verifying cluster health and shard allocation . | Query the Cluster health API endpoint to see information about the health of your cluster. You should see a status of green, which indicates that all primary and replica shards are allocated: curl -s \"https://localhost:9201/_cluster/health?pretty\" -ku admin:admin . copy . Example response . { \"cluster_name\" : \"opensearch-dev-cluster\", \"status\" : \"green\", \"timed_out\" : false, \"number_of_nodes\" : 4, \"number_of_data_nodes\" : 4, \"discovered_master\" : true, \"discovered_cluster_manager\" : true, \"active_primary_shards\" : 16, \"active_shards\" : 36, \"relocating_shards\" : 0, \"initializing_shards\" : 0, \"unassigned_shards\" : 0, \"delayed_unassigned_shards\" : 0, \"number_of_pending_tasks\" : 0, \"number_of_in_flight_fetch\" : 0, \"task_max_waiting_in_queue_millis\" : 0, \"active_shards_percent_as_number\" : 100.0 } . | Query the CAT shards API endpoint to see how shards are allocated after the cluster is upgrade: curl -s \"https://localhost:9201/_cat/shards\" -ku admin:admin . copy . Example response . security-auditlog-2023.02.27 0 r STARTED 4 80.5kb 172.20.0.13 os-node-03 security-auditlog-2023.02.27 0 p STARTED 4 80.5kb 172.20.0.11 os-node-01 security-auditlog-2023.03.08 0 p STARTED 30 95.2kb 172.20.0.13 os-node-03 security-auditlog-2023.03.08 0 r STARTED 30 123.8kb 172.20.0.11 os-node-01 ecommerce 0 p STARTED 4675 3.9mb 172.20.0.12 os-node-02 ecommerce 0 r STARTED 4675 3.9mb 172.20.0.13 os-node-03 .kibana_1 0 p STARTED 3 5.9kb 172.20.0.12 os-node-02 .kibana_1 0 r STARTED 3 5.9kb 172.20.0.11 os-node-01 .kibana_92668751_admin_1 0 p STARTED 33 37.3kb 172.20.0.13 os-node-03 .kibana_92668751_admin_1 0 r STARTED 33 37.3kb 172.20.0.11 os-node-01 opensearch_dashboards_sample_data_logs 0 p STARTED 14074 9.1mb 172.20.0.12 os-node-02 opensearch_dashboards_sample_data_logs 0 r STARTED 14074 9.1mb 172.20.0.14 os-node-04 security-auditlog-2023.02.28 0 p STARTED 6 26.2kb 172.20.0.11 os-node-01 security-auditlog-2023.02.28 0 r STARTED 6 26.2kb 172.20.0.14 os-node-04 .opendistro-reports-definitions 0 p STARTED 0 208b 172.20.0.12 os-node-02 .opendistro-reports-definitions 0 r STARTED 0 208b 172.20.0.13 os-node-03 .opendistro-reports-definitions 0 r STARTED 0 208b 172.20.0.14 os-node-04 security-auditlog-2023.03.06 0 r STARTED 53 174.6kb 172.20.0.12 os-node-02 security-auditlog-2023.03.06 0 p STARTED 53 174.6kb 172.20.0.14 os-node-04 .kibana_101107607_newuser_1 0 r STARTED 1 5.1kb 172.20.0.13 os-node-03 .kibana_101107607_newuser_1 0 p STARTED 1 5.1kb 172.20.0.11 os-node-01 .opendistro_security 0 r STARTED 10 64.5kb 172.20.0.12 os-node-02 .opendistro_security 0 r STARTED 10 64.5kb 172.20.0.13 os-node-03 .opendistro_security 0 r STARTED 10 64.5kb 172.20.0.11 os-node-01 .opendistro_security 0 p STARTED 10 64.5kb 172.20.0.14 os-node-04 .kibana_-152937574_admintenant_1 0 r STARTED 1 5.1kb 172.20.0.12 os-node-02 .kibana_-152937574_admintenant_1 0 p STARTED 1 5.1kb 172.20.0.14 os-node-04 security-auditlog-2023.03.07 0 r STARTED 37 175.7kb 172.20.0.12 os-node-02 security-auditlog-2023.03.07 0 p STARTED 37 175.7kb 172.20.0.14 os-node-04 .kibana_92668751_admin_2 0 p STARTED 34 38.6kb 172.20.0.13 os-node-03 .kibana_92668751_admin_2 0 r STARTED 34 38.6kb 172.20.0.11 os-node-01 .kibana_2 0 p STARTED 3 6kb 172.20.0.13 os-node-03 .kibana_2 0 r STARTED 3 6kb 172.20.0.14 os-node-04 .opendistro-reports-instances 0 r STARTED 0 208b 172.20.0.12 os-node-02 .opendistro-reports-instances 0 r STARTED 0 208b 172.20.0.11 os-node-01 .opendistro-reports-instances 0 p STARTED 0 208b 172.20.0.14 os-node-04 . | . Verifying data consistency . You need to query the ecommerce index again in order to confirm that the sample data is still present: . | Compare the response to this query with the response you received in the last step of Indexing data with the REST API: curl -H 'Content-Type: application/json' \\ -X GET \"https://localhost:9201/ecommerce/_search?pretty=true&amp;filter_path=hits.total\" \\ -d'{\"query\":{\"match\":{\"customer_first_name\":\"Sonya\"}}}' \\ -ku admin:admin . copy . Example response . { \"hits\" : { \"total\" : { \"value\" : 106, \"relation\" : \"eq\" } } } . | Open a web browser and navigate to port 5601 on your Docker host (for example, https://HOST_ADDRESS:5601). | Enter the default username (admin) and password (admin). | On the OpenSearch Dashboards Home page, select the Menu button in the upper-left corner of the web interface to open the Navigation pane. | Select Dashboard. | Choose [Logs] Web Traffic to open the dashboard that was created when you added sample data earlier in the process. | When you are done reviewing the dashboard, select the Profile button. Choose Log out so you can log in as a different user. | Enter the username and password you created before upgrading, then select Log in. | . ",
    "url": "https://vagimeli.github.io/install-and-configure/upgrade-opensearch/appendix/rolling-upgrade-lab/#validating-the-upgrade",
    "relUrl": "/install-and-configure/upgrade-opensearch/appendix/rolling-upgrade-lab/#validating-the-upgrade"
  },"123": {
    "doc": "Rolling upgrade lab",
    "title": "Next steps",
    "content": "Review the following resoures to learn more about how OpenSearch works: . | REST API reference | Quickstart guide for OpenSearch Dashboards | About Security in OpenSearch | . ",
    "url": "https://vagimeli.github.io/install-and-configure/upgrade-opensearch/appendix/rolling-upgrade-lab/#next-steps",
    "relUrl": "/install-and-configure/upgrade-opensearch/appendix/rolling-upgrade-lab/#next-steps"
  },"124": {
    "doc": "Rolling upgrade lab",
    "title": "Rolling upgrade lab",
    "content": " ",
    "url": "https://vagimeli.github.io/install-and-configure/upgrade-opensearch/appendix/rolling-upgrade-lab/",
    "relUrl": "/install-and-configure/upgrade-opensearch/appendix/rolling-upgrade-lab/"
  },"125": {
    "doc": "Upgrading OpenSearch",
    "title": "Upgrading OpenSearch",
    "content": "The OpenSearch Project releases regular updates that include new features, enhancements, and bug fixes. OpenSearch uses Semantic Versioning, which means that breaking changes are only introduced between major version releases. To learn about upcoming features and fixes, review the OpenSearch Project Roadmap on GitHub. To view a list of previous releases or to learn more about how OpenSearch uses versioning, see Release Schedule and Maintenance Policy. We recognize that users are excited about upgrading OpenSearch in order to enjoy the latest features, and we will continue to expand on these upgrade and migration documents to cover additional topics, such as upgrading OpenSearch Dashboards and preserving custom configurations, such as for plugins. To see what’s coming next or to make a request for future content, leave a comment on the upgrade and migration documentation meta issue in the OpenSearch Project on GitHub. If you would like a specific process to be added or would like to contribute, create an issue on GitHub. See the Contributor Guidelines to learn how you can help. ",
    "url": "https://vagimeli.github.io/install-and-configure/upgrade-opensearch/index/",
    "relUrl": "/install-and-configure/upgrade-opensearch/index/"
  },"126": {
    "doc": "Upgrading OpenSearch",
    "title": "Workflow considerations",
    "content": "Take time to plan the process before making any changes to your cluster. For example, consider the following questions: . | How long will the upgrade process take? | If your cluster is being used in production, how impactful is downtime? | Do you have infrastructure in place to stand up the new cluster in a testing or development environment before you move it into production, or do you need to upgrade the production hosts directly? | . The answers to questions like these will help you determine which upgrade path will work best in your environment. At a minimum, you should be: . | Reviewing breaking changes. | Reviewing the OpenSearch tools compatibility matrices. | Reviewing plugin compatibility. | Backing up configuration files. | Creating a snapshot. | . Stop any nonessential indexing before you begin the upgrade procedure to eliminate unnecessary resource demands on the cluster while you perform the upgrade. Reviewing breaking changes . It’s important to determine how the new version of OpenSearch will integrate with your environment. Review Breaking changes before beginning any upgrade procedures to determine whether you will need to make adjustments to your workflow. For example, upstream or downstream components might need to be modified to be compatible with an API change (see meta issue #2589). Reviewing the OpenSearch tools compatibility matrices . If your OpenSearch cluster interacts with other services in your environment, like Logstash or Beats, then you should check the OpenSearch tools compatibility matrices to determine whether other components will need to be upgraded. Reviewing plugin compatibility . Review the plugins you use to determine compatibility with the target version of OpenSearch. Official OpenSearch Project plugins can be found in the OpenSearch Project repository on GitHub. If you use any third-party plugins, then you should check the documentation for those plugins to determine whether they are compatible. Go to Available plugins to see a reference table that highlights version compatibility for bundled OpenSearch plugins. Major, minor, and patch plugin versions must match OpenSearch major, minor, and patch versions in order to be compatible. For example, plugin versions 2.3.0.x work only with OpenSearch 2.3.0. Backing up configuration files . Mitigate the risk of data loss by backing up any important files before you start an upgrade. Generally, these files will be located in either of two directories: . | opensearch/config | opensearch-dashboards/config | . Some examples include opensearch.yml, opensearch_dashboards.yml, plugin configuration files, and TLS certificates. Once you identify which files you want to back up, copy them to remote storage for safety. If you use security features, make sure to read A word of caution for information about backing up and restoring your security settings. Creating a snapshot . We recommend that you back up your cluster state and indexes using snapshots. Snapshots you take before an upgrade can be used as restore points if you need to roll back the cluster to its original version. You can further reduce the risk of data loss by storing your snapshots on external storage, such as a mounted Network File System (NFS) or a cloud storage solution like those listed in the following table. | Snapshot repository location | Required OpenSearch plugin | . | Amazon Simple Storage Service (Amazon S3) | repository-s3 | . | Google Cloud Storage (GCS) | repository-gcs | . | Apache Hadoop Distributed File System (HDFS) | repository-hdfs | . | Microsoft Azure Blob Storage | repository-azure | . ",
    "url": "https://vagimeli.github.io/install-and-configure/upgrade-opensearch/index/#workflow-considerations",
    "relUrl": "/install-and-configure/upgrade-opensearch/index/#workflow-considerations"
  },"127": {
    "doc": "Upgrading OpenSearch",
    "title": "Upgrade methods",
    "content": "Choose an appropriate method for upgrading your cluster to a new version of OpenSearch based on your requirements: . | A rolling upgrade upgrades nodes one at a time without stopping the cluster. | A cluster restart upgrade upgrades services while the cluster is stopped. | . Upgrades spanning more than a single major version of OpenSearch will require additional effort due to the need for reindexing. For more information, refer to the Reindex API. See the Index compatibility reference table included later in this guide for help planning your data migration. Rolling upgrade . A rolling upgrade is a great option if you want to keep your cluster operational throughout the process. Data may continue to be ingested, analyzed, and queried as nodes are individually stopped, upgraded, and restarted. A variation of the rolling upgrade referred to as “node replacement” follows exactly the same process except that hosts and containers are not reused for the new node. You might perform node replacement if you are upgrading the underlying host(s) as well. OpenSearch nodes cannot join a cluster if the cluster manager is running a newer version of OpenSearch than the node requesting membership. To avoid this issue, upgrade the cluster-manager-eligible nodes last. See Rolling Upgrade for more information about the process. Cluster restart upgrade . OpenSearch administrators might choose to perform a cluster restart upgrade for several reasons, such as if the administrator doesn’t want to perform maintenance on a running cluster or if the cluster is being migrated to a different environment. Unlike a rolling upgrade, where only one node is offline at a time, a cluster restart upgrade requires you to stop OpenSearch and OpenSearch Dashboards on all nodes in the cluster before proceeding. After the nodes are stopped, a new version of OpenSearch is installed. Then OpenSearch is started and the cluster bootstraps to the new version. ",
    "url": "https://vagimeli.github.io/install-and-configure/upgrade-opensearch/index/#upgrade-methods",
    "relUrl": "/install-and-configure/upgrade-opensearch/index/#upgrade-methods"
  },"128": {
    "doc": "Upgrading OpenSearch",
    "title": "Compatibility",
    "content": "OpenSearch nodes are compatible with other OpenSearch nodes running any other minor version within the same major version release. For example, 1.1.0 is compatible with 1.3.7 because they are part of the same major version (1.x). Additionally, OpenSearch nodes and indexes are backward compatible with the previous major version. That means, for example, that an index created by an OpenSearch node running any 1.x version can be restored from a snapshot to an OpenSearch cluster running any 2.x version. OpenSearch 1.x nodes are compatible with nodes running Elasticsearch 7.x, but the longevity of a mixed-version environment should not extend beyond cluster upgrade activities. Index compatibility is determined by the version of Apache Lucene that created the index. If an index was created by an OpenSearch cluster running version 1.0.0, then the index can be used by any other OpenSearch cluster running up to the latest 1.x or 2.x release. See the Index compatibility reference table for Lucene versions running in OpenSearch 1.0.0 and later and Elasticsearch 6.8 and later. If your upgrade path spans more than a single major version and you want to retain any existing indexes, then you can use the Reindex API to make your indexes compatible with the target version of OpenSearch before upgrading. For example, if your cluster is currently running Elasticsearch 6.8 and you want to upgrade to OpenSearch 2.x, then you must first upgrade to OpenSearch 1.x, recreate your indexes using the Reindex API, and finally upgrade to 2.x. One alternative to reindexing is to reingest data from the origin, such as by replaying a data stream or ingesting data from a database. Index compatibility reference . If you plan to retain old indexes after the OpenSearch version upgrade, then you might need to reindex or reingest the data. Refer to the following table for Lucene versions across recent OpenSearch and Elasticsearch releases. | Lucene Version | OpenSearch Version | Elasticsearch Version | . | 9.4.2 | 2.5.02.4.1 | 8.6 | . | 9.4.1 | 2.4.0 | &#8212; | . | 9.4.0 | &#8212; | 8.5 | . | 9.3.0 | 2.3.02.2.x | 8.4 | . | 9.2.0 | 2.1.0 | 8.3 | . | 9.1.0 | 2.0.x | 8.2 | . | 9.0.0 | &#8212; | 8.18.0 | . | 8.11.1 | &#8212; | 7.17 | . | 8.10.1 | 1.3.x1.2.x | 7.16 | . | 8.9.0 | 1.1.0 | 7.157.14 | . | 8.8.2 | 1.0.0 | 7.13 | . | 8.8.0 | &#8212; | 7.12 | . | 8.7.0 | &#8212; | 7.117.10 | . | 8.6.2 | &#8212; | 7.9 | . | 8.5.1 | &#8212; | 7.87.7 | . | 8.4.0 | &#8212; | 7.6 | . | 8.3.0 | &#8212; | 7.5 | . | 8.2.0 | &#8212; | 7.4 | . | 8.1.0 | &#8212; | 7.3 | . | 8.0.0 | &#8212; | 7.27.1 | . | 7.7.3 | &#8212; | 6.8 | . A dash (&#8212;) indicates that there is no product version containing the specified version of Apache Lucene. ",
    "url": "https://vagimeli.github.io/install-and-configure/upgrade-opensearch/index/#compatibility",
    "relUrl": "/install-and-configure/upgrade-opensearch/index/#compatibility"
  },"129": {
    "doc": "Rolling Upgrade",
    "title": "Rolling Upgrade",
    "content": "Rolling upgrades, sometimes referred to as “node replacement upgrades,” can be performed on running clusters with virtually no downtime. Nodes are individually stopped and upgraded in place. Alternatively, nodes can be stopped and replaced, one at a time, by hosts running the new version. During this process you can continue to index and query data in your cluster. This document serves as a high-level, platform-agnostic overview of the rolling upgrade procedure. For specific examples of commands, scripts, and configuration files, refer to the Appendix. ",
    "url": "https://vagimeli.github.io/install-and-configure/upgrade-opensearch/rolling-upgrade/",
    "relUrl": "/install-and-configure/upgrade-opensearch/rolling-upgrade/"
  },"130": {
    "doc": "Rolling Upgrade",
    "title": "Preparing to upgrade",
    "content": "Review Upgrading OpenSearch for recommendations about backing up your configuration files and creating a snapshot of the cluster state and indexes before you make any changes to your OpenSearch cluster. Important: OpenSearch nodes cannot be downgraded. If you need to revert the upgrade, then you will need to perform a fresh installation of OpenSearch and restore the cluster from a snapshot. Take a snapshot and store it in a remote repository before beginning the upgrade procedure. ",
    "url": "https://vagimeli.github.io/install-and-configure/upgrade-opensearch/rolling-upgrade/#preparing-to-upgrade",
    "relUrl": "/install-and-configure/upgrade-opensearch/rolling-upgrade/#preparing-to-upgrade"
  },"131": {
    "doc": "Rolling Upgrade",
    "title": "Performing the upgrade",
    "content": ". | Verify the health of your OpenSearch cluster before you begin. You should resolve any index or shard allocation issues prior to upgrading to ensure that your data is preserved. A status of green indicates that all primary and replica shards are allocated. See Cluster health for more information. The following command queries the _cluster/health API endpoint: GET \"/_cluster/health?pretty\" . The response should look similar to the following example: . { \"cluster_name\":\"opensearch-dev-cluster\", \"status\":\"green\", \"timed_out\":false, \"number_of_nodes\":4, \"number_of_data_nodes\":4, \"active_primary_shards\":1, \"active_shards\":4, \"relocating_shards\":0, \"initializing_shards\":0, \"unassigned_shards\":0, \"delayed_unassigned_shards\":0, \"number_of_pending_tasks\":0, \"number_of_in_flight_fetch\":0, \"task_max_waiting_in_queue_millis\":0, \"active_shards_percent_as_number\":100.0 } . | Disable shard replication to prevent shard replicas from being created while nodes are being taken offline. This stops the movement of Lucene index segments on nodes in your cluster. You can disable shard replication by querying the _cluster/settings API endpoint: PUT \"/_cluster/settings?pretty\" { \"persistent\": { \"cluster.routing.allocation.enable\": \"primaries\" } } . The response should look similar to the following example: . { \"acknowledged\" : true, \"persistent\" : { \"cluster\" : { \"routing\" : { \"allocation\" : { \"enable\" : \"primaries\" } } } }, \"transient\" : { } } . | Perform a flush operation on the cluster to commit transaction log entries to the Lucene index: POST \"/_flush?pretty\" . The response should look similar to the following example: . { \"_shards\" : { \"total\" : 4, \"successful\" : 4, \"failed\" : 0 } } . | Review your cluster and identify the first node to upgrade. Eligible cluster manager nodes should be upgraded last because OpenSearch nodes can join a cluster with manager nodes running an older version, but they cannot join a cluster with all manager nodes running a newer version. | Query the _cat/nodes endpoint to identify which node was promoted to cluster manager. The following command includes additional query parameters that request only the name, version, node.role, and master headers. Note that OpenSearch 1.x versions use the term “master,” which has been deprecated and replaced by “cluster_manager” in OpenSearch 2.x and later. GET \"/_cat/nodes?v&amp;h=name,version,node.role,master\" | column -t . The response should look similar to the following example: . name version node.role master os-node-01 7.10.2 dimr - os-node-04 7.10.2 dimr - os-node-03 7.10.2 dimr - os-node-02 7.10.2 dimr * . | Stop the node you are upgrading. Do not delete the volume associated with the container when you delete the container. The new OpenSearch container will use the existing volume. Deleting the volume will result in data loss. | Confirm that the associated node has been dismissed from the cluster by querying the _cat/nodes API endpoint: GET \"/_cat/nodes?v&amp;h=name,version,node.role,master\" | column -t . The response should look similar to the following example: . name version node.role master os-node-02 7.10.2 dimr * os-node-04 7.10.2 dimr - os-node-03 7.10.2 dimr - . os-node-01 is no longer listed because the container has been stopped and deleted. | Deploy a new container running the desired version of OpenSearch and mapped to the same volume as the container you deleted. | Query the _cat/nodes endpoint after OpenSearch is running on the new node to confirm that it has joined the cluster: GET \"/_cat/nodes?v&amp;h=name,version,node.role,master\" | column -t . The response should look similar to the following example: . name version node.role master os-node-02 7.10.2 dimr * os-node-04 7.10.2 dimr - os-node-01 7.10.2 dimr - os-node-03 7.10.2 dimr - . In the example output, the new OpenSearch node reports a running version of 7.10.2 to the cluster. This is the result of compatibility.override_main_response_version, which is used when connecting to a cluster with legacy clients that check for a version. You can manually confirm the version of the node by calling the /_nodes API endpoint, as in the following command. Replace &lt;nodeName&gt; with the name of your node. See Nodes API to learn more. GET \"/_nodes/&lt;nodeName&gt;?pretty=true\" | jq -r '.nodes | .[] | \"\\(.name) v\\(.version)\"' . The response should look similar to the following example: . os-node-01 v1.3.7 . | Repeat steps 5 through 9 for each node in your cluster. Remember to upgrade an eligible cluster manager node last. After replacing the last node, query the _cat/nodes endpoint to confirm that all nodes have joined the cluster. The cluster is now bootstrapped to the new version of OpenSearch. You can verify the cluster version by querying the _cat/nodes API endpoint: GET \"/_cat/nodes?v&amp;h=name,version,node.role,master\" | column -t . The response should look similar to the following example: . name version node.role master os-node-04 1.3.7 dimr - os-node-02 1.3.7 dimr * os-node-01 1.3.7 dimr - os-node-03 1.3.7 dimr - . | Reenable shard replication: PUT \"/_cluster/settings?pretty\" { \"persistent\": { \"cluster.routing.allocation.enable\": \"all\" } } . The response should look similar to the following example: . { \"acknowledged\" : true, \"persistent\" : { \"cluster\" : { \"routing\" : { \"allocation\" : { \"enable\" : \"all\" } } } }, \"transient\" : { } } . | Confirm that the cluster is healthy: GET \"/_cluster/health?pretty\" . The response should look similar to the following example: . { \"cluster_name\" : \"opensearch-dev-cluster\", \"status\" : \"green\", \"timed_out\" : false, \"number_of_nodes\" : 4, \"number_of_data_nodes\" : 4, \"discovered_master\" : true, \"active_primary_shards\" : 1, \"active_shards\" : 4, \"relocating_shards\" : 0, \"initializing_shards\" : 0, \"unassigned_shards\" : 0, \"delayed_unassigned_shards\" : 0, \"number_of_pending_tasks\" : 0, \"number_of_in_flight_fetch\" : 0, \"task_max_waiting_in_queue_millis\" : 0, \"active_shards_percent_as_number\" : 100.0 } . | The upgrade is now complete, and you can begin enjoying the latest features and fixes! | . Related articles . | OpenSearch configuration | Performance analyzer | Install and configure OpenSearch Dashboards | About Security in OpenSearch | . ",
    "url": "https://vagimeli.github.io/install-and-configure/upgrade-opensearch/rolling-upgrade/#performing-the-upgrade",
    "relUrl": "/install-and-configure/upgrade-opensearch/rolling-upgrade/#performing-the-upgrade"
  },"132": {
    "doc": "Migrating from Kibana OSS to OpenSearch Dashboards",
    "title": "Migrating from Kibana OSS to OpenSearch Dashboards",
    "content": "Kibana OSS stores its visualizations and dashboards in one or more indexes (.kibana*) on the Elasticsearch OSS cluster. As such, the most important step is to leave those indexes intact as you migrate from Elasticsearch OSS to OpenSearch. Consider exporting all Kibana objects prior to starting the migration. In Kibana, choose Stack Management, Saved Objects, Export objects. | After you migrate your Elasticsearch OSS cluster to OpenSearch, stop Kibana. | For safety, make a backup copy of &lt;kibana-dir&gt;/config/kibana.yml. | Extract the OpenSearch Dashboards tarball to a new directory. | Port your settings from &lt;kibana-dir&gt;/config/kibana.yml to &lt;dashboards-dir&gt;/config/opensearch_dashboards.yml. In general, settings with elasticsearch in their names map to opensearch (for example, elasticsearch.shardTimeout and opensearch.shardTimeout) and settings with kibana in their names map to opensearchDashboards (for example, kibana.defaultAppId and opensearchDashboards.defaultAppId). Most other settings use the same names. For a full list of OpenSearch Dashboards settings, see opensearch_dashboards.yml. | If your OpenSearch cluster uses the Security plugin, preserve and modify the default settings in opensearch_dashboards.yml, particularly opensearch.username and opensearch.password. If you disabled the Security plugin on your OpenSearch cluster, remove or comment out all opensearch_security settings. Then run rm -rf plugins/security-dashboards/ to remove the Security plugin. | Start OpenSearch Dashboards: ./bin/opensearch-dashboards . | Log in, and verify that your saved searches, visualizations, and dashboards are present. | . ",
    "url": "https://vagimeli.github.io/upgrade-to/dashboards-upgrade-to/",
    "relUrl": "/upgrade-to/dashboards-upgrade-to/"
  },"133": {
    "doc": "Migrating Docker clusters to OpenSearch",
    "title": "Migrating Docker clusters to OpenSearch",
    "content": "If you use a container orchestration system like Kubernetes (or manage your containers manually) and want to avoid downtime, think of the process not as an upgrade of each node, but as a decommissioning and replacement of each node. One by one, add OpenSearch nodes to the cluster and remove Elasticsearch OSS nodes, pointing to existing data volumes as necessary and allowing time for all indexes to return to a green status prior to proceeding. If you use Docker Compose, we highly recommend that you perform what amounts to a cluster restart upgrade. Update your cluster configuration with new images, new settings, and new environment variables, and test it. Then stop and start the cluster. This process requires downtime, but takes very few steps and lets you continue to treat the cluster as a single entity that you can reliably deploy and redeploy. The most important step is to leave your data volumes intact. Don’t run docker-compose down -v. ",
    "url": "https://vagimeli.github.io/upgrade-to/docker-upgrade-to/",
    "relUrl": "/upgrade-to/docker-upgrade-to/"
  },"134": {
    "doc": "About the migration process",
    "title": "About the migration process",
    "content": "The process of migrating from Elasticsearch OSS to OpenSearch varies depending on your current version of Elasticsearch OSS, installation type, tolerance for downtime, and cost-sensitivity. Rather than concrete steps to cover every situation, we have general guidance for the process. Three approaches exist: . | Use a snapshot to migrate your Elasticsearch OSS data to a new OpenSearch cluster. This method may incur downtime. | Perform a restart upgrade or a rolling upgrade on your existing nodes. A restart upgrade involves upgrading the entire cluster and restarting it, whereas a rolling upgrade requires upgrading and restarting nodes in the cluster one by one. | Replace existing Elasticsearch OSS nodes with new OpenSearch nodes. Node replacement is most popular when upgrading Docker clusters. | . Regardless of your approach, to safeguard against data loss, we recommend that you take a snapshot of all indexes prior to any migration. If your existing clients include a version check, such as recent versions of Logstash OSS and Filebeat OSS, check compatibility before upgrading. ",
    "url": "https://vagimeli.github.io/upgrade-to/index/",
    "relUrl": "/upgrade-to/index/"
  },"135": {
    "doc": "About the migration process",
    "title": "Upgrading from Open Distro",
    "content": "For steps to upgrade from Open Distro to OpenSearch, refer to the blog post How To: Upgrade from Open Distro to OpenSearch. ",
    "url": "https://vagimeli.github.io/upgrade-to/index/#upgrading-from-open-distro",
    "relUrl": "/upgrade-to/index/#upgrading-from-open-distro"
  },"136": {
    "doc": "Using snapshots to migrate data",
    "title": "Using snapshots to migrate data",
    "content": "One popular approach is to take a snapshot of your Elasticsearch OSS 6.x or 7.x indexes, create an OpenSearch cluster, restore the snapshot on the new cluster, and point your clients to the new host. The snapshot approach can mean running two clusters in parallel, but lets you validate that the OpenSearch cluster is working in a way that meets your needs prior to modifying the Elasticsearch OSS cluster. ",
    "url": "https://vagimeli.github.io/upgrade-to/snapshot-migrate/",
    "relUrl": "/upgrade-to/snapshot-migrate/"
  },"137": {
    "doc": "Migrating from Elasticsearch OSS to OpenSearch",
    "title": "Migrating from Elasticsearch OSS to OpenSearch",
    "content": "If you want to migrate from an existing Elasticsearch OSS cluster to OpenSearch and find the snapshot approach unappealing, you can migrate your existing nodes from Elasticsearch OSS to OpenSearch. If your existing cluster runs an older version of Elasticsearch OSS, the first step is to upgrade to version 6.x or 7.x. Elasticsearch OSS supports two types of upgrades: rolling and cluster restart. | Rolling upgrades let you shut down one node at a time for minimal disruption of service. Rolling upgrades work between minor versions (for example, 6.5 to 6.8) and also support a single path to the next major version (for example, 6.8 to 7.10.2). Performing these upgrades might require intermediate upgrades to arrive at your desired version and can affect cluster performance as nodes leave and rejoin, but the cluster remains available throughout the process. | Cluster restart upgrades require you to shut down all nodes, perform the upgrade, and restart the cluster. Cluster restart upgrades work between minor versions (for example, 6.5 to 6.8) and the next major version (for example, 6.x to 7.10.2). Cluster restart upgrades are faster to perform and require fewer intermediate upgrades, but require downtime. | . ",
    "url": "https://vagimeli.github.io/upgrade-to/upgrade-to/",
    "relUrl": "/upgrade-to/upgrade-to/"
  },"138": {
    "doc": "Migrating from Elasticsearch OSS to OpenSearch",
    "title": "Migration paths",
    "content": "| Elasticsearch OSS version | Rolling upgrade path | Cluster restart upgrade path | . | 5.x | Upgrade to 5.6, upgrade to 6.8, reindex all 5.x indexes, upgrade to 7.10.2, and migrate to OpenSearch. | Upgrade to 6.8, reindex all 5.x indexes, and migrate to OpenSearch. | . | 6.x | Upgrade to 6.8, upgrade to 7.10.2, and migrate to OpenSearch. | Migrate to OpenSearch. | . | 7.x | Migrate to OpenSearch. | Migrate to OpenSearch. | . If you are migrating an Open Distro for Elasticsearch cluster, we recommend first upgrading to ODFE 1.13 and then migrating to OpenSearch. ",
    "url": "https://vagimeli.github.io/upgrade-to/upgrade-to/#migration-paths",
    "relUrl": "/upgrade-to/upgrade-to/#migration-paths"
  },"139": {
    "doc": "Migrating from Elasticsearch OSS to OpenSearch",
    "title": "Upgrade Elasticsearch OSS",
    "content": ". | Disable shard allocation to prevent Elasticsearch OSS from replicating shards as you shut down nodes: . PUT _cluster/settings { \"persistent\": { \"cluster.routing.allocation.enable\": \"primaries\" } } . | Stop Elasticsearch OSS on one node (rolling upgrade) or all nodes (cluster restart upgrade). On Linux distributions that use systemd, use this command: . sudo systemctl stop elasticsearch.service . For tarball installations, find the process ID (ps aux) and kill it (kill &lt;pid&gt;). | Upgrade the node (rolling) or all nodes (cluster restart). The exact command varies by package manager, but likely looks something like this: . sudo yum install elasticsearch-oss-7.10.2 --enablerepo=elasticsearch . For tarball installations, extract to a new directory to ensure you do not overwrite your config, data, and logs directories. Ideally, these directories should have their own, independent paths and not be colocated with the Elasticsearch application directory. Then set the ES_PATH_CONF environment variable to the directory that contains elasticsearch.yml (for example, /etc/elasticesarch/). In elasticsearch.yml, set path.data and path.logs to your data and logs directories (for example, /var/lib/elasticsearch and /var/log/opensearch). | Restart Elasticsearch OSS on the node (rolling) or all nodes (cluster restart). On Linux distributions that use systemd, use this command: . sudo systemctl start elasticsearch.service . For tarball installations, run ./bin/elasticsearch -d. | Wait for the node to rejoin the cluster (rolling) or for the cluster to start (cluster restart). Check the _nodes summary to verify that all nodes are available and running the expected version: . # Elasticsearch OSS curl -XGET 'localhost:9200/_nodes/_all?pretty=true' # Open Distro for Elasticsearch with Security plugin enabled curl -XGET 'https://localhost:9200/_nodes/_all?pretty=true' -u 'admin:admin' -k . Specifically, check the nodes.&lt;node-id&gt;.version portion of the response. Also check _cat/indices?v for a green status on all indexes. | (Rolling) Repeat steps 2–5 until all nodes are using the new version. | After all nodes are using the new version, re-enable shard allocation: . PUT _cluster/settings { \"persistent\": { \"cluster.routing.allocation.enable\": \"all\" } } . | If you upgraded from 5.x to 6.x, reindex all indexes. | Repeat all steps as necessary until you arrive at your desired Elasticsearch OSS version. | . ",
    "url": "https://vagimeli.github.io/upgrade-to/upgrade-to/#upgrade-elasticsearch-oss",
    "relUrl": "/upgrade-to/upgrade-to/#upgrade-elasticsearch-oss"
  },"140": {
    "doc": "Migrating from Elasticsearch OSS to OpenSearch",
    "title": "Migrate to OpenSearch",
    "content": ". | Disable shard allocation to prevent Elasticsearch OSS from replicating shards as you shut down nodes: . PUT _cluster/settings { \"persistent\": { \"cluster.routing.allocation.enable\": \"primaries\" } } . | Stop Elasticsearch OSS on one node (rolling upgrade) or all nodes (cluster restart upgrade). On Linux distributions that use systemd, use this command: . sudo systemctl stop elasticsearch.service . For tarball installations, find the process ID (ps aux) and kill it (kill &lt;pid&gt;). | Upgrade the node (rolling) or all nodes (cluster restart). | Extract the OpenSearch tarball to a new directory to ensure you do not overwrite your Elasticsearch OSS config, data, and logs directories. | (Optional) Copy or move your Elasticsearch OSS data and logs directories to new paths. For example, you might move /var/lib/elasticsearch to /var/lib/opensearch. | Set the OPENSEARCH_PATH_CONF environment variable to the directory that contains opensearch.yml (for example, /etc/opensearch). | In opensearch.yml, set path.data and path.logs. You might also want to disable the Security plugin for now. opensearch.yml might look something like this: . path.data: /var/lib/opensearch path.logs: /var/log/opensearch plugins.security.disabled: true . | Port your settings from elasticsearch.yml to opensearch.yml. Most settings use the same names. At a minimum, specify cluster.name, node.name, discovery.seed_hosts, and cluster.initial_cluster_manager_nodes. | (Optional) If you’re actively connecting to the cluster with legacy clients that check for a particular version number, such as Logstash OSS, add a compatibility setting to opensearch.yml: . compatibility.override_main_response_version: true . | (Optional) Add your certificates to your config directory, add them to opensearch.yml, and initialize the Security plugin. | . | Start OpenSearch on the node (rolling) or all nodes (cluster restart). For the tarball, run ./bin/opensearch -d. | Wait for the OpenSearch node to rejoin the cluster (rolling) or for the cluster to start (cluster restart). Check the _nodes summary to verify that all nodes are available and running the expected version: . # Security plugin disabled curl -XGET 'localhost:9200/_nodes/_all?pretty=true' # Security plugin enabled curl -XGET -k -u 'admin:admin' 'https://localhost:9200/_nodes/_all?pretty=true' . Specifically, check the nodes.&lt;node-id&gt;.version portion of the response. Also check _cat/indices?v for a green status on all indexes. | (Rolling) Repeat steps 2–5 until all nodes are using OpenSearch. | After all nodes are using the new version, re-enable shard allocation: . PUT _cluster/settings { \"persistent\": { \"cluster.routing.allocation.enable\": \"all\" } } . | . ",
    "url": "https://vagimeli.github.io/upgrade-to/upgrade-to/#migrate-to-opensearch",
    "relUrl": "/upgrade-to/upgrade-to/#migrate-to-opensearch"
  },"141": {
    "doc": "Migrating from Elasticsearch OSS to OpenSearch",
    "title": "Upgrade tool",
    "content": "The opensearch-upgrade tool lets you automate some of the steps in Migrate to OpenSearch, eliminating the need for error-prone manual operations. The opensearch-upgrade tool performs the following functions: . | Imports any existing configurations and applies it to the new installation of OpenSearch. | Installs any existing core plugins. | . Limitations . The opensearch-upgrade tool doesn’t perform an end-to-end upgrade: . | You need to run the tool on each node of the cluster individually as part of the upgrade process. | The tool doesn’t provide a rollback option after you’ve upgraded a node, so make sure you follow best practices and take backups. | You must install all community plugins (if available) manually. | The tool only validates any keystore settings at service start-up time, so you must manually remove any unsupported settings for the service to start. | . Using the upgrade tool . To perform a rolling upgrade using the OpenSearch tarball distribution: . Check Migration paths to make sure that the version you’re upgrading to is supported and whether you need to upgrade to a supported Elasticsearch OSS version first. | Disable shard allocation to prevent Elasticsearch OSS from replicating shards as you shut down nodes: . PUT _cluster/settings { \"persistent\": { \"cluster.routing.allocation.enable\": \"primaries\" } } . | On any one of the nodes, download and extract the OpenSearch tarball to a new directory. | Make sure the following environment variables are set: . | ES_HOME - Path to the existing Elasticsearch installation home. export ES_HOME=/home/workspace/upgrade-demo/node1/elasticsearch-7.10.2 . | ES_PATH_CONF - Path to the existing Elasticsearch config directory. export ES_PATH_CONF=/home/workspace/upgrade-demo/node1/os-config . | OPENSEARCH_HOME - Path to the OpenSearch installation home. export OPENSEARCH_HOME=/home/workspace/upgrade-demo/node1/opensearch-1.0.0 . | OPENSEARCH_PATH_CONF - Path to the OpenSearch config directory. export OPENSEARCH_PATH_CONF=/home/workspace/upgrade-demo/node1/opensearch-config . | . | The opensearch-upgrade tool is in the bin directory of the distribution. Run the following command from the distribution home: . Make sure you run this tool as the same user running the current Elasticsearch service./bin/opensearch-upgrade . | Stop Elasticsearch OSS on the node. On Linux distributions that use systemd, use this command: . sudo systemctl stop elasticsearch.service . For tarball installations, find the process ID (ps aux) and kill it (kill &lt;pid&gt;). | Start OpenSearch on the node: ./bin/opensearch -d. | Repeat steps 2–6 until all nodes are using the new version. | After all nodes are using the new version, re-enable shard allocation: . PUT _cluster/settings { \"persistent\": { \"cluster.routing.allocation.enable\": \"all\" } } . | . How it works . Behind the scenes, the opensearch-upgrade tool performs the following tasks in sequence: . | Looks for a valid Elasticsearch installation on the current node. After it finds the installation, it reads the elasticsearch.yml file to get the endpoint details and connects to the locally running Elasticsearch service. If the tool can’t find an Elasticsearch installation, it tries to get the path from the ES_HOME location. | Verifies if the existing version of Elasticsearch is compatible with the OpenSearch version. It prints a summary of the information gathered to the console and prompts you for a confirmation to proceed. | Imports the settings from the elasticsearch.yml config file into the opensearch.yml config file. | Copies across any custom JVM options from the $ES_PATH_CONF/jvm.options.d directory into the $OPENSEARCH_PATH_CONF/jvm.options.d directory. Similarly, it also imports the logging configurations from the $ES_PATH_CONF/log4j2.properties file into the $OPENSEARCH_PATH_CONF/log4j2.properties file. | Installs the core plugins that you’ve currently installed in the $ES_HOME/plugins directory. You must install all other third-party community plugins manually. | Imports the secure settings from the elasticsearch.keystore file (if any) into the opensearch.keystore file. If the keystore file is password protected, the opensearch-upgrade tool prompts you to enter the password. | . ",
    "url": "https://vagimeli.github.io/upgrade-to/upgrade-to/#upgrade-tool",
    "relUrl": "/upgrade-to/upgrade-to/#upgrade-tool"
  },"142": {
    "doc": "Data streams",
    "title": "Data streams",
    "content": "If you’re ingesting continuously generated time-series data such as logs, events, and metrics into OpenSearch, you’re likely in a scenario where the number of documents grows rapidly and you don’t need to update older documents. A typical workflow to manage time-series data involves multiple steps, such as creating a rollover index alias, defining a write index, and defining common mappings and settings for the backing indexes. Data streams simplify this process and enforce a setup that best suits time-series data, such as being designed primarily for append-only data and ensuring that each document has a timestamp field. A data stream is internally composed of multiple backing indexes. Search requests are routed to all the backing indexes, while indexing requests are routed to the latest write index. ISM policies let you automatically handle index rollovers or deletions. ",
    "url": "https://vagimeli.github.io/im-plugin/data-streams/",
    "relUrl": "/im-plugin/data-streams/"
  },"143": {
    "doc": "Data streams",
    "title": "Get started with data streams",
    "content": "Step 1: Create an index template . To create a data stream, you first need to create an index template that configures a set of indexes as a data stream. The data_stream object indicates that it’s a data stream and not a regular index template. The index pattern matches with the name of the data stream: . PUT _index_template/logs-template { \"index_patterns\": [ \"my-data-stream\", \"logs-*\" ], \"data_stream\": {}, \"priority\": 100 } . In this case, each ingested document must have an @timestamp field. You also have the ability to define your own custom timestamp field as a property in the data_stream object. You can also add index mappings and other settings here, just as you would for a regular index template. PUT _index_template/logs-template-nginx { \"index_patterns\": \"logs-nginx\", \"data_stream\": { \"timestamp_field\": { \"name\": \"request_time\" } }, \"priority\": 200, \"template\": { \"settings\": { \"number_of_shards\": 1, \"number_of_replicas\": 0 } } } . In this case, logs-nginx index matches both the logs-template and logs-template-nginx templates. When you have a tie, OpenSearch selects the matching index template with the higher priority value. Step 2: Create a data stream . After you create an index template, you can create a data stream. You can use the data stream API to explicitly create a data stream. The data stream API initializes the first backing index: . PUT _data_stream/logs-redis PUT _data_stream/logs-nginx . You can also directly start ingesting data without creating a data stream. Because we have a matching index template with a data_stream object, OpenSearch automatically creates the data stream: . POST logs-staging/_doc { \"message\": \"login attempt failed\", \"@timestamp\": \"2013-03-01T00:00:00\" } . To see information about a specific data stream: . GET _data_stream/logs-nginx . Example response . { \"data_streams\" : [ { \"name\" : \"logs-nginx\", \"timestamp_field\" : { \"name\" : \"request_time\" }, \"indices\" : [ { \"index_name\" : \".ds-logs-nginx-000001\", \"index_uuid\" : \"-VhmuhrQQ6ipYCmBhn6vLw\" } ], \"generation\" : 1, \"status\" : \"GREEN\", \"template\" : \"logs-template-nginx\" } ] } . You can see the name of the timestamp field, the list of the backing indexes, and the template that’s used to create the data stream. You can also see the health of the data stream, which represents the lowest status of all its backing indexes. To see more insights about the data stream, use the _stats endpoint: . GET _data_stream/logs-nginx/_stats . Example response . { \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"failed\" : 0 }, \"data_stream_count\" : 1, \"backing_indices\" : 1, \"total_store_size_bytes\" : 208, \"data_streams\" : [ { \"data_stream\" : \"logs-nginx\", \"backing_indices\" : 1, \"store_size_bytes\" : 208, \"maximum_timestamp\" : 0 } ] } . To see information about all data streams, use the following request: . GET _data_stream . Step 3: Ingest data into the data stream . To ingest data into a data stream, you can use the regular indexing APIs. Make sure every document that you index has a timestamp field. If you try to ingest a document that doesn’t have a timestamp field, you get an error. POST logs-redis/_doc { \"message\": \"login attempt\", \"@timestamp\": \"2013-03-01T00:00:00\" } . Step 4: Searching a data stream . You can search a data stream just like you search a regular index or an index alias. The search operation applies to all of the backing indexes (all data present in the stream). GET logs-redis/_search { \"query\": { \"match\": { \"message\": \"login\" } } } . Example response . { \"took\" : 514, \"timed_out\" : false, \"_shards\" : { \"total\" : 5, \"successful\" : 5, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 1, \"relation\" : \"eq\" }, \"max_score\" : 0.2876821, \"hits\" : [ { \"_index\" : \".ds-logs-redis-000001\", \"_type\" : \"_doc\", \"_id\" : \"-rhVmXoBL6BAVWH3mMpC\", \"_score\" : 0.2876821, \"_source\" : { \"message\" : \"login attempt\", \"@timestamp\" : \"2013-03-01T00:00:00\" } } ] } } . Step 5: Rollover a data stream . A rollover operation creates a new backing index that becomes the data stream’s new write index. To perform manual rollover operation on the data stream: . POST logs-redis/_rollover . Example response . { \"acknowledged\" : true, \"shards_acknowledged\" : true, \"old_index\" : \".ds-logs-redis-000001\", \"new_index\" : \".ds-logs-redis-000002\", \"rolled_over\" : true, \"dry_run\" : false, \"conditions\" : { } } . If you now perform a GET operation on the logs-redis data stream, you see that the generation ID is incremented from 1 to 2. You can also set up an Index State Management (ISM) policy to automate the rollover process for the data stream. The ISM policy is applied to the backing indexes at the time of their creation. When you associate a policy to a data stream, it only affects the future backing indexes of that data stream. You also don’t need to provide the rollover_alias setting, because the ISM policy infers this information from the backing index. Step 6: Manage data streams in OpenSearch Dashboards . To manage data streams from OpenSearch Dashboards, open OpenSearch Dashboards, choose Index Management, select Indices or Policy managed indices. You see a toggle switch for data streams that you can use to show or hide indexes belonging to a data stream. When you enable this switch, you see a data stream multi-select dropdown menu that you can use for filtering data streams. You also see a data stream column that shows you the name of the data stream the index is contained in. You can select one or more data streams and apply an ISM policy on them. You can also apply a policy on any individual backing index. You can performing visualizations on a data stream just like you would on a regular index or index alias. Step 7: Delete a data stream . The delete operation first deletes the backing indexes of a data stream and then deletes the data stream itself. To delete a data stream and all of its hidden backing indexes: . DELETE _data_stream/&lt;name_of_data_stream&gt; . You can use wildcards to delete more than one data stream. We recommend deleting data from a data stream using an ISM policy. You can also use asynchronous search, SQL, and PPL to query your data stream directly. You can also use the Security plugin to define granular permissions for the data stream name. ",
    "url": "https://vagimeli.github.io/im-plugin/data-streams/#get-started-with-data-streams",
    "relUrl": "/im-plugin/data-streams/#get-started-with-data-streams"
  },"144": {
    "doc": "Index aliases",
    "title": "Index aliases",
    "content": "An alias is a virtual index name that can point to one or more indexes. If your data is spread across multiple indexes, rather than keeping track of which indexes to query, you can create an alias and query it instead. For example, if you’re storing logs into indexes based on the month and you frequently query the logs for the previous two months, you can create a last_2_months alias and update the indexes it points to each month. Because you can change the indexes an alias points to at any time, referring to indexes using aliases in your applications allows you to reindex your data without any downtime. . | Create aliases | Add or remove indexes | Manage aliases | Add aliases at index creation | Create filtered aliases | Index alias options | Delete aliases | . ",
    "url": "https://vagimeli.github.io/im-plugin/index-alias/",
    "relUrl": "/im-plugin/index-alias/"
  },"145": {
    "doc": "Index aliases",
    "title": "Create aliases",
    "content": "To create an alias, use a POST request: . POST _aliases . Use the actions method to specify the list of actions that you want to perform. This command creates an alias named alias1 and adds index-1 to this alias: . POST _aliases { \"actions\": [ { \"add\": { \"index\": \"index-1\", \"alias\": \"alias1\" } } ] } . You should see the following response: . { \"acknowledged\": true } . If this request fails, make sure the index that you’re adding to the alias already exists. You can also create an alias using one of the following requests: . PUT &lt;index&gt;/_aliases/&lt;alias name&gt; POST &lt;index&gt;/_aliases/&lt;alias name&gt; PUT &lt;index&gt;/_alias/&lt;alias name&gt; POST &lt;index&gt;/_alias/&lt;alias name&gt; . The &lt;index&gt; in the above requests can be an index name, a comma-separated list of index names, or a wildcard expression. Use _all to refer to all indexes. To check if alias1 refers to index-1, run one of the following commands: . GET /_alias/alias1 GET /index-1/_alias/alias1 . To get the mappings and settings information of the indexes that the alias references, run the following command: . GET alias1 . ",
    "url": "https://vagimeli.github.io/im-plugin/index-alias/#create-aliases",
    "relUrl": "/im-plugin/index-alias/#create-aliases"
  },"146": {
    "doc": "Index aliases",
    "title": "Add or remove indexes",
    "content": "You can perform multiple actions in the same _aliases operation. For example, the following command removes index-1 and adds index-2 to alias1: . POST _aliases { \"actions\": [ { \"remove\": { \"index\": \"index-1\", \"alias\": \"alias1\" } }, { \"add\": { \"index\": \"index-2\", \"alias\": \"alias1\" } } ] } . The add and remove actions occur atomically, which means that at no point will alias1 point to both index-1 and index-2. You can also add indexes based on an index pattern: . POST _aliases { \"actions\": [ { \"add\": { \"index\": \"index*\", \"alias\": \"alias1\" } } ] } . ",
    "url": "https://vagimeli.github.io/im-plugin/index-alias/#add-or-remove-indexes",
    "relUrl": "/im-plugin/index-alias/#add-or-remove-indexes"
  },"147": {
    "doc": "Index aliases",
    "title": "Manage aliases",
    "content": "To list the mapping of aliases to indexes, run the following command: . GET _cat/aliases?v . Example response . alias index filter routing.index routing.search alias1 index-1 * - - . To check which indexes an alias points to, run the following command: . GET _alias/alias1 . Example response . { \"index-2\": { \"aliases\": { \"alias1\": {} } } } . Conversely, to find which alias points to a specific index, run the following command: . GET /index-2/_alias/* . To get all index names and their aliases, run the following command: . GET /_alias . To check if an alias exists, run one of the following commands: . HEAD /alias1/_alias/ HEAD /_alias/alias1/ HEAD index-1/_alias/alias1/ . ",
    "url": "https://vagimeli.github.io/im-plugin/index-alias/#manage-aliases",
    "relUrl": "/im-plugin/index-alias/#manage-aliases"
  },"148": {
    "doc": "Index aliases",
    "title": "Add aliases at index creation",
    "content": "You can add an index to an alias as you create the index: . PUT index-1 { \"aliases\": { \"alias1\": {} } } . ",
    "url": "https://vagimeli.github.io/im-plugin/index-alias/#add-aliases-at-index-creation",
    "relUrl": "/im-plugin/index-alias/#add-aliases-at-index-creation"
  },"149": {
    "doc": "Index aliases",
    "title": "Create filtered aliases",
    "content": "You can create a filtered alias to access a subset of documents or fields from the underlying indexes. This command adds only a specific timestamp field to alias1: . POST _aliases { \"actions\": [ { \"add\": { \"index\": \"index-1\", \"alias\": \"alias1\", \"filter\": { \"term\": { \"timestamp\": \"1574641891142\" } } } } ] } . ",
    "url": "https://vagimeli.github.io/im-plugin/index-alias/#create-filtered-aliases",
    "relUrl": "/im-plugin/index-alias/#create-filtered-aliases"
  },"150": {
    "doc": "Index aliases",
    "title": "Index alias options",
    "content": "You can specify the options shown in the following table. | Option | Valid values | Description | Required | . | index | String | The name of the index that the alias points to. | Yes | . | alias | String | The name of the alias. | No | . | filter | Object | Add a filter to the alias. | No | . | routing | String | Limit search to an associated shard value. You can specify search_routing and index_routing independently. | No | . | is_write_index | String | Specify the index that accepts any write operations to the alias. If this value is not specified, then no write operations are allowed. | No | . ",
    "url": "https://vagimeli.github.io/im-plugin/index-alias/#index-alias-options",
    "relUrl": "/im-plugin/index-alias/#index-alias-options"
  },"151": {
    "doc": "Index aliases",
    "title": "Delete aliases",
    "content": "To delete one or more aliases from an index, use the following request: . DELETE &lt;index&gt;/_alias/&lt;alias&gt; DELETE &lt;index&gt;/_aliases/&lt;alias&gt; . Both &lt;index&gt; and &lt;alias&gt; in the above request support comma-separated lists and wildcard expressions. Use _all in place of &lt;alias&gt; to delete all aliases for the indexes listed in &lt;index&gt;. For example, if alias1 refers to index-1 and index-2, you can run the following command to remove alias1 from index-1: . DELETE index-1/_alias/alias1 . After you run the request above, alias1 no longer refers to index-1, but still refers to index-2. ",
    "url": "https://vagimeli.github.io/im-plugin/index-alias/#delete-aliases",
    "relUrl": "/im-plugin/index-alias/#delete-aliases"
  },"152": {
    "doc": "Index rollups",
    "title": "Index rollups",
    "content": "Time series data increases storage costs, strains cluster health, and slows down aggregations over time. Index rollup lets you periodically reduce data granularity by rolling up old data into summarized indexes. You pick the fields that interest you and use index rollup to create a new index with only those fields aggregated into coarser time buckets. You can store months or years of historical data at a fraction of the cost with the same query performance. For example, say you collect CPU consumption data every five seconds and store it on a hot node. Instead of moving older data to a read-only warm node, you can roll up or compress this data with only the average CPU consumption per day or with a 10% decrease in its interval every week. You can use index rollup in three ways: . | Use the index rollup API for an on-demand index rollup job that operates on an index that’s not being actively ingested such as a rolled-over index. For example, you can perform an index rollup operation to reduce data collected at a five minute interval to a weekly average for trend analysis. | Use the OpenSearch Dashboards UI to create an index rollup job that runs on a defined schedule. You can also set it up to roll up your indexes as it’s being actively ingested. For example, you can continuously roll up Logstash indexes from a five second interval to a one hour interval. | Specify the index rollup job as an ISM action for complete index management. This allows you to roll up an index after a certain event such as a rollover, index age reaching a certain point, index becoming read-only, and so on. You can also have rollover and index rollup jobs running in sequence, where the rollover first moves the current index to a warm node and then the index rollup job creates a new index with the minimized data on the hot node. | . ",
    "url": "https://vagimeli.github.io/im-plugin/index-rollups/index/",
    "relUrl": "/im-plugin/index-rollups/index/"
  },"153": {
    "doc": "Index rollups",
    "title": "Create an Index Rollup Job",
    "content": "To get started, choose Index Management in OpenSearch Dashboards. Select Rollup Jobs and choose Create rollup job. Step 1: Set up indexes . | In the Job name and description section, specify a unique name and an optional description for the index rollup job. | In the Indices section, select the source and target index. The source index is the one that you want to roll up. The source index remains as is, the index rollup job creates a new index referred to as a target index. The target index is where the index rollup results are saved. For target index, you can either type in a name for a new index or you select an existing index. | Choose Next | . After you create an index rollup job, you can’t change your index selections. Step 2: Define aggregations and metrics . Select the attributes with the aggregations (terms and histograms) and metrics (avg, sum, max, min, and value count) that you want to roll up. Make sure you don’t add a lot of highly granular attributes, because you won’t save much space. For example, consider a dataset of cities and demographics within those cities. You can aggregate based on cities and specify demographics within a city as metrics. The order in which you select attributes is critical. A city followed by a demographic is different from a demographic followed by a city. | In the Time aggregation section, select a timestamp field. Choose between a Fixed or Calendar interval type and specify the interval and timezone. The index rollup job uses this information to create a date histogram for the timestamp field. | (Optional) Add additional aggregations for each field. You can choose terms aggregation for all field types and histogram aggregation only for numeric fields. | (Optional) Add additional metrics for each field. You can choose between All, Min, Max, Sum, Avg, or Value Count. | Choose Next. | . Step 3: Specify schedule . Specify a schedule to roll up your indexes as it’s being ingested. The index rollup job is enabled by default. | Specify if the data is continuous or not. | For roll up execution frequency, select Define by fixed interval and specify the Rollup interval and the time unit or Define by cron expression and add in a cron expression to select the interval. To learn how to define a cron expression, see Alerting. | Specify the number of pages per execution process. A larger number means faster execution and more cost for memory. | (Optional) Add a delay to the roll up executions. This is the amount of time the job waits for data ingestion to accommodate any processing time. For example, if you set this value to 10 minutes, an index rollup that executes at 2 PM to roll up 1 PM to 2 PM of data starts at 2:10 PM. | Choose Next. | . Step 4: Review and create . Review your configuration and select Create. Step 5: Search the target index . You can use the standard _search API to search the target index. Make sure that the query matches the constraints of the target index. For example, if you don’t set up terms aggregations on a field, you don’t receive results for terms aggregations. If you don’t set up the maximum aggregations, you don’t receive results for maximum aggregations. You can’t access the internal structure of the data in the target index because the plugin automatically rewrites the query in the background to suit the target index. This is to make sure you can use the same query for the source and target index. To query the target index, set size to 0: . GET target_index/_search { \"size\": 0, \"query\": { \"match_all\": {} }, \"aggs\": { \"avg_cpu\": { \"avg\": { \"field\": \"cpu_usage\" } } } } . Consider a scenario where you collect rolled up data from 1 PM to 9 PM in hourly intervals and live data from 7 PM to 11 PM in minutely intervals. If you execute an aggregation over these in the same query, for 7 PM to 9 PM, you see an overlap of both rolled up data and live data because they get counted twice in the aggregations. ",
    "url": "https://vagimeli.github.io/im-plugin/index-rollups/index/#create-an-index-rollup-job",
    "relUrl": "/im-plugin/index-rollups/index/#create-an-index-rollup-job"
  },"154": {
    "doc": "Index rollups",
    "title": "Sample Walkthrough",
    "content": "This walkthrough uses the OpenSearch Dashboards sample e-commerce data. To add that sample data, log in to OpenSearch Dashboards, choose Home and Try our sample data. For Sample eCommerce orders, choose Add data. Then run a search: . GET opensearch_dashboards_sample_data_ecommerce/_search . Example response . { \"took\": 23, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 4675, \"relation\": \"eq\" }, \"max_score\": 1, \"hits\": [ { \"_index\": \"opensearch_dashboards_sample_data_ecommerce\", \"_type\": \"_doc\", \"_id\": \"jlMlwXcBQVLeQPrkC_kQ\", \"_score\": 1, \"_source\": { \"category\": [ \"Women's Clothing\", \"Women's Accessories\" ], \"currency\": \"EUR\", \"customer_first_name\": \"Selena\", \"customer_full_name\": \"Selena Mullins\", \"customer_gender\": \"FEMALE\", \"customer_id\": 42, \"customer_last_name\": \"Mullins\", \"customer_phone\": \"\", \"day_of_week\": \"Saturday\", \"day_of_week_i\": 5, \"email\": \"selena@mullins-family.zzz\", \"manufacturer\": [ \"Tigress Enterprises\" ], \"order_date\": \"2021-02-27T03:56:10+00:00\", \"order_id\": 581553, \"products\": [ { \"base_price\": 24.99, \"discount_percentage\": 0, \"quantity\": 1, \"manufacturer\": \"Tigress Enterprises\", \"tax_amount\": 0, \"product_id\": 19240, \"category\": \"Women's Clothing\", \"sku\": \"ZO0064500645\", \"taxless_price\": 24.99, \"unit_discount_amount\": 0, \"min_price\": 12.99, \"_id\": \"sold_product_581553_19240\", \"discount_amount\": 0, \"created_on\": \"2016-12-24T03:56:10+00:00\", \"product_name\": \"Blouse - port royal\", \"price\": 24.99, \"taxful_price\": 24.99, \"base_unit_price\": 24.99 }, { \"base_price\": 10.99, \"discount_percentage\": 0, \"quantity\": 1, \"manufacturer\": \"Tigress Enterprises\", \"tax_amount\": 0, \"product_id\": 17221, \"category\": \"Women's Accessories\", \"sku\": \"ZO0085200852\", \"taxless_price\": 10.99, \"unit_discount_amount\": 0, \"min_price\": 5.06, \"_id\": \"sold_product_581553_17221\", \"discount_amount\": 0, \"created_on\": \"2016-12-24T03:56:10+00:00\", \"product_name\": \"Snood - rose\", \"price\": 10.99, \"taxful_price\": 10.99, \"base_unit_price\": 10.99 } ], \"sku\": [ \"ZO0064500645\", \"ZO0085200852\" ], \"taxful_total_price\": 35.98, \"taxless_total_price\": 35.98, \"total_quantity\": 2, \"total_unique_products\": 2, \"type\": \"order\", \"user\": \"selena\", \"geoip\": { \"country_iso_code\": \"MA\", \"location\": { \"lon\": -8, \"lat\": 31.6 }, \"region_name\": \"Marrakech-Tensift-Al Haouz\", \"continent_name\": \"Africa\", \"city_name\": \"Marrakesh\" }, \"event\": { \"dataset\": \"sample_ecommerce\" } } } ] } } ... Create an index rollup job. This example picks the order_date, customer_gender, geoip.city_name, geoip.region_name, and day_of_week fields and rolls them into an example_rollup target index: . PUT _plugins/_rollup/jobs/example { \"rollup\": { \"enabled\": true, \"schedule\": { \"interval\": { \"period\": 1, \"unit\": \"Minutes\", \"start_time\": 1602100553 } }, \"last_updated_time\": 1602100553, \"description\": \"An example policy that rolls up the sample ecommerce data\", \"source_index\": \"opensearch_dashboards_sample_data_ecommerce\", \"target_index\": \"example_rollup\", \"page_size\": 1000, \"delay\": 0, \"continuous\": false, \"dimensions\": [ { \"date_histogram\": { \"source_field\": \"order_date\", \"fixed_interval\": \"60m\", \"timezone\": \"America/Los_Angeles\" } }, { \"terms\": { \"source_field\": \"customer_gender\" } }, { \"terms\": { \"source_field\": \"geoip.city_name\" } }, { \"terms\": { \"source_field\": \"geoip.region_name\" } }, { \"terms\": { \"source_field\": \"day_of_week\" } } ], \"metrics\": [ { \"source_field\": \"taxless_total_price\", \"metrics\": [ { \"avg\": {} }, { \"sum\": {} }, { \"max\": {} }, { \"min\": {} }, { \"value_count\": {} } ] }, { \"source_field\": \"total_quantity\", \"metrics\": [ { \"avg\": {} }, { \"max\": {} } ] } ] } } . You can query the example_rollup index for the terms aggregations on the fields set up in the rollup job. You get back the same response that you would on the original opensearch_dashboards_sample_data_ecommerce source index: . POST example_rollup/_search { \"size\": 0, \"query\": { \"bool\": { \"must\": {\"term\": { \"geoip.region_name\": \"California\" } } } }, \"aggregations\": { \"daily_numbers\": { \"terms\": { \"field\": \"day_of_week\" }, \"aggs\": { \"per_city\": { \"terms\": { \"field\": \"geoip.city_name\" }, \"aggregations\": { \"average quantity\": { \"avg\": { \"field\": \"total_quantity\" } } } }, \"total_revenue\": { \"sum\": { \"field\": \"taxless_total_price\" } } } } } } . Sample Response . { \"took\" : 14, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 281, \"relation\" : \"eq\" }, \"max_score\" : null, \"hits\" : [ ] }, \"aggregations\" : { \"daily_numbers\" : { \"doc_count_error_upper_bound\" : 0, \"sum_other_doc_count\" : 0, \"buckets\" : [ { \"key\" : \"Friday\", \"doc_count\" : 59, \"total_revenue\" : { \"value\" : 4858.84375 }, \"per_city\" : { \"doc_count_error_upper_bound\" : 0, \"sum_other_doc_count\" : 0, \"buckets\" : [ { \"key\" : \"Los Angeles\", \"doc_count\" : 59, \"average quantity\" : { \"value\" : 2.305084745762712 } } ] } }, { \"key\" : \"Saturday\", \"doc_count\" : 46, \"total_revenue\" : { \"value\" : 3547.203125 }, \"per_city\" : { \"doc_count_error_upper_bound\" : 0, \"sum_other_doc_count\" : 0, \"buckets\" : [ { \"key\" : \"Los Angeles\", \"doc_count\" : 46, \"average quantity\" : { \"value\" : 2.260869565217391 } } ] } }, { \"key\" : \"Tuesday\", \"doc_count\" : 45, \"total_revenue\" : { \"value\" : 3983.28125 }, \"per_city\" : { \"doc_count_error_upper_bound\" : 0, \"sum_other_doc_count\" : 0, \"buckets\" : [ { \"key\" : \"Los Angeles\", \"doc_count\" : 45, \"average quantity\" : { \"value\" : 2.2888888888888888 } } ] } }, { \"key\" : \"Sunday\", \"doc_count\" : 44, \"total_revenue\" : { \"value\" : 3308.1640625 }, \"per_city\" : { \"doc_count_error_upper_bound\" : 0, \"sum_other_doc_count\" : 0, \"buckets\" : [ { \"key\" : \"Los Angeles\", \"doc_count\" : 44, \"average quantity\" : { \"value\" : 2.090909090909091 } } ] } }, { \"key\" : \"Thursday\", \"doc_count\" : 40, \"total_revenue\" : { \"value\" : 2876.125 }, \"per_city\" : { \"doc_count_error_upper_bound\" : 0, \"sum_other_doc_count\" : 0, \"buckets\" : [ { \"key\" : \"Los Angeles\", \"doc_count\" : 40, \"average quantity\" : { \"value\" : 2.3 } } ] } }, { \"key\" : \"Monday\", \"doc_count\" : 38, \"total_revenue\" : { \"value\" : 2673.453125 }, \"per_city\" : { \"doc_count_error_upper_bound\" : 0, \"sum_other_doc_count\" : 0, \"buckets\" : [ { \"key\" : \"Los Angeles\", \"doc_count\" : 38, \"average quantity\" : { \"value\" : 2.1578947368421053 } } ] } }, { \"key\" : \"Wednesday\", \"doc_count\" : 38, \"total_revenue\" : { \"value\" : 3202.453125 }, \"per_city\" : { \"doc_count_error_upper_bound\" : 0, \"sum_other_doc_count\" : 0, \"buckets\" : [ { \"key\" : \"Los Angeles\", \"doc_count\" : 38, \"average quantity\" : { \"value\" : 2.236842105263158 } } ] } } ] } } } . ",
    "url": "https://vagimeli.github.io/im-plugin/index-rollups/index/#sample-walkthrough",
    "relUrl": "/im-plugin/index-rollups/index/#sample-walkthrough"
  },"155": {
    "doc": "Index rollups",
    "title": "The doc_count field",
    "content": "The doc_count field in bucket aggregations contains the number of documents collected in each bucket. When calculating the bucket’s doc_count, the number of documents is incremented by the number of the pre-aggregated documents in each summary document. The doc_count returned from rollup searches represents the total number of matching documents from the source index. The document count for each bucket is the same whether you search the source index or the rollup target index. ",
    "url": "https://vagimeli.github.io/im-plugin/index-rollups/index/#the-doc_count-field",
    "relUrl": "/im-plugin/index-rollups/index/#the-doc_count-field"
  },"156": {
    "doc": "Index rollups",
    "title": "Query string queries",
    "content": "To take advantage of shorter and more easily written strings in Query DSL, you can use query strings to simplify search queries in rollup indexes. To use query strings, add the following fields to your rollup search request: . \"query\": { \"query_string\": { \"query\": \"field_name:field_value\" } } . The following example uses a query string with a * wildcard operator to search inside a rollup index called my_server_logs_rollup: . GET my_server_logs_rollup/_search { \"size\": 0, \"query\": { \"query_string\": { \"query\": \"email* OR inventory\", \"default_field\": \"service_name\" } }, \"aggs\": { \"service_name\": { \"terms\": { \"field\": \"service_name\" }, \"aggs\": { \"region\": { \"terms\": { \"field\": \"region\" }, \"aggs\": { \"average quantity\": { \"avg\": { \"field\": \"cpu_usage\" } } } } } } } } . For more information about query string query parameters, see Query string query. ",
    "url": "https://vagimeli.github.io/im-plugin/index-rollups/index/#query-string-queries",
    "relUrl": "/im-plugin/index-rollups/index/#query-string-queries"
  },"157": {
    "doc": "Index rollups",
    "title": "Dynamic target index",
    "content": "In ISM rollup, the target_index field may contain a template that is compiled at the time of each rollup indexing. For example, if you specify the target_index field as rollup_ndx-{{ctx.source_index}}, the source index log-000001 will roll up into a target index rollup_ndx-log-000001. This allows you to roll up data into multiple time-based indexes, with one rollup job created for each source index. The source_index parameter in {{ctx.source_index}} cannot contain wildcards. ",
    "url": "https://vagimeli.github.io/im-plugin/index-rollups/index/#dynamic-target-index",
    "relUrl": "/im-plugin/index-rollups/index/#dynamic-target-index"
  },"158": {
    "doc": "Index rollups",
    "title": "Searching multiple rollup indexes",
    "content": "When data is rolled up into multiple target indexes, you can run one search across all of the rollup indexes. To search multiple target indexes that have the same rollup, specify the index names as a comma-separated list or a wildcard pattern. For example, with target_index as rollup_ndx-{{ctx.source_index}} and source indexes that start with log, specify the rollup_ndx-log* pattern. Or, to search for rolled up log-000001 and log-000002 indexes, specify the rollup_ndx-log-000001,rollup_ndx-log-000002 list. You cannot search a mix of rollup and non-rollup indexes with the same query. ",
    "url": "https://vagimeli.github.io/im-plugin/index-rollups/index/#searching-multiple-rollup-indexes",
    "relUrl": "/im-plugin/index-rollups/index/#searching-multiple-rollup-indexes"
  },"159": {
    "doc": "Index rollups",
    "title": "Example",
    "content": "The following example demonstrates the doc_count field, dynamic index names, and searching multiple rollup indexes with the same rollup. Step 1: Add an index template for ISM to manage the rolling over of the indexes aliased by log: . PUT _index_template/ism_rollover { \"index_patterns\": [\"log*\"], \"template\": { \"settings\": { \"plugins.index_state_management.rollover_alias\": \"log\" } } } . Step 2: Set up an ISM rollover policy to roll over any index whose name starts with log* after one document is uploaded to it, and then roll up the individual backing index. The target index name is dynamically generated from the source index name by prepending the string rollup_ndx- to the source index name. PUT _plugins/_ism/policies/rollover_policy { \"policy\": { \"description\": \"Example rollover policy.\", \"default_state\": \"rollover\", \"states\": [ { \"name\": \"rollover\", \"actions\": [ { \"rollover\": { \"min_doc_count\": 1 } } ], \"transitions\": [ { \"state_name\": \"rp\" } ] }, { \"name\": \"rp\", \"actions\": [ { \"rollup\": { \"ism_rollup\": { \"target_index\": \"rollup_ndx-{{ctx.source_index}}\", \"description\": \"Example rollup job\", \"page_size\": 200, \"dimensions\": [ { \"date_histogram\": { \"source_field\": \"ts\", \"fixed_interval\": \"60m\", \"timezone\": \"America/Los_Angeles\" } }, { \"terms\": { \"source_field\": \"message.keyword\" } } ], \"metrics\": [ { \"source_field\": \"msg_size\", \"metrics\": [ { \"sum\": {} } ] } ] } } } ], \"transitions\": [] } ], \"ism_template\": { \"index_patterns\": [\"log*\"], \"priority\": 100 } } } . Step 3: Create an index named log-000001 and set up an alias log for it. PUT log-000001 { \"aliases\": { \"log\": { \"is_write_index\": true } } } . Step 4: Index four documents into the index created above. Two of the documents have the message “Success”, and two have the message “Error”. POST log/_doc?refresh=true { \"ts\" : \"2022-08-26T09:28:48-04:00\", \"message\": \"Success\", \"msg_size\": 10 } . POST log/_doc?refresh=true { \"ts\" : \"2022-08-26T10:06:25-04:00\", \"message\": \"Error\", \"msg_size\": 20 } . POST log/_doc?refresh=true { \"ts\" : \"2022-08-26T10:23:54-04:00\", \"message\": \"Error\", \"msg_size\": 30 } . POST log/_doc?refresh=true { \"ts\" : \"2022-08-26T10:53:41-04:00\", \"message\": \"Success\", \"msg_size\": 40 } . Once you index the first document, the rollover action is executed. This action creates the index log-000002 with rollover_policy attached to it. Then the rollup action is executed, which creates the rollup index rollup_ndx-log-000001. To monitor the status of rollover and rollup index creation, you can use the ISM explain API: GET _plugins/_ism/explain . Step 5: Search the rollup index. GET rollup_ndx-log-*/_search { \"size\": 0, \"query\": { \"match_all\": {} }, \"aggregations\": { \"message_numbers\": { \"terms\": { \"field\": \"message.keyword\" }, \"aggs\": { \"per_message\": { \"terms\": { \"field\": \"message.keyword\" }, \"aggregations\": { \"sum_message\": { \"sum\": { \"field\": \"msg_size\" } } } } } } } } . The response contains two buckets, “Error” and “Success”, and the document count for each bucket is 2: . { \"took\" : 30, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 4, \"relation\" : \"eq\" }, \"max_score\" : null, \"hits\" : [ ] }, \"aggregations\" : { \"message_numbers\" : { \"doc_count_error_upper_bound\" : 0, \"sum_other_doc_count\" : 0, \"buckets\" : [ { \"key\" : \"Success\", \"doc_count\" : 2, \"per_message\" : { \"doc_count_error_upper_bound\" : 0, \"sum_other_doc_count\" : 0, \"buckets\" : [ { \"key\" : \"Success\", \"doc_count\" : 2, \"sum_message\" : { \"value\" : 50.0 } } ] } }, { \"key\" : \"Error\", \"doc_count\" : 2, \"per_message\" : { \"doc_count_error_upper_bound\" : 0, \"sum_other_doc_count\" : 0, \"buckets\" : [ { \"key\" : \"Error\", \"doc_count\" : 2, \"sum_message\" : { \"value\" : 50.0 } } ] } } ] } } } . ",
    "url": "https://vagimeli.github.io/im-plugin/index-rollups/index/#example",
    "relUrl": "/im-plugin/index-rollups/index/#example"
  },"160": {
    "doc": "Index rollups API",
    "title": "Index rollups API",
    "content": "Use the index rollup operations to programmatically work with index rollup jobs. . | Create or update an index rollup job | Get an index rollup job | Delete an index rollup job | Start or stop an index rollup job | Explain an index rollup job | . ",
    "url": "https://vagimeli.github.io/im-plugin/index-rollups/rollup-api/",
    "relUrl": "/im-plugin/index-rollups/rollup-api/"
  },"161": {
    "doc": "Index rollups API",
    "title": "Create or update an index rollup job",
    "content": "Introduced 1.0 . Creates or updates an index rollup job. You must provide the seq_no and primary_term parameters. Request . PUT _plugins/_rollup/jobs/&lt;rollup_id&gt; // Create PUT _plugins/_rollup/jobs/&lt;rollup_id&gt;?if_seq_no=1&amp;if_primary_term=1 // Update { \"rollup\": { \"source_index\": \"nyc-taxi-data\", \"target_index\": \"rollup-nyc-taxi-data\", \"schedule\": { \"interval\": { \"period\": 1, \"unit\": \"Days\" } }, \"description\": \"Example rollup job\", \"enabled\": true, \"page_size\": 200, \"delay\": 0, \"roles\": [ \"rollup_all\", \"nyc_taxi_all\", \"example_rollup_index_all\" ], \"continuous\": false, \"dimensions\": [ { \"date_histogram\": { \"source_field\": \"tpep_pickup_datetime\", \"fixed_interval\": \"1h\", \"timezone\": \"America/Los_Angeles\" } }, { \"terms\": { \"source_field\": \"PULocationID\" } } ], \"metrics\": [ { \"source_field\": \"passenger_count\", \"metrics\": [ { \"avg\": {} }, { \"sum\": {} }, { \"max\": {} }, { \"min\": {} }, { \"value_count\": {} } ] } ] } } . You can specify the following options. | Options | Description | Type | Required | . | source_index | The name of the detector. | String | Yes | . | target_index | Specify the target index that the rolled up data is ingested into. You can either create a new target index or use an existing index. The target index cannot be a combination of raw and rolled up data. This field supports dynamically generated index names like rollup_{{ctx.source_index}}, where source_index cannot contain wildcards. | String | Yes | . | schedule | Schedule of the index rollup job which can be an interval or a cron expression. | Object | Yes | . | schedule.interval | Specify the frequency of execution of the rollup job. | Object | No | . | schedule.interval.start_time | Start time of the interval. | Timestamp | Yes | . | schedule.interval.period | Define the interval period. | String | Yes | . | schedule.interval.unit | Specify the time unit of the interval. | String | Yes | . | schedule.interval.cron | Optionally, specify a cron expression to define therollup frequency. | List | No | . | schedule.interval.cron.expression | Specify a Unix cron expression. | String | Yes | . | schedule.interval.cron.timezone | Specify timezones as defined by the IANA Time Zone Database. Defaults to UTC. | String | No | . | description | Optionally, describe the rollup job. | String | No | . | enabled | When true, the index rollup job is scheduled. Default is true. | Boolean | Yes | . | continuous | Specify whether or not the index rollup job continuously rolls up data forever or just executes over the current data set once and stops. Default is false. | Boolean | Yes | . | error_notification | Set up a Mustache message template for error notifications. For example, if an index rollup job fails, the system sends a message to a Slack channel. | Object | No | . | page_size | Specify the number of buckets to paginate at a time during rollup. | Number | Yes | . | delay | The number of milliseconds to delay execution of the index rollup job. | Long | No | . | dimensions | Specify aggregations to create dimensions for the roll up time window. Supported groups are terms, histogram, and date_histogram. For more information, see Bucket Aggregations. | Array | Yes | . | metrics | Specify a list of objects that represent the fields and metrics that you want to calculate. Supported metrics are sum, max, min, value_count and avg. For more information, see Metric Aggregations. | Array | No | . Example response . { \"_id\": \"&lt;rollup_id&gt;\", \"_version\": 3, \"_seq_no\": 1, \"_primary_term\": 1, \"rollup\": { \"rollup_id\": \"&lt;rollup_id&gt;\", \"enabled\": true, \"schedule\": { \"interval\": { \"start_time\": 1680159934649, \"period\": 1, \"unit\": \"Days\", \"schedule_delay\": 0 } }, \"last_updated_time\": 1680159934649, \"enabled_time\": 1680159934649, \"description\": \"Example rollup job\", \"schema_version\": 17, \"source_index\": \"nyc-taxi-data\", \"target_index\": \"rollup-nyc-taxi-data\", \"metadata_id\": null, \"page_size\": 200, \"delay\": 0, \"continuous\": false, \"dimensions\": [ { \"date_histogram\": { \"fixed_interval\": \"1h\", \"source_field\": \"tpep_pickup_datetime\", \"target_field\": \"tpep_pickup_datetime\", \"timezone\": \"America/Los_Angeles\" } }, { \"terms\": { \"source_field\": \"PULocationID\", \"target_field\": \"PULocationID\" } } ], \"metrics\": [ { \"source_field\": \"passenger_count\", \"metrics\": [ { \"avg\": {} }, { \"sum\": {} }, { \"max\": {} }, { \"min\": {} }, { \"value_count\": {} } ] } ] } } . ",
    "url": "https://vagimeli.github.io/im-plugin/index-rollups/rollup-api/#create-or-update-an-index-rollup-job",
    "relUrl": "/im-plugin/index-rollups/rollup-api/#create-or-update-an-index-rollup-job"
  },"162": {
    "doc": "Index rollups API",
    "title": "Get an index rollup job",
    "content": "Introduced 1.0 . Returns all information about an index rollup job based on the rollup_id. Request . GET _plugins/_rollup/jobs/&lt;rollup_id&gt; . Example response . { \"_id\": \"my_rollup\", \"_seqNo\": 1, \"_primaryTerm\": 1, \"rollup\": { ... } } . ",
    "url": "https://vagimeli.github.io/im-plugin/index-rollups/rollup-api/#get-an-index-rollup-job",
    "relUrl": "/im-plugin/index-rollups/rollup-api/#get-an-index-rollup-job"
  },"163": {
    "doc": "Index rollups API",
    "title": "Delete an index rollup job",
    "content": "Introduced 1.0 . Deletes an index rollup job based on the rollup_id. Request . DELETE _plugins/_rollup/jobs/&lt;rollup_id&gt; . Example response . 200 OK . ",
    "url": "https://vagimeli.github.io/im-plugin/index-rollups/rollup-api/#delete-an-index-rollup-job",
    "relUrl": "/im-plugin/index-rollups/rollup-api/#delete-an-index-rollup-job"
  },"164": {
    "doc": "Index rollups API",
    "title": "Start or stop an index rollup job",
    "content": "Introduced 1.0 . Start or stop an index rollup job. Request . POST _plugins/_rollup/jobs/&lt;rollup_id&gt;/_start POST _plugins/_rollup/jobs/&lt;rollup_id&gt;/_stop . Example response . 200 OK . ",
    "url": "https://vagimeli.github.io/im-plugin/index-rollups/rollup-api/#start-or-stop-an-index-rollup-job",
    "relUrl": "/im-plugin/index-rollups/rollup-api/#start-or-stop-an-index-rollup-job"
  },"165": {
    "doc": "Index rollups API",
    "title": "Explain an index rollup job",
    "content": "Introduced 1.0 . Returns detailed metadata information about the index rollup job and its current progress. Request . GET _plugins/_rollup/jobs/&lt;rollup_id&gt;/_explain . Example response . { \"example_rollup\": { \"rollup_id\": \"example_rollup\", \"last_updated_time\": 1602014281, \"continuous\": { \"next_window_start_time\": 1602055591, \"next_window_end_time\": 1602075591 }, \"status\": \"running\", \"failure_reason\": null, \"stats\": { \"pages_processed\": 342, \"documents_processed\": 489359, \"rollups_indexed\": 3420, \"index_time_in_ms\": 30495, \"search_time_in_ms\": 584922 } } } . ",
    "url": "https://vagimeli.github.io/im-plugin/index-rollups/rollup-api/#explain-an-index-rollup-job",
    "relUrl": "/im-plugin/index-rollups/rollup-api/#explain-an-index-rollup-job"
  },"166": {
    "doc": "Settings",
    "title": "Index rollup settings",
    "content": "We don’t recommend changing these settings; the defaults should work well for most use cases. All settings are available using the OpenSearch _cluster/settings operation. None require a restart, and all can be marked persistent or transient. | Setting | Default | Description | . | plugins.rollup.search.backoff_millis | 1000 milliseconds | The backoff time between retries for failed rollup jobs. | . | plugins.rollup.search.backoff_count | 5 | How many retries the plugin should attempt for failed rollup jobs. | . | plugins.rollup.search.search_all_jobs | false | Whether OpenSearch should return all jobs that match all specified search terms. If disabled, OpenSearch returns just one, as opposed to all, of the jobs that matches the search terms. | . | plugins.rollup.dashboards.enabled | true | Whether rollups are enabled in OpenSearch Dashboards. | . | plugins.rollup.enabled | true | Whether the rollup plugin is enabled. | . | plugins.ingest.backoff_millis | 1000 milliseconds | The backoff time between data ingestions for rollup jobs. | . | plugins.ingest.backoff_count | 5 | How many retries the plugin should attempt for failed ingestions. | . ",
    "url": "https://vagimeli.github.io/im-plugin/index-rollups/settings/#index-rollup-settings",
    "relUrl": "/im-plugin/index-rollups/settings/#index-rollup-settings"
  },"167": {
    "doc": "Settings",
    "title": "Settings",
    "content": " ",
    "url": "https://vagimeli.github.io/im-plugin/index-rollups/settings/",
    "relUrl": "/im-plugin/index-rollups/settings/"
  },"168": {
    "doc": "Index templates",
    "title": "Index templates",
    "content": "Index templates let you initialize new indexes with predefined mappings and settings. For example, if you continuously index log data, you can define an index template so that all of these indexes have the same number of shards and replicas. Create a template . To create an index template, use a PUT or POST request: . PUT _index_template/&lt;template name&gt; POST _index_template/&lt;template name&gt; . This command creates a template named daily_logs and applies it to any new index whose name matches the pattern logs-2020-01-* and also adds it to the my_logs alias: . PUT _index_template/daily_logs { \"index_patterns\": [ \"logs-2020-01-*\" ], \"template\": { \"aliases\": { \"my_logs\": {} }, \"settings\": { \"number_of_shards\": 2, \"number_of_replicas\": 1 }, \"mappings\": { \"properties\": { \"timestamp\": { \"type\": \"date\", \"format\": \"yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis\" }, \"value\": { \"type\": \"double\" } } } } } . You should see the following response: . { \"acknowledged\": true } . If you create an index named logs-2020-01-01, you can see that it has the mappings and settings from the template: . PUT logs-2020-01-01 GET logs-2020-01-01 . { \"logs-2020-01-01\": { \"aliases\": { \"my_logs\": {} }, \"mappings\": { \"properties\": { \"timestamp\": { \"type\": \"date\", \"format\": \"yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis\" }, \"value\": { \"type\": \"double\" } } }, \"settings\": { \"index\": { \"creation_date\": \"1578107970779\", \"number_of_shards\": \"2\", \"number_of_replicas\": \"1\", \"uuid\": \"U1vMDMOHSAuS2IzPcPHpOA\", \"version\": { \"created\": \"7010199\" }, \"provided_name\": \"logs-2020-01-01\" } } } } . Any additional indexes that match this pattern—logs-2020-01-02, logs-2020-01-03, and so on—will inherit the same mappings and settings. Index patterns cannot contain any of the following characters: :, \", +, /, \\, |, ?, #, &gt;, and &lt;. Retrieve a template . To list all index templates: . GET _cat/templates GET /_index_template . To find a template by its name: . GET _index_template/daily_logs . To get a list of all templates that match a pattern: . GET _index_template/daily* . To check if a specific template exists: . HEAD _index_template/&lt;name&gt; . Configure multiple templates . You can create multiple index templates for your indexes. If the index name matches more than one template, OpenSearch takes the mappings and settings from the template with the highest priority and applies it to the index. For example, say you have the following two templates that both match the logs-2020-01-02 index and there’s a conflict in the number_of_shards field: . Template 1 . PUT _index_template/template-01 { \"index_patterns\": [ \"logs*\" ], \"priority\": 0, \"template\": { \"settings\": { \"number_of_shards\": 2, \"number_of_replicas\": 2 } } } . Template 2 . PUT _index_template/template-02 { \"index_patterns\": [ \"logs-2020-01-*\" ], \"priority\": 1, \"template\": { \"settings\": { \"number_of_shards\": 3 } } } . Because template-02 has a higher priority value, it takes precedence over template-01 . The logs-2020-01-02 index would have the number_of_shards value as 3 and the number_of_replicas as the default value 1. Delete a template . You can delete an index template using its name: . DELETE _index_template/daily_logs . ",
    "url": "https://vagimeli.github.io/im-plugin/index-templates/",
    "relUrl": "/im-plugin/index-templates/"
  },"169": {
    "doc": "Index templates",
    "title": "Composable index templates",
    "content": "Managing multiple index templates has the following challenges: . | If you have duplication between index templates, storing these index templates results in a bigger cluster state. | If you want to make a change across all your index templates, you have to manually make the change for each template. | . You can use composable index templates to overcome these challenges. Composable index templates let you abstract common settings, mappings, and aliases into a reusable building block called a component template. You can combine component templates to compose an index template. Settings and mappings that you specify directly in the create index request override any settings or mappings specified in an index template and its component templates. Create a component template . Let’s define two component templates⁠—component_template_1 and component_template_2: . Component template 1 . PUT _component_template/component_template_1 { \"template\": { \"mappings\": { \"properties\": { \"@timestamp\": { \"type\": \"date\" } } } } } . Component template 2 . PUT _component_template/component_template_2 { \"template\": { \"mappings\": { \"properties\": { \"ip_address\": { \"type\": \"ip\" } } } } } . Use component templates to create an index template . When creating index templates, you need to include the component templates in a composed_of list. OpenSearch applies the component templates in the order in which you specify them within the index template. The settings, mappings, and aliases that you specify inside the index template are applied last. PUT _index_template/daily_logs { \"index_patterns\": [ \"logs-2020-01-*\" ], \"template\": { \"aliases\": { \"my_logs\": {} }, \"settings\": { \"number_of_shards\": 2, \"number_of_replicas\": 1 }, \"mappings\": { \"properties\": { \"timestamp\": { \"type\": \"date\", \"format\": \"yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis\" }, \"value\": { \"type\": \"double\" } } } }, \"priority\": 200, \"composed_of\": [ \"component_template_1\", \"component_template_2\" ], \"version\": 3, \"_meta\": { \"description\": \"using component templates\" } } . If you create an index named logs-2020-01-01, you can see that it derives its mappings and settings from both the component templates: . PUT logs-2020-01-01 GET logs-2020-01-01 . Example response . { \"logs-2020-01-01\": { \"aliases\": { \"my_logs\": {} }, \"mappings\": { \"properties\": { \"@timestamp\": { \"type\": \"date\" }, \"ip_address\": { \"type\": \"ip\" }, \"timestamp\": { \"type\": \"date\", \"format\": \"yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis\" }, \"value\": { \"type\": \"double\" } } }, \"settings\": { \"index\": { \"creation_date\": \"1625382479459\", \"number_of_shards\": \"2\", \"number_of_replicas\": \"1\", \"uuid\": \"rYUlpOXDSUSuZifQLPfa5A\", \"version\": { \"created\": \"7100299\" }, \"provided_name\": \"logs-2020-01-01\" } } } } . ",
    "url": "https://vagimeli.github.io/im-plugin/index-templates/#composable-index-templates",
    "relUrl": "/im-plugin/index-templates/#composable-index-templates"
  },"170": {
    "doc": "Index templates",
    "title": "Index template options",
    "content": "You can specify the following template options: . | Option | Type | Description | Required | . | template | Object | Specify index settings, mappings, and aliases. | No | . | priority | Integer | The priority of the index template. | No | . | composed_of | String array | The names of component templates applied on a new index together with the current template. | No | . | version | Integer | Specify a version number to simplify template management. Default is null. | No | . | _meta | Object | Specify meta information about the template. | No | . ",
    "url": "https://vagimeli.github.io/im-plugin/index-templates/#index-template-options",
    "relUrl": "/im-plugin/index-templates/#index-template-options"
  },"171": {
    "doc": "Index transforms",
    "title": "Index transforms",
    "content": "Whereas index rollup jobs let you reduce data granularity by rolling up old data into condensed indexes, transform jobs let you create a different, summarized view of your data centered around certain fields, so you can visualize or analyze the data in different ways. For example, suppose that you have airline data that’s scattered across multiple fields and categories, and you want to view a summary of the data that’s organized by airline, quarter, and then price. You can use a transform job to create a new, summarized index that’s organized by those specific categories. You can use transform jobs in two ways: . | Use the OpenSearch Dashboards UI to specify the index you want to transform and any optional data filters you want to use to filter the original index. Then select the fields you want to transform and the aggregations to use in the transformation. Finally, define a schedule for your job to follow. | Use the transforms API to specify all the details about your job: the index you want to transform, target groups you want the transformed index to have, any aggregations you want to use to group columns, and a schedule for your job to follow. | . OpenSearch Dashboards provides a detailed summary of the jobs you created and their relevant information, such as associated indexes and job statuses. You can review and edit your job’s details and selections before creation, and even preview a transformed index’s data as you’re choosing which fields to transform. However, you can also use the REST API to create transform jobs and preview transform job results, but you must know all of the necessary settings and parameters to submit them as part of the HTTP request body. Submitting your transform job configurations as JSON scripts offers you more portability, allowing you to share and replicate your transform jobs, which is harder to do using OpenSearch Dashboards. Your use cases will help you decide which method to use to create transform jobs. ",
    "url": "https://vagimeli.github.io/im-plugin/index-transforms/index/",
    "relUrl": "/im-plugin/index-transforms/index/"
  },"172": {
    "doc": "Index transforms",
    "title": "Create a transform job",
    "content": "If you don’t have any data in your cluster, you can use the sample flight data within OpenSearch Dashboards to try out transform jobs. Otherwise, after launching OpenSearch Dashboards, choose Index Management. Select Transform Jobs, and choose Create Transform Job. Step 1: Choose indexes . | In the Job name and description section, specify a name and an optional description for your job. | In the Indices section, select the source and target index. You can either select an existing target index or create a new one by entering a name for your new index. If you want to transform just a subset of your source index, choose Edit data filter, and use the OpenSearch query DSL to specify a subset of your source index. For more information about the OpenSearch query DSL, see query DSL. | Choose Next. | . Step 2: Select fields to transform . After specifying the indexes, you can select the fields you want to use in your transform job, as well as whether to use groupings or aggregations. You can use groupings to place your data into separate buckets in your transformed index. For example, if you want to group all of the airport destinations within the sample flight data, you can group the DestAirportID field into a target field of DestAirportID_terms field, and you can find the grouped airport IDs in your transformed index after the transform job finishes. On the other hand, aggregations let you perform simple calculations. For example, you can include an aggregation in your transform job to define a new field of sum_of_total_ticket_price that calculates the sum of all airplane tickets, and then analyze the newly summer data within your transformed index. | In the data table, select the fields you want to transform and expand the drop-down menu within the column header to choose the grouping or aggregation you want to use. Currently, transform jobs support histogram, date_histogram, and terms groupings. For more information about groupings, see Bucket Aggregations. In terms of aggregations, you can select from sum, avg, max, min, value_count, percentiles, and scripted_metric. For more information about aggregations, see Metric Aggregations. | Repeat step 1 for any other fields that you want to transform. | After selecting the fields that you want to transform and verifying the transformation, choose Next. | . Step 3: Specify a schedule . You can configure transform jobs to run once or multiple times on a schedule. Transform jobs are enabled by default. | Choose whether the job should be continuous. Continuous jobs execute at each transform execution interval and incrementally transform newly modified buckets, which can include new data added to the source indexes. Non-continuous jobs execute only once. | For transformation execution interval, specify a transform interval in minutes, hours, or days. This interval dicatates how often continuous jobs should execute, and non-continuous jobs execute once after the interval elapses. | Under Advanced, specify an optional amount for Pages per execution. A larger number means more data is processed in each search request, but also uses more memory and causes higher latency. Exceeding allowed memory limits can cause exceptions and errors to occur. | Choose Next. | . Step 4: Review and confirm details . After confirming your transform job’s details are correct, choose Create Transform Job. If you want to edit any part of the job, choose Edit of the section you want to change, and make the necessary changes. You can’t change aggregations or groupings after creating a job. Step 5: Search through the transformed index. Once the transform job finishes, you can use the _search API operation to search the target index. GET &lt;target_index&gt;/_search . For example, after running a transform job that transforms the flight data based on a DestAirportID field, you can run the following request that returns all of the fields that have a value of SFO. Sample Request . GET finished_flight_job/_search { \"query\": { \"match\": { \"DestAirportID_terms\" : \"SFO\" } } } . Sample Response . { \"took\" : 3, \"timed_out\" : false, \"_shards\" : { \"total\" : 5, \"successful\" : 5, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 4, \"relation\" : \"eq\" }, \"max_score\" : 3.845883, \"hits\" : [ { \"_index\" : \"finished_flight_job\", \"_id\" : \"dSNKGb8U3OJOmC4RqVCi1Q\", \"_score\" : 3.845883, \"_source\" : { \"transform._id\" : \"sample_flight_job\", \"transform._doc_count\" : 14, \"Carrier_terms\" : \"Dashboards Airlines\", \"DestAirportID_terms\" : \"SFO\" } }, { \"_index\" : \"finished_flight_job\", \"_id\" : \"_D7oqOy7drx9E-MG96U5RA\", \"_score\" : 3.845883, \"_source\" : { \"transform._id\" : \"sample_flight_job\", \"transform._doc_count\" : 14, \"Carrier_terms\" : \"Logstash Airways\", \"DestAirportID_terms\" : \"SFO\" } }, { \"_index\" : \"finished_flight_job\", \"_id\" : \"YuZ8tOt1OsBA54e84WuAEw\", \"_score\" : 3.6988301, \"_source\" : { \"transform._id\" : \"sample_flight_job\", \"transform._doc_count\" : 11, \"Carrier_terms\" : \"ES-Air\", \"DestAirportID_terms\" : \"SFO\" } }, { \"_index\" : \"finished_flight_job\", \"_id\" : \"W_-e7bVmH6eu8veJeK8ZxQ\", \"_score\" : 3.6988301, \"_source\" : { \"transform._id\" : \"sample_flight_job\", \"transform._doc_count\" : 10, \"Carrier_terms\" : \"JetBeats\", \"DestAirportID_terms\" : \"SFO\" } } ] } } . ",
    "url": "https://vagimeli.github.io/im-plugin/index-transforms/index/#create-a-transform-job",
    "relUrl": "/im-plugin/index-transforms/index/#create-a-transform-job"
  },"173": {
    "doc": "Transforms APIs",
    "title": "Transforms APIs",
    "content": "Aside from using OpenSearch Dashboards, you can also use the REST API to create, start, stop, and complete other operations relative to transform jobs. | Create a transform job . | Request format | Path parameters | Request body fields | . | Update a transform job . | Request format | Query parameters | Request body fields | . | Get a transform job’s details . | Request format | Query parameters | . | Start a transform job . | Request format | . | Stop a transform job . | Request format | . | Get the status of a transform job . | Request format | . | Preview a transform job’s results | Delete a transform job . | Request format | . | . ",
    "url": "https://vagimeli.github.io/im-plugin/index-transforms/transforms-apis/",
    "relUrl": "/im-plugin/index-transforms/transforms-apis/"
  },"174": {
    "doc": "Transforms APIs",
    "title": "Create a transform job",
    "content": "Introduced 1.0 . Creates a transform job. Request format . PUT _plugins/_transform/&lt;transform_id&gt; . Path parameters . | Parameter | Data Type | Description | . | transform_id | String | Transform ID | . Request body fields . You can specify the following options in the HTTP request body: . | Option | Data Type | Description | Required | . | enabled | Boolean | If true, the transform job is enabled at creation. | No | . | continuous | Boolean | Specifies whether the transform job should be continuous. Continuous jobs execute every time they are scheduled according to the schedule field and run based off of newly transformed buckets as well as any new data added to source indexes. Non-continuous jobs execute only once. Default is false. | No | . | schedule | Object | The schedule for the transform job. | Yes | . | start_time | Integer | The Unix epoch time of the transform job’s start time. | Yes | . | description | String | Describes the transform job. | No | . | metadata_id | String | Any metadata to be associated with the transform job. | No | . | source_index | String | The source index containing the data to be transformed. | Yes | . | target_index | String | The target index the newly transformed data is added to. You can create a new index or update an existing one. | Yes | . | data_selection_query | Object | The query DSL to use to filter a subset of the source index for the transform job. See query domain-specific language(DSL) for more information. | Yes | . | page_size | Integer | The number of buckets IM processes and indexes concurrently. A higher number results in better performance, but it requires more memory. If your machine runs out of memory, Index Management (IM) automatically adjusts this field and retries until the operation succeeds. | Yes | . | groups | Array | Specifies the grouping(s) to use in the transform job. Supported groups are terms, histogram, and date_histogram. For more information, see Bucket Aggregations. | Yes if not using aggregations. | . | source_field | String | The field(s) to transform. | Yes | . | aggregations | Object | The aggregations to use in the transform job. Supported aggregations are sum, max, min, value_count, avg, scripted_metric, and percentiles. For more information, see Metric Aggregations. | Yes if not using groups. | . Sample Request . The following request creates a transform job with the id sample: . PUT _plugins/_transform/sample { \"transform\": { \"enabled\": true, \"continuous\": true, \"schedule\": { \"interval\": { \"period\": 1, \"unit\": \"Minutes\", \"start_time\": 1602100553 } }, \"description\": \"Sample transform job\", \"source_index\": \"sample_index\", \"target_index\": \"sample_target\", \"data_selection_query\": { \"match_all\": {} }, \"page_size\": 1, \"groups\": [ { \"terms\": { \"source_field\": \"customer_gender\", \"target_field\": \"gender\" } }, { \"terms\": { \"source_field\": \"day_of_week\", \"target_field\": \"day\" } } ], \"aggregations\": { \"quantity\": { \"sum\": { \"field\": \"total_quantity\" } } } } } . Sample Response . { \"_id\": \"sample\", \"_version\": 7, \"_seq_no\": 13, \"_primary_term\": 1, \"transform\": { \"transform_id\": \"sample\", \"schema_version\": 7, \"continuous\": true, \"schedule\": { \"interval\": { \"start_time\": 1621467964243, \"period\": 1, \"unit\": \"Minutes\" } }, \"metadata_id\": null, \"updated_at\": 1621467964243, \"enabled\": true, \"enabled_at\": 1621467964243, \"description\": \"Sample transform job\", \"source_index\": \"sample_index\", \"data_selection_query\": { \"match_all\": { \"boost\": 1.0 } }, \"target_index\": \"sample_target\", \"roles\": [], \"page_size\": 1, \"groups\": [ { \"terms\": { \"source_field\": \"customer_gender\", \"target_field\": \"gender\" } }, { \"terms\": { \"source_field\": \"day_of_week\", \"target_field\": \"day\" } } ], \"aggregations\": { \"quantity\": { \"sum\": { \"field\": \"total_quantity\" } } } } } . ",
    "url": "https://vagimeli.github.io/im-plugin/index-transforms/transforms-apis/#create-a-transform-job",
    "relUrl": "/im-plugin/index-transforms/transforms-apis/#create-a-transform-job"
  },"175": {
    "doc": "Transforms APIs",
    "title": "Update a transform job",
    "content": "Introduced 1.0 . Updates the transform job if transform_id already exists. For this request you must specify the sequence number and primary term of the transform to be updated. To get these, use the Get a transform job’s details API call. Request format . PUT _plugins/_transform/&lt;transform_id&gt;?if_seq_no=&lt;seq_no&gt;&amp;if_primary_term=&lt;primary_term&gt; . Query parameters . The update operation supports the following query parameters: . | Parameter | Description | Required | . | seq_no | Only perform the transform operation if the last operation that changed the transform job has the specified sequence number. | Yes | . | primary_term | Only perform the transform operation if the last operation that changed the transform job has the specified sequence term. | Yes | . Request body fields . You can update the following fields. | Option | Data Type | Description | . | schedule | Object | The schedule for the transform job. Contains the fields interval.start_time, interval.period, and interval.unit. | . | start_time | Integer | The Unix epoch start time of the transform job. | . | period | Integer | How often to execute the transform job. | . | unit | String | The unit of time associated with the execution period. Available options are Minutes, Hours, and Days. | . | description | Integer | Describes the transform job. | . | page_size | Integer | The number of buckets IM processes and indexes concurrently. A higher number results in better performance, but it requires more memory. If your machine runs out of memory, IM automatically adjusts this field and retries until the operation succeeds. | . Sample Request . The following request updates a transform job with the id sample, sequence number 13, and primary term 1: . PUT _plugins/_transform/sample?if_seq_no=13&amp;if_primary_term=1 { \"transform\": { \"enabled\": true, \"schedule\": { \"interval\": { \"period\": 1, \"unit\": \"Minutes\", \"start_time\": 1602100553 } }, \"description\": \"Sample transform job\", \"source_index\": \"sample_index\", \"target_index\": \"sample_target\", \"data_selection_query\": { \"match_all\": {} }, \"page_size\": 1, \"groups\": [ { \"terms\": { \"source_field\": \"customer_gender\", \"target_field\": \"gender\" } }, { \"terms\": { \"source_field\": \"day_of_week\", \"target_field\": \"day\" } } ], \"aggregations\": { \"quantity\": { \"sum\": { \"field\": \"total_quantity\" } } } } } . Sample Response . PUT _plugins/_transform/sample?if_seq_no=13&amp;if_primary_term=1 { \"transform\": { \"enabled\": true, \"schedule\": { \"interval\": { \"period\": 1, \"unit\": \"Minutes\", \"start_time\": 1602100553 } }, \"description\": \"Sample transform job\", \"source_index\": \"sample_index\", \"target_index\": \"sample_target\", \"data_selection_query\": { \"match_all\": {} }, \"page_size\": 1, \"groups\": [ { \"terms\": { \"source_field\": \"customer_gender\", \"target_field\": \"gender\" } }, { \"terms\": { \"source_field\": \"day_of_week\", \"target_field\": \"day\" } } ], \"aggregations\": { \"quantity\": { \"sum\": { \"field\": \"total_quantity\" } } } } } . ",
    "url": "https://vagimeli.github.io/im-plugin/index-transforms/transforms-apis/#update-a-transform-job",
    "relUrl": "/im-plugin/index-transforms/transforms-apis/#update-a-transform-job"
  },"176": {
    "doc": "Transforms APIs",
    "title": "Get a transform job’s details",
    "content": "Introduced 1.0 . Returns a transform job’s details. Request format . GET _plugins/_transform/&lt;transform_id&gt; . Sample Request . The following request returns the details of the transform job with the id sample: . GET _plugins/_transform/sample . Sample Response . { \"_id\": \"sample\", \"_version\": 7, \"_seq_no\": 13, \"_primary_term\": 1, \"transform\": { \"transform_id\": \"sample\", \"schema_version\": 7, \"continuous\": true, \"schedule\": { \"interval\": { \"start_time\": 1621467964243, \"period\": 1, \"unit\": \"Minutes\" } }, \"metadata_id\": null, \"updated_at\": 1621467964243, \"enabled\": true, \"enabled_at\": 1621467964243, \"description\": \"Sample transform job\", \"source_index\": \"sample_index\", \"data_selection_query\": { \"match_all\": { \"boost\": 1.0 } }, \"target_index\": \"sample_target\", \"roles\": [], \"page_size\": 1, \"groups\": [ { \"terms\": { \"source_field\": \"customer_gender\", \"target_field\": \"gender\" } }, { \"terms\": { \"source_field\": \"day_of_week\", \"target_field\": \"day\" } } ], \"aggregations\": { \"quantity\": { \"sum\": { \"field\": \"total_quantity\" } } } } } . You can also get details of all transform jobs by omitting transform_id. Sample Request . The following request returns the details of all transform jobs: . GET _plugins/_transform/ . Sample Response . { \"total_transforms\": 1, \"transforms\": [ { \"_id\": \"sample\", \"_seq_no\": 13, \"_primary_term\": 1, \"transform\": { \"transform_id\": \"sample\", \"schema_version\": 7, \"continuous\": true, \"schedule\": { \"interval\": { \"start_time\": 1621467964243, \"period\": 1, \"unit\": \"Minutes\" } }, \"metadata_id\": null, \"updated_at\": 1621467964243, \"enabled\": true, \"enabled_at\": 1621467964243, \"description\": \"Sample transform job\", \"source_index\": \"sample_index\", \"data_selection_query\": { \"match_all\": { \"boost\": 1.0 } }, \"target_index\": \"sample_target\", \"roles\": [], \"page_size\": 1, \"groups\": [ { \"terms\": { \"source_field\": \"customer_gender\", \"target_field\": \"gender\" } }, { \"terms\": { \"source_field\": \"day_of_week\", \"target_field\": \"day\" } } ], \"aggregations\": { \"quantity\": { \"sum\": { \"field\": \"total_quantity\" } } } } } ] } . Query parameters . You can specify the following GET API operation’s query parameters to filter the results. | Parameter | Description | Required | . | from | The starting transform to return. Default is 0. | No | . | size | Specifies the number of transforms to return. Default is 10. | No | . | search | The search term to use to filter results. | No | . | sortField | The field to sort results with. | No | . | sortDirection | Specifies the direction to sort results in. Can be ASC or DESC. Default is ASC. | No | . Sample Request . The following request returns two results starting from transform 8: . GET _plugins/_transform?size=2&amp;from=8 . Sample Response . { \"total_transforms\": 18, \"transforms\": [ { \"_id\": \"sample8\", \"_seq_no\": 93, \"_primary_term\": 1, \"transform\": { \"transform_id\": \"sample8\", \"schema_version\": 7, \"schedule\": { \"interval\": { \"start_time\": 1622063596812, \"period\": 1, \"unit\": \"Minutes\" } }, \"metadata_id\": \"y4hFAB2ZURQ2dzY7BAMxWA\", \"updated_at\": 1622063657233, \"enabled\": false, \"enabled_at\": null, \"description\": \"Sample transform job\", \"source_index\": \"sample_index3\", \"data_selection_query\": { \"match_all\": { \"boost\": 1.0 } }, \"target_index\": \"sample_target3\", \"roles\": [], \"page_size\": 1, \"groups\": [ { \"terms\": { \"source_field\": \"customer_gender\", \"target_field\": \"gender\" } }, { \"terms\": { \"source_field\": \"day_of_week\", \"target_field\": \"day\" } } ], \"aggregations\": { \"quantity\": { \"sum\": { \"field\": \"total_quantity\" } } } } }, { \"_id\": \"sample9\", \"_seq_no\": 98, \"_primary_term\": 1, \"transform\": { \"transform_id\": \"sample9\", \"schema_version\": 7, \"schedule\": { \"interval\": { \"start_time\": 1622063598065, \"period\": 1, \"unit\": \"Minutes\" } }, \"metadata_id\": \"x8tCIiYMTE3veSbIJkit5A\", \"updated_at\": 1622063658388, \"enabled\": false, \"enabled_at\": null, \"description\": \"Sample transform job\", \"source_index\": \"sample_index4\", \"data_selection_query\": { \"match_all\": { \"boost\": 1.0 } }, \"target_index\": \"sample_target4\", \"roles\": [], \"page_size\": 1, \"groups\": [ { \"terms\": { \"source_field\": \"customer_gender\", \"target_field\": \"gender\" } }, { \"terms\": { \"source_field\": \"day_of_week\", \"target_field\": \"day\" } } ], \"aggregations\": { \"quantity\": { \"sum\": { \"field\": \"total_quantity\" } } } } } ] } . ",
    "url": "https://vagimeli.github.io/im-plugin/index-transforms/transforms-apis/#get-a-transform-jobs-details",
    "relUrl": "/im-plugin/index-transforms/transforms-apis/#get-a-transform-jobs-details"
  },"177": {
    "doc": "Transforms APIs",
    "title": "Start a transform job",
    "content": "Introduced 1.0 . Transform jobs created using the API are automatically enabled, but if you ever need to enable a job, you can use the start API operation. Request format . POST _plugins/_transform/&lt;transform_id&gt;/_start . Sample Request . The following request starts the transform job with the ID sample: . POST _plugins/_transform/sample/_start . Sample Response . { \"acknowledged\": true } . ",
    "url": "https://vagimeli.github.io/im-plugin/index-transforms/transforms-apis/#start-a-transform-job",
    "relUrl": "/im-plugin/index-transforms/transforms-apis/#start-a-transform-job"
  },"178": {
    "doc": "Transforms APIs",
    "title": "Stop a transform job",
    "content": "Introduced 1.0 . Stops a transform job. Request format . POST _plugins/_transform/&lt;transform_id&gt;/_stop . Sample Request . The following request stops the transform job with the ID sample: . POST _plugins/_transform/sample/_stop . Sample Response . { \"acknowledged\": true } . ",
    "url": "https://vagimeli.github.io/im-plugin/index-transforms/transforms-apis/#stop-a-transform-job",
    "relUrl": "/im-plugin/index-transforms/transforms-apis/#stop-a-transform-job"
  },"179": {
    "doc": "Transforms APIs",
    "title": "Get the status of a transform job",
    "content": "Introduced 1.0 . Returns the status and metadata of a transform job. Request format . GET _plugins/_transform/&lt;transform_id&gt;/_explain . Sample Request . The following request returns the details of the transform job with the ID sample: . GET _plugins/_transform/sample/_explain . Sample Response . { \"sample\": { \"metadata_id\": \"PzmjweME5xbgkenl9UpsYw\", \"transform_metadata\": { \"continuous_stats\": { \"last_timestamp\": 1621883525672, \"documents_behind\": { \"sample_index\": 72 } }, \"transform_id\": \"sample\", \"last_updated_at\": 1621883525873, \"status\": \"finished\", \"failure_reason\": \"null\", \"stats\": { \"pages_processed\": 0, \"documents_processed\": 0, \"documents_indexed\": 0, \"index_time_in_millis\": 0, \"search_time_in_millis\": 0 } } } } . ",
    "url": "https://vagimeli.github.io/im-plugin/index-transforms/transforms-apis/#get-the-status-of-a-transform-job",
    "relUrl": "/im-plugin/index-transforms/transforms-apis/#get-the-status-of-a-transform-job"
  },"180": {
    "doc": "Transforms APIs",
    "title": "Preview a transform job’s results",
    "content": "Introduced 1.0 . Returns a preview of what a transformed index would look like. Sample Request . POST _plugins/_transform/_preview { \"transform\": { \"enabled\": false, \"schedule\": { \"interval\": { \"period\": 1, \"unit\": \"Minutes\", \"start_time\": 1602100553 } }, \"description\": \"test transform\", \"source_index\": \"sample_index\", \"target_index\": \"sample_target\", \"data_selection_query\": { \"match_all\": {} }, \"page_size\": 10, \"groups\": [ { \"terms\": { \"source_field\": \"customer_gender\", \"target_field\": \"gender\" } }, { \"terms\": { \"source_field\": \"day_of_week\", \"target_field\": \"day\" } } ], \"aggregations\": { \"quantity\": { \"sum\": { \"field\": \"total_quantity\" } } } } } . Sample Response . { \"documents\" : [ { \"quantity\" : 862.0, \"gender\" : \"FEMALE\", \"day\" : \"Friday\" }, { \"quantity\" : 682.0, \"gender\" : \"FEMALE\", \"day\" : \"Monday\" }, { \"quantity\" : 772.0, \"gender\" : \"FEMALE\", \"day\" : \"Saturday\" }, { \"quantity\" : 669.0, \"gender\" : \"FEMALE\", \"day\" : \"Sunday\" }, { \"quantity\" : 887.0, \"gender\" : \"FEMALE\", \"day\" : \"Thursday\" } ] } . ",
    "url": "https://vagimeli.github.io/im-plugin/index-transforms/transforms-apis/#preview-a-transform-jobs-results",
    "relUrl": "/im-plugin/index-transforms/transforms-apis/#preview-a-transform-jobs-results"
  },"181": {
    "doc": "Transforms APIs",
    "title": "Delete a transform job",
    "content": "Introduced 1.0 . Deletes a transform job. This operation does not delete the source or target indexes. Request format . DELETE _plugins/_transform/&lt;transform_id&gt; . Sample Request . The following request deletes the transform job with the ID sample: . DELETE _plugins/_transform/sample . Sample Response . { \"took\": 205, \"errors\": false, \"items\": [ { \"delete\": { \"_index\": \".opensearch-ism-config\", \"_id\": \"sample\", \"_version\": 4, \"result\": \"deleted\", \"forced_refresh\": true, \"_shards\": { \"total\": 2, \"successful\": 1, \"failed\": 0 }, \"_seq_no\": 6, \"_primary_term\": 1, \"status\": 200 } } ] } . ",
    "url": "https://vagimeli.github.io/im-plugin/index-transforms/transforms-apis/#delete-a-transform-job",
    "relUrl": "/im-plugin/index-transforms/transforms-apis/#delete-a-transform-job"
  },"182": {
    "doc": "Managing indexes",
    "title": "Managing indexes",
    "content": "OpenSearch Dashboards . You index data using the OpenSearch REST API. Two APIs exist: the index API and the _bulk API. For situations in which new data arrives incrementally (for example, customer orders from a small business), you might use the index API to add documents individually as they arrive. For situations in which the flow of data is less frequent (for example, weekly updates to a marketing website), you might prefer to generate a file and send it to the _bulk API. For large numbers of documents, lumping requests together and using the _bulk API offers superior performance. If your documents are enormous, however, you might need to index them individually. ",
    "url": "https://vagimeli.github.io/im-plugin/index/",
    "relUrl": "/im-plugin/index/"
  },"183": {
    "doc": "Managing indexes",
    "title": "Introduction to indexing",
    "content": "Before you can search data, you must index it. Indexing is the method by which search engines organize data for fast retrieval. The resulting structure is called, fittingly, an index. In OpenSearch, the basic unit of data is a JSON document. Within an index, OpenSearch identifies each document using a unique ID. A request to the index API looks like this: . PUT &lt;index&gt;/_doc/&lt;id&gt; { \"A JSON\": \"document\" } . A request to the _bulk API looks a little different, because you specify the index and ID in the bulk data: . POST _bulk { \"index\": { \"_index\": \"&lt;index&gt;\", \"_id\": \"&lt;id&gt;\" } } { \"A JSON\": \"document\" } . Bulk data must conform to a specific format, which requires a newline character (\\n) at the end of every line, including the last line. This is the basic format: . Action and metadata\\n Optional document\\n Action and metadata\\n Optional document\\n . The document is optional, because delete actions don’t require a document. The other actions (index, create, and update) all require a document. If you specifically want the action to fail if the document already exists, use the create action instead of the index action. To index bulk data using the curl command, navigate to the folder where you have your file saved and run the following command: . curl -H \"Content-Type: application/x-ndjson\" -POST https://localhost:9200/data/_bulk -u 'admin:admin' --insecure --data-binary \"@data.json\" . If any one of the actions in the _bulk API fail, OpenSearch continues to execute the other actions. Examine the items array in the response to figure out what went wrong. The entries in the items array are in the same order as the actions specified in the request. OpenSearch automatically creates an index when you add a document to an index that doesn’t already exist. It also automatically generates an ID if you don’t specify an ID in the request. This simple example automatically creates the movies index, indexes the document, and assigns it a unique ID: . POST movies/_doc { \"title\": \"Spirited Away\" } . Automatic ID generation has a clear downside: because the indexing request didn’t specify a document ID, you can’t easily update the document at a later time. Also, if you run this request 10 times, OpenSearch indexes this document as 10 different documents with unique IDs. To specify an ID of 1, use the following request (note the use of PUT instead of POST): . PUT movies/_doc/1 { \"title\": \"Spirited Away\" } . Because you must specify an ID, if you run this command 10 times, you still have just one document indexed with the _version field incremented to 10. Indexes default to one primary shard and one replica. If you want to specify non-default settings, create the index before adding documents: . PUT more-movies { \"settings\": { \"number_of_shards\": 6, \"number_of_replicas\": 2 } } . ",
    "url": "https://vagimeli.github.io/im-plugin/index/#introduction-to-indexing",
    "relUrl": "/im-plugin/index/#introduction-to-indexing"
  },"184": {
    "doc": "Managing indexes",
    "title": "Naming restrictions for indexes",
    "content": "OpenSearch indexes have the following naming restrictions: . | All letters must be lowercase. | Index names can’t begin with underscores (_) or hyphens (-). | Index names can’t contain spaces, commas, or the following characters: . :, \", *, +, /, \\, |, ?, #, &gt;, or &lt; . | . ",
    "url": "https://vagimeli.github.io/im-plugin/index/#naming-restrictions-for-indexes",
    "relUrl": "/im-plugin/index/#naming-restrictions-for-indexes"
  },"185": {
    "doc": "Managing indexes",
    "title": "Read data",
    "content": "After you index a document, you can retrieve it by sending a GET request to the same endpoint that you used for indexing: . GET movies/_doc/1 { \"_index\" : \"movies\", \"_type\" : \"_doc\", \"_id\" : \"1\", \"_version\" : 1, \"_seq_no\" : 0, \"_primary_term\" : 1, \"found\" : true, \"_source\" : { \"title\" : \"Spirited Away\" } } . You can see the document in the _source object. If the document is not found, the found key is false and the _source object is not part of the response. To retrieve multiple documents with a single command, use the _mget operation. The format for retrieving multiple documents is similar to the _bulk operation, where you must specify the index and ID in the request body: . GET _mget { \"docs\": [ { \"_index\": \"&lt;index&gt;\", \"_id\": \"&lt;id&gt;\" }, { \"_index\": \"&lt;index&gt;\", \"_id\": \"&lt;id&gt;\" } ] } . To only return specific fields in a document: . GET _mget { \"docs\": [ { \"_index\": \"&lt;index&gt;\", \"_id\": \"&lt;id&gt;\", \"_source\": \"field1\" }, { \"_index\": \"&lt;index&gt;\", \"_id\": \"&lt;id&gt;\", \"_source\": \"field2\" } ] } . To check if a document exists: . HEAD movies/_doc/&lt;doc-id&gt; . If the document exists, you get back a 200 OK response, and if it doesn’t, you get back a 404 - Not Found error. ",
    "url": "https://vagimeli.github.io/im-plugin/index/#read-data",
    "relUrl": "/im-plugin/index/#read-data"
  },"186": {
    "doc": "Managing indexes",
    "title": "Update data",
    "content": "To update existing fields or to add new fields, send a POST request to the _update operation with your changes in a doc object: . POST movies/_update/1 { \"doc\": { \"title\": \"Castle in the Sky\", \"genre\": [\"Animation\", \"Fantasy\"] } } . Note the updated title field and new genre field: . GET movies/_doc/1 { \"_index\" : \"movies\", \"_type\" : \"_doc\", \"_id\" : \"1\", \"_version\" : 2, \"_seq_no\" : 1, \"_primary_term\" : 1, \"found\" : true, \"_source\" : { \"title\" : \"Castle in the Sky\", \"genre\" : [ \"Animation\", \"Fantasy\" ] } } . The document also has an incremented _version field. Use this field to keep track of how many times a document is updated. POST requests make partial updates to documents. To altogether replace a document, use a PUT request: . PUT movies/_doc/1 { \"title\": \"Spirited Away\" } . The document with ID of 1 will contain only the title field, because the entire document will be replaced with the document indexed in this PUT request. Use the upsert object to conditionally update documents based on whether they already exist. Here, if the document exists, its title field changes to Castle in the Sky. If it doesn’t, OpenSearch indexes the document in the upsert object. POST movies/_update/2 { \"doc\": { \"title\": \"Castle in the Sky\" }, \"upsert\": { \"title\": \"Only Yesterday\", \"genre\": [\"Animation\", \"Fantasy\"], \"date\": 1993 } } . Example response . { \"_index\" : \"movies\", \"_type\" : \"_doc\", \"_id\" : \"2\", \"_version\" : 2, \"result\" : \"updated\", \"_shards\" : { \"total\" : 2, \"successful\" : 1, \"failed\" : 0 }, \"_seq_no\" : 3, \"_primary_term\" : 1 } . Each update operation for a document has a unique combination of the _seq_no and _primary_term values. OpenSearch first writes your updates to the primary shard and then sends this change to all the replica shards. An uncommon issue can occur if multiple users of your OpenSearch-based application make updates to existing documents in the same index. In this situation, another user can read and update a document from a replica before it receives your update from the primary shard. Your update operation then ends up updating an older version of the document. In the best case, you and the other user make the same changes, and the document remains accurate. In the worst case, the document now contains out-of-date information. To prevent this situation, use the _seq_no and _primary_term values in the request header: . POST movies/_update/2?if_seq_no=3&amp;if_primary_term=1 { \"doc\": { \"title\": \"Castle in the Sky\", \"genre\": [\"Animation\", \"Fantasy\"] } } . If the document is updated after we retrieved it, the _seq_no and _primary_term values are different and our update operation fails with a 409 — Conflict error. When using the _bulk API, specify the _seq_no and _primary_term values within the action metadata. ",
    "url": "https://vagimeli.github.io/im-plugin/index/#update-data",
    "relUrl": "/im-plugin/index/#update-data"
  },"187": {
    "doc": "Managing indexes",
    "title": "Delete data",
    "content": "To delete a document from an index, use a DELETE request: . DELETE movies/_doc/1 . The DELETE operation increments the _version field. If you add the document back to the same ID, the _version field increments again. This behavior occurs because OpenSearch deletes the document _source, but retains its metadata. ",
    "url": "https://vagimeli.github.io/im-plugin/index/#delete-data",
    "relUrl": "/im-plugin/index/#delete-data"
  },"188": {
    "doc": "Managing indexes",
    "title": "Next steps",
    "content": ". | The Index Management (IM) plugin lets you automate recurring index management activities and reduce storage costs. For more information, see Index State Management. | For instructions on how to reindex data, see Reindex data. | . ",
    "url": "https://vagimeli.github.io/im-plugin/index/#next-steps",
    "relUrl": "/im-plugin/index/#next-steps"
  },"189": {
    "doc": "ISM API",
    "title": "ISM API",
    "content": "Use the index state management operations to programmatically work with policies and managed indexes. . | Create policy | Add policy | Update policy | Get policy | Remove policy from index | Update managed index policy | Retry failed index | Explain index | Delete policy | Error prevention validation | . ",
    "url": "https://vagimeli.github.io/im-plugin/ism/api/",
    "relUrl": "/im-plugin/ism/api/"
  },"190": {
    "doc": "ISM API",
    "title": "Create policy",
    "content": "Introduced 1.0 . Creates a policy. Example request . PUT _plugins/_ism/policies/policy_1 { \"policy\": { \"description\": \"ingesting logs\", \"default_state\": \"ingest\", \"states\": [ { \"name\": \"ingest\", \"actions\": [ { \"rollover\": { \"min_doc_count\": 5 } } ], \"transitions\": [ { \"state_name\": \"search\" } ] }, { \"name\": \"search\", \"actions\": [], \"transitions\": [ { \"state_name\": \"delete\", \"conditions\": { \"min_index_age\": \"5m\" } } ] }, { \"name\": \"delete\", \"actions\": [ { \"delete\": {} } ], \"transitions\": [] } ] } } . Example response . { \"_id\": \"policy_1\", \"_version\": 1, \"_primary_term\": 1, \"_seq_no\": 7, \"policy\": { \"policy\": { \"policy_id\": \"policy_1\", \"description\": \"ingesting logs\", \"last_updated_time\": 1577990761311, \"schema_version\": 1, \"error_notification\": null, \"default_state\": \"ingest\", \"states\": [ { \"name\": \"ingest\", \"actions\": [ { \"rollover\": { \"min_doc_count\": 5 } } ], \"transitions\": [ { \"state_name\": \"search\" } ] }, { \"name\": \"search\", \"actions\": [], \"transitions\": [ { \"state_name\": \"delete\", \"conditions\": { \"min_index_age\": \"5m\" } } ] }, { \"name\": \"delete\", \"actions\": [ { \"delete\": {} } ], \"transitions\": [] } ] } } } . ",
    "url": "https://vagimeli.github.io/im-plugin/ism/api/#create-policy",
    "relUrl": "/im-plugin/ism/api/#create-policy"
  },"191": {
    "doc": "ISM API",
    "title": "Add policy",
    "content": "Introduced 1.0 . Adds a policy to an index. This operation does not change the policy if the index already has one. Example request . POST _plugins/_ism/add/index_1 { \"policy_id\": \"policy_1\" } . Example response . { \"updated_indices\": 1, \"failures\": false, \"failed_indices\": [] } . If you use a wildcard * while adding a policy to an index, the ISM plugin interprets * as all indexes, including system indexes like .opendistro-security, which stores users, roles, and tenants. A delete action in your policy might accidentally delete all user roles and tenants in your cluster. Don’t use the broad * wildcard, and instead add a prefix, such as my-logs*, when specifying indexes with the _ism/add API. ",
    "url": "https://vagimeli.github.io/im-plugin/ism/api/#add-policy",
    "relUrl": "/im-plugin/ism/api/#add-policy"
  },"192": {
    "doc": "ISM API",
    "title": "Update policy",
    "content": "Introduced 1.0 . Updates a policy. Use the seq_no and primary_term parameters to update an existing policy. If these numbers don’t match the existing policy or the policy doesn’t exist, ISM throws an error. It’s possible that the policy currently applied to your index isn’t the most up-to-date policy available. To see what policy is currently applied to your index, see Explain index. To get the most up-to-date version of a policy, see Get policy. Example request . PUT _plugins/_ism/policies/policy_1?if_seq_no=7&amp;if_primary_term=1 { \"policy\": { \"description\": \"ingesting logs\", \"default_state\": \"ingest\", \"states\": [ { \"name\": \"ingest\", \"actions\": [ { \"rollover\": { \"min_doc_count\": 5 } } ], \"transitions\": [ { \"state_name\": \"search\" } ] }, { \"name\": \"search\", \"actions\": [], \"transitions\": [ { \"state_name\": \"delete\", \"conditions\": { \"min_index_age\": \"5m\" } } ] }, { \"name\": \"delete\", \"actions\": [ { \"delete\": {} } ], \"transitions\": [] } ] } } . Example response . { \"_id\": \"policy_1\", \"_version\": 2, \"_primary_term\": 1, \"_seq_no\": 10, \"policy\": { \"policy\": { \"policy_id\": \"policy_1\", \"description\": \"ingesting logs\", \"last_updated_time\": 1577990934044, \"schema_version\": 1, \"error_notification\": null, \"default_state\": \"ingest\", \"states\": [ { \"name\": \"ingest\", \"actions\": [ { \"rollover\": { \"min_doc_count\": 5 } } ], \"transitions\": [ { \"state_name\": \"search\" } ] }, { \"name\": \"search\", \"actions\": [], \"transitions\": [ { \"state_name\": \"delete\", \"conditions\": { \"min_index_age\": \"5m\" } } ] }, { \"name\": \"delete\", \"actions\": [ { \"delete\": {} } ], \"transitions\": [] } ] } } } . ",
    "url": "https://vagimeli.github.io/im-plugin/ism/api/#update-policy",
    "relUrl": "/im-plugin/ism/api/#update-policy"
  },"193": {
    "doc": "ISM API",
    "title": "Get policy",
    "content": "Introduced 1.0 . Gets the policy by policy_id. Example request . GET _plugins/_ism/policies/policy_1 . Example response . { \"_id\": \"policy_1\", \"_version\": 2, \"_seq_no\": 10, \"_primary_term\": 1, \"policy\": { \"policy_id\": \"policy_1\", \"description\": \"ingesting logs\", \"last_updated_time\": 1577990934044, \"schema_version\": 1, \"error_notification\": null, \"default_state\": \"ingest\", \"states\": [ { \"name\": \"ingest\", \"actions\": [ { \"rollover\": { \"min_doc_count\": 5 } } ], \"transitions\": [ { \"state_name\": \"search\" } ] }, { \"name\": \"search\", \"actions\": [], \"transitions\": [ { \"state_name\": \"delete\", \"conditions\": { \"min_index_age\": \"5m\" } } ] }, { \"name\": \"delete\", \"actions\": [ { \"delete\": {} } ], \"transitions\": [] } ] } } . ",
    "url": "https://vagimeli.github.io/im-plugin/ism/api/#get-policy",
    "relUrl": "/im-plugin/ism/api/#get-policy"
  },"194": {
    "doc": "ISM API",
    "title": "Remove policy from index",
    "content": "Introduced 1.0 . Removes any ISM policy from the index. Example request . POST _plugins/_ism/remove/index_1 . Example response . { \"updated_indices\": 1, \"failures\": false, \"failed_indices\": [] } . ",
    "url": "https://vagimeli.github.io/im-plugin/ism/api/#remove-policy-from-index",
    "relUrl": "/im-plugin/ism/api/#remove-policy-from-index"
  },"195": {
    "doc": "ISM API",
    "title": "Update managed index policy",
    "content": "Introduced 1.0 . Updates the managed index policy to a new policy (or to a new version of the policy). You can use an index pattern to update multiple indexes at once. When updating multiple indexes, you might want to include a state filter to only affect certain managed indexes. The change policy filters out all the existing managed indexes and only applies the change to the ones in the state that you specify. You can also explicitly specify the state that the managed index transitions to after the change policy takes effect. A policy change is an asynchronous background process. The changes are queued and are not executed immediately by the background process. This delay in execution protects the currently running managed indexes from being put into a broken state. If the policy you are changing to has only some small configuration changes, then the change takes place immediately. For example, if the policy changes the min_index_age parameter in a rollover condition from 1000d to 100d, this change takes place immediately in its next execution. If the change modifies the state, actions, or the order of actions of the current state the index is in, then the change happens at the end of its current state before transitioning to a new state. In this example, the policy applied on the index_1 index is changed to policy_1, which could either be a completely new policy or an updated version of its existing policy. The process only applies the change if the index is currently in the searches state. After this change in policy takes place, index_1 transitions to the delete state. Example request . POST _plugins/_ism/change_policy/index_1 { \"policy_id\": \"policy_1\", \"state\": \"delete\", \"include\": [ { \"state\": \"searches\" } ] } . Example response . { \"updated_indices\": 0, \"failures\": false, \"failed_indices\": [] } . ",
    "url": "https://vagimeli.github.io/im-plugin/ism/api/#update-managed-index-policy",
    "relUrl": "/im-plugin/ism/api/#update-managed-index-policy"
  },"196": {
    "doc": "ISM API",
    "title": "Retry failed index",
    "content": "Introduced 1.0 . Retries the failed action for an index. For the retry call to succeed, ISM must manage the index, and the index must be in a failed state. You can use index patterns (*) to retry multiple failed indexes. Example request . POST _plugins/_ism/retry/index_1 { \"state\": \"delete\" } . Example response . { \"updated_indices\": 0, \"failures\": false, \"failed_indices\": [] } . ",
    "url": "https://vagimeli.github.io/im-plugin/ism/api/#retry-failed-index",
    "relUrl": "/im-plugin/ism/api/#retry-failed-index"
  },"197": {
    "doc": "ISM API",
    "title": "Explain index",
    "content": "Introduced 1.0 . Gets the current state of the index. You can use index patterns to get the status of multiple indexes. Example request . GET _plugins/_ism/explain/index_1 . Example response . { \"index_1\": { \"index.plugins.index_state_management.policy_id\": \"policy_1\" } } . Optionally, you can add the show_policy parameter to your request’s path to get the policy that is currently applied to your index, which is useful for seeing whether the policy applied to your index is the latest one. To get the most up-to-date policy, see Get Policy API. Example request . GET _plugins/_ism/explain/index_1?show_policy=true . Example response . { \"index_1\": { \"index.plugins.index_state_management.policy_id\": \"sample-policy\", \"index.opendistro.index_state_management.policy_id\": \"sample-policy\", \"index\": \"index_1\", \"index_uuid\": \"gCFlS_zcTdih8xyxf3jQ-A\", \"policy_id\": \"sample-policy\", \"enabled\": true, \"policy\": { \"policy_id\": \"sample-policy\", \"description\": \"ingesting logs\", \"last_updated_time\": 1647284980148, \"schema_version\": 13, \"error_notification\": null, \"default_state\": \"ingest\", \"states\": [...], \"ism_template\": null } }, \"total_managed_indices\": 1 } . The plugins.index_state_management.policy_id setting is deprecated starting from ODFE version 1.13.0. We retain this field in the response API for consistency. ",
    "url": "https://vagimeli.github.io/im-plugin/ism/api/#explain-index",
    "relUrl": "/im-plugin/ism/api/#explain-index"
  },"198": {
    "doc": "ISM API",
    "title": "Delete policy",
    "content": "Introduced 1.0 . Deletes the policy by policy_id. Example request . DELETE _plugins/_ism/policies/policy_1 . Example response . { \"_index\": \".opendistro-ism-config\", \"_id\": \"policy_1\", \"_version\": 3, \"result\": \"deleted\", \"forced_refresh\": true, \"_shards\": { \"total\": 2, \"successful\": 2, \"failed\": 0 }, \"_seq_no\": 15, \"_primary_term\": 1 } . ",
    "url": "https://vagimeli.github.io/im-plugin/ism/api/#delete-policy",
    "relUrl": "/im-plugin/ism/api/#delete-policy"
  },"199": {
    "doc": "ISM API",
    "title": "Error prevention validation",
    "content": "Introduced 2.4 . ISM allows you to run an action automatically. However, running an action can fail for a variety of reasons. You can use error prevention validation to test an action in order to rule out failures. To enable error prevention validation, set the plugins.index_state_management.validation_service.enabled setting to true: . PUT _cluster/settings { \"persistent\":{ \"plugins.index_state_management.validation_action.enabled\": true } } . Example response . { \"acknowledged\" : true, \"persistent\" : { \"plugins\" : { \"index_state_management\" : { \"validation_action\" : { \"enabled\" : \"true\" } } } }, \"transient\" : { } } . To check an error prevention validation status and message, pass validate_action=true to the _plugins/_ism/explain endpoint: . GET _plugins/_ism/explain/test-000001?validate_action=true . Example response . The response contains an additional validate object with a validation message and status: . { \"test-000001\" : { \"index.plugins.index_state_management.policy_id\" : \"test_rollover\", \"index.opendistro.index_state_management.policy_id\" : \"test_rollover\", \"index\" : \"test-000001\", \"index_uuid\" : \"CgKsxFmQSIa8dWqpbSJmyA\", \"policy_id\" : \"test_rollover\", \"policy_seq_no\" : -2, \"policy_primary_term\" : 0, \"rolled_over\" : false, \"index_creation_date\" : 1667410460649, \"state\" : { \"name\" : \"rollover\", \"start_time\" : 1667410766045 }, \"action\" : { \"name\" : \"rollover\", \"start_time\" : 1667411127803, \"index\" : 0, \"failed\" : false, \"consumed_retries\" : 0, \"last_retry_time\" : 0 }, \"step\" : { \"name\" : \"attempt_rollover\", \"start_time\" : 1667411127803, \"step_status\" : \"starting\" }, \"retry_info\" : { \"failed\" : true, \"consumed_retries\" : 0 }, \"info\" : { \"message\" : \"Previous action was not able to update IndexMetaData.\" }, \"enabled\" : false, \"validate\" : { \"validation_message\" : \"Missing rollover_alias index setting [index=test-000001]\", \"validation_status\" : \"re_validating\" } }, \"total_managed_indices\" : 1 } . If you pass validate_action=false or do not pass a validate_action value to the _plugins/_ism/explain endpoint, the response will not contain an error prevention validation status and message: . GET _plugins/_ism/explain/test-000001?validate_action=false . Or: . GET _plugins/_ism/explain/test-000001 . Example response . { \"test-000001\" : { \"index.plugins.index_state_management.policy_id\" : \"test_rollover\", \"index.opendistro.index_state_management.policy_id\" : \"test_rollover\", \"index\" : \"test-000001\", \"index_uuid\" : \"CgKsxFmQSIa8dWqpbSJmyA\", \"policy_id\" : \"test_rollover\", \"policy_seq_no\" : -2, \"policy_primary_term\" : 0, \"rolled_over\" : false, \"index_creation_date\" : 1667410460649, \"state\" : { \"name\" : \"rollover\", \"start_time\" : 1667410766045 }, \"action\" : { \"name\" : \"rollover\", \"start_time\" : 1667411127803, \"index\" : 0, \"failed\" : false, \"consumed_retries\" : 0, \"last_retry_time\" : 0 }, \"step\" : { \"name\" : \"attempt_rollover\", \"start_time\" : 1667411127803, \"step_status\" : \"starting\" }, \"retry_info\" : { \"failed\" : true, \"consumed_retries\" : 0 }, \"info\" : { \"message\" : \"Previous action was not able to update IndexMetaData.\" }, \"enabled\" : false }, \"total_managed_indices\" : 1 } . ",
    "url": "https://vagimeli.github.io/im-plugin/ism/api/#error-prevention-validation",
    "relUrl": "/im-plugin/ism/api/#error-prevention-validation"
  },"200": {
    "doc": "ISM Error Prevention API",
    "title": "ISM Error Prevention API",
    "content": "The ISM Error Prevention API allows you to enable Index State Management (ISM) error prevention and check the validation status and message. ",
    "url": "https://vagimeli.github.io/im-plugin/ism/error-prevention/api/",
    "relUrl": "/im-plugin/ism/error-prevention/api/"
  },"201": {
    "doc": "ISM Error Prevention API",
    "title": "Enable error prevention validation",
    "content": "You can configure error prevention validation by setting the plugins.index_state_management.validation_service.enabled parameter. Example request . PUT _cluster/settings { \"persistent\":{ \"plugins.index_state_management.validation_action.enabled\": true } } . Example response . { \"acknowledged\" : true, \"persistent\" : { \"plugins\" : { \"index_state_management\" : { \"validation_action\" : { \"enabled\" : \"true\" } } } }, \"transient\" : { } } . ",
    "url": "https://vagimeli.github.io/im-plugin/ism/error-prevention/api/#enable-error-prevention-validation",
    "relUrl": "/im-plugin/ism/error-prevention/api/#enable-error-prevention-validation"
  },"202": {
    "doc": "ISM Error Prevention API",
    "title": "Check validation status and message via the Explain API",
    "content": "Pass the validate_action=true path parameter in the Explain API URI to see the validation status and message. Example request . GET _plugins/_ism/explain/test-000001?validate_action=true . Example response . { \"test-000001\" : { \"index.plugins.index_state_management.policy_id\" : \"test_rollover\", \"index.opendistro.index_state_management.policy_id\" : \"test_rollover\", \"index\" : \"test-000001\", \"index_uuid\" : \"CgKsxFmQSIa8dWqpbSJmyA\", \"policy_id\" : \"test_rollover\", \"policy_seq_no\" : -2, \"policy_primary_term\" : 0, \"rolled_over\" : false, \"index_creation_date\" : 1667410460649, \"state\" : { \"name\" : \"rollover\", \"start_time\" : 1667410766045 }, \"action\" : { \"name\" : \"rollover\", \"start_time\" : 1667411127803, \"index\" : 0, \"failed\" : false, \"consumed_retries\" : 0, \"last_retry_time\" : 0 }, \"step\" : { \"name\" : \"attempt_rollover\", \"start_time\" : 1667411127803, \"step_status\" : \"starting\" }, \"retry_info\" : { \"failed\" : true, \"consumed_retries\" : 0 }, \"info\" : { \"message\" : \"Previous action was not able to update IndexMetaData.\" }, \"enabled\" : false, \"validate\" : { \"validation_message\" : \"Missing rollover_alias index setting [index=test-000001]\", \"validation_status\" : \"re_validating\" } }, \"total_managed_indices\" : 1 } . If you pass the parameter without a value or false, then it doesn’t return the validation status and message. Only if you pass validate_action=true will the response will return the validation status and message. Example request . GET _plugins/_ism/explain/test-000001?validate_action=false --- OR --- GET _plugins/_ism/explain/test-000001 . Example response . { \"test-000001\" : { \"index.plugins.index_state_management.policy_id\" : \"test_rollover\", \"index.opendistro.index_state_management.policy_id\" : \"test_rollover\", \"index\" : \"test-000001\", \"index_uuid\" : \"CgKsxFmQSIa8dWqpbSJmyA\", \"policy_id\" : \"test_rollover\", \"policy_seq_no\" : -2, \"policy_primary_term\" : 0, \"rolled_over\" : false, \"index_creation_date\" : 1667410460649, \"state\" : { \"name\" : \"rollover\", \"start_time\" : 1667410766045 }, \"action\" : { \"name\" : \"rollover\", \"start_time\" : 1667411127803, \"index\" : 0, \"failed\" : false, \"consumed_retries\" : 0, \"last_retry_time\" : 0 }, \"step\" : { \"name\" : \"attempt_rollover\", \"start_time\" : 1667411127803, \"step_status\" : \"starting\" }, \"retry_info\" : { \"failed\" : true, \"consumed_retries\" : 0 }, \"info\" : { \"message\" : \"Previous action was not able to update IndexMetaData.\" }, \"enabled\" : false }, \"total_managed_indices\" : 1 } . ",
    "url": "https://vagimeli.github.io/im-plugin/ism/error-prevention/api/#check-validation-status-and-message-via-the-explain-api",
    "relUrl": "/im-plugin/ism/error-prevention/api/#check-validation-status-and-message-via-the-explain-api"
  },"203": {
    "doc": "ISM Error Prevention",
    "title": "ISM error prevention",
    "content": "Error prevention validates Index State Management (ISM) actions before they are performed in order to prevent actions from failing. It also outputs additional information from the action validation results in the response of the Index Explain API. Validation rules and troubleshooting of each action are listed in the following sections. . | rollover | delete | force_merge | replica_count | open | read_only | read_write | close | index_priority | snapshot | transition | . ",
    "url": "https://vagimeli.github.io/im-plugin/ism/error-prevention/index/#ism-error-prevention",
    "relUrl": "/im-plugin/ism/error-prevention/index/#ism-error-prevention"
  },"204": {
    "doc": "ISM Error Prevention",
    "title": "rollover",
    "content": "ISM does not perform a rollover action for an index under any of these conditions: . | The index is not the write index. | The index does not have an alias. | The rollover policy does not contain a rollover_alias index setting. | Skipping of a rollover action has occured. | The index has already been rolled over using the alias successfully. | . ",
    "url": "https://vagimeli.github.io/im-plugin/ism/error-prevention/index/#rollover",
    "relUrl": "/im-plugin/ism/error-prevention/index/#rollover"
  },"205": {
    "doc": "ISM Error Prevention",
    "title": "delete",
    "content": "ISM does not perform a delete action for an index under any of these conditions: . | The index does not exist. | The index name is invalid. | The index is the write index for a data stream. | . ",
    "url": "https://vagimeli.github.io/im-plugin/ism/error-prevention/index/#delete",
    "relUrl": "/im-plugin/ism/error-prevention/index/#delete"
  },"206": {
    "doc": "ISM Error Prevention",
    "title": "force_merge",
    "content": "ISM does not perform a force_merge action for an index if its dataset is too large and exceeds the threshold. ",
    "url": "https://vagimeli.github.io/im-plugin/ism/error-prevention/index/#force_merge",
    "relUrl": "/im-plugin/ism/error-prevention/index/#force_merge"
  },"207": {
    "doc": "ISM Error Prevention",
    "title": "replica_count",
    "content": "ISM does not perform a replica_count action for an index under any of these conditions: . | The amount of data exceeds the threshold. | The number of shards exceeds the maximum. | . ",
    "url": "https://vagimeli.github.io/im-plugin/ism/error-prevention/index/#replica_count",
    "relUrl": "/im-plugin/ism/error-prevention/index/#replica_count"
  },"208": {
    "doc": "ISM Error Prevention",
    "title": "open",
    "content": "ISM does not perform an open action for an index under any of these conditions: . | The index is blocked. | The number of shards exceeds the maximum. | . ",
    "url": "https://vagimeli.github.io/im-plugin/ism/error-prevention/index/#open",
    "relUrl": "/im-plugin/ism/error-prevention/index/#open"
  },"209": {
    "doc": "ISM Error Prevention",
    "title": "read_only",
    "content": "ISM does not perform a read_only action for an index under any of these conditions: . | The index is blocked. | The amount of data exceeds the threshold. | . ",
    "url": "https://vagimeli.github.io/im-plugin/ism/error-prevention/index/#read_only",
    "relUrl": "/im-plugin/ism/error-prevention/index/#read_only"
  },"210": {
    "doc": "ISM Error Prevention",
    "title": "read_write",
    "content": "ISM does not perform a read_write action for an index if the index is blocked. ",
    "url": "https://vagimeli.github.io/im-plugin/ism/error-prevention/index/#read_write",
    "relUrl": "/im-plugin/ism/error-prevention/index/#read_write"
  },"211": {
    "doc": "ISM Error Prevention",
    "title": "close",
    "content": "ISM does not perform a close action for an index under any of these conditions: . | The index does not exist. | The index name is invalid. | . ",
    "url": "https://vagimeli.github.io/im-plugin/ism/error-prevention/index/#close",
    "relUrl": "/im-plugin/ism/error-prevention/index/#close"
  },"212": {
    "doc": "ISM Error Prevention",
    "title": "index_priority",
    "content": "ISM does not perform an index_priority action for an index that does not have read-only-allow-delete permission. ",
    "url": "https://vagimeli.github.io/im-plugin/ism/error-prevention/index/#index_priority",
    "relUrl": "/im-plugin/ism/error-prevention/index/#index_priority"
  },"213": {
    "doc": "ISM Error Prevention",
    "title": "snapshot",
    "content": "ISM does not perform a snapshot action for an index under any of these conditions: . | The index does not exist. | The index name is invalid. | . ",
    "url": "https://vagimeli.github.io/im-plugin/ism/error-prevention/index/#snapshot",
    "relUrl": "/im-plugin/ism/error-prevention/index/#snapshot"
  },"214": {
    "doc": "ISM Error Prevention",
    "title": "transition",
    "content": "ISM does not perform a transition action for an index under any of these conditions: . | The index does not exist. | The index name is invalid. | . ",
    "url": "https://vagimeli.github.io/im-plugin/ism/error-prevention/index/#transition",
    "relUrl": "/im-plugin/ism/error-prevention/index/#transition"
  },"215": {
    "doc": "ISM Error Prevention",
    "title": "ISM Error Prevention",
    "content": " ",
    "url": "https://vagimeli.github.io/im-plugin/ism/error-prevention/index/",
    "relUrl": "/im-plugin/ism/error-prevention/index/"
  },"216": {
    "doc": "ISM Error Prevention resolutions",
    "title": "ISM error prevention resolutions",
    "content": "Resolutions of errors for each validation rule action are listed in the following sections. . | The index is not the write index | The index does not have an alias | Skipping rollover action is true | This index has already been rolled over successfully | The rollover policy misses rollover_alias index setting | Data too large and exceeding the threshold | Maximum shards exceeded | The index is a write index for some data stream | The index is blocked | . ",
    "url": "https://vagimeli.github.io/im-plugin/ism/error-prevention/resolutions/#ism-error-prevention-resolutions",
    "relUrl": "/im-plugin/ism/error-prevention/resolutions/#ism-error-prevention-resolutions"
  },"217": {
    "doc": "ISM Error Prevention resolutions",
    "title": "The index is not the write index",
    "content": "To confirm that the index is a write index, run the following request: . GET &lt;index&gt;/_alias?pretty . If the response does not contain \"is_write_index\" : true, the index is not a write index. The following example confirms that the index is a write index: . { \"&lt;index&gt;\" : { \"aliases\" : { \"&lt;index_alias&gt;\" : { \"is_write_index\" : true } } } } . To set the index as a write index, run the following request: . PUT &lt;index&gt; { \"aliases\": { \"&lt;index_alias&gt;\" : { \"is_write_index\" : true } } } . ",
    "url": "https://vagimeli.github.io/im-plugin/ism/error-prevention/resolutions/#the-index-is-not-the-write-index",
    "relUrl": "/im-plugin/ism/error-prevention/resolutions/#the-index-is-not-the-write-index"
  },"218": {
    "doc": "ISM Error Prevention resolutions",
    "title": "The index does not have an alias",
    "content": "If the index does not have an alias, you can add one by running the following request: . POST _aliases { \"actions\": [ { \"add\": { \"index\": \"&lt;target_index&gt;\", \"alias\": \"&lt;index_alias&gt;\" } } ] } . ",
    "url": "https://vagimeli.github.io/im-plugin/ism/error-prevention/resolutions/#the-index-does-not-have-an-alias",
    "relUrl": "/im-plugin/ism/error-prevention/resolutions/#the-index-does-not-have-an-alias"
  },"219": {
    "doc": "ISM Error Prevention resolutions",
    "title": "Skipping rollover action is true",
    "content": "In the event that skipping a rollover action occurs, run the following request: . GET &lt;target_index&gt;/_settings?pretty . If you receive the response in the first example, you can reset it by running the request in the second example: . { \"index\": { \"opendistro.index_state_management.rollover_skip\": true } } . PUT &lt;target_index&gt;/_settings { \"index\": { \"index_state_management.rollover_skip\": false } } . ",
    "url": "https://vagimeli.github.io/im-plugin/ism/error-prevention/resolutions/#skipping-rollover-action-is-true",
    "relUrl": "/im-plugin/ism/error-prevention/resolutions/#skipping-rollover-action-is-true"
  },"220": {
    "doc": "ISM Error Prevention resolutions",
    "title": "This index has already been rolled over successfully",
    "content": "Remove the rollover policy from the index to prevent this error from reoccurring. ",
    "url": "https://vagimeli.github.io/im-plugin/ism/error-prevention/resolutions/#this-index-has-already-been-rolled-over-successfully",
    "relUrl": "/im-plugin/ism/error-prevention/resolutions/#this-index-has-already-been-rolled-over-successfully"
  },"221": {
    "doc": "ISM Error Prevention resolutions",
    "title": "The rollover policy misses rollover_alias index setting",
    "content": "Add a rollover_alias index setting to the rollover policy to resolve this issue. Run the following request: . PUT _index_template/ism_rollover { \"index_patterns\": [\"&lt;index_patterns_in_rollover_policy&gt;\"], \"template\": { \"settings\": { \"plugins.index_state_management.rollover_alias\": \"&lt;rollover_alias&gt;\" } } } . ",
    "url": "https://vagimeli.github.io/im-plugin/ism/error-prevention/resolutions/#the-rollover-policy-misses-rollover_alias-index-setting",
    "relUrl": "/im-plugin/ism/error-prevention/resolutions/#the-rollover-policy-misses-rollover_alias-index-setting"
  },"222": {
    "doc": "ISM Error Prevention resolutions",
    "title": "Data too large and exceeding the threshold",
    "content": "Check the JVM information and increase the heap memory. ",
    "url": "https://vagimeli.github.io/im-plugin/ism/error-prevention/resolutions/#data-too-large-and-exceeding-the-threshold",
    "relUrl": "/im-plugin/ism/error-prevention/resolutions/#data-too-large-and-exceeding-the-threshold"
  },"223": {
    "doc": "ISM Error Prevention resolutions",
    "title": "Maximum shards exceeded",
    "content": "The shard limit per node, or per index, causes this issue to occur. Check whether there is a total_shards_per_node limit by running the following request: . GET /_cluster/settings . If the response contains total_shards_per_node, increase its value temporarily by running the following request: . PUT _cluster/settings { \"transient\":{ \"cluster.routing.allocation.total_shards_per_node\":100 } } . To check whether there is a shard limit for an index, run the following request: . GET &lt;index&gt;/_settings/index.routing- . If the response contains the setting in the first example, increase its value or set it to -1 for unlimited shards, as shown in the second example: . \"index\" : { \"routing\" : { \"allocation\" : { \"total_shards_per_node\" : \"10\" } } } . PUT &lt;index&gt;/_settings {\"index.routing.allocation.total_shards_per_node\":-1} . ",
    "url": "https://vagimeli.github.io/im-plugin/ism/error-prevention/resolutions/#maximum-shards-exceeded",
    "relUrl": "/im-plugin/ism/error-prevention/resolutions/#maximum-shards-exceeded"
  },"224": {
    "doc": "ISM Error Prevention resolutions",
    "title": "The index is a write index for some data stream",
    "content": "If you still want to delete the index, check your data stream settings and change the write index. ",
    "url": "https://vagimeli.github.io/im-plugin/ism/error-prevention/resolutions/#the-index-is-a-write-index-for-some-data-stream",
    "relUrl": "/im-plugin/ism/error-prevention/resolutions/#the-index-is-a-write-index-for-some-data-stream"
  },"225": {
    "doc": "ISM Error Prevention resolutions",
    "title": "The index is blocked",
    "content": "Generally, the index is blocked because disk usage has exceeded the flood-stage watermark and the index has a read-only-allow-delete block. To resolve this issue, you can: . | Remove the -index.blocks.read_only_allow_delete- parameter. | Temporarily increase the disk watermarks. | Temporarily disable the disk allocation threshold. | . To prevent the issue from reoccurring, it is better to reduce the usage of the disk by increasing disk space, adding new nodes, or removing data or indexes that are no longer needed. Remove -index.blocks.read_only_allow_delete- by running the following request: . PUT &lt;index&gt;/_settings { \"index.blocks.read_only_allow_delete\": null } . Increase the low disk watermarks by running the following request: . PUT _cluster/settings { \"transient\": { \"cluster\": { \"routing\": { \"allocation\": { \"disk\": { \"watermark\": { \"low\": \"25.0gb\" } } } } } } } . Disable the disk allocation threshold by running the following request: . PUT _cluster/settings { \"transient\": { \"cluster\": { \"routing\": { \"allocation\": { \"disk\": { \"threshold_enabled\" : false } } } } } } . ",
    "url": "https://vagimeli.github.io/im-plugin/ism/error-prevention/resolutions/#the-index-is-blocked",
    "relUrl": "/im-plugin/ism/error-prevention/resolutions/#the-index-is-blocked"
  },"226": {
    "doc": "ISM Error Prevention resolutions",
    "title": "ISM Error Prevention resolutions",
    "content": " ",
    "url": "https://vagimeli.github.io/im-plugin/ism/error-prevention/resolutions/",
    "relUrl": "/im-plugin/ism/error-prevention/resolutions/"
  },"227": {
    "doc": "Index State Management",
    "title": "Index State Management",
    "content": "OpenSearch Dashboards . If you analyze time-series data, you likely prioritize new data over old data. You might periodically perform certain operations on older indexes, such as reducing replica count or deleting them. Index State Management (ISM) is a plugin that lets you automate these periodic, administrative operations by triggering them based on changes in the index age, index size, or number of documents. Using the ISM plugin, you can define policies that automatically handle index rollovers or deletions to fit your use case. For example, you can define a policy that moves your index into a read_only state after 30 days and then deletes it after a set period of 90 days. You can also set up the policy to send you a notification message when the index is deleted. You might want to perform an index rollover after a certain amount of time or run a force_merge operation on an index during off-peak hours to improve search performance during peak hours. To use the ISM plugin, your user role needs to be mapped to the all_access role that gives you full access to the cluster. To learn more, see Users and roles. ",
    "url": "https://vagimeli.github.io/im-plugin/ism/index/",
    "relUrl": "/im-plugin/ism/index/"
  },"228": {
    "doc": "Index State Management",
    "title": "Get started with ISM",
    "content": "To get started, choose Index Management in OpenSearch Dashboards. Step 1: Set up policies . A policy is a set of rules that describes how an index should be managed. For information about creating a policy, see Policies. You can use the visual editor or JSON editor to create policies. Compared to the JSON editor, the visual editor offers a more structured way of defining policies by separating the process into creating error notifications, defining ISM templates, and adding states. We recommend using the visual editor if you want to see pre-defined fields, such as which actions you can assign to a state or under what conditions a state can transition into a destination state. Visual editor . | Choose the Index Policies tab. | Choose Create policy. | Choose Visual editor. | In the Policy info section, enter a policy ID and an optional description. | In the Error notification section, set up an optional error notification that gets sent whenever a policy execution fails. For more information, see Error notifications. If you’re using auto rollovers in your policy, we recommend setting up error notifications, which notify you of unexpectedly large indexes if rollovers fail. | In ISM templates, enter any ISM template patterns to automatically apply this policy to future indexes. For example, if you specify a template of sample-index*, the ISM plugin automatically applies this policy to any indexes whose names start with sample-index. Your pattern cannot contain any of the following characters: :, \", +, /, \\, |, ?, #, &gt;, and &lt;. | In States, add any states you want to include in the policy. Each state has actions the plugin executes when the index enters a certain state, and transitions, which have conditions that, when met, transition the index into a destination state. The first state you create in a policy is automatically set as the initial state. Each policy must have at least one state, but actions and transitions are optional. | Choose Create. | . JSON editor . | Choose the Index Policies tab. | Choose Create policy. | Choose JSON editor. | In the Name policy section, enter a policy ID. | In the Define policy section, enter your policy. | Choose Create. | . After you create a policy, your next step is to attach it to an index or indexes. You can set up an ism_template in the policy so when an index that matches the ISM template pattern is created, the plugin automatically attaches the policy to the index. The following example demonstrates how to create a policy that automatically gets attached to all indexes whose names start with index_name-. PUT _plugins/_ism/policies/policy_id { \"policy\": { \"description\": \"Example policy.\", \"default_state\": \"...\", \"states\": [...], \"ism_template\": { \"index_patterns\": [\"index_name-*\"], \"priority\": 100 } } } . If you have more than one template that matches an index pattern, ISM uses the priority value to determine which template to apply. For an example ISM template policy, see Sample policy with ISM template for auto rollover. Older versions of the plugin include the policy_id in an index template, so when an index is created that matches the index template pattern, the index will have the policy attached to it: . PUT _index_template/&lt;template_name&gt; { \"index_patterns\": [ \"index_name-*\" ], \"template\": { \"settings\": { \"opendistro.index_state_management.policy_id\": \"policy_id\" } } } . The opendistro.index_state_management.policy_id setting is deprecated. You can continue to automatically manage newly created indexes with the ISM template field. Step 2: Attach policies to indexes . | Choose indexes. | Choose the index or indexes that you want to attach your policy to. | Choose Apply policy. | From the Policy ID menu, choose the policy that you created. You can see a preview of your policy. | If your policy includes a rollover operation, specify a rollover alias. Make sure that the alias that you enter already exists. For more information about the rollover operation, see rollover. | Choose Apply. | . After you attach a policy to an index, ISM creates a job that runs every 5 minutes by default to perform policy actions, check conditions, and transition the index into different states. To change the default time interval for this job, see Settings. ISM does not run jobs if the cluster state is red. Step 3: Manage indexes . | Choose Managed indexes. | To change your policy, see Change Policy. | To attach a rollover alias to your index, select your policy and choose Add rollover alias. Make sure that the alias that you enter already exists. For more information about the rollover operation, see rollover. | To remove a policy, choose your policy, and then choose Remove policy. | To retry a policy, choose your policy, and then choose Retry policy. | . For information about managing your policies, see Managed indexes. ",
    "url": "https://vagimeli.github.io/im-plugin/ism/index/#get-started-with-ism",
    "relUrl": "/im-plugin/ism/index/#get-started-with-ism"
  },"229": {
    "doc": "Managed indexes",
    "title": "Managed indexes",
    "content": "You can change or update a policy using the managed index operations. This table lists the fields of managed index operations. | Parameter | Description | Type | Required | Read Only | . | name | The name of the managed index policy. | string | Yes | No | . | index | The name of the managed index that this policy is managing. | string | Yes | No | . | index_uuid | The uuid of the index. | string | Yes | No | . | enabled | When true, the managed index is scheduled and run by the scheduler. | boolean | Yes | No | . | enabled_time | The time the managed index was last enabled. If the managed index process is disabled, then this is null. | timestamp | Yes | Yes | . | last_updated_time | The time the managed index was last updated. | timestamp | Yes | Yes | . | schedule | The schedule of the managed index job. | object | Yes | No | . | policy_id | The name of the policy used by this managed index. | string | Yes | No | . | policy_seq_no | The sequence number of the policy used by this managed index. | number | Yes | No | . | policy_primary_term | The primary term of the policy used by this managed index. | number | Yes | No | . | policy_version | The version of the policy used by this managed index. | number | Yes | Yes | . | policy | The cached JSON of the policy for the policy_version that’s used during runs. If the policy is null, it means that this is the first execution of the job and the latest policy document is read in/saved. | object | No | No | . | change_policy | The information regarding what policy and state to change to. | object | No | No | . | policy_name | The name of the policy to update to. To update to the latest version, set this to be the same as the current policy_name. | string | No | Yes | . | state | The state of the managed index after it finishes updating. If no state is specified, it’s assumed that the policy structure did not change. | string | No | Yes | . The following example shows a managed index policy: . { \"managed_index\": { \"name\": \"my_index\", \"index\": \"my_index\", \"index_uuid\": \"sOKSOfkdsoSKeofjIS\", \"enabled\": true, \"enabled_time\": 1553112384, \"last_updated_time\": 1553112384, \"schedule\": { \"interval\": { \"period\": 1, \"unit\": \"MINUTES\", \"start_time\": 1553112384 } }, \"policy_id\": \"log_rotation\", \"policy_version\": 1, \"policy\": {...}, \"change_policy\": null } } . ",
    "url": "https://vagimeli.github.io/im-plugin/ism/managedindexes/",
    "relUrl": "/im-plugin/ism/managedindexes/"
  },"230": {
    "doc": "Managed indexes",
    "title": "Change policy",
    "content": "You can change any managed index policy, but ISM has a few constraints in place to make sure that policy changes don’t break indexes. If an index is stuck in its current state, never proceeding, and you want to update its policy immediately, make sure that the new policy includes the same state—same name, same actions, same order—as the old policy. In this case, even if the policy is in the middle of executing an action, ISM applies the new policy. If you update the policy without including an identical state, ISM updates the policy only after all actions in the current state finish executing. Alternately, you can choose a specific state in your old policy after which you want the new policy to take effect. To change a policy using OpenSearch Dashboards, do the following: . | Under Index Management, choose the indexes that you want to attach the new policy to. | To attach the new policy to indexes in specific states, choose Choose state filters, and then choose those states. | Under Choose New Policy, choose the new policy. | To start the new policy for indexes in the current state, choose Keep indices in their current state after the policy takes effect. | To start the new policy in a specific state, choose Start from a chosen state after changing policies, and then choose the default start state in your new policy. | . ",
    "url": "https://vagimeli.github.io/im-plugin/ism/managedindexes/#change-policy",
    "relUrl": "/im-plugin/ism/managedindexes/#change-policy"
  },"231": {
    "doc": "Policies",
    "title": "Policies",
    "content": "Policies are JSON documents that define the following: . | The states that an index can be in, including the default state for new indexes. For example, you might name your states “hot,” “warm,” “delete,” and so on. For more information, see States. | Any actions that you want the plugin to take when an index enters a state, such as performing a rollover. For more information, see Actions. | The conditions that must be met for an index to move into a new state, known as transitions. For example, if an index is more than eight weeks old, you might want to move it to the “delete” state. For more information, see Transitions. | . In other words, a policy defines the states that an index can be in, the actions to perform when in a state, and the conditions that must be met to transition between states. You have complete flexibility in the way you can design your policies. You can create any state, transition to any other state, and specify any number of actions in each state. This table lists the relevant fields of a policy. | Field | Description | Type | Required | Read Only | . | policy_id | The name of the policy. | string | Yes | Yes | . | description | A human-readable description of the policy. | string | Yes | No | . | ism_template | Specify an ISM template pattern that matches the index to apply the policy. | nested list of objects | No | No | . | last_updated_time | The time the policy was last updated. | timestamp | Yes | Yes | . | error_notification | The destination and message template for error notifications. The destination could be Amazon Chime, Slack, or a webhook URL. | object | No | No | . | default_state | The default starting state for each index that uses this policy. | string | Yes | No | . | states | The states that you define in the policy. | nested list of objects | Yes | No | . | States | Actions | ISM supported operations . | force_merge | read_only | read_write | replica_count | shrink | close | open | delete | rollover | notification | snapshot | index_priority | allocation | rollup | . | Transitions | Error notifications | Sample policy with ISM template for auto rollover | Example policy with ISM templates for the alias action | Example policy | . ",
    "url": "https://vagimeli.github.io/im-plugin/ism/policies/",
    "relUrl": "/im-plugin/ism/policies/"
  },"232": {
    "doc": "Policies",
    "title": "States",
    "content": "A state is the description of the status that the managed index is currently in. A managed index can be in only one state at a time. Each state has associated actions that are executed sequentially on entering a state and transitions that are checked after all the actions have been completed. This table lists the parameters that you can define for a state. | Field | Description | Type | Required | . | name | The name of the state. | string | Yes | . | actions | The actions to execute after entering a state. For more information, see Actions. | nested list of objects | Yes | . | transitions | The next states and the conditions required to transition to those states. If no transitions exist, the policy assumes that it’s complete and can now stop managing the index. For more information, see Transitions. | nested list of objects | Yes | . ",
    "url": "https://vagimeli.github.io/im-plugin/ism/policies/#states",
    "relUrl": "/im-plugin/ism/policies/#states"
  },"233": {
    "doc": "Policies",
    "title": "Actions",
    "content": "Actions are the steps that the policy sequentially executes on entering a specific state. ISM executes actions in the order in which they are defined. For example, if you define actions [A,B,C,D], ISM executes action A, and then goes into a sleep period based on the cluster setting plugins.index_state_management.job_interval. Once the sleep period ends, ISM continues to execute the remaining actions. However, if ISM cannot successfully execute action A, the operation ends, and actions B, C, and D do not get executed. Optionally, you can define an action’s timeout period, which, if exceeded, forcibly fails the action. For example, if timeout is set to 1d, and ISM has not completed the action within one day, even after retries, the action fails. This table lists the parameters that you can define for an action. | Parameter | Description | Type | Required | Default | . | timeout | The timeout period for the action. Accepts time units for minutes, hours, and days. | time unit | No | - | . | retry | The retry configuration for the action. | object | No | Specific to action | . The retry operation has the following parameters: . | Parameter | Description | Type | Required | Default | . | count | The number of retry counts. | number | Yes | - | . | backoff | The backoff policy type to use when retrying. Valid values are Exponential, Constant, and Linear. | string | No | Exponential | . | delay | The time to wait between retries. Accepts time units for minutes, hours, and days. | time unit | No | 1 minute | . The following example action has a timeout period of one hour. The policy retries this action three times with an exponential backoff policy, with a delay of 10 minutes between each retry: . \"actions\": { \"timeout\": \"1h\", \"retry\": { \"count\": 3, \"backoff\": \"exponential\", \"delay\": \"10m\" } } . For a list of available unit types, see Supported units. ",
    "url": "https://vagimeli.github.io/im-plugin/ism/policies/#actions",
    "relUrl": "/im-plugin/ism/policies/#actions"
  },"234": {
    "doc": "Policies",
    "title": "ISM supported operations",
    "content": "ISM supports the following operations: . | force_merge | read_only | read_write | replica_count | shrink | close | open | delete | rollover | notification | snapshot | index_priority | allocation | rollup | . force_merge . Reduces the number of Lucene segments by merging the segments of individual shards. This operation attempts to set the index to a read-only state before starting the merging process. | Parameter | Description | Type | Required | . | max_num_segments | The number of segments to reduce the shard to. | number | Yes | . | wait_for_completion | Boolean | When set to false, the request returns immediately instead of after the operation is finished. To monitor the operation status, use the Tasks API with the task ID returned by the request. Default is true. |   | . | task_execution_timeout | Time | The explicit task execution timeout. Only useful when wait_for_completion is set to false. Default is 1h. | No | . { \"force_merge\": { \"max_num_segments\": 1 } } . read_only . Sets a managed index to be read only. { \"read_only\": {} } . Set the index setting index.blocks.write to true for a managed index. *Note: this block does not prevent the index from refreshing. read_write . Sets a managed index to be writeable. { \"read_write\": {} } . replica_count . Sets the number of replicas to assign to an index. | Parameter | Description | Type | Required | . | number_of_replicas | Defines the number of replicas to assign to an index. | number | Yes | . { \"replica_count\": { \"number_of_replicas\": 2 } } . For information about setting replicas, see Primary and replica shards. shrink . Allows you to reduce the number of primary shards in your indexes. With this action, you can specify: . | The number of primary shards that the target index should contain. | A max shard size for the primary shards in the target index. | Specify a percentage to shrink the number of primary shards in the target index. | . \"shrink\": { \"num_new_shards\": 1, \"target_index_name_template\": { \"source\": \"_shrunken\" }, \"aliases\": [ { \"my-alias\": {} } ], \"force_unsafe\": false } . | Parameter | Description | Type | Example | Required | . | num_new_shards | The maximum number of primary shards in the shrunken index. | integer | 5 | Yes, however it cannot be used with max_shard_size or percentage_of_source_shards | . | max_shard_size | The maximum size in bytes of a shard for the target index. | keyword | 5gb | Yes, however it cannot be used with num_new_shards or percentage_of_source_shards | . | percentage_of_source_shards | Percentage of the number of original primary shards to shrink. This parameter indicates the minimum percentage to use when shrinking the number of primary shards. Must be between 0.0 and 1.0, exclusive. | Percentage | 0.5 | Yes, however it cannot be used with max_shard_size or num_new_shards | . | target_index_name_template | The name of the shrunken index. Accepts strings and the Mustache variables and. | string or Mustache template | {\"source\": \"_shrunken\"} | No | . | aliases | Aliases to add to the new index. | object | myalias | No, but must be an array of alias objects | . | force_unsafe | If true, executes the shrink action even if there are no replicas. | boolean | false | No | . If you want to add aliases to the action, the parameter must include an array of alias objects. For example, . \"aliases\": [ { \"my-alias\": {} }, { \"my-second-alias\": { \"is_write_index\": false, \"filter\": { \"multi_match\": { \"query\": \"QUEEN\", \"fields\": [\"speaker\", \"text_entry\"] } }, \"index_routing\" : \"1\", \"search_routing\" : \"1\" } }, ] . close . Closes the managed index. { \"close\": {} } . Closed indexes remain on disk, but consume no CPU or memory. You can’t read from, write to, or search closed indexes. Closing an index is a good option if you need to retain data for longer than you need to actively search it and have sufficient disk space on your data nodes. If you need to search the data again, reopening a closed index is simpler than restoring an index from a snapshot. open . Opens a managed index. { \"open\": {} } . delete . Deletes a managed index. { \"delete\": {} } . rollover . Rolls an alias over to a new index when the managed index meets one of the rollover conditions. ISM checks the conditions for operations on every execution of the policy based on the set interval, not continuously. The rollover will be performed if the value has reached or has exceeded the configured limit when the check is performed. For example with min_size configured to a value of 100GiB, ISM might check the index at 99 GiB and not perform the rollover. However, if the index has grown past the limit (e.g., 105GiB) by the next check, the operation is performed. If you need to skip the rollover action, you can set the index setting index.plugins.index_state_management.rollover_skip to true. For example, if you receive the error message “Missing alias or not the write index…”, you can set the index.plugins.index_state_management.rollover_skip parameter to true and retry to skip rollover action. The index format must match the pattern: ^.*-\\d+$. For example, (logs-000001). Set index.plugins.index_state_management.rollover_alias as the alias to rollover. | Parameter | Description | Type | Example | Required | . | min_size | The minimum size of the total primary shard storage (not counting replicas) required to roll over the index. For example, if you set min_size to 100 GiB and your index has 5 primary shards and 5 replica shards of 20 GiB each, the total size of all primary shards is 100 GiB, so the rollover occurs. See Important note above. | string | 20gb or 5mb | No | . | min_primary_shard_size | The minimum storage size of a single primary shard required to roll over the index. For example, if you set min_primary_shard_size to 30 GiB and one of the primary shards in the index has a size greater than the condition, the rollover occurs. See Important note above. | string | 20gb or 5mb | No | . | min_doc_count | The minimum number of documents required to roll over the index. See Important note above. | number | 2000000 | No | . | min_index_age | The minimum age required to roll over the index. Index age is the time between its creation and the present. Supported units are d (days), h (hours), m (minutes), s (seconds), ms (milliseconds), and micros (microseconds). See Important note above. | string | 5d or 7h | No | . { \"rollover\": { \"min_size\": \"50gb\" } } . { \"rollover\": { \"min_primary_shard_size\": \"30gb\" } } . { \"rollover\": { \"min_doc_count\": 100000000 } } . { \"rollover\": { \"min_index_age\": \"30d\" } } . notification . Sends you a notification. | Parameter | Description | Type | Required | . | destination | The destination URL. | Slack, Amazon Chime, or webhook URL | Yes | . | message_template | The text of the message. You can add variables to your messages using Mustache templates. | object | Yes | . The destination system must return a response otherwise the notification operation throws an error. Example 1: Chime notification . { \"notification\": { \"destination\": { \"chime\": { \"url\": \"&lt;url&gt;\" } }, \"message_template\": { \"source\": \"the index is {{ctx.index}}\" } } } . Example 2: Custom webhook notification . { \"notification\": { \"destination\": { \"custom_webhook\": { \"url\": \"https://&lt;your_webhook&gt;\" } }, \"message_template\": { \"source\": \"the index is {{ctx.index}}\" } } } . Example 3: Slack notification . { \"notification\": { \"destination\": { \"slack\": { \"url\": \"https://hooks.slack.com/services/xxx/xxxxxx\" } }, \"message_template\": { \"source\": \"the index is {{ctx.index}}\" } } } . You can use ctx variables in your message to represent a number of policy parameters based on the past executions of your policy. For example, if your policy has a rollover action, you can use {{ctx.action.name}} in your message to represent the name of the rollover. The following ctx variable options are available for every policy: . Guaranteed variables . | Parameter | Description | Type | . | index | The name of the index. | string | . | index_uuid | The uuid of the index. | string | . | policy_id | The name of the policy. | string | . snapshot . Back up your cluster’s indexes and state. For more information about snapshots, see Take and restore snapshots. The snapshot operation has the following parameters: . | Parameter | Description | Type | Required | Default | . | repository | The repository name that you register through the native snapshot API operations. | string | Yes | - | . | snapshot | The name of the snapshot. Accepts strings and the Mustache variables and. If the Mustache variables are invalid, then the snapshot name defaults to the index’s name. | string or Mustache template | Yes | - | . { \"snapshot\": { \"repository\": \"my_backup\", \"snapshot\": \"\" } } . index_priority . Set the priority for the index in a specific state. Unallocated shards of indexes are recovered in the order of their priority, whenever possible. The indexes with higher priority values are recovered first followed by the indexes with lower priority values. The index_priority operation has the following parameter: . | Parameter | Description | Type | Required | Default | . | priority | The priority for the index as soon as it enters a state. | number | Yes | 1 | . \"actions\": [ { \"index_priority\": { \"priority\": 50 } } ] . allocation . Allocate the index to a node with a specific attribute set like this. For example, setting require to warm moves your data only to “warm” nodes. The allocation operation has the following parameters: . | Parameter | Description | Type | Required | . | require | Allocate the index to a node with a specified attribute. | string | Yes | . | include | Allocate the index to a node with any of the specified attributes. | string | Yes | . | exclude | Don’t allocate the index to a node with any of the specified attributes. | string | Yes | . | wait_for | Wait for the policy to execute before allocating the index to a node with a specified attribute. | string | Yes | . \"actions\": [ { \"allocation\": { \"require\": { \"temp\": \"warm\" } } } ] . rollup . Index rollup lets you periodically reduce data granularity by rolling up old data into summarized indexes. Rollup jobs can be continuous or non-continuous. A rollup job created using an ISM policy can only be non-continuous. Path and HTTP methods . PUT _plugins/_rollup/jobs/&lt;rollup_id&gt; GET _plugins/_rollup/jobs/&lt;rollup_id&gt; DELETE _plugins/_rollup/jobs/&lt;rollup_id&gt; POST _plugins/_rollup/jobs/&lt;rollup_id&gt;/_start POST _plugins/_rollup/jobs/&lt;rollup_id&gt;/_stop GET _plugins/_rollup/jobs/&lt;rollup_id&gt;/_explain . Sample ISM rollup policy . { \"policy\": { \"description\": \"Sample rollup\" , \"default_state\": \"rollup\", \"states\": [ { \"name\": \"rollup\", \"actions\": [ { \"rollup\": { \"ism_rollup\": { \"description\": \"Creating rollup through ISM\", \"target_index\": \"target\", \"page_size\": 1000, \"dimensions\": [ { \"date_histogram\": { \"fixed_interval\": \"60m\", \"source_field\": \"order_date\", \"target_field\": \"order_date\", \"timezone\": \"America/Los_Angeles\" } }, { \"terms\": { \"source_field\": \"customer_gender\", \"target_field\": \"customer_gender\" } }, { \"terms\": { \"source_field\": \"day_of_week\", \"target_field\": \"day_of_week\" } } ], \"metrics\": [ { \"source_field\": \"taxless_total_price\", \"metrics\": [ { \"sum\": {} } ] }, { \"source_field\": \"total_quantity\", \"metrics\": [ { \"avg\": {} }, { \"max\": {} } ] } ] } } } ], \"transitions\": [] } ] } } . Request fields . Request fields are required when creating an ISM policy. You can reference the Index rollups API page for request field options. Adding a rollup policy in Dashboards . To add a rollup policy in Dashboards, follow the steps below. | Select the menu button on the top-left of the Dashboards user interface. | In the Dashboards menu, select Index Management. | On the next screen select Rollup jobs. | Select the Create rollup button. | Follow the steps in the Create rollup job wizard. | Add a name for the policy in the Name box. | You can reference the Index rollups API page to configure the rollup policy. | Finally, select the Create button on the bottom-right of the Dashboards user interface. | . ",
    "url": "https://vagimeli.github.io/im-plugin/ism/policies/#ism-supported-operations",
    "relUrl": "/im-plugin/ism/policies/#ism-supported-operations"
  },"235": {
    "doc": "Policies",
    "title": "Transitions",
    "content": "Transitions define the conditions that need to be met for a state to change. After all actions in the current state are completed, the policy starts checking the conditions for transitions. ISM evaluates transitions in the order in which they are defined. For example, if you define transitions: [A,B,C,D], ISM iterates through this list of transitions until it finds a transition that evaluates to true, it then stops and sets the next state to the one defined in that transition. On its next execution, ISM dismisses the rest of the transitions and starts in that new state. If you don’t specify any conditions in a transition and leave it empty, then it’s assumed to be the equivalent of always true. This means that the policy transitions the index to this state the moment it checks. This table lists the parameters you can define for transitions. | Parameter | Description | Type | Required | . | state_name | The name of the state to transition to if the conditions are met. | string | Yes | . | conditions | List the conditions for the transition. | list | Yes | . The conditions object has the following parameters: . | Parameter | Description | Type | Required | . | min_index_age | The minimum age of the index required to transition. | string | No | . | min_rollover_age | The minimum age required after a rollover has occurred to transition to the next state. | string | No | . | min_doc_count | The minimum document count of the index required to transition. | number | No | . | min_size | The minimum size of the total primary shard storage (not counting replicas) required to transition. For example, if you set min_size to 100 GiB and your index has 5 primary shards and 5 replica shards of 20 GiB each, the total size of all primary shards is 100 GiB, so your index is transitioned to the next state. | string | No | . | cron | The cron job that triggers the transition if no other transition happens first. | object | No | . | cron.cron.expression | The cron expression that triggers the transition. | string | Yes | . | cron.cron.timezone | The timezone that triggers the transition. | string | Yes | . The following example transitions the index to a cold state after a period of 30 days: . \"transitions\": [ { \"state_name\": \"cold\", \"conditions\": { \"min_index_age\": \"30d\" } } ] . ISM checks the conditions on every execution of the policy based on the set interval. This example uses the cron condition to transition indexes every Saturday at 5:00 PT: . \"transitions\": [ { \"state_name\": \"cold\", \"conditions\": { \"cron\": { \"cron\": { \"expression\": \"* 17 * * SAT\", \"timezone\": \"America/Los_Angeles\" } } } } ] . Note that this condition does not execute at exactly 5:00 PM; the job still executes based off the job_interval setting. Due to this variance in start time and the amount of time that it can take for actions to complete prior to checking transition conditions, we recommend against overly narrow cron expressions. For example, don’t use 15 17 * * SAT (5:15 PM on Saturday). A window of an hour, which this example uses, is generally sufficient, but you might increase it to 2–3 hours to avoid missing the window and having to wait a week for the transition to occur. Alternately, you could use a broader expression such as * * * * SAT,SUN to have the transition occur at any time during the weekend. For information on writing cron expressions, see Cron expression reference. ",
    "url": "https://vagimeli.github.io/im-plugin/ism/policies/#transitions",
    "relUrl": "/im-plugin/ism/policies/#transitions"
  },"236": {
    "doc": "Policies",
    "title": "Error notifications",
    "content": "The error_notification operation sends you a notification if your managed index fails. It notifies a single destination or notification channel with a custom message. Set up error notifications at the policy level: . { \"policy\": { \"description\": \"hot warm delete workflow\", \"default_state\": \"hot\", \"schema_version\": 1, \"error_notification\": { }, \"states\": [ ] } } . | Parameter | Description | Type | Required | . | destination | The destination URL. | Slack, Amazon Chime, or webhook URL | Yes if channel isn’t specified | . | channel | A notification channel’s ID | string | Yes if destination isn’t specified | . | message_template | The text of the message. You can add variables to your messages using Mustache templates. | object | Yes | . The destination system must return a response otherwise the error_notification operation throws an error. Example 1: Chime notification . { \"error_notification\": { \"destination\": { \"chime\": { \"url\": \"&lt;url&gt;\" } }, \"message_template\": { \"source\": \"The index {{ctx.index}} failed during policy execution.\" } } } . Example 2: Custom webhook notification . { \"error_notification\": { \"destination\": { \"custom_webhook\": { \"url\": \"https://&lt;your_webhook&gt;\" } }, \"message_template\": { \"source\": \"The index {{ctx.index}} failed during policy execution.\" } } } . Example 3: Slack notification . { \"error_notification\": { \"destination\": { \"slack\": { \"url\": \"https://hooks.slack.com/services/xxx/xxxxxx\" } }, \"message_template\": { \"source\": \"The index {{ctx.index}} failed during policy execution.\" } } } . Example 4: Using a notification channel . { \"error_notification\": { \"channel\": { \"id\": \"some-channel-config-id\" }, \"message_template\": { \"source\": \"The index {{ctx.index}} failed during policy execution.\" } } } . You can use the same options for ctx variables as the notification operation. ",
    "url": "https://vagimeli.github.io/im-plugin/ism/policies/#error-notifications",
    "relUrl": "/im-plugin/ism/policies/#error-notifications"
  },"237": {
    "doc": "Policies",
    "title": "Sample policy with ISM template for auto rollover",
    "content": "The following sample template policy is for a rollover use case. If you want to skip rollovers for an index, set index.plugins.index_state_management.rollover_skip to true in the settings of that index. | Create a policy with an ism_template field: . PUT _plugins/_ism/policies/rollover_policy { \"policy\": { \"description\": \"Example rollover policy.\", \"default_state\": \"rollover\", \"states\": [ { \"name\": \"rollover\", \"actions\": [ { \"rollover\": { \"min_doc_count\": 1 } } ], \"transitions\": [] } ], \"ism_template\": { \"index_patterns\": [\"log*\"], \"priority\": 100 } } } . You need to specify the index_patterns field. If you don’t specify a value for priority, it defaults to 0. | Set up a template with the rollover_alias as log : . PUT _index_template/ism_rollover { \"index_patterns\": [\"log*\"], \"template\": { \"settings\": { \"plugins.index_state_management.rollover_alias\": \"log\" } } } . | Create an index with the log alias: . PUT log-000001 { \"aliases\": { \"log\": { \"is_write_index\": true } } } . | Index a document to trigger the rollover condition: . POST log/_doc { \"message\": \"dummy\" } . | Verify if the policy is attached to the log-000001 index: . GET _plugins/_ism/explain/log-000001?pretty . | . ",
    "url": "https://vagimeli.github.io/im-plugin/ism/policies/#sample-policy-with-ism-template-for-auto-rollover",
    "relUrl": "/im-plugin/ism/policies/#sample-policy-with-ism-template-for-auto-rollover"
  },"238": {
    "doc": "Policies",
    "title": "Example policy with ISM templates for the alias action",
    "content": "The following example policy is for an alias action use case. In the following example, the first job will trigger the rollover action, and a new index will be created. Next, another document is added to the two indexes. The new job will then cause the second index to point to the log alias, and the older index will be removed due to the alias action. First, create an ISM policy: . PUT /_plugins/_ism/policies/rollover_policy?pretty { \"policy\": { \"description\": \"Example rollover policy.\", \"default_state\": \"rollover\", \"states\": [ { \"name\": \"rollover\", \"actions\": [ { \"rollover\": { \"min_doc_count\": 1 } } ], \"transitions\": [{ \"state_name\": \"alias\", \"conditions\": { \"min_doc_count\": \"2\" } }] }, { \"name\": \"alias\", \"actions\": [ { \"alias\": { \"actions\": [ { \"remove\": { \"alias\": \"log\" } } ] } } ] } ], \"ism_template\": { \"index_patterns\": [\"log*\"], \"priority\": 100 } } } . Next, create an index template on which to enable the policy: . PUT /_index_template/ism_rollover? { \"index_patterns\": [\"log*\"], \"template\": { \"settings\": { \"plugins.index_state_management.rollover_alias\": \"log\" } } } . copy . Next, change the cluster settings to trigger jobs every minute: . PUT /_cluster/settings?pretty=true { \"persistent\" : { \"plugins.index_state_management.job_interval\" : 1 } } . copy . Next, create a new index: . PUT /log-000001 { \"aliases\": { \"log\": { \"is_write_index\": true } } } . copy . Finally, add a document to the index to trigger the job: . POST /log-000001/_doc { \"message\": \"dummy\" } . copy . You can verify these steps using the Alias and Index API: . GET /_cat/indices?pretty . copy . GET /_cat/aliases?pretty . copy . Note: The index and remove_index parameters are not allowed with alias action policies. Only the add and remove alias action parameters are allowed. ",
    "url": "https://vagimeli.github.io/im-plugin/ism/policies/#example-policy-with-ism-templates-for-the-alias-action",
    "relUrl": "/im-plugin/ism/policies/#example-policy-with-ism-templates-for-the-alias-action"
  },"239": {
    "doc": "Policies",
    "title": "Example policy",
    "content": "The following example policy implements a hot, warm, and delete workflow. You can use this policy as a template to prioritize resources to your indexes based on their levels of activity. In this case, an index is initially in a hot state. After a day, it changes to a warm state, where the number of replicas increases to 5 to improve the read performance. After 30 days, the policy moves this index into a delete state. The service sends a notification to a Chime room that the index is being deleted, and then permanently deletes it. { \"policy\": { \"description\": \"hot warm delete workflow\", \"default_state\": \"hot\", \"schema_version\": 1, \"states\": [ { \"name\": \"hot\", \"actions\": [ { \"rollover\": { \"min_index_age\": \"1d\", \"min_primary_shard_size\": \"30gb\" } } ], \"transitions\": [ { \"state_name\": \"warm\" } ] }, { \"name\": \"warm\", \"actions\": [ { \"replica_count\": { \"number_of_replicas\": 5 } } ], \"transitions\": [ { \"state_name\": \"delete\", \"conditions\": { \"min_index_age\": \"30d\" } } ] }, { \"name\": \"delete\", \"actions\": [ { \"notification\": { \"destination\": { \"chime\": { \"url\": \"&lt;URL&gt;\" } }, \"message_template\": { \"source\": \"The index {{ctx.index}} is being deleted\" } } }, { \"delete\": {} } ] } ], \"ism_template\": { \"index_patterns\": [\"log*\"], \"priority\": 100 } } } . This diagram shows the states, transitions, and actions of the above policy as a finite-state machine. For more information about finite-state machines, see Wikipedia. ",
    "url": "https://vagimeli.github.io/im-plugin/ism/policies/#example-policy",
    "relUrl": "/im-plugin/ism/policies/#example-policy"
  },"240": {
    "doc": "Settings",
    "title": "ISM settings",
    "content": "We don’t recommend changing these settings; the defaults should work well for most use cases. Index State Management (ISM) stores its configuration in the .opendistro-ism-config index. Don’t modify this index without using the ISM API operations. All settings are available using the OpenSearch _cluster/settings operation. None require a restart, and all can be marked persistent or transient. | Setting | Default | Description | . | plugins.index_state_management.enabled | True | Specifies whether ISM is enabled or not. | . | plugins.index_state_management.job_interval | 5 minutes | The interval at which the managed index jobs are run. | . | plugins.index_state_management.jitter | 0.6 | A randomized delay that is added to a job’s base run time to prevent a surge of activity from all indexes at the same time. A value of 0.6 means a delay of 0-60% of a job interval is added to the base interval. For example, if you have a base interval time of 30 minutes, a value of 0.6 means an amount anywhere between 0 to 18 minutes gets added to your job interval. Maximum is 1, which means an additional interval time of 100%. This maximum cannot exceed plugins.jobscheduler.jitter_limit, which also has a default of 0.6. For example, if plugins.index_state_management.jitter is set to 0.8, ISM uses plugins.jobscheduler.jitter_limit of 0.6 instead. | . | plugins.index_state_management.coordinator.sweep_period | 10 minutes | How often the routine background sweep is run. | . | plugins.index_state_management.coordinator.backoff_millis | 50 milliseconds | The backoff time between retries for failures in the ManagedIndexCoordinator (such as when we update managed indexes). | . | plugins.index_state_management.coordinator.backoff_count | 2 | The count of retries for failures in the ManagedIndexCoordinator. | . | plugins.index_state_management.history.enabled | True | Specifies whether audit history is enabled or not. The logs from ISM are automatically indexed to a logs document. | . | plugins.index_state_management.history.max_docs | 2,500,000 | The maximum number of documents before rolling over the audit history index. | . | plugins.index_state_management.history.max_age | 24 hours | The maximum age before rolling over the audit history index. | . | plugins.index_state_management.history.rollover_check_period | 8 hours | The time between rollover checks for the audit history index. | . | plugins.index_state_management.history.rollover_retention_period | 30 days | How long audit history indexes are kept. | . | plugins.index_state_management.allow_list | All actions | List of actions that you can use. | . ",
    "url": "https://vagimeli.github.io/im-plugin/ism/settings/#ism-settings",
    "relUrl": "/im-plugin/ism/settings/#ism-settings"
  },"241": {
    "doc": "Settings",
    "title": "Settings",
    "content": " ",
    "url": "https://vagimeli.github.io/im-plugin/ism/settings/",
    "relUrl": "/im-plugin/ism/settings/"
  },"242": {
    "doc": "Reindex data",
    "title": "Reindex data",
    "content": "After creating an index, you might need to make an extensive change such as adding a new field to every document or combining multiple indexes to form a new one. Rather than deleting your index, making the change offline, and then indexing your data again, you can use the reindex operation. With the reindex operation, you can copy all or a subset of documents that you select through a query to another index. Reindex is a POST operation. In its most basic form, you specify a source index and a destination index. Reindexing can be an expensive operation depending on the size of your source index. We recommend you disable replicas in your destination index by setting number_of_replicas to 0 and re-enable them once the reindex process is complete. . | Reindex all documents | Reindex from a remote cluster | Reindex a subset of documents | Combine one or more indexes | Reindex only unique documents | Transform documents during reindexing | Update documents in the current index | Source index options | Destination index options | . ",
    "url": "https://vagimeli.github.io/im-plugin/reindex-data/",
    "relUrl": "/im-plugin/reindex-data/"
  },"243": {
    "doc": "Reindex data",
    "title": "Reindex all documents",
    "content": "You can copy all documents from one index to another. You first need to create a destination index with your desired field mappings and settings or you can copy the ones from your source index: . PUT destination { \"mappings\":{ \"Add in your desired mappings\" }, \"settings\":{ \"Add in your desired settings\" } } . This reindex command copies all the documents from a source index to a destination index: . POST _reindex { \"source\":{ \"index\":\"source\" }, \"dest\":{ \"index\":\"destination\" } } . If the destination index is not already created, the reindex operation creates a new destination index with default configurations. ",
    "url": "https://vagimeli.github.io/im-plugin/reindex-data/#reindex-all-documents",
    "relUrl": "/im-plugin/reindex-data/#reindex-all-documents"
  },"244": {
    "doc": "Reindex data",
    "title": "Reindex from a remote cluster",
    "content": "You can copy documents from an index in a remote cluster. Use the remote option to specify the remote hostname and the required login credentials. This command reaches out to a remote cluster, logs in with the username and password, and copies all the documents from the source index in that remote cluster to the destination index in your local cluster: . POST _reindex { \"source\":{ \"remote\":{ \"host\":\"https://&lt;REST_endpoint_of_remote_cluster&gt;:9200\", \"username\":\"YOUR_USERNAME\", \"password\":\"YOUR_PASSWORD\" }, \"index\": \"source\" }, \"dest\":{ \"index\":\"destination\" } } . You can specify the following options: . | Options | Valid values | Description | Required | . | host | String | The REST endpoint of the remote cluster. | Yes | . | username | String | The username to log into the remote cluster. | No | . | password | String | The password to log into the remote cluster. | No | . | socket_timeout | Time Unit | The wait time for socket reads (default 30s). | No | . | connect_timeout | Time Unit | The wait time for remote connection timeouts (default 30s). | No | . ",
    "url": "https://vagimeli.github.io/im-plugin/reindex-data/#reindex-from-a-remote-cluster",
    "relUrl": "/im-plugin/reindex-data/#reindex-from-a-remote-cluster"
  },"245": {
    "doc": "Reindex data",
    "title": "Reindex a subset of documents",
    "content": "You can copy a specific set of documents that match a search query. This command copies only a subset of documents matched by a query operation to the destination index: . POST _reindex { \"source\":{ \"index\":\"source\", \"query\": { \"match\": { \"field_name\": \"text\" } } }, \"dest\":{ \"index\":\"destination\" } } . For a list of all query operations, see Full-text queries. ",
    "url": "https://vagimeli.github.io/im-plugin/reindex-data/#reindex-a-subset-of-documents",
    "relUrl": "/im-plugin/reindex-data/#reindex-a-subset-of-documents"
  },"246": {
    "doc": "Reindex data",
    "title": "Combine one or more indexes",
    "content": "You can combine documents from one or more indexes by adding the source indexes as a list. This command copies all documents from two source indexes to one destination index: . POST _reindex { \"source\":{ \"index\":[ \"source_1\", \"source_2\" ] }, \"dest\":{ \"index\":\"destination\" } } . Make sure the number of shards for your source and destination indexes is the same. ",
    "url": "https://vagimeli.github.io/im-plugin/reindex-data/#combine-one-or-more-indexes",
    "relUrl": "/im-plugin/reindex-data/#combine-one-or-more-indexes"
  },"247": {
    "doc": "Reindex data",
    "title": "Reindex only unique documents",
    "content": "You can copy only documents missing from a destination index by setting the op_type option to create. In this case, if a document with the same ID already exists, the operation ignores the one from the source index. To ignore all version conflicts of documents, set the conflicts option to proceed. POST _reindex { \"conflicts\":\"proceed\", \"source\":{ \"index\":\"source\" }, \"dest\":{ \"index\":\"destination\", \"op_type\":\"create\" } } . ",
    "url": "https://vagimeli.github.io/im-plugin/reindex-data/#reindex-only-unique-documents",
    "relUrl": "/im-plugin/reindex-data/#reindex-only-unique-documents"
  },"248": {
    "doc": "Reindex data",
    "title": "Transform documents during reindexing",
    "content": "You can transform your data during the reindexing process using the script option. We recommend Painless for scripting in OpenSearch. This command runs the source index through a Painless script that increments a number field inside an account object before copying it to the destination index: . POST _reindex { \"source\":{ \"index\":\"source\" }, \"dest\":{ \"index\":\"destination\" }, \"script\":{ \"lang\":\"painless\", \"source\":\"ctx._account.number++\" } } . You can also specify an ingest pipeline to transform your data during the reindexing process. You would first have to create a pipeline with processors defined. You have a number of different processors available to use in your ingest pipeline. Here’s a sample ingest pipeline that defines a split processor that splits a text field based on a space separator and stores it in a new word field. The script processor is a Painless script that finds the length of the word field and stores it in a new word_count field. The remove processor removes the test field. PUT _ingest/pipeline/pipeline-test { \"description\": \"Splits the text field into a list. Computes the length of the 'word' field and stores it in a new 'word_count' field. Removes the 'test' field.\", \"processors\": [ { \"split\": { \"field\": \"text\", \"separator\": \"\\\\s+\", \"target_field\": \"word\" }, } { \"script\": { \"lang\": \"painless\", \"source\": \"ctx.word_count = ctx.word.length\" } }, { \"remove\": { \"field\": \"test\" } } ] } . After creating a pipeline, you can use the reindex operation: . POST _reindex { \"source\": { \"index\": \"source\", }, \"dest\": { \"index\": \"destination\", \"pipeline\": \"pipeline-test\" } } . ",
    "url": "https://vagimeli.github.io/im-plugin/reindex-data/#transform-documents-during-reindexing",
    "relUrl": "/im-plugin/reindex-data/#transform-documents-during-reindexing"
  },"249": {
    "doc": "Reindex data",
    "title": "Update documents in the current index",
    "content": "To update the data in your current index itself without copying it to a different index, use the update_by_query operation. The update_by_query operation is POST operation that you can perform on a single index at a time. POST &lt;index_name&gt;/_update_by_query . If you run this command with no parameters, it increments the version number for all documents in the index. ",
    "url": "https://vagimeli.github.io/im-plugin/reindex-data/#update-documents-in-the-current-index",
    "relUrl": "/im-plugin/reindex-data/#update-documents-in-the-current-index"
  },"250": {
    "doc": "Reindex data",
    "title": "Source index options",
    "content": "You can specify the following options for your source index: . | Option | Valid values | Description | Required | . | index | String | The name of the source index. You can provide multiple source indexes as a list. | Yes | . | max_docs | Integer | The maximum number of documents to reindex. | No | . | query | Object | The search query to use for the reindex operation. | No | . | size | Integer | The number of documents to reindex. | No | . | slice | String | Specify manual or automatic slicing to parallelize reindexing. | No | . ",
    "url": "https://vagimeli.github.io/im-plugin/reindex-data/#source-index-options",
    "relUrl": "/im-plugin/reindex-data/#source-index-options"
  },"251": {
    "doc": "Reindex data",
    "title": "Destination index options",
    "content": "You can specify the following options for your destination index: . | Option | Valid values | Description | Required | . | index | String | The name of the destination index. | Yes | . | version_type | Enum | The version type for the indexing operation. Valid values: internal, external, external_gt, external_gte. | No | . ",
    "url": "https://vagimeli.github.io/im-plugin/reindex-data/#destination-index-options",
    "relUrl": "/im-plugin/reindex-data/#destination-index-options"
  },"252": {
    "doc": "Index management security",
    "title": "Index management security",
    "content": "Using the Security plugin with index management lets you limit non-admin users to certain actions. For example, you might want to set up your security such that a group of users can only read ISM policies, while others can create, delete, or change policies. All index management data are protected as system indexes, and only a super admin or an admin with a Transport Layer Security (TLS) certificate can access system indexes. For more information, see System indexes. ",
    "url": "https://vagimeli.github.io/im-plugin/security/",
    "relUrl": "/im-plugin/security/"
  },"253": {
    "doc": "Index management security",
    "title": "Basic permissions",
    "content": "The Security plugin comes with one role that offers full access to index management: index_management_full_access. For a description of the role’s permissions, see Predefined roles. With security enabled, users not only need the correct index management permissions, but they also need permissions to execute actions to involved indexes. For example, if a user wants to use the REST API to attach a policy that executes a rollup job to an index named system-logs, they would need the permissions to attach a policy and execute a rollup job, as well as access to system-logs. Finally, with the exceptions of Create Policy, Get Policy, and Delete Policy, users also need the indices:admin/opensearch/ism/managedindex permission to execute ISM APIs. ",
    "url": "https://vagimeli.github.io/im-plugin/security/#basic-permissions",
    "relUrl": "/im-plugin/security/#basic-permissions"
  },"254": {
    "doc": "Index management security",
    "title": "(Advanced) Limit access by backend role",
    "content": "You can use backend roles to configure fine-grained access to index management policies and actions. For example, users of different departments in an organization might view different policies depending on what roles and permissions they are assigned. First, ensure your users have the appropriate backend roles. Backend roles usually come from an LDAP server or SAML provider. However, if you use the internal user database, you can use the REST API to add them manually. Use the REST API to enable the following setting: . PUT _cluster/settings { \"transient\": { \"plugins.index_management.filter_by_backend_roles\": \"true\" } } . With security enabled, only users who share at least one backend role can see and execute the policies and actions relevant to their roles. For example, consider a scenario with three users: John and Jill, who have the backend role helpdesk_staff, and Jane, who has the backend role phone_operator. John wants to create a policy that performs a rollup job on an index named airline_data, so John would need a backend role that has permissions to access that index, create relevant policies, and execute relevant actions, and Jill would be able to access the same index, policy, and job. However, Jane cannot access or edit those resources or actions. ",
    "url": "https://vagimeli.github.io/im-plugin/security/#advanced-limit-access-by-backend-role",
    "relUrl": "/im-plugin/security/#advanced-limit-access-by-backend-role"
  },"255": {
    "doc": "Customizing your branding",
    "title": "Customizing your branding",
    "content": "Introduced 1.2 . By default, OpenSearch Dashboards uses the OpenSearch logo, but if you want to use custom branding elements such as the favicon or main Dashboards logo, you can do so by editing opensearch_dashboards.yml or by including a custom opensearch_dashboards.yml file when you start your OpenSearch cluster. For example, if you’re using Docker to start your OpenSearch cluster, include the following lines in the opensearch-dashboards section of your docker-compose.yml file: . volumes: - ./opensearch_dashboards.yml:/usr/share/opensearch-dashboards/config/opensearch_dashboards.yml . Doing so replaces the Docker image’s default opensearch_dashboards.yml with your custom opensearch_dashboards.yml file, so be sure to include your desired settings as well. For example, if you want to configure TLS for OpenSearch Dashboards, see Configure TLS for OpenSearch Dashboards. Re-launch OpenSearch Dashboards, and OpenSearch Dashboards now uses your custom elements. ",
    "url": "https://vagimeli.github.io/dashboards/branding/",
    "relUrl": "/dashboards/branding/"
  },"256": {
    "doc": "Customizing your branding",
    "title": "Branding elements",
    "content": "The following elements in OpenSearch Dashboards are customizable: . | Setting | Corresponding branding element | . | logo | Header logo. See #1 in the image. | . | mark | OpenSearch Dashboards mark. See #2 in the image. | . | loadingLogo | Loading logo used when OpenSearch Dashboards is starting. See #3 in the image. | . | faviconUrl | Website icon. Loads next to the application title. See #4 in the image. | . | applicationTitle | The application’s title. See #5 in the image. | . To consolidate navigation controls and reduce the space the header takes up on the page, see Condensed header. To start using your own branding elements in OpenSearch Dashboards, first uncomment this section of opensearch_dashboards.yml: . # opensearchDashboards.branding: # logo: # defaultUrl: \"\" # darkModeUrl: \"\" # mark: # defaultUrl: \"\" # darkModeUrl: \"\" # loadingLogo: # defaultUrl: \"\" # darkModeUrl: \"\" # faviconUrl: \"\" # applicationTitle: \"\" . Add the URLs you want to use as branding elements to the appropriate setting. Valid image types are SVG, PNG, and GIF. Customization of dark mode Dashboards is also available, but you first must supply a valid link to defaultUrl, and then link to your preferred image with darkModeUrl. If you don’t provide a darkModeUrl link, then Dashboards uses the provided defaultUrl element for dark mode. You are not required to customize all branding elements, so if you wanted to, it’s perfectly valid to change just the logo or any other element. Leave unchanged elements as commented. The following example demonstrates how to use SVG files as logos but leaves the other elements as defaults. logo: defaultUrl: \"https://example.com/validUrl.svg\" darkModeUrl: \"https://example.com/validDarkModeUrl.svg\" # mark: # defaultUrl: \"\" # darkModeUrl: \"\" # loadingLogo: # defaultUrl: \"\" # darkModeUrl: \"\" # faviconUrl: \"\" applicationTitle: \"My custom application\" . We recommend linking to images that are hosted on a web server, but if you really want to use locally hosted images, save your images inside assets, and then configure opensearch_dashboards.yml to use the correct paths. You can access locally stored images through the ui/assets folder. The following example assumes the default port of 5601 that Dashboards uses and demonstrates how to link to locally stored images. logo: defaultUrl: \"https://localhost:5601/ui/assets/my-own-image.svg\" darkModeUrl: \"https://localhost:5601/ui/assets/dark-mode-my-own-image.svg\" mark: defaultUrl: \"https://localhost:5601/ui/assets/my-own-image2.svg\" darkModeUrl: \"https://localhost:5601/ui/assets/dark-mode-my-own-image2.svg\" # loadingLogo: # defaultUrl: \"\" # darkModeUrl: \"\" # faviconUrl: \"\" applicationTitle: \"My custom application\" . Condensed header . The condensed header view reduces the footprint of the header and frees up space on the page by combining navigational elements into a single header bar. The current default view remains close in appearance to the two-bar header offered in the previous version of Dashboards, with minor differences. To specify the condensed header, add the configuration property useExpandedHeader to the opensearch_dashboards.yml file and set the value to false, as the following example illustrates. # opensearchDashboards.branding: # logo: defaultUrl: \"https://example.com/sample.svg\" darkModeUrl: \"https://example.com/dark-mode-sample.svg\" # mark: # defaultUrl: \"\" # darkModeUrl: \"\" # loadingLogo: # defaultUrl: \"\" # darkModeUrl: \"\" # faviconUrl: \"\" applicationTitle: \"my custom application\" useExpandedHeader: false . In a future release, default behavior will become useExpandedHeader: false. If you want to retain the default view in subsequent releases, you can explicitly set the property to true in advance. Alternatively, you can also do this when upgrading. The condensed view header appears as in the example below. | Header element | Description | . | OpenSearch logo | See #1. Functions as the home button. | . | Header bar | See #2. A single header bar used for all navigation controls. | . The default view remains close to the traditional view, with minor changes. | Header element | Description | . | Home button | See #1. Returns to the home page and provides an indication when a page is loading. | . | Header label | See #2. The label also functions as a home button. | . | Navigation controls | See #3. Additional navigation controls on right-side insertion points. | . Preserving nagivation elements in the default view . You can continue using the top header bar in the default view for custom navigation links (such as menu items and plugins). Follow the steps below to keep these elements in the top header in the default view. | Replace the property coreStart.chrome.navControls.registerRight(...) with coreStart.chrome.navControls.registerExpandedRight(...) and then replace the property coreStart.chrome.navControls.registerCenter(...) with coreStart.chrome.navControls.registerExpandedCenter(...) . | Make sure the configuration property useExpandedHeader is explicitly set to true. | . ",
    "url": "https://vagimeli.github.io/dashboards/branding/#branding-elements",
    "relUrl": "/dashboards/branding/#branding-elements"
  },"257": {
    "doc": "Customizing your branding",
    "title": "Sample configuration",
    "content": "The following configuration enables the Security plugin and SSL within OpenSearch Dashboards and uses custom branding elements to replace the OpenSearch logo and application title. server.host: \"0\" opensearch.hosts: [\"https://localhost:9200\"] opensearch.ssl.verificationMode: none opensearch.username: \"kibanaserver\" opensearch.password: \"kibanaserver\" opensearch.requestHeadersAllowlist: [ authorization,securitytenant ] #server.ssl.enabled: true #server.ssl.certificate: /path/to/your/server/certificate #server.ssl.key: /path/to/your/server/key opensearch_security.multitenancy.enabled: true opensearch_security.multitenancy.tenants.preferred: [\"Private\", \"Global\"] opensearch_security.readonly_mode.roles: [\"kibana_read_only\"] # Use this setting if you are running opensearch-dashboards without https opensearch_security.cookie.secure: false opensearchDashboards.branding: logo: defaultUrl: \"https://example.com/sample.svg\" darkModeUrl: \"https://example.com/dark-mode-sample.svg\" # mark: # defaultUrl: \"\" # darkModeUrl: \"\" # loadingLogo: # defaultUrl: \"\" # darkModeUrl: \"\" # faviconUrl: \"\" applicationTitle: \"Just some testing\" . ",
    "url": "https://vagimeli.github.io/dashboards/branding/#sample-configuration",
    "relUrl": "/dashboards/branding/#sample-configuration"
  },"258": {
    "doc": "Creating dashboards",
    "title": "Creating dashboards",
    "content": "The Dashboard application in OpenSearch Dashboards lets you visually represent your analytical, operational, and strategic data to help you quickly understand the trends in your data, giving you a high-level view of key metrics, simplifying data exploration, and delivering insights when and where you need them. In this tutorial you’ll learn the basics of creating a dashboard using the Dashboard application and OpenSearch sample data. The sample dataset has existing sample visualizations, and you can use those visualizations or create new visualizations for the dashboard. In this tutorial, you’ll do both. Once you’ve completed this tutorial, you’ll have learned the foundations of creating a new dashboard with multiple panels in OpenSearch Dashboards. This OpenSearch Playground dashboard example shows you what’s possible with OpenSearch Dashboards. ",
    "url": "https://vagimeli.github.io/dashboards/dashboard/index/",
    "relUrl": "/dashboards/dashboard/index/"
  },"259": {
    "doc": "Creating dashboards",
    "title": "Getting familiar with the UI",
    "content": "Before getting started, let’s get familiar with the Dashboard UI. The UI comprises the following main components: . | The navigation panel (A) on the left contains the OpenSearch Dashboards applications. | The search bar (B) lets you search for documents and other objects and add filters. | The filter (C) lets you narrow a dashboard’s results. | The toolbar (D) contains frequently used commands and shortcuts. | The time filter (E) lets you customize the time and date. | The panel (F) allows you to add existing visualizations to the dashboard or create new ones for the dashboard. | . ",
    "url": "https://vagimeli.github.io/dashboards/dashboard/index/#getting-familiar-with-the-ui",
    "relUrl": "/dashboards/dashboard/index/#getting-familiar-with-the-ui"
  },"260": {
    "doc": "Creating dashboards",
    "title": "Defining terminology",
    "content": "The following is some useful terminology for working with OpenSearch Dashboards and the Dashboard application: . | Dashboards is the abbreviated name for OpenSearch Dashboards. OpenSearch Dashboards is an open-source visualization tool designed to work with OpenSearch. | Dashboard is the OpenSearch Dashboards application used to track, analyze, and display data. | dashboard or dashboards are common names for a tool used to visually display data. | Panel is a term used to refer to a visualization displayed on a dashboard. The terms panel and visualization may be used interchangeably throughout this and other Dashboards documentation. | . The following tutorial assumes you’re either using your existing installation of OpenSearch Dashboards or using the OpenSearch Playground. Depending on which one you use, certain capabilities may not be available. For example, sample datasets may not be included in your existing installation, and saving a dashboard isn’t an option in the OpenSearch Playground. ",
    "url": "https://vagimeli.github.io/dashboards/dashboard/index/#defining-terminology",
    "relUrl": "/dashboards/dashboard/index/#defining-terminology"
  },"261": {
    "doc": "Creating dashboards",
    "title": "Creating a dashboard and adding an existing visualization",
    "content": "To create a dashboard and add a sample visualization: . | Connect to https://localhost:5601. The username and password are admin. Alternatively, go to the OpenSearch Playground. | On the top menu, go to OpenSearch Dashboards &gt; Dashboard. | From the Dashboards panel, choose Create Dashboard. | Choose the calendar icon and set the time filter to Last 30 days. | From the panel, choose Add an existing. | From the Add panels window, choose [eCommerce] Promotion Tracking, and then choose x to close the panel. | . You’ve now created the following basic dashboard with a single panel, which you’ll continue using throughout this tutorial. ",
    "url": "https://vagimeli.github.io/dashboards/dashboard/index/#creating-a-dashboard-and-adding-an-existing-visualization",
    "relUrl": "/dashboards/dashboard/index/#creating-a-dashboard-and-adding-an-existing-visualization"
  },"262": {
    "doc": "Creating dashboards",
    "title": "Creating visualizations",
    "content": "Continuing with the dashboard you created in the preceding steps, you’ll create a new visualization and save it to the dashboard: . | From the dashboard toolbar, choose Create new. | From the New Visualization window, choose Gauge and then select the index pattern opensearch_dashboards_sample_data_ecommerce. | From the toolbar, choose Save. | In the Save visualization window, enter a title for the visualization. For example, the title for the gauge chart panel is [eCommerce] Orders. | Choose Save and return. | . The gauge chart visualization is now saved and you are taken back to the dashboard. You’ll see two visualizations on the dashboard, like the following. ",
    "url": "https://vagimeli.github.io/dashboards/dashboard/index/#creating-visualizations",
    "relUrl": "/dashboards/dashboard/index/#creating-visualizations"
  },"263": {
    "doc": "Creating dashboards",
    "title": "Adding subsequent panels",
    "content": "Continuing with the dashboard you created in the preceding steps, you’ll add an existing visualization to the dashboard: . | From the dashboard toolbar, choose Add. | From the Add panels window, choose [eCommerce] Sales by Category. | Choose x to close the Add panels window. | . You’ll see an area chart visualization display on the dashboard, as shown in the following image. ",
    "url": "https://vagimeli.github.io/dashboards/dashboard/index/#adding-subsequent-panels",
    "relUrl": "/dashboards/dashboard/index/#adding-subsequent-panels"
  },"264": {
    "doc": "Creating dashboards",
    "title": "Saving dashboards",
    "content": "When you’ve finalized your dashboard, save it. If you’re saving a new dashboard: . | In the toolbar, choose Save. | In the Save dashboard window, enter the Title. The Description is optional. | To save the time filter to the dashboard, select Store time with dashboard. | Choose Save. | . ",
    "url": "https://vagimeli.github.io/dashboards/dashboard/index/#saving-dashboards",
    "relUrl": "/dashboards/dashboard/index/#saving-dashboards"
  },"265": {
    "doc": "Creating dashboards",
    "title": "Customizing the look of a panel",
    "content": "To customize the panels, you’ll need to be in edit mode: . | Choose Edit at the top right of the toolbar. | . If you see Create new at the top right of the toolbar, you’re already in edit mode. Displaying a legend can give readers more information, while hiding a legend can give the panel a cleaner look. If you want to display or hide the panel legend: . | Choose the list icon in the panel’s lower left corner. | . If you want to change the color of the panel legend: . | From the visualization legend, select a category and then select a color from the flyout. The area chart updates with your change. | . This color change is only saved for the current panel and dashboard and doesn’t affect the saved visualization. If you want to change the color of the panel legend in the visualization: . | Choose the gear icon on the area chart panel. | From the Options window, select Edit visualization. | From the visualization legend, select a category and then select a color from the flyout. The area chart updates with your change. | Choose Save and return. | . This color change affects the saved visualization and any dashboard that links to the visualization. If you want to display, hide, or customize the panel title: . | Choose the gear icon on the panel. | From the Options window, select Edit panel title. | From the Customize panel, enter a title under Panel title or toggle the Show panel title to hide the title. | Choose Save. | . Changing panel titles only affects the particular panel on the particular dashboard and won’t affect any other panel containing that same visualization or any other dashboard. ",
    "url": "https://vagimeli.github.io/dashboards/dashboard/index/#customizing-the-look-of-a-panel",
    "relUrl": "/dashboards/dashboard/index/#customizing-the-look-of-a-panel"
  },"266": {
    "doc": "Creating dashboards",
    "title": "Arranging panels",
    "content": "To organize panels, arrange them side by side, or resize them, you can use these options: . | To move a panel, select and hold the panel title or the top of the panel and drag to the new location. | To resize a panel, choose the resize icon in the panel’s lower-right corner and drag to the new dimensions. | To view a panel in full screen mode, choose the gear icon (edit mode) or vertical ellipsis (⋮) at the top right of the panel and select Maximize panel. To minimize the full screen mode, choose the gear icon or vertical ellipsis and select Minimize. | . The following is an example of a customized dashboard created by using this tutorial. ",
    "url": "https://vagimeli.github.io/dashboards/dashboard/index/#arranging-panels",
    "relUrl": "/dashboards/dashboard/index/#arranging-panels"
  },"267": {
    "doc": "Integrating plugins into a dashboard",
    "title": "Integrating plugins into a dashboard",
    "content": "Observability is a collection of plugins and applications that let you visualize data-driven events by using Piped Processing Language to explore, discover, and query data stored in OpenSearch. Observability provides a unified experience for collecting and monitoring metrics, logs, and traces from common data sources. With data collection and monitoring in one place, you have full-stack, end-to-end observability of your entire infrastructure. As of OpenSearch 2.7, you can manage your observability plugins with Observability Dashboards or Dashboard instead of the plugins page. This feature provides you: . | Instant access to installed plugins: The dashboard displays all installed plugins in one place. | Improved efficiency: With a list of plugins readily available from a dashboard, you can enable, disable, update, or remove plugins in the OpenSearch Dashboards UI. | Better troubleshooting: Viewing a list of plugins from a dashboard can help you quickly identify which plugins may be causing a problem. | Enhanced security: With a list of plugins readily available from a dashboard, you can easily see if any outdated or vulnerable plugins are present and then quickly remove or update them, minimizing or avoiding security risks. | Improved website performance: Viewing a list of plugins from a dashboard can help you identify any plugins that may be slowing down your website or causing performance issues. | . Get familiar with the basics of managing plugins from the Dashboard app in less than 20 seconds in the following video. ",
    "url": "https://vagimeli.github.io/dashboards/dashboard/plugins-dashboards/",
    "relUrl": "/dashboards/dashboard/plugins-dashboards/"
  },"268": {
    "doc": "Integrating plugins into a dashboard",
    "title": "Viewing a list of installed plugins",
    "content": "To view a list of installed plugins from the Dashboard app, follow these steps: . | From the OpenSearch Dashboards main menu, select Dashboard. | View the list of items and select your plugin. Plugins are categorized automatically as the Observability Dashboard data type, which you can filter in order to concentrate on just what you want to see. | . ",
    "url": "https://vagimeli.github.io/dashboards/dashboard/plugins-dashboards/#viewing-a-list-of-installed-plugins",
    "relUrl": "/dashboards/dashboard/plugins-dashboards/#viewing-a-list-of-installed-plugins"
  },"269": {
    "doc": "Integrating plugins into a dashboard",
    "title": "Adding and removing plugins",
    "content": "To add a plugin from the Dashboard app, follow these steps: . | From the OpenSearch Dashboards main menu, select Dashboard. | In the Dashboards window, select Create &gt; Dashboard. | In the Create operational panel window, enter a name in the Name field and then select Create. The plugin is added to both the Observability app and the Dashboard app. | . You can remove a plugin from the Dashboard app by selecting the edit icon under the Actions column and then selecting Delete. ",
    "url": "https://vagimeli.github.io/dashboards/dashboard/plugins-dashboards/#adding-and-removing-plugins",
    "relUrl": "/dashboards/dashboard/plugins-dashboards/#adding-and-removing-plugins"
  },"270": {
    "doc": "Integrating plugins into a dashboard",
    "title": "Staying updated about OpenSearch Dashboards plugins",
    "content": "The OpenSearch plugins repository on GitHub is a great way to keep track of and contribute to tasks, features, enhancements, and bugs. The OpenSearch Project team welcomes your input. ",
    "url": "https://vagimeli.github.io/dashboards/dashboard/plugins-dashboards/#staying-updated-about-opensearch-dashboards-plugins",
    "relUrl": "/dashboards/dashboard/plugins-dashboards/#staying-updated-about-opensearch-dashboards-plugins"
  },"271": {
    "doc": "Dev Tools",
    "title": "Dev Tools",
    "content": "Dev Tools allows you to set up your OpenSearch Dashboards environment, identify and fix bugs, and customize your dashboards’ appearance and behavior. To access the Dev Tools console, select Dev Tools in the menu on the OpenSearch Dashboards home page. You’ll see an interface like the one shown in the following image. ",
    "url": "https://vagimeli.github.io/dashboards/dev-tools/index-dev/",
    "relUrl": "/dashboards/dev-tools/index-dev/"
  },"272": {
    "doc": "Running queries in the Dev Tools console",
    "title": "Running queries in the Dev Tools console",
    "content": "You can use the OpenSearch Dev Tools Console to send queries to OpenSearch. ",
    "url": "https://vagimeli.github.io/dashboards/dev-tools/run-queries/",
    "relUrl": "/dashboards/dev-tools/run-queries/"
  },"273": {
    "doc": "Running queries in the Dev Tools console",
    "title": "Navigating to the console",
    "content": "To open the console, select Dev Tools on the main OpenSearch Dashboards page: . You can open the console from any other page by navigating to the main menu and selecting Management &gt; Dev Tools. ",
    "url": "https://vagimeli.github.io/dashboards/dev-tools/run-queries/#navigating-to-the-console",
    "relUrl": "/dashboards/dev-tools/run-queries/#navigating-to-the-console"
  },"274": {
    "doc": "Running queries in the Dev Tools console",
    "title": "Writing queries",
    "content": "Write your queries in the editor pane on the left side of the console: . You can collapse and expand parts of your query by selecting the small triangles next to the line numbers. To learn more about writing queries in OpenSearch domain-specific language (DSL), see Query DSL. Comments . Use # at the beginning of a line to write single-line comments. Autocomplete . OpenSearch provides autocomplete suggestions for fields, indexes and their aliases, and templates. To configure autocomplete preferences, update them in Console Settings. ",
    "url": "https://vagimeli.github.io/dashboards/dev-tools/run-queries/#writing-queries",
    "relUrl": "/dashboards/dev-tools/run-queries/#writing-queries"
  },"275": {
    "doc": "Running queries in the Dev Tools console",
    "title": "Sending the request",
    "content": "To send a query to OpenSearch, select the query by placing the cursor anywhere in the query text. Then choose the play icon () on the upper right of the request or press Ctrl/Cmd+Enter: . OpenSearch displays the response in the response pane on the right side of the console: . ",
    "url": "https://vagimeli.github.io/dashboards/dev-tools/run-queries/#sending-the-request",
    "relUrl": "/dashboards/dev-tools/run-queries/#sending-the-request"
  },"276": {
    "doc": "Running queries in the Dev Tools console",
    "title": "Working in the cURL and console formats",
    "content": "The console uses an easier syntax to format REST requests than the curl command. For example, the following curl command runs a search query: . curl -XGET http://localhost:9200/shakespeare/_search?pretty -H 'Content-Type: application/json' -d' { \"query\": { \"match\": { \"text_entry\": \"To be, or not to be\" } } }' . The same query has a simpler syntax in the console format: . GET shakespeare/_search { \"query\": { \"match\": { \"text_entry\": \"To be, or not to be\" } } } . If you paste a curl command directly into the console, the command is automatically converted into the format the console uses. To import a query in cURL format, select the query, select the wrench icon (), and choose Copy as cURL: . ",
    "url": "https://vagimeli.github.io/dashboards/dev-tools/run-queries/#working-in-the-curl-and-console-formats",
    "relUrl": "/dashboards/dev-tools/run-queries/#working-in-the-curl-and-console-formats"
  },"277": {
    "doc": "Running queries in the Dev Tools console",
    "title": "Viewing documentation",
    "content": "To view the OpenSearch documentation, select the wrench icon () and choose Open documentation. ",
    "url": "https://vagimeli.github.io/dashboards/dev-tools/run-queries/#viewing-documentation",
    "relUrl": "/dashboards/dev-tools/run-queries/#viewing-documentation"
  },"278": {
    "doc": "Running queries in the Dev Tools console",
    "title": "Auto indenting",
    "content": "To use auto indent, select the queries that you want to format, select the wrench icon (), and choose Auto indent. Auto indenting a collapsed query expands it. Auto indenting a well-formatted query puts the request body on a single line. This is useful for working with bulk APIs. ",
    "url": "https://vagimeli.github.io/dashboards/dev-tools/run-queries/#auto-indenting",
    "relUrl": "/dashboards/dev-tools/run-queries/#auto-indenting"
  },"279": {
    "doc": "Running queries in the Dev Tools console",
    "title": "Viewing your request history",
    "content": "You can view up to the 500 most recent requests that OpenSearch ran successfully. To view request history, select History from the top menu. If you select the request you want to view from the left pane, the query is shown in the right pane. To copy the query into the editor pane, select the query text and then select Apply. To clear the history, select Clear. ",
    "url": "https://vagimeli.github.io/dashboards/dev-tools/run-queries/#viewing-your-request-history",
    "relUrl": "/dashboards/dev-tools/run-queries/#viewing-your-request-history"
  },"280": {
    "doc": "Running queries in the Dev Tools console",
    "title": "Updating the console settings",
    "content": "To update your preferences, select Settings from the top menu: . ",
    "url": "https://vagimeli.github.io/dashboards/dev-tools/run-queries/#updating-the-console-settings",
    "relUrl": "/dashboards/dev-tools/run-queries/#updating-the-console-settings"
  },"281": {
    "doc": "Running queries in the Dev Tools console",
    "title": "Using keyboard shortcuts",
    "content": "To view all available keyboard shortcuts, select Help from the top menu. ",
    "url": "https://vagimeli.github.io/dashboards/dev-tools/run-queries/#using-keyboard-shortcuts",
    "relUrl": "/dashboards/dev-tools/run-queries/#using-keyboard-shortcuts"
  },"282": {
    "doc": "Using Dashboards Query Language",
    "title": "Using Dashboards Query Language",
    "content": "Dashboards Query Language (DQL) is a simple text-based query language for filtering data in OpenSearch Dashboards. Similar to Query DSL, DQL uses an HTTP request body. For example, to display your site visitor data for a host in the United States, you would enter geo.dest:US in the search field, as shown in the following image. Before you can search data in Dashboards, you must index it. In OpenSearch, the basic unit of data is a JSON document. Within an index, OpenSearch identifies each document using a unique ID. To learn more about indexing in OpenSearch, see Index data. ",
    "url": "https://vagimeli.github.io/dashboards/discover/dql/",
    "relUrl": "/dashboards/discover/dql/"
  },"283": {
    "doc": "Using Dashboards Query Language",
    "title": "Searching with terms queries",
    "content": "The most basic query specifies the search term, for example: . host:www.example.com . To access an object’s nested field, list the complete path to the field separated by periods. For example, use the following path to retrieve the lat field in the coordinates object: . coordinates.lat:43.7102 . DQL supports leading and trailing wildcards, so you can search for any terms that match your pattern, for example: . host.keyword:*.example.com/* . To check whether a field exists or has any data, use a wildcard to see whether Dashboards returns any results,for example: . host.keyword:* . ",
    "url": "https://vagimeli.github.io/dashboards/discover/dql/#searching-with-terms-queries",
    "relUrl": "/dashboards/discover/dql/#searching-with-terms-queries"
  },"284": {
    "doc": "Using Dashboards Query Language",
    "title": "Searching with Boolean queries",
    "content": "To mix and match or combine multiple queries for more refined results, you can use the Boolean operators and, or, and not. DQL is not case sensitive, so AND and and are the same, for example: . host.keyword:www.example.com and response.keyword:200 . You also can use multiple Boolean operators in one query, for example: . geo.dest:US or response.keyword:200 and host.keyword:www.example.com . Remember that Boolean operators follow the logical precedence order of not, and, and or, so if you have an expression like the one in the preceding example, response.keyword:200 and host.keyword:www.example.com is evaluated first. To avoid confusion, use parentheses to dictate the order in which you want to evaluate operands. If you want to evaluate geo.dest:US or response.keyword:200 first, you can use an expression like the following: . (geo.dest:US or response.keyword:200) and host.keyword:www.example.com . ",
    "url": "https://vagimeli.github.io/dashboards/discover/dql/#searching-with-boolean-queries",
    "relUrl": "/dashboards/discover/dql/#searching-with-boolean-queries"
  },"285": {
    "doc": "Using Dashboards Query Language",
    "title": "Querying dates and ranges",
    "content": "DQL supports numeric inequalities, for example, bytes &gt;= 15 and memory &lt; 15. You can use the same method to find a date before or after the date specified in the query. &gt; indicates a search for a date after the specified date, and &lt; returns dates before the specified date, for example, @timestamp &gt; \"2020-12-14T09:35:33. ",
    "url": "https://vagimeli.github.io/dashboards/discover/dql/#querying-dates-and-ranges",
    "relUrl": "/dashboards/discover/dql/#querying-dates-and-ranges"
  },"286": {
    "doc": "Using Dashboards Query Language",
    "title": "Querying nested fields",
    "content": "Searching a document with nested fields requires you to specify the full path of the field to be retrieved. In the following example document, the superheroes field has nested objects: . { \"superheroes\":[ { \"hero-name\": \"Superman\", \"real-identity\": \"Clark Kent\", \"age\": 28 }, { \"hero-name\": \"Batman\", \"real-identity\": \"Bruce Wayne\", \"age\": 26 }, { \"hero-name\": \"Flash\", \"real-identity\": \"Barry Allen\", \"age\": 28 }, { \"hero-name\": \"Robin\", \"real-identity\": \"Dick Grayson\", \"age\": 15 } ] } . copy . To retrieve documents that match a specific field using DQL, specify the field, for example: . superheroes: {hero-name: Superman} . copy . To retrieve documents that match multiple fields, specify all the fields, for example: . superheroes: {hero-name: Superman} and superheroes: {hero-name: Batman} . copy . You can combine multiple Boolean and range queries to create a more refined query, for example: . superheroes: {hero-name: Superman and age &lt; 50} . copy . ",
    "url": "https://vagimeli.github.io/dashboards/discover/dql/#querying-nested-fields",
    "relUrl": "/dashboards/discover/dql/#querying-nested-fields"
  },"287": {
    "doc": "Using Dashboards Query Language",
    "title": "Querying doubly nested objects",
    "content": "If a document has doubly nested objects (objects nested inside other objects), retrieve a field value by specifying the full path to the field. In the following example document, the superheroes object is nested inside the justice-league object: . { \"justice-league\": [ { \"superheroes\":[ { \"hero-name\": \"Superman\", \"real-identity\": \"Clark Kent\", \"age\": 28 }, { \"hero-name\": \"Batman\", \"real-identity\": \"Bruce Wayne\", \"age\": 26 }, { \"hero-name\": \"Flash\", \"real-identity\": \"Barry Allen\", \"age\": 28 }, { \"hero-name\": \"Robin\", \"real-identity\": \"Dick Grayson\", \"age\": 15 } ] } ] } . copy . The following image shows the query result using the example notation justice-league.superheroes: {hero-name:Superman}. ",
    "url": "https://vagimeli.github.io/dashboards/discover/dql/#querying-doubly-nested-objects",
    "relUrl": "/dashboards/discover/dql/#querying-doubly-nested-objects"
  },"288": {
    "doc": "Exploring data",
    "title": "Exploring data",
    "content": "Discover in OpenSearch Dashboards helps you extract insights and get value out of data assets across your organization. Discover enables you to: . | Explore data. You can explore, customize, and filter data as well as search data using Dashboards Query Language (DQL). | Analyze data. You can analyze data, view individual documents, and create tables summarizing data contents. | Visualize data. You can display findings from your saved searches in a single dashboard that combines different data visualization types. | . ",
    "url": "https://vagimeli.github.io/dashboards/discover/index-discover/",
    "relUrl": "/dashboards/discover/index-discover/"
  },"289": {
    "doc": "Exploring data",
    "title": "Try it: Exploring sample data with Discover",
    "content": "This tutorial shows you how to use Discover to analyze and understand a sample dataset. At the end of this tutorial, you should be ready to use Discover with your own data. Before starting this tutorial, make sure you’ve added the Sample flight data. See Quickstart guide for OpenSearch Dashboards for information about how to get started. Setting up data . Watch the following short video or start with the tutorial steps to learn how to set up a sample dataset in Discover. | Verify access to OpenSearch Dashboards by connecting to http://localhost:5601 from a browser. The default username and password are admin. | On the Home page, choose Discover in the navigation pane. | On the index pattern toolbar, select the opensearch_dashboards_sample_data_flights dataset. | On the time filter toolbar, choose the calendar icon and then change the time range to Last 7 days. | . Exploring the data fields . In the Discover panel, you’ll see a table that shows all the documents that match your search. The table includes a list of data fields that are available in the document table, as shown in the following image. Follow these steps to explore the data fields: . | View the list of Available fields. | Choose Cancelled to view the values (true and false). | Choose the plus (+) sign to add the field to the document table. The field will be automatically added to Selected fields and the document table. | Select FlightDelay from the Available fields list, and then choose the plus (+) sign to add the field to the document table. | Optional: Rearrange the table columns by selecting the table header and then choosing Move left or Move right. | . ",
    "url": "https://vagimeli.github.io/dashboards/discover/index-discover/#try-it-exploring-sample-data-with-discover",
    "relUrl": "/dashboards/discover/index-discover/#try-it-exploring-sample-data-with-discover"
  },"290": {
    "doc": "Exploring data",
    "title": "Searching data",
    "content": "You can use the search toolbar or enter a DQL query in the DevTools console to search data in Dashboards, as shown in the following image. The search toolbar is best for basic queries, such as searching by a field name. DQL is best for complex queries, such as searching data using a term, string, Boolean, date, range, or nested query. Follow these steps to search data: . | In the search toolbar, enter the Boolean query. For example, enter FlightDelay:true AND FlightDelayMin &gt;= 60 to search the data for flights delayed by 60 minutes or more. | Choose Update. | Optional: Choose the arrow (&gt;) in a table row to expand the row and view the document table details. | . ",
    "url": "https://vagimeli.github.io/dashboards/discover/index-discover/#searching-data",
    "relUrl": "/dashboards/discover/index-discover/#searching-data"
  },"291": {
    "doc": "Exploring data",
    "title": "Filtering data",
    "content": "Filters allow you to refine sets of documents to subsets of those documents. For example, you can filter data to include or exclude certain fields, as shown in the following image. Follow these steps to filter data: . | In the filter bar, choose Add filter. | Select options from the Field, Operator, and Value dropdown lists. For example, Cancelled, is, and true. | Choose Save. | To remove the filter, choose the close icon (x) next to the filter name. | Optional: Add more filters to further explore the data. | . ",
    "url": "https://vagimeli.github.io/dashboards/discover/index-discover/#filtering-data",
    "relUrl": "/dashboards/discover/index-discover/#filtering-data"
  },"292": {
    "doc": "Exploring data",
    "title": "Analyzing data in the document table",
    "content": "You can view the document table fields to better understand the data and gather insights for more informed decision-making: . | Choose the arrow icon (&gt;) to expand a table row. | View the fields and details. | Switch between the Table and JSON tabs to view the different formats, as shown in the following image. | . ",
    "url": "https://vagimeli.github.io/dashboards/discover/index-discover/#analyzing-data-in-the-document-table",
    "relUrl": "/dashboards/discover/index-discover/#analyzing-data-in-the-document-table"
  },"293": {
    "doc": "Exploring data",
    "title": "Saving the search",
    "content": "Saving a search saves the query text, filters, and current data view. To save your search to use it later, generate a report, or build visualizations and dashboards: . | Choose the save icon in the toolbar. | Give the search a title, and then choose Save. | Choose the save icon to access the saved search, as shown in the following image. | . ",
    "url": "https://vagimeli.github.io/dashboards/discover/index-discover/#saving-the-search",
    "relUrl": "/dashboards/discover/index-discover/#saving-the-search"
  },"294": {
    "doc": "Exploring data",
    "title": "Visualizing the search",
    "content": "You can quickly visualize an aggregated field from Discover: . | From the Available fields list, select FlightDelayType and then choose Visualize, as shown in the following image. | . Dashboards creates a visualization for this field, which in this case is a basic bar chart, as shown in the following image. ",
    "url": "https://vagimeli.github.io/dashboards/discover/index-discover/#visualizing-the-search",
    "relUrl": "/dashboards/discover/index-discover/#visualizing-the-search"
  },"295": {
    "doc": "Adding multiple data sources",
    "title": "Adding multiple data sources",
    "content": "Use OpenSearch Dashboards to dynamically manage data sources, create index patterns based on those data sources, run queries against a specific data source, and combine visualizations in one dashboard. Learn how to enable the data_source setting in Dashboards; add credentials, data source connections, and index patterns; and combine visualizations in a single dashboard in this tutorial. ",
    "url": "https://vagimeli.github.io/dashboards/discover/multi-data-sources/",
    "relUrl": "/dashboards/discover/multi-data-sources/"
  },"296": {
    "doc": "Adding multiple data sources",
    "title": "Modifying the YAML file settings for multiple data sources",
    "content": "This tutorial uses a preconfigured data source and index pattern for which you aren’t required to configure settings. However, you have to enable the multiple data sources feature because it is disabled by default. To enable multiple data sources: . | Navigate to your Dashboards home directory, for example, in Docker, /usr/share/opensearch-dashboards. | Open your local copy of the Dashboards configuration file, opensearch_dashboards.yml. If you don’t have a copy, opensearch_dashboards.yml is available on GitHub. | Set data_source.enabled: to true and save the YAML file. | Restart the Dashboards container. | Verify that the configuration settings were created and configured properly by connecting to Dashboards and viewing the Stack Management console. Data Sources appears in the sidebar, as shown in the following image. | . ",
    "url": "https://vagimeli.github.io/dashboards/discover/multi-data-sources/#modifying-the-yaml-file-settings-for-multiple-data-sources",
    "relUrl": "/dashboards/discover/multi-data-sources/#modifying-the-yaml-file-settings-for-multiple-data-sources"
  },"297": {
    "doc": "Adding multiple data sources",
    "title": "Creating a data source connection",
    "content": "A data source connection specifies the parameters needed to connect to a data source. These parameters form a connection string for the data source. In Dashboards, you can add new data source connections or edit existing connections. To create a new data source connection: . | Go to http://localhost:5601 and log in with the username admin and password admin. If you’re running the Security plugin, go to https://localhost:5601. | From the OpenSearch Dashboards main menu, select Stack Management, Data Sources, and then Create data source connection. | Add information to each field to configure Connection Details, Endpoint URL, and Authentication Method. In the Connection Details window, enter a title. Entering a description is optional. In the Endpoint window, enter the Endpoint URL. For this tutorial, use the URL http://localhost:5601/app/management/opensearch-dashboards/dataSources. In the Authentication window, select an Authentication Method. The options are: . | No authentication: No authentication is used to connect to the data source. | Username &amp; Password: A basic username and password are used to connect to the data source. | AWS Sigv4: An AWS Signature Version 4 authenticating request is used to connect to the data source. AWS Sigv4 requires an access key and a secret key. First specify the Region, and then enter the Access Key and Secret Key for authorization. For information about available AWS Regions for AWS accounts, see Available Regions. For more about Sigv4 authentication requests, see Authenticating Requests (AWS Signature Version 4). | . When you select the authentication method, the applicable fields appear for the selected method. Enter the required details. After you have entered the appropriate details in all of the required fields, the Test connection and Create data source connection buttons become active. You can select Test connection to confirm that the connection is valid. | Select Create data source connection to save your settings. The connection is created. The active window returns to the Data Sources main page, and the new connection appears in the list of data sources. | Delete the data source connection by selecting the check box to the left of the title and then choosing Delete 1 connection. Selecting multiple check boxes for multiple connections is supported. | . Editing and updating a data source connection . To make changes to the data source connection, select a connection in the list on the Data Sources main page. The Connection Details window opens. To make changes to Connection Details, edit one or both of the Title and Description fields and select Save changes in the lower-right corner of the screen. You can also cancel changes here. To change the Authentication Method, choose a different authentication method, enter your credentials (if applicable), and then select Save changes in the lower-right corner of the screen. The changes are saved. When Username &amp; Password is the selected authentication method, you can update the password by choosing Update stored password next to the Password field. In the pop-up window, enter a a new password in the first field and then enter it again in the second field to confirm. Select Update stored password in the pop-up window. The new password is saved. Select Test connection to confirm that the connection is valid. When AWS Sigv4 is the selected authentication method, you can update the credentials by selecting Update stored AWS credential. In the pop-up window, enter a new access key in the first field and a new secret key in the second field. Select Update stored AWS credential in the pop-up window. The new credentials are saved. Select Test connection in the upper-right corner of the screen to confirm that the connection is valid. To delete the data source connection, select the trash can icon (). ",
    "url": "https://vagimeli.github.io/dashboards/discover/multi-data-sources/#creating-a-data-source-connection",
    "relUrl": "/dashboards/discover/multi-data-sources/#creating-a-data-source-connection"
  },"298": {
    "doc": "Adding multiple data sources",
    "title": "Creating an index pattern",
    "content": "Use index patterns to access the OpenSearch data that you want to explore. Learn how to load your own data and create an index pattern in the following steps. This tutorial uses the preconfigured index pattern opensearch_dashboards_sample_data_ecommerce Default. | In the Dashboards console, select Index Patterns and then Create index pattern. | Select Use external data source connection. | Start typing in the Search data sources field to search for the data source you created earlier and then select the data source and Next step. | In the Dashboards console, choose Index Patterns and then Create index pattern. | Choose Use external data source connection. | Start typing in the Search data sources field to search for the data source you created earlier, select the data source, and then select Next step. | Add an Index pattern name to define the index pattern and then choose Next step. | Choose an option for the Time field and then select Create index pattern. | . ",
    "url": "https://vagimeli.github.io/dashboards/discover/multi-data-sources/#creating-an-index-pattern",
    "relUrl": "/dashboards/discover/multi-data-sources/#creating-an-index-pattern"
  },"299": {
    "doc": "Adding multiple data sources",
    "title": "Searching data",
    "content": "Before you start searching for data, set up the time filter. The sample index pattern used for this tutorial contains time-based data. You can set a time filter that displays only the data within a specified time range, and you can choose the time filter to change the time range or select a specific time range in the histogram. Setting the time filter . To set the time filter: . | In the Dashboards console, select Discover and confirm that the index pattern being used is opensearch_dashboards_sample_data_ecommerce. | Select the calendar icon () to change the time field. The default time period is Last 15 minutes. | Change the time field to a particular time period, for example, Last 7 days, and then select Refresh. | Change start or end times by selecting the start or end time in the search bar. | In the pop-up window, choose Absolute, Relative, or Now and then specify the date, for example, as shown in the following image. | . Selecting a time range from the histogram . To select a time range for the histogram, choose from the following options: . | Select the bar that represents the time range you want to zoom in on. | Select the bar and drag to view a specific time range. You must start the selection with the cursor over the background of the chart (the cursor changes to a plus sign when you hover over a valid start point). | Select the dropdown and then select an interval. | . ",
    "url": "https://vagimeli.github.io/dashboards/discover/multi-data-sources/#searching-data",
    "relUrl": "/dashboards/discover/multi-data-sources/#searching-data"
  },"300": {
    "doc": "Adding multiple data sources",
    "title": "Selecting multiple data sources in the Dev Tools console",
    "content": "Selecting multiple data sources in the Dev Tools console allows you to work with a broader range of data and gain deeper insight into your code and applications. Watch the video to see it in action, and then try it out in the following steps. | Locate your copy of opensearch_dashboards.yml and open it in the editor of your choice. | Set data_source.enabled to true. | Connect to OpenSearch Dashboards and select Dev Tools in the menu. | Enter the following query in the editor pane of the Console and then select the play button: . GET /_cat/indices . | From the DataSource dropdown menu, select a data source and then query the source. | Repeat the preceding steps for each data source you want to select. | . ",
    "url": "https://vagimeli.github.io/dashboards/discover/multi-data-sources/#selecting-multiple-data-sources-in-the-dev-tools-console",
    "relUrl": "/dashboards/discover/multi-data-sources/#selecting-multiple-data-sources-in-the-dev-tools-console"
  },"301": {
    "doc": "Adding multiple data sources",
    "title": "Creating data visualizations for a dashboard",
    "content": "To create data visualizations for a dashboard, follow these steps: . | In the Dashboards console, choose Visualize and then Create visualization. | Select the visualization type. For this tutorial, choose Line. | Select a source. For this tutorial, choose the index pattern opensearch_dashboards_sample_data_ecommerce. | Under Buckets, choose Add and then X-axis. | In the Aggregation field, choose Date Histogram and then choose Update. | Choose Save and add the file name. | . ",
    "url": "https://vagimeli.github.io/dashboards/discover/multi-data-sources/#creating-data-visualizations-for-a-dashboard",
    "relUrl": "/dashboards/discover/multi-data-sources/#creating-data-visualizations-for-a-dashboard"
  },"302": {
    "doc": "Adding multiple data sources",
    "title": "Connecting visualizations in a single dashboard",
    "content": "To connect your visualizations in a single dashboard, follow these steps: . | In the Dashboards console, choose Dashboard and then Create dashboard. | Choose Add an existing and then select the data you want to add. | Choose Save and add the dashboard name in the Title field. This tutorial uses preconfigured dashboards, so you won’t be able to save your dashboard. | Click on the white space left of Add panels to view the visualizations in a single dashboard. | . Your dashboard might look like the one in the following image. ",
    "url": "https://vagimeli.github.io/dashboards/discover/multi-data-sources/#connecting-visualizations-in-a-single-dashboard",
    "relUrl": "/dashboards/discover/multi-data-sources/#connecting-visualizations-in-a-single-dashboard"
  },"303": {
    "doc": "Adding multiple data sources",
    "title": "Understanding feature limitations",
    "content": "This feature has the following limitations: . | The multiple data sources feature is supported for index-pattern-based visualizations only. | The visualization types Time Series Visual Builder (TSVB), Vega and Vega-Lite, and timeline are not supported. | External plugins, such as Gantt chart, and non-visualization plugins, such as the developer console, are not supported. | . ",
    "url": "https://vagimeli.github.io/dashboards/discover/multi-data-sources/#understanding-feature-limitations",
    "relUrl": "/dashboards/discover/multi-data-sources/#understanding-feature-limitations"
  },"304": {
    "doc": "Setting the time filter",
    "title": "Setting the time filter",
    "content": "You can change the time range to display dashboard data over minutes, hours, days, weeks, months, or years. The default time range is Last 15 minutes. You can change the time range at the dashboard level or under Stack Management &gt; Advanced Settings &gt; Time filter defaults. To change the time range at the dashboard level, perform the following steps: . | From an OpenSearch Dashboards application (Discover, Dashboard, or Visualize), select the time clock or calendar icon. | Select one of the time filter options, as shown in the following image: . | Quick select: Choose a time based on the last or next number of seconds, minutes, hours, days, or another time unit. | Commonly used: Choose a common time range like Today, Last 7 days, or Last 30 days. | Recently used date ranges: Select a previously used time range. | Refresh every: Set an automatic refresh period. | . | Choose Show dates to set start and end times, and then select anywhere inside the toolbar to access the time filter pop-up window, as shown in the following image. | Select Absolute, Relative, or Now and specify ranges. | Choose Update to apply changes, as shown in the following image. | . ",
    "url": "https://vagimeli.github.io/dashboards/discover/time-filter/",
    "relUrl": "/dashboards/discover/time-filter/"
  },"305": {
    "doc": "Component templates",
    "title": "Component templates",
    "content": "Introduced 2.7 . Component templates allow you to create a single index pattern that matches multiple indexes. This pattern can include wildcards or regular expressions, enabling you to apply the same setting or mapping to multiple indexes simultaneously. Using them with index templates can provide a powerful tool for managing large volumes of data. You can create an index template that defines the basic structure and settings of your indexes and then use the component templates to apply the settings to all indexes that match a specific pattern or set of criteria. You can create component templates using the Index Management UI. The UI maximizes ease of use for common indexing and data stream administrative operations such as create, read, update, delete (CRUD) and mapping indexes; CRUD and mapping aliases; reindexing; and open/close, shrink, and split indexes, along with the monitoring of actions and logging of audit records. The following GIF demonstrates creating a component template. ",
    "url": "https://vagimeli.github.io/dashboards/im-dashboards/component-templates/",
    "relUrl": "/dashboards/im-dashboards/component-templates/"
  },"306": {
    "doc": "Component templates",
    "title": "Prerequisites",
    "content": "This tutorial is intended for admin users who manage OpenSearch clusters and are familiar with index management in OpenSearch Dashboards. ",
    "url": "https://vagimeli.github.io/dashboards/im-dashboards/component-templates/#prerequisites",
    "relUrl": "/dashboards/im-dashboards/component-templates/#prerequisites"
  },"307": {
    "doc": "Component templates",
    "title": "Key terms",
    "content": "It’s helpful to understand the following terms before starting this tutorial: . | Component template refers to a reusable building block with settings, mappings, and aliases that can be attached to an index template. | Index template refers to a predefined structure used to organize and store data in a database or search index. | . ",
    "url": "https://vagimeli.github.io/dashboards/im-dashboards/component-templates/#key-terms",
    "relUrl": "/dashboards/im-dashboards/component-templates/#key-terms"
  },"308": {
    "doc": "Component templates",
    "title": "Creating component templates using the Index Management UI",
    "content": "You can use predefined OpenSearch Dashboards component templates or customize your own, either by creating original templates or by modifying existing templates. Predefined component templates include preconfigured charts, tables, and graphs and are a good starting point for users who are new to OpenSearch Dashboards. Alternatively, customized template components provide you with options for tailoring reports and visualizations that meet your specific requirements and preferences. To create template components using the UI, follow these steps: . | On the OpenSearch Dashboards main page, select Index Management in the navigation menu. | In the Index Management window, select Templates &gt; Component templates. | Select Create and then define the component template settings. | To configure aliases, settings, and mappings, toggle Use configuration, as shown in the following image. | Enter details in the aliases, settings, and mappings fields. | Select Create component template. | . When you create component templates, those templates apply only to new index templates that you create and not to existing index templates. ",
    "url": "https://vagimeli.github.io/dashboards/im-dashboards/component-templates/#creating-component-templates-using-the-index-management-ui",
    "relUrl": "/dashboards/im-dashboards/component-templates/#creating-component-templates-using-the-index-management-ui"
  },"309": {
    "doc": "Component templates",
    "title": "Associating component templates with index templates",
    "content": "To associate a component template with an index template, follow these steps: . | In the Index Management navigation menu, select Templates. | In the Templates window, select Create template. | Select Component template as the method for defining your template. | In the Component template pane, select Associate component template, as shown in the following image. | In the Associate component template pop-up window, select the component templates that you want to associate with your index template. | Select Associate. | Select Preview template to view the template settings. | Select Create template. | . ",
    "url": "https://vagimeli.github.io/dashboards/im-dashboards/component-templates/#associating-component-templates-with-index-templates",
    "relUrl": "/dashboards/im-dashboards/component-templates/#associating-component-templates-with-index-templates"
  },"310": {
    "doc": "Data streams",
    "title": "Data streams",
    "content": "Introduced 2.6 . In OpenSearch Dashboards, the Index Management application allows you to view and manage data streams as shown in the following image. ",
    "url": "https://vagimeli.github.io/dashboards/im-dashboards/datastream/",
    "relUrl": "/dashboards/im-dashboards/datastream/"
  },"311": {
    "doc": "Data streams",
    "title": "Viewing a data stream",
    "content": "To view a data stream and its health status, choose Data streams under Index management as shown in the following image. The following are the three data stream health statuses: . | Green: All primary and replica shards are assigned. | Yellow: At least one replica shard is not assigned. | Red: At least one primary shard is not assigned. | . ",
    "url": "https://vagimeli.github.io/dashboards/im-dashboards/datastream/#viewing-a-data-stream",
    "relUrl": "/dashboards/im-dashboards/datastream/#viewing-a-data-stream"
  },"312": {
    "doc": "Data streams",
    "title": "Creating a data stream",
    "content": "To create a data stream, perform the following steps: . | Under Index Management, choose Data streams. | Choose Create data stream. | Enter a name for the data stream under Data stream name. | Ensure that you have a matching index template. This will be populated under Matching index template, as shown in the following image. | The Inherited settings from template and Index alias sections are read-only, and display the backing indexes that are contained in the data stream. | The number of primary shards, number of replicas, and the refresh interval are inherited from the template, as shown in the following image. | Choose Create data stream. | . ",
    "url": "https://vagimeli.github.io/dashboards/im-dashboards/datastream/#creating-a-data-stream",
    "relUrl": "/dashboards/im-dashboards/datastream/#creating-a-data-stream"
  },"313": {
    "doc": "Data streams",
    "title": "Deleting a data stream",
    "content": "To delete a data stream, perform the following steps: . | Under Index Management, choose Data streams. | Select the data stream that you want to delete. | Choose Actions, and then choose Delete. | . ",
    "url": "https://vagimeli.github.io/dashboards/im-dashboards/datastream/#deleting-a-data-stream",
    "relUrl": "/dashboards/im-dashboards/datastream/#deleting-a-data-stream"
  },"314": {
    "doc": "Data streams",
    "title": "Rolling over a data stream",
    "content": "To perform a rollover operation on a data stream, perform the following steps: . | Under Index Management, choose Data streams. | Choose Actions, and then choose Roll over, as shown in the following image. | Under Configure source, select the source data stream on which you want to perform the rollover operation. | Choose Roll over, as shown in the following image. | . ",
    "url": "https://vagimeli.github.io/dashboards/im-dashboards/datastream/#rolling-over-a-data-stream",
    "relUrl": "/dashboards/im-dashboards/datastream/#rolling-over-a-data-stream"
  },"315": {
    "doc": "Data streams",
    "title": "Force merging data streams",
    "content": "To perform a force merge operation on two or more indexes, perform the following steps: . | Under Index Management, choose Data streams. | Select the data streams on which you want to perform the force merge operation. | Choose Actions, and then choose Force merge. | Under Configure source index, specify the data streams you want to force merge. | Optionally, under Advanced settings you can to choose to Flush indices or Only expunge delete and then specify the Max number of segments to merge to as shown in the following image. | . ",
    "url": "https://vagimeli.github.io/dashboards/im-dashboards/datastream/#force-merging-data-streams",
    "relUrl": "/dashboards/im-dashboards/datastream/#force-merging-data-streams"
  },"316": {
    "doc": "Force merge",
    "title": "Force merge",
    "content": "Introduced 2.6 . OpenSearch Dashboards allows you to perform a force merge operation on two or more indexes with Index Management. ",
    "url": "https://vagimeli.github.io/dashboards/im-dashboards/forcemerge/",
    "relUrl": "/dashboards/im-dashboards/forcemerge/"
  },"317": {
    "doc": "Force merge",
    "title": "Force merging indexes",
    "content": "To perform a force merge operation on two or more indexes, perform the following steps: . | Under Index Management, choose Indices. | Select the indexes you want to force merge. | Choose Actions, and then choose Force merge, as shown in the following image. | Under Configure source index, specify the indexes you want to force merge. | Optionally, under Advanced settings you can to choose to Flush indices or Only expunge delete and then specify the Max number of segments to merge to as shown in the following image. | . ",
    "url": "https://vagimeli.github.io/dashboards/im-dashboards/forcemerge/#force-merging-indexes",
    "relUrl": "/dashboards/im-dashboards/forcemerge/#force-merging-indexes"
  },"318": {
    "doc": "Force merge",
    "title": "Force merging data streams",
    "content": "To perform a force merge operation on two or more indexes, perform the following steps: . | Under Index Management, choose Data streams. | Select the data streams you want to force merge. | Choose Actions, and then choose Force merge. | Under Configure source index, specify the data streams you want to force merge. | Optionally, under Advanced settings you can to choose to Flush indices or Only expunge delete and then specify the Max number of segments to merge to as shown in the following image. | . ",
    "url": "https://vagimeli.github.io/dashboards/im-dashboards/forcemerge/#force-merging-data-streams",
    "relUrl": "/dashboards/im-dashboards/forcemerge/#force-merging-data-streams"
  },"319": {
    "doc": "Indexes",
    "title": "Indexes",
    "content": "Introduced 2.5 . In the Index Management section, you can perform the operations available in the Index API. ",
    "url": "https://vagimeli.github.io/dashboards/im-dashboards/index-management/",
    "relUrl": "/dashboards/im-dashboards/index-management/"
  },"320": {
    "doc": "Indexes",
    "title": "Index policies",
    "content": "Policies are configurations that define the possible states of an index, the actions to perform when an index enters a given state, and the conditions that must be met to transition between states: . | States: The possible states of an index, including the default state for new indexes. For example, you might name your states hot, warm, or delete. For more information, see States. | Actions: Any actions that you want the plugin to take when an index enters a given state, such as performing a rollover. For more information, see Actions. | Transitions: The conditions that must be met for an index to move into a new state. For example, if an index is more than 8 weeks old, you might want to move it to the delete state. For more information, see Transitions. | . You can also upload a JSON document to specify an index policy. You have complete flexibility in designing your policies. You can create any state, transition to any other state, and specify any number of actions in each state. To attach policies to indexes, perform the following steps: . | Under Index Management, choose Index policies. | Select the index or indexes to which you want to attach your policy. | Choose the Apply policy button. | From the Policy ID menu, select the policy that you created. View the preview of your policy. | (Optional): Specify a rollover alias if your policy includes a rollover operation. Make sure that the alias already exists. For more information about the rollover operation, see rollover. | Choose the Apply button. | . After you attach a policy to an index, Index State Management (ISM) creates a job that runs every 5 minutes by default to perform policy actions, check conditions, and transition the index into different states. To change the default time interval for this job, see Settings. Policy jobs don’t run if the cluster state is red. ",
    "url": "https://vagimeli.github.io/dashboards/im-dashboards/index-management/#index-policies",
    "relUrl": "/dashboards/im-dashboards/index-management/#index-policies"
  },"321": {
    "doc": "Indexes",
    "title": "Managed indexes",
    "content": "To attach policies to indexes, perform the following steps: . | Under Index Management, choose Manage Indices. | Select the index or indexes to which you want to attach your policy. | Choose the Change policy button. | Choose the Apply policy button. | . ",
    "url": "https://vagimeli.github.io/dashboards/im-dashboards/index-management/#managed-indexes",
    "relUrl": "/dashboards/im-dashboards/index-management/#managed-indexes"
  },"322": {
    "doc": "Indexes",
    "title": "Indexes",
    "content": "The Indices section displays a list of indexes in your OpenSearch cluster. For each index, you can see its health status (green, yellow, or red), policy (if the index is managed by a policy), status, total size, primary sizes, total documents, deleted documents, primaries, and replicas. The following are the three index health statuses: . | Green: All primary and replica shards are assigned. | Yellow: At least one replica shard is not assigned. | Red: At least one primary shard is not assigned. | . Creating an index . While you can create an index by using a document as a base, you can also create an empty index for later use. To create an index, select the Create Index button located under the Indices section of Index Management. Then define the index by setting the following parameters: . | Index name | Number of primary shards | Number of replicas | Refresh interval | . You can also add fields and objects using either the visual editor or the JSON editor. The Advanced settings allow you to upload a JSON configuration. Applying a policy . If you analyze time-series data, you likely want to prioritize new data over old data. You might periodically perform certain operations on older indexes, such as reducing replica count or deleting them. ISM is a plugin that lets you automate these periodic administrative operations by triggering them based on changes in the index age, index size, or number of documents. You can define policies that automatically handle index rollovers or deletions to fit your use case. For example, you can define a policy that moves your index into a read_only state after 30 days and then deletes it after a set period of 90 days. You can also set up the policy to send you a notification message when the index is deleted. You might want to perform an index rollover after a certain amount of time or run a force_merge operation on an index during off-peak hours to improve search performance during peak hours. To apply a policy, select the index to which you want to apply the policy from the Indices list under Index Management. Then select the Actions button and select Apply policy from the dropdown list as shown in the following image. Closing an index . The close index operation closes an index. Once an index is closed, you cannot add data to it or search for any data within the index. To close an index, select the index you want to close from the Indices list under Index Management. Then select the Actions button and select Close from the dropdown list. Opening an index . The open index operation opens a closed index, letting you add data to it or search for data within the index. To open an index, select the index you want to open from the Indices list under Index Management. Then select the Actions button and select Open from the dropdown list. Reindexing an index . The reindex operation lets you copy all of your data or a subset of data from a source index into a destination index. To reindex an index, select the index from the Indices list under Index Management. Then select the Actions button and select Reindex from the dropdown list as shown in the following image. Shrinking an index . The shrink index operation copies all of the data in an existing index into a new index with fewer primary shards. To shrink an index, select the index you want to shrink from the Indices list under Index Management. Then choose the Actions button and choose Shrink from the dropdown list as shown in the following image. Splitting an index . The split index operation splits an existing read-only index into a new index, splitting each primary shard into a number of primary shards in the new index. To split an index, select the index you want to split from the Indices list under Index Management. Then choose the Actions button and choose Split from the dropdown list as shown in the following image. Deleting an index . If you no longer need an index, you can use the delete index operation to delete it. To delete an index, select the index you want to delete from the Indices list under Index Management. Then select the Actions button and select Delete from the dropdown list. ",
    "url": "https://vagimeli.github.io/dashboards/im-dashboards/index-management/",
    "relUrl": "/dashboards/im-dashboards/index-management/"
  },"323": {
    "doc": "Indexes",
    "title": "Templates",
    "content": "Index templates let you initialize new indexes with predefined mappings and settings. For example, if you continuously index log data, you can define an index template so that all of the indexes have the same number of shards and replicas as shown in the following image. Creating a template . To create a template, choose the Create template button on the Templates page under Index Management. Next, define the template: . | Enter the template name. | Select the template type. | Specify any index patterns you would like to use. | Set the priority of the template. | Select an index alias. | Set the number of primary shards. | Set the number of replicas. | Set the refresh intervals. | Add fields and objects for your index mapping using either the visual editor or the JSON editor. | Under Advanced Settings you can specify advanced index settings with a comma-delimited list as shown in the following image. | . Editing a template . To edit a template, select the template you want to edit from the list of templates. Next, select the Actions dropdown list and select the Edit option. Deleting a template . To delete a template, select the template you want to delete from the list of templates. Next, select the Actions dropdown list and select the Delete option. ",
    "url": "https://vagimeli.github.io/dashboards/im-dashboards/index-management/#templates",
    "relUrl": "/dashboards/im-dashboards/index-management/#templates"
  },"324": {
    "doc": "Indexes",
    "title": "Aliases",
    "content": "An alias is a virtual index name that can point to one or more indexes. If your data is spread across multiple indexes, rather than keeping track of which indexes to query, you can create an alias and query it instead as shown in the following image. To create an alias, perform the following steps: . | Choose the Create Alias button on the Aliases page under Index Management. | Specify the alias name. | Enter the index, or index patterns, to be included in the alias. | Choose Create alias as shown in the following image. | . To edit an alias, perform the following steps: . | Select the alias you want to edit. | Choose the Actions button. | Choose Edit from the dropdown list. | . To delete an alias, perform the following steps: . | Select the alias you want to edit. | Choose the Actions button. | Choose Delete from the dropdown list. | . ",
    "url": "https://vagimeli.github.io/dashboards/im-dashboards/index-management/#aliases",
    "relUrl": "/dashboards/im-dashboards/index-management/#aliases"
  },"325": {
    "doc": "Indexes",
    "title": "Rollup jobs",
    "content": "The Rollup Jobs section under Index Management allows you to create or update index rollup jobs. To create a rollup job, perform the following steps: . | Choose the Create rollup job button on the Rollup Jobs page under Index Management. | Set the name, source index, and target index. | Choose Next. | Set the timestamp field and interval type. | Optionally, set additional aggregations and metrics. | Choose Next. | Under Schedule, check or uncheck Enable job by default. | Set the Continuous, Execution frequency, Rollup interval, and Pages per execution settings. | Additionally, you can set an execution delay. | Choose Next. | Review the settings for the rollup job and choose Create. | . You can also enable and disable rollup jobs by choosing the corresponding buttons on the Rollup Jobs page. ",
    "url": "https://vagimeli.github.io/dashboards/im-dashboards/index-management/#rollup-jobs",
    "relUrl": "/dashboards/im-dashboards/index-management/#rollup-jobs"
  },"326": {
    "doc": "Indexes",
    "title": "Transform jobs",
    "content": "You can create, start, stop, and complete operations with transform jobs. To create a transform job, perform the following steps: . | Choose the Create transform job button on the Transform Jobs page under Index Management. | Set the name, source index, and target index. | Choose Next. | Select the fields to transform. From the table, select a field you want to transform by choosing + next to the field name. | Choose Next. | Check or uncheck Job enabled by default. | Set the transform execution interval and whether the schedule is continuous. | Optionally, set pages per execution under the Advanced dropdown list. | Choose Next. | Review the settings for the rollup job and choose Create. | . You can also enable and disable rollup jobs by choosing the corresponding buttons on the Transform Jobs page. ",
    "url": "https://vagimeli.github.io/dashboards/im-dashboards/index-management/#transform-jobs",
    "relUrl": "/dashboards/im-dashboards/index-management/#transform-jobs"
  },"327": {
    "doc": "Indexes",
    "title": "Long-running operation status check",
    "content": "Certain index operations take additional time to complete (usually more than 30 seconds, but up to tens of minutes or hours). This is tracked in the index status column on the Indices page. You can check the status of the reindex, shrink, and split operations because they are one-time, non-recursive operations. ",
    "url": "https://vagimeli.github.io/dashboards/im-dashboards/index-management/#long-running-operation-status-check",
    "relUrl": "/dashboards/im-dashboards/index-management/#long-running-operation-status-check"
  },"328": {
    "doc": "Indexes",
    "title": "Security integration",
    "content": "Permission control is managed with existing permissions or action groups that are enforced at the API level. There is currently no UI-level permission control. Users with permission to access the ISM plugin are able to view new pages. They can also make changes if they have permission to run the related APIs. ",
    "url": "https://vagimeli.github.io/dashboards/im-dashboards/index-management/#security-integration",
    "relUrl": "/dashboards/im-dashboards/index-management/#security-integration"
  },"329": {
    "doc": "Indexes",
    "title": "Error handling",
    "content": "Similar to API calls, if the operation fails immediately, you will be notified with an error message. However, if it is a long-running operation, you will be notified of the failure at the time of failure, or you can check the index status on the Indices page. ",
    "url": "https://vagimeli.github.io/dashboards/im-dashboards/index-management/#error-handling",
    "relUrl": "/dashboards/im-dashboards/index-management/#error-handling"
  },"330": {
    "doc": "Index management in Dashboards",
    "title": "Index management in Dashboards",
    "content": "Introduced 2.5 . Previously, users relied on REST APIs or YAML configurations for basic administrative operations and interventions. This release takes the first step toward a unified administration panel in OpenSearch Dashboards with the launch of several index management UI enhancements. The new interface provides a more user-friendly way to run common indexing and data stream operations. Now you can perform create, read, update, and delete (CRUD) and mapping operations for indexes, index templates, and aliases through the UI. Additionally, you can open, close, reindex, shrink, and split indexes. The UI runs index status and data validation before submitting requests and lets you compare changes with previously saved settings before making updates. ",
    "url": "https://vagimeli.github.io/dashboards/im-dashboards/index/",
    "relUrl": "/dashboards/im-dashboards/index/"
  },"331": {
    "doc": "Rollover",
    "title": "Rollover",
    "content": "Introduced 2.6 . OpenSearch Dashboards allows you to perform an index rollover operation with Index Management. ",
    "url": "https://vagimeli.github.io/dashboards/im-dashboards/rollover/",
    "relUrl": "/dashboards/im-dashboards/rollover/"
  },"332": {
    "doc": "Rollover",
    "title": "Data streams",
    "content": "To perform a rollover operation on a data stream, perform the following steps: . | Under Index Management, choose Data streams. | Choose Actions, and then choose Roll over, as shown in the following image. | Under Configure source, select the source data stream on which you want to perform the rollover operation. | Choose Roll over, as shown in the following image. | . ",
    "url": "https://vagimeli.github.io/dashboards/im-dashboards/rollover/#data-streams",
    "relUrl": "/dashboards/im-dashboards/rollover/#data-streams"
  },"333": {
    "doc": "Rollover",
    "title": "Aliases",
    "content": "To perform a rollover operation on an alias, perform the following steps: . | Under Index Management, choose Aliases. | Choose Actions, and then choose Roll over, as shown in the following image. | Under Configure source, select the source alias on which you want to perform the rollover operation. | If the alias does not contain a write index, you are prompted to assign a write index, as shown in the following image. | Under Configure a new rollover index and on the Define index pane, specify an index name and an optional index alias. | Under Index settings, specify the number of primary shards, the number of replicas, and the refresh interval, as shown in the following image. | Choose Roll over. | . ",
    "url": "https://vagimeli.github.io/dashboards/im-dashboards/rollover/#aliases",
    "relUrl": "/dashboards/im-dashboards/rollover/#aliases"
  },"334": {
    "doc": "OpenSearch Dashboards",
    "title": "OpenSearch Dashboards",
    "content": "OpenSearch Dashboards is the user interface that lets you visualize your OpenSearch data and run and scale your OpenSearch clusters. ",
    "url": "https://vagimeli.github.io/dashboards/index/",
    "relUrl": "/dashboards/index/"
  },"335": {
    "doc": "OpenSearch Dashboards",
    "title": "Getting started",
    "content": "| Concept | Description | . | OpenSearch Dashboards Quickstart | Learn about the basic concepts and features of OpenSearch Dashboards. | . | OpenSearch Playground | Explore features in OpenSearch Dashboards without downloading or installing anything. | . | Install and configure OpenSearch Dashboards | Get started with OpenSearch Dashboards. | . | Create visualizations | Learn about visualizing data in OpenSearch Dashboards. | . | Explore and query data | Learn how to explore and query data in OpenSearch. | . ",
    "url": "https://vagimeli.github.io/dashboards/index/#getting-started",
    "relUrl": "/dashboards/index/#getting-started"
  },"336": {
    "doc": "OpenSearch Dashboards",
    "title": "Observability",
    "content": "| Concept | Description | . | Observability in OpenSearch Dashboards | Observe, monitor, and secure data and improve performance across tools and workflows. | . ",
    "url": "https://vagimeli.github.io/dashboards/index/#observability",
    "relUrl": "/dashboards/index/#observability"
  },"337": {
    "doc": "OpenSearch Dashboards",
    "title": "Dev Tools",
    "content": "| Concept | Description | . | Dev Tools | Learn how to run OpenSearch queries in an integrated console. | . ",
    "url": "https://vagimeli.github.io/dashboards/index/#dev-tools",
    "relUrl": "/dashboards/index/#dev-tools"
  },"338": {
    "doc": "Quickstart guide",
    "title": "Quickstart guide",
    "content": "This quickstart guide covers the core concepts that you need to understand to get started with OpenSearch Dashboards. You’ll learn how to: . | Add sample data. | Explore and inspect data. | Visualize data. | . Before you get started, make sure you’ve installed OpenSearch and OpenSearch Dashboards. For information on installation and configuration, see Install and configure OpenSearch and Install and configure OpenSearch Dashboards. ",
    "url": "https://vagimeli.github.io/dashboards/quickstart/",
    "relUrl": "/dashboards/quickstart/"
  },"339": {
    "doc": "Quickstart guide",
    "title": "Adding sample data",
    "content": "Sample datasets come with visualizations, dashboards, and other tools to help you explore Dashboards before you add your own data. To add sample data, perform the following steps: . | Verify access to OpenSearch Dashboards by connecting to http://localhost:5601 from a browser. The default username and password are admin. | On the OpenSearch Dashboards Home page, choose Add sample data. | Choose Add data to add the datasets, as shown in the following image. | . ",
    "url": "https://vagimeli.github.io/dashboards/quickstart/#adding-sample-data",
    "relUrl": "/dashboards/quickstart/#adding-sample-data"
  },"340": {
    "doc": "Quickstart guide",
    "title": "Exploring and inspecting data",
    "content": "In Discover, you can: . | Choose data to explore, set a time range for that data, search it using Dashboards Query Language (DQL), and filter the results. | Explore the data, view individual documents, and create tables summarizing the data’s contents. | Visualize your findings. | . ",
    "url": "https://vagimeli.github.io/dashboards/quickstart/#exploring-and-inspecting-data",
    "relUrl": "/dashboards/quickstart/#exploring-and-inspecting-data"
  },"341": {
    "doc": "Quickstart guide",
    "title": "Try it: Getting familiar with Discover",
    "content": ". | On the OpenSearch Dashboards Home page, choose Discover. | Change the time filter to Last 7 days, as shown in the following image. | Search using the DQL query FlightDelay:true AND DestCountry: US AND FlightDelayMin &gt;= 60 and then choose Update. You should see results for US-bound flights delayed by 60 minutes or more, as shown in the following image. | To filter data, choose Add filter and then select an Available field. For example, select FlightDelayType, is, and Weather delay from the Field, Operator, and Value dropdown lists, as shown in the following image. | . ",
    "url": "https://vagimeli.github.io/dashboards/quickstart/#try-it-getting-familiar-with-discover",
    "relUrl": "/dashboards/quickstart/#try-it-getting-familiar-with-discover"
  },"342": {
    "doc": "Quickstart guide",
    "title": "Visualizing data",
    "content": "Raw data can be difficult to comprehend and use. Data visualizations help you prepare and present data in a visual form. In Dashboard you can: . | Display data in a single view. | Build dynamic dashboards. | Create and share reports. | Embed analytics to differentiate your applications. | . ",
    "url": "https://vagimeli.github.io/dashboards/quickstart/#visualizing-data",
    "relUrl": "/dashboards/quickstart/#visualizing-data"
  },"343": {
    "doc": "Quickstart guide",
    "title": "Try it: Getting familiar with Dashboard",
    "content": ". | On the OpenSearch Dashboards Home page, choose Dashboard. | Choose [Flights] Global Flight Data in the Dashboards window, as shown in the following image. | To add panels to the dashboard, choose Edit and then Add from the toolbar. | In the Add panels window, choose the existing panel [Flights] Delay Buckets. You’ll see a pop-up window on the lower right confirming that you’ve added the panel. | Select x to close the Add panels window. | View the added panel [Flights] Delay Buckets, which is added as the last panel on the dashboard, as shown in the following image. | . ",
    "url": "https://vagimeli.github.io/dashboards/quickstart/#try-it-getting-familiar-with-dashboard",
    "relUrl": "/dashboards/quickstart/#try-it-getting-familiar-with-dashboard"
  },"344": {
    "doc": "Quickstart guide",
    "title": "Try it: Creating a visualization panel",
    "content": "Continuing with the preceding dashboard, you’ll create a bar chart comparing the number of canceled flights and delayed flights to delay type and then add the panel to the dashboard: . | Change the default time range from 24 hours to Last 7 days. | In the toolbar, choose Edit, then Create new. | Select VisBuilder in the New Visualizations window. | In the Data Source dropdown list, choose opensearch_dashboards_sample_data_flights. | Drag the fields Cancelled and FlightDelay to the y-axis column. | Drag the field FlightDelayType to the x-axis column. | Choose Save and name the visualization in the Title field. | Choose Save and return. The following bar chart is added as the last panel on the dashboard, as shown in the following image. | . ",
    "url": "https://vagimeli.github.io/dashboards/quickstart/#try-it-creating-a-visualization-panel",
    "relUrl": "/dashboards/quickstart/#try-it-creating-a-visualization-panel"
  },"345": {
    "doc": "Quickstart guide",
    "title": "Interacting with data",
    "content": "Interactive dashboards allow you analyze data in more depth and filter it in several ways. In Dashboards, you can interact directly with data on a dashboard by using dashboard-level filters. For example, continuing with the preceding dashboard, you can filter to show delays and cancellations for a specific airline. ",
    "url": "https://vagimeli.github.io/dashboards/quickstart/#interacting-with-data",
    "relUrl": "/dashboards/quickstart/#interacting-with-data"
  },"346": {
    "doc": "Quickstart guide",
    "title": "Try it: Interacting with the sample flight data",
    "content": ". | On the [Flights] Airline Carrier panel, choose OpenSearch-Air. The dashboard updates automatically. | Choose Save to save the customized dashboard. | . Alternatively, you can apply filters using the dashboard toolbar: . | In the dashboard toolbar, choose Add filter. | From the Field, Operator, and Value dropdown lists, choose Carrier, is, and OpenSearch-Air, respectively, as shown in the following image. | Choose Save. The dashboard updates automatically, and the result is the dashboard shown in the following image. | . ",
    "url": "https://vagimeli.github.io/dashboards/quickstart/#try-it-interacting-with-the-sample-flight-data",
    "relUrl": "/dashboards/quickstart/#try-it-interacting-with-the-sample-flight-data"
  },"347": {
    "doc": "Quickstart guide",
    "title": "Next steps",
    "content": ". | Visualize data. To learn more about data visualizations in OpenSearch Dashboards, see Building data visualizations. | Create dashboards. To learn more about creating dashboards in OpenSearch Dashboards, see Creating dashboards. | Explore data. To learn more about exploring data in OpenSearch Dashboards, see Exploring data. | . ",
    "url": "https://vagimeli.github.io/dashboards/quickstart/#next-steps",
    "relUrl": "/dashboards/quickstart/#next-steps"
  },"348": {
    "doc": "Creating and requesting a visualization report",
    "title": "Creating and requesting a visualization report",
    "content": "First, you need to get the URL for the visualization that you want to download as an image file or PDF. To generate a visualization report, you need to specify the Dashboards URL. Open the visualization for which you want to generate a report, and select Share &gt; Permalinks &gt; Generate link as Shapshot &gt; Short URL &gt; Copy link, as shown in the following image. You will need to add the URL with the -u argument when you request the report in the CLI. Example: Requesting a PNG file . The following command requests a report in PNG format with basic authentication and sends the report to an email address using Amazon SES: . opensearch-reporting-cli -u https://localhost:5601/app/dashboards#/view/7adfa750-4c81-11e8-b3d7-01146121b73d -a basic -c admin:Test@1234 -e ses -s &lt;email address&gt; -r &lt;email address&gt; -f png . Example: Requesting a PDF file . The following command requests a PDF file and specifies the recipient’s email address: . opensearch-reporting-cli -u https://localhost:5601/app/dashboards#/view/7adfa750-4c81-11e8-b3d7-01146121b73d -a basic -c admin:Test@1234 -e ses -s &lt;email address&gt; -r &lt;email address&gt; -f pdf . Upon success, the file will be sent to the specified email address. The following image shows an example PDF report. Example: Requesting a CSV file . The following command generates a report that contains all table content in CSV format and sends the report to an email address using Amazon SES transport: . opensearch-reporting-cli -u https://localhost:5601/app/dashboards#/view/7adfa750-4c81-11e8-b3d7-01146121b73d -f csv -a basic -c admin:Test@1234 -e ses -s &lt;email address&gt; -r &lt;email address&gt; . Upon success, the email will be sent to the specified email address with the CSV file attached. ",
    "url": "https://vagimeli.github.io/dashboards/reporting-cli/rep-cli-create/",
    "relUrl": "/dashboards/reporting-cli/rep-cli-create/"
  },"349": {
    "doc": "Scheduling reports with the cron utility",
    "title": "Scheduling reports with the cron utility",
    "content": "You can use the cron command-line utility to initiate a report request with the Reporting CLI that runs periodically at any date or time interval. Follow the cron expression syntax to specify the date and time that precedes the command that you want to initiate. To learn about the cron expression syntax, see Cron expression reference. To get help with cron, open the man page by running the following command: . man cron . Prerequisites . | You need a machine with cron installed. | You need to install the Reporting CLI. See Downloading and installing the Reporting CLI tool | . ",
    "url": "https://vagimeli.github.io/dashboards/reporting-cli/rep-cli-cron/",
    "relUrl": "/dashboards/reporting-cli/rep-cli-cron/"
  },"350": {
    "doc": "Scheduling reports with the cron utility",
    "title": "Specifying the report details",
    "content": "Open the crontab editor by running the following command: . crontab -e . In the crontab editor, enter the report request. The following example shows a cron report that runs every day at 8:00 AM: . 0 8 * * * opensearch-reporting-cli -u https://playground.opensearch.org/app/dashboards#/view/084aed50-6f48-11ed-a3d5-1ddbf0afc873 -e ses -s &lt;sender_email&gt; -r &lt;recipient_email&gt; . ",
    "url": "https://vagimeli.github.io/dashboards/reporting-cli/rep-cli-cron/#specifying-the-report-details",
    "relUrl": "/dashboards/reporting-cli/rep-cli-cron/#specifying-the-report-details"
  },"351": {
    "doc": "Using environment variables with the Reporting CLI",
    "title": "Using environment variables with the Reporting CLI",
    "content": "Instead of explicitly providing values in the command line, you can save them as environment variables. The Reporting CLI reads environment variables from the current directory inside the project. To set the environment variables in Linux, use the following command: . export NAME=VALUE . Each line should use the format NAME=VALUE. Each line that starts with a hashtag (#) is considered to be a comment. Quotation marks (“) don’t get any special handling. Values from the command line argument have higher priority than the environment file. For example, if you add the file name as test in the .env file and also add the --filename report command option, the generated report’s name will be report. Example: Requesting a PNG report with environment variables set . The following command requests a report with basic authentication in PNG format: . opensearch-reporting-cli --url https://localhost:5601/app/dashboards#/view/7adfa750-4c81-11e8-b3d7-01146121b73d --format png --auth basic --credentials admin:admin . Upon success, the report will download to the current directory. ",
    "url": "https://vagimeli.github.io/dashboards/reporting-cli/rep-cli-env-var/",
    "relUrl": "/dashboards/reporting-cli/rep-cli-env-var/"
  },"352": {
    "doc": "Using environment variables with the Reporting CLI",
    "title": "Using Amazon SES to request an email with a report attachment",
    "content": "To use Amazon SES as the email transport mechanism, the following prerequisites apply: . | The sender’s email address must be verified by Amazon SES. The AWS Command Line Interface (AWS CLI) is required to interact with Amazon SES. To configure basic settings used by the AWS CLI, see Quick configuration with aws configure in the AWS Command Line Interface user guide. | Amazon SES transport requires the ses:SendRawEmail role: | . { \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"ses:SendRawEmail\", \"Resource\": \"*\" } ] } . The following command requests an email with the report attached: . opensearch-reporting-cli --url https://localhost:5601/app/dashboards#/view/7adfa750-4c81-11e8-b3d7-01146121b73d --transport ses --from &lt;sender_email_id&gt; --to &lt;recipient_email_id&gt; . The following command uses default values for all other options. You can also set OPENSEARCH_FROM, OPENSEARCH_TO, and OPENSEARCH_TRANSPORT in your .env file and use the following command: . opensearch-reporting-cli --url https://localhost:5601/app/dashboards#/view/7adfa750-4c81-11e8-b3d7-01146121b73d . To modify the body of your email, you can edit the index.hbs file. Example: Sending a report to an email address with SMTP . To send a report to an email address with SMTP transport, you need to set the options OPENSEARCH_SMTP_HOST, OPENSEARCH_SMTP_PORT, OPENSEARCH_SMTP_USER, OPENSEARCH_SMTP_PASSWORD, and OPENSEARCH_SMTP_SECURE in your .env file. Once the transport options are set in your .env file, you can send the email using the following command: . opensearch-reporting-cli --url https://localhost:5601/app/dashboards#/view/7adfa750-4c81-11e8-b3d7-01146121b73d --transport smtp --from &lt;sender_email_id&gt; --to &lt;recipient_email_id&gt; . You can choose to set options using either your .env file or the command line argument values in any combination. Make sure to specify all required values to avoid errors. To modify the body of your email, you can edit the index.hbs file. ",
    "url": "https://vagimeli.github.io/dashboards/reporting-cli/rep-cli-env-var/#using-amazon-ses-to-request-an-email-with-a-report-attachment",
    "relUrl": "/dashboards/reporting-cli/rep-cli-env-var/#using-amazon-ses-to-request-an-email-with-a-report-attachment"
  },"353": {
    "doc": "Using environment variables with the Reporting CLI",
    "title": "Limitations",
    "content": "The following limitations apply to environment variable usage with the Reporting CLI: . | Supported platforms are Windows x86, Windows x64, Mac Intel, Mac ARM, Linux x86, and Linux x64. For any other platform, users can take advantage of the CHROMIUM_PATH environment variable to use custom Chromium. | If a URL contains an exclamation point (!), then the history expansion needs to be disabled temporarily. Depending on which shell you are using, you can disable history expansion using one of the following commands: . | For bash, use set +H. | For zsh, use setopt nobanghist. | . Alternatively, you can add a URL value as an environment variable using this format: URL=\"&lt;url-with-!&gt;\". | All command options only accept lowercase letters. | . ",
    "url": "https://vagimeli.github.io/dashboards/reporting-cli/rep-cli-env-var/#limitations",
    "relUrl": "/dashboards/reporting-cli/rep-cli-env-var/#limitations"
  },"354": {
    "doc": "Using environment variables with the Reporting CLI",
    "title": "Troubleshooting",
    "content": "To resolve MessageRejected: Email address is not verified, see Why am I getting a 400 “message rejected” error with the message “Email address is not verified” from Amazon SES? in the AWS Knowledge Center. ",
    "url": "https://vagimeli.github.io/dashboards/reporting-cli/rep-cli-env-var/#troubleshooting",
    "relUrl": "/dashboards/reporting-cli/rep-cli-env-var/#troubleshooting"
  },"355": {
    "doc": "Creating reports with the Reporting CLI",
    "title": "Creating reports with the Reporting CLI",
    "content": "You can programmatically create dashboard reports in PDF or PNG format with the Reporting CLI without using OpenSearch Dashboards or the Reporting plugin. This allows you to create reports automatically within your email workflows. If you want to download a CSV file, you need to have the Reporting plugin installed. For any dashboard view, you can request a report in PNG or PDF format to be sent to an email address. This can be useful for sending reports to multiple email recipients with an email alias. The only dashboard application that supports creating a CSV report is Discover. With the Reporting CLI, you can specify options for your report in the command line. The report is sent to an email address as a PDF attachment by default. You can also request a PNG image or a CSV file with the --formats argument. You can download the report to the directory in which you are running the Reporting CLI, or you can email the report by specifying Amazon Simple Email Service (Amazon SES) or SMTP for the email transport option. You can connect to OpenSearch with any of the following authentication types: . | Basic – Basic HTTP authentication. Use -a basic. | Cognito – Authentication through Amazon Cognito. Use -a cognito. | SAML – Authentication between an identity provider and a service provider. Use -a saml. Okta provides the SAML third-party authentication. | No auth – No authentication. Use -a none. Authentication defaults to No auth if the -a flag is not specified. | . To learn more about Amazon Cognito, see What is Amazon Cognito?. ",
    "url": "https://vagimeli.github.io/dashboards/reporting-cli/rep-cli-index/",
    "relUrl": "/dashboards/reporting-cli/rep-cli-index/"
  },"356": {
    "doc": "Downloading and installing the Reporting CLI tool",
    "title": "Downloading and installing the Reporting CLI tool",
    "content": "You can download and install the Reporting CLI tool from either the npm software registry or the OpenSearch.org Artifacts hub. Refer to the following sections for instructions. To learn more about the npm software registry, see the npm documentation. ",
    "url": "https://vagimeli.github.io/dashboards/reporting-cli/rep-cli-install/",
    "relUrl": "/dashboards/reporting-cli/rep-cli-install/"
  },"357": {
    "doc": "Downloading and installing the Reporting CLI tool",
    "title": "Downloading and installing the Reporting CLI from npm",
    "content": "To download and install the Reporting CLI from npm, run the following command to initiate installation: . npm i @opensearch-project/reporting-cli . ",
    "url": "https://vagimeli.github.io/dashboards/reporting-cli/rep-cli-install/#downloading-and-installing-the-reporting-cli-from-npm",
    "relUrl": "/dashboards/reporting-cli/rep-cli-install/#downloading-and-installing-the-reporting-cli-from-npm"
  },"358": {
    "doc": "Downloading and installing the Reporting CLI tool",
    "title": "Downloading and installing the Reporting CLI from OpenSearch.org",
    "content": "You can download the opensearch-reporting-cli tool from the OpenSearch.org Artifacts hub. Next, run the following command to install the .tar archive: . npm install -g opensearch-reporting-cli-1.0.0.tgz . To provide better security for artifacts, we recommend that you verify signatures by downloading the Reporting CLI signature file. To learn more about verifying signatures, see How to verify signatures for downloadable artifacts. ",
    "url": "https://vagimeli.github.io/dashboards/reporting-cli/rep-cli-install/#downloading-and-installing-the-reporting-cli-from-opensearchorg",
    "relUrl": "/dashboards/reporting-cli/rep-cli-install/#downloading-and-installing-the-reporting-cli-from-opensearchorg"
  },"359": {
    "doc": "Scheduling reports with AWS Lambda",
    "title": "Scheduling reports with AWS Lambda",
    "content": "You can use AWS Lambda with the Reporting CLI tool to specify an AWS Lambda function to trigger the report generation. This requires that you use an AMD64 system and Docker. Prerequisites . To use the Reporting CLI with AWS Lambda, you need to do the following preliminary steps. | Get an AWS account. For instructions, see Creating an AWS account in the AWS Account Management reference guide. | Set up an Amazon Elastic Container Registry (ECR). For instructions, see Getting started with Amazon ECR using the AWS Management Console. | . ",
    "url": "https://vagimeli.github.io/dashboards/reporting-cli/rep-cli-lambda/",
    "relUrl": "/dashboards/reporting-cli/rep-cli-lambda/"
  },"360": {
    "doc": "Scheduling reports with AWS Lambda",
    "title": "Step 1: Create a container image with a Dockerfile",
    "content": "You need to assemble the container image by running a Dockerfile. When you run the Dockerfile, it downloads the OpenSearch artifact required to use the Reporting CLI. To learn more about Dockerfiles, see Dockerfile reference. Copy the following sample configurations into a Dockerfile: . # Define function directory ARG FUNCTION_DIR=\"/function\" # Base image of the docker container FROM node:lts-slim as build-image # Include global arg in this stage of the build ARG FUNCTION_DIR # AWS Lambda runtime dependencies RUN apt-get update &amp;&amp; \\ apt-get install -y \\ g++ \\ make \\ unzip \\ libcurl4-openssl-dev \\ autoconf \\ automake \\ libtool \\ cmake \\ python3 \\ libkrb5-dev \\ curl # Copy function code WORKDIR ${FUNCTION_DIR} RUN npm install @opensearch-project/reporting-cli &amp;&amp; npm install aws-lambda-ric # Build Stage 2: Copy Build Stage 1 files in to Stage 2. Install chrome, then remove chrome to keep the dependencies. FROM node:lts-slim # Include global arg in this stage of the build ARG FUNCTION_DIR # Set working directory to function root directory WORKDIR ${FUNCTION_DIR} # Copy in the build image dependencies COPY --from=build-image ${FUNCTION_DIR} ${FUNCTION_DIR} # Install latest chrome dev package and fonts to support major char sets (Chinese, Japanese, Arabic, Hebrew, Thai and a few others) # Note: this installs the necessary libs to make the bundled version of Chromium that Puppeteer installs, work. RUN apt-get update \\ &amp;&amp; apt-get install -y wget gnupg \\ &amp;&amp; wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | apt-key add - \\ &amp;&amp; sh -c 'echo \"deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main\" &gt;&gt; /etc/apt/sources.list.d/google.list' \\ &amp;&amp; apt-get update \\ &amp;&amp; apt-get install -y google-chrome-stable fonts-ipafont-gothic fonts-wqy-zenhei fonts-thai-tlwg fonts-kacst fonts-freefont-ttf libxss1 \\ --no-install-recommends \\ &amp;&amp; apt-get remove -y google-chrome-stable \\ &amp;&amp; rm -rf /var/lib/apt/lists/* ENTRYPOINT [\"/usr/local/bin/npx\", \"aws-lambda-ric\"] ENV HOME=\"/tmp\" CMD [ \"/function/node_modules/@opensearch-project/reporting-cli/src/index.handler\" ] . Next, run the following build command within the same directory that contains the Dockerfile: . docker build -t opensearch-reporting-cli . ",
    "url": "https://vagimeli.github.io/dashboards/reporting-cli/rep-cli-lambda/#step-1-create-a-container-image-with-a-dockerfile",
    "relUrl": "/dashboards/reporting-cli/rep-cli-lambda/#step-1-create-a-container-image-with-a-dockerfile"
  },"361": {
    "doc": "Scheduling reports with AWS Lambda",
    "title": "Step 2: Create a private repository with Amazon ECR",
    "content": "You need to follow the instructions to create an image repository, see Getting started with Amazon ECR using the AWS Management Console. Give your repository the name opensearch-reporting-cli. In addition to the Amazon ECR instructions, you need to make several adjustments for the Reporting CLI to function properly as described in the following steps in this procedure. ",
    "url": "https://vagimeli.github.io/dashboards/reporting-cli/rep-cli-lambda/#step-2-create-a-private-repository-with-amazon-ecr",
    "relUrl": "/dashboards/reporting-cli/rep-cli-lambda/#step-2-create-a-private-repository-with-amazon-ecr"
  },"362": {
    "doc": "Scheduling reports with AWS Lambda",
    "title": "Step 3: Push the image to the private repository",
    "content": "You need to get several commands from the AWS ECR Console to run within the Dockerfile directory. | After you create your repository, select it from Private repositories. | Choose view push commands. | Copy and run each command shown in Push commands for opensearch-reporting-cli sequentially in the Dockerfile directory. | . For more details about Docker push commands, see Pushing a Docker image in the Amazon ECR user guide. ",
    "url": "https://vagimeli.github.io/dashboards/reporting-cli/rep-cli-lambda/#step-3-push-the-image-to-the-private-repository",
    "relUrl": "/dashboards/reporting-cli/rep-cli-lambda/#step-3-push-the-image-to-the-private-repository"
  },"363": {
    "doc": "Scheduling reports with AWS Lambda",
    "title": "Step 4: Create a Lambda function with the container image",
    "content": "Now that you have a container image created for the Reporting CLI, you need to create a function defined as the container image. | Open the AWS Lambda console and choose Functions. | Choose Create function, then choose Container image and fill in a name for the function. | In Container image URI, choose Browse images and select opensearch-reporting-cli for the image repository. | In Images select the image, and choose Select image. | In Architecture, choose x86_64. | Choose Create function. | Go to Lambda &gt; functions and choose the function you created. | Choose Configuration &gt; General configuration &gt; Edit timeout and set the timeout in lambda to 5 minutes to allow the Reporting CLI to generate the report. | Change the Ephemeral storage setting to at least 1024MB. The default setting is not a sufficient storage amount to support report generation. | Next, test the function either by providing values JSON format or by providing AWS Lambda environment variables. | . | If the function contains fixed values, such as email address you do not need a JSON file. You can specify an environment variable in AWS Lambda. | If the function takes a variable key-value pair, then you need to specify the values in the JSON with the same naming convention as command options, for example the --credentials option requires the username and password. | . The following example shows fixed values provided for the sender and recipient email addresses: . { \"url\": \"https://playground.opensearch.org/app/dashboards#/view/084aed50-6f48-11ed-a3d5-1ddbf0afc873\", \"transport\": \"ses\", \"from\": \"sender@amazon.com\", \"to\": \"recipient@amazon.com\", \"subject\": \"Test lambda docker image\" } . To learn more about AWS Lambda functions, see Deploying Lambda functions as container images in the AWS Lambda documentation. ",
    "url": "https://vagimeli.github.io/dashboards/reporting-cli/rep-cli-lambda/#step-4-create-a-lambda-function-with-the-container-image",
    "relUrl": "/dashboards/reporting-cli/rep-cli-lambda/#step-4-create-a-lambda-function-with-the-container-image"
  },"364": {
    "doc": "Scheduling reports with AWS Lambda",
    "title": "Step 5: Add the trigger to start the AWS Lambda function",
    "content": "Set the trigger to start running the report. AWS Lambda can use any AWS service as a trigger, such as SNS, S3, or an AWS CloudWatch EventBridge. | In the Triggers section, choose Add trigger. | Select a trigger from the list. For example, you can set an AWS CloudWatch Event. To learn more about Amazon ECR events you can schedule, see Sample events from Amazon ECR. | Choose Test to initiate the function. | . ",
    "url": "https://vagimeli.github.io/dashboards/reporting-cli/rep-cli-lambda/#step-5-add-the-trigger-to-start-the-aws-lambda-function",
    "relUrl": "/dashboards/reporting-cli/rep-cli-lambda/#step-5-add-the-trigger-to-start-the-aws-lambda-function"
  },"365": {
    "doc": "Scheduling reports with AWS Lambda",
    "title": "(Optional) Step 6: Add the role permission for Amazon SES",
    "content": "If you want to use Amazon SES for the email transport, you need to set up permissions. | Select Configuration and choose Execution role. | In Summary, choose Permissions. | Select {}JSON to open the JSON policy editor. | Add the permissions for the Amazon SES resource that you want to use. | . The following example provides the resource ARN for the send email action: . { \"Effect\": \"Allow\", \"Action\": [ \"ses:SendEmail\", \"ses:SendRawEmail\" ], \"Resource\": \"arn:aws:ses:us-west-2:555555511111:identity/username@amazon.com\" } . To learn more about setting role permissions, see Permissions in the AWS Lambda user guide. ",
    "url": "https://vagimeli.github.io/dashboards/reporting-cli/rep-cli-lambda/#optional-step-6-add-the-role-permission-for-amazon-ses",
    "relUrl": "/dashboards/reporting-cli/rep-cli-lambda/#optional-step-6-add-the-role-permission-for-amazon-ses"
  },"366": {
    "doc": "Reporting CLI options",
    "title": "Reporting CLI options",
    "content": "You can use any of the following arguments with the opensearch-reporting-cli tool. | Argument | Description | Acceptable values and usage | Environment variable | . | -u, --url | The URL for the visualization. | Obtain from OpenSearch Dashboards &gt; Visualize &gt; Share &gt; Permalinks &gt; Copy link. | OPENSEARCH_URL | . | -a, --auth | The authentication type for the report. | You can specify either Basic basic, Cognito cognito, SAML saml, or No Auth none. If no value is specified, the Reporting CLI tool defaults to no authentication, type none. Basic, Cognito, and SAML require credentials with the -c flag. | N/A | . | -c, --credentials | The OpenSearch login credentials. | Enter your username and password separated by a colon. For example, username:password. Required for Basic, Cognito, and SAML authentication types. | OPENSEARCH_USERNAME and OPENSEARCH_PASSWORD | . | -t, --tenant | The tenants in OpenSearch Dashboards. | The default tenant is private. | N/A | . | -f, --format | The file format for the report. | Can be either pdf, png, or csv. The default is pdf. | N/A | . | -w, --width | The window width in pixels for the report. | Default is 1680. | N/A | . | -l, --height | The minimum window height in pixels for the report. | Default is 600. | N/A | . | -n, --filename | The file name of the report. | Default is reporting. | opensearch-report-YYY-MM-DDTHH-mm-ss.sssZ | . | -e, --transport | The transport mechanism for sending the email. | For Amazon SES, specify ses. Amazon SES requires an AWS configuration on your system to store the credentials. For SMTP, use smtp and also specify the login credentials with --smtpusername and --smtppassword. | OPENSEARCH_TRANSPORT | . | -s, --from | The email address of the sender. | For example, user@amazon.com. | OPENSEARCH_FROM | . | -r, --to | The email address of the recipient. | For example, user@amazon.com. | OPENSEARCH_TO | . | --smtphost | The hostname of the SMTP server. | For example, SMTP_HOST. | OPENSEARCH_SMTP_HOST | . | --smtpport | The port for the SMTP connection. | For example, SMTP_PORT. | OPENSEARCH_SMTP_PORT | . | --smtpsecure | Specifies to use TLS when connecting to the server. | For example, SMTP_SECURE. | OPENSEARCH_SMTP_SECURE | . | --smtpusername | The SMTP username. | For example, SMTP_USERNAME. | OPENSEARCH_SMTP_USERNAME | . | --smtppassword | The SMTP password. | For example, SMTP_PASSWORD. | OPENSEARCH_SMTP_PASSWORD | . | --subject | The email subject text encased in quotes. | Can be any string. The default is “This is an email containing your dashboard report”. | OPENSEARCH_EMAIL_SUBJECT | . | --note | The email body, either a string or a path to a text file. | The default note is “Hi,\\nHere is the latest report!” | OPENSEARCH_EMAIL_NOTE | . | -h, --help | Specifies to display the list of optional arguments from the command line. | N/A |   | . ",
    "url": "https://vagimeli.github.io/dashboards/reporting-cli/rep-cli-options/",
    "relUrl": "/dashboards/reporting-cli/rep-cli-options/"
  },"367": {
    "doc": "Reporting CLI options",
    "title": "Getting help",
    "content": "To get a list of all available CLI arguments, run the following command: . $ opensearch-reporting-cli -h . ",
    "url": "https://vagimeli.github.io/dashboards/reporting-cli/rep-cli-options/#getting-help",
    "relUrl": "/dashboards/reporting-cli/rep-cli-options/#getting-help"
  },"368": {
    "doc": "Creating reports with the Dashboards interface",
    "title": "Creating reports with the Dashboards interface",
    "content": "You can use OpenSearch Dashboards to create PNG, PDF, and CSV reports. To create reports, you must have the correct permissions. For a summary of the predefined roles and the permissions they grant, see the Security plugin. CSV reports have a non-configurable 10,000 row limit. They have no explicit size limit (for example, MB), but extremely large documents could cause report generation to fail with an out of memory error from the V8 JavaScript engine. ",
    "url": "https://vagimeli.github.io/dashboards/reporting/",
    "relUrl": "/dashboards/reporting/"
  },"369": {
    "doc": "Creating reports with the Dashboards interface",
    "title": "Generating reports with the interface",
    "content": "To generate a report from the interface: . | From the navigation panel, choose Reporting. | For dashboards, visualizations, or notebooks, choose Download PDF or Download PNG. If you’re creating a report from the Discover page, choose Generate CSV. | . Reports generate asynchronously in the background and might take a few minutes, depending on the size of the report. A notification appears when your report is ready to download. | To create a schedule-based report, choose Create report definition. Then proceed to Create reports using a definition. This option pre-fills many of the fields for you based on the visualization, dashboard, or data you were viewing. | . ",
    "url": "https://vagimeli.github.io/dashboards/reporting/#generating-reports-with-the-interface",
    "relUrl": "/dashboards/reporting/#generating-reports-with-the-interface"
  },"370": {
    "doc": "Creating reports with the Dashboards interface",
    "title": "Creating reports using a definition",
    "content": "Definitions let you generate reports on a periodic schedule. | From the navigation panel, choose Reporting. | Choose Create. | Under Report settings, enter a name and optional description for your report. | Choose the Report source (i.e. the page from which the report is generated). You can generate reports from the Dashboard, Visualize, Discover (saved search), or Notebooks pages. | Select your dashboard, visualization, saved search, or notebook. Then choose a time range for the report. | Choose an appropriate file format for the report. | (Optional) Add a header or footer to the report. Headers and footers are only available for dashboard, visualization, and notebook reports. | Under Report trigger, choose either On demand or Schedule. For scheduled reports, select either Recurring or Cron based. You can receive reports daily or at some other time interval, and Cron expressions give you more flexibility. See Cron expression reference for more information. | Choose Create. | . ",
    "url": "https://vagimeli.github.io/dashboards/reporting/#creating-reports-using-a-definition",
    "relUrl": "/dashboards/reporting/#creating-reports-using-a-definition"
  },"371": {
    "doc": "Creating reports with the Dashboards interface",
    "title": "Troubleshooting",
    "content": "Chromium fails to launch with OpenSearch Dashboards . While creating a report for dashboards or visualizations, you might see a the following error: . This problem can occur for two reasons: . | You don’t have the correct version of headless-chrome to match the operating system on which OpenSearch Dashboards is running. Download the correct version. | You’re missing additional dependencies. Install the required dependencies for your operating system from the additional libraries section. | . Characters not loading in reports . You might encounter an issue where UTF-8 encoded characters look fine in your browser, but they don’t load in your generated reports because you’re missing the required font dependencies. Install the font dependencies, and then generate your reports again. ",
    "url": "https://vagimeli.github.io/dashboards/reporting/#troubleshooting",
    "relUrl": "/dashboards/reporting/#troubleshooting"
  },"372": {
    "doc": "Managing search telemetry settings",
    "title": "Managing search telemetry settings",
    "content": "You can use search telemetry to analyze search request performance by success or failure in OpenSearch Dashboards. OpenSearch stores telemetry data in the .kibana_1 index. Because there are thousands of concurrent search requests from OpenSearch Dashboards, the heavy traffic can cause significant load in an OpenSearch cluster. OpenSearch clusters perform better with search telemetry turned off. ",
    "url": "https://vagimeli.github.io/dashboards/search-telemetry/",
    "relUrl": "/dashboards/search-telemetry/"
  },"373": {
    "doc": "Managing search telemetry settings",
    "title": "Turning on search telemetry",
    "content": "Search usage telemetry is turned off by default. To turn it on, you need to set data.search.usageTelemetry.enabled to true in the opensearch_dashboards.yml file. You can find the OpenSearch Dashboards YAML file in the opensearch-project repository on GitHub. Turning on telemetry in the opensearch_dashboards.yml file overrides the default search telemetry setting of false in the Data plugin configuration file. Turning search telemetry on or off . The following table shows the data.search.usageTelemetry.enabled values you can set in opensearch_dashboards.yml to turn search telemetry on or off. | OpenSearch Dashboards YAML value | Search telemetry status: on or off | . | true | On | . | false | Off | . | none | Off | . Sample opensearch_dashboards.yml with telemetry enabled . This OpenSearch Dashboards YAML file excerpt shows the telemetry setting set to true to turn on search telemetry: . # Set the value of this setting to false to suppress # search usage telemetry to reduce the load of the OpenSearch cluster. data.search.usageTelemetry.enabled: true . ",
    "url": "https://vagimeli.github.io/dashboards/search-telemetry/#turning-on-search-telemetry",
    "relUrl": "/dashboards/search-telemetry/#turning-on-search-telemetry"
  },"374": {
    "doc": "Snapshot management in Dashboards",
    "title": "Snapshot management",
    "content": "You can set up Snapshot Management (SM) in OpenSearch Dashboards. Snapshots are backups of a cluster’s indexes and state. The state includes cluster settings, node information, index metadata (mappings, settings, templates), and shard allocation. Snapshots have two main uses: . | Recovering from failure . For example, if cluster health goes red, you might restore the red indexes from a snapshot. | Migrating from one cluster to another . For example, if you’re moving from a proof of concept to a production cluster, you might take a snapshot of the former and restore it on the latter. | . You can take and restore snapshots using snapshot management in OpenSearch Dashboards. If you need to automate snapshots creation, you can use a snapshot policy. ",
    "url": "https://vagimeli.github.io/dashboards/sm-dashboards/#snapshot-management",
    "relUrl": "/dashboards/sm-dashboards/#snapshot-management"
  },"375": {
    "doc": "Snapshot management in Dashboards",
    "title": "Creating a repository",
    "content": "Before you create an SM policy, you need to set up a repository for snapshots. | On the top menu bar, go to OpenSearch Plugins &gt; Snapshot Management. | In the left panel, under Snapshot Management, select Repositories. | Choose the Create Repository button. | Enter the repository name, type, and location. | (Optional) Select Advanced Settings and enter additional settings for this repository as a JSON object. Example: { \"chunk_size\": null, \"compress\": false, \"max_restore_bytes_per_sec\": \"40m\", \"max_snapshot_bytes_per_sec\": \"40m\", \"readonly\": false } . | Choose the Add button. | . ",
    "url": "https://vagimeli.github.io/dashboards/sm-dashboards/#creating-a-repository",
    "relUrl": "/dashboards/sm-dashboards/#creating-a-repository"
  },"376": {
    "doc": "Snapshot management in Dashboards",
    "title": "Deleting a repository",
    "content": "To delete a snapshot repository configuration, select the repository from the Repositories list and then choose the Delete button. ",
    "url": "https://vagimeli.github.io/dashboards/sm-dashboards/#deleting-a-repository",
    "relUrl": "/dashboards/sm-dashboards/#deleting-a-repository"
  },"377": {
    "doc": "Snapshot management in Dashboards",
    "title": "Creating an SM policy",
    "content": "Create an SM policy to set up automatic snapshots. An SM policy defines an automated snapshot creation schedule and an optional automated deletion schedule. | On the top menu bar, go to OpenSearch Plugins &gt; Snapshot Management. | In the left panel, under Snapshot Management, select Snapshot Policies. | Select the Create Policy button. | In the Policy settings section: . | Enter the policy name. | (Optional) Enter the policy description. | . | In the Source and destination section: . | Select or enter source indexes either as a list or as an index pattern. | Select a repository for snapshots. To create a new repository, select the Create button. | . | In the Snapshot schedule section: . | Select the desired snapshot frequency or enter a custom cron expression for snapshot frequency. | Select the start time and time zone. | . | In the Retention period section: . | Choose to retain all snapshots or specify retention conditions (the maximum age of retained snapshots). | (Optional) In Additional settings, select the minimum and maximum number of retained snapshots, deletion frequency, and deletion start time. | . | In the Notifications section, select the snapshot activities you want to be notified about. | (Optional) In the Advanced settings section, select the desired options: . | Include cluster state in snapshots | Ignore unavailable indices | Allow partial snapshots | . | Select the Create button. | . ",
    "url": "https://vagimeli.github.io/dashboards/sm-dashboards/#creating-an-sm-policy",
    "relUrl": "/dashboards/sm-dashboards/#creating-an-sm-policy"
  },"378": {
    "doc": "Snapshot management in Dashboards",
    "title": "View, edit, or delete an SM policy",
    "content": "You can view, edit, or delete an SM policy on the policy details page. | On the top menu bar, go to OpenSearch Plugins &gt; Snapshot Management. | In the left panel, under Snapshot Management, select Snapshot Policies. | Click on the Policy name of the policy you want to view, edit, or delete. The policy settings, snapshot schedule, snapshot retention period, notifications, and last creation and deletion are displayed in the policy details page. If a snapshot creation or deletion fails, you can view information about the failure in the Last Creation/Deletion section. To view the failure message, click on the cause in the Info column. | To edit or delete the SM policy, select the Edit or Delete button. | . ",
    "url": "https://vagimeli.github.io/dashboards/sm-dashboards/#view-edit-or-delete-an-sm-policy",
    "relUrl": "/dashboards/sm-dashboards/#view-edit-or-delete-an-sm-policy"
  },"379": {
    "doc": "Snapshot management in Dashboards",
    "title": "Enable, disable, or delete SM policies",
    "content": ". | On the top menu bar, go to OpenSearch Plugins &gt; Snapshot Management. | In the left panel, under Snapshot Management, select Snapshot Policies. | Select one or more policies in the list. | To enable or disable selected SM policies, select the Enable or Disable button. To delete selected SM policies, in the Actions list, select the Delete option. | . ",
    "url": "https://vagimeli.github.io/dashboards/sm-dashboards/#enable-disable-or-delete-sm-policies",
    "relUrl": "/dashboards/sm-dashboards/#enable-disable-or-delete-sm-policies"
  },"380": {
    "doc": "Snapshot management in Dashboards",
    "title": "View snapshots",
    "content": ". | On the top menu bar, go to OpenSearch Plugins &gt; Snapshot Management. | In the left panel, under Snapshot Management, select Snapshots. All automatically or manually taken snapshots appear in the list. | To view a snapshot, click on its Name. | . ",
    "url": "https://vagimeli.github.io/dashboards/sm-dashboards/#view-snapshots",
    "relUrl": "/dashboards/sm-dashboards/#view-snapshots"
  },"381": {
    "doc": "Snapshot management in Dashboards",
    "title": "Take a snapshot",
    "content": "Use the steps below to take a snapshot manually: . | On the top menu bar, go to OpenSearch Plugins &gt; Snapshot Management. | In the left panel, under Snapshot Management, select Snapshots. | Select the Take snapshot button. | Enter the snapshot name. | Select or enter source indexes either as a list or as an index pattern. | Select a repository for the snapshot. | (Optional) In the Advanced options section, select the desired options: . | Include cluster state in snapshots | Ignore unavailable indices | Allow partial snapshots | . | Choose the Add button. | . ",
    "url": "https://vagimeli.github.io/dashboards/sm-dashboards/#take-a-snapshot",
    "relUrl": "/dashboards/sm-dashboards/#take-a-snapshot"
  },"382": {
    "doc": "Snapshot management in Dashboards",
    "title": "Deleting a snapshot",
    "content": "The Delete button deletes a snapshot from a repository. | To view a list of your repositories, choose Repositories under the Snapshot Management section. | To view a list of your snapshots, choose Snapshots under the Snapshot Management section. | . ",
    "url": "https://vagimeli.github.io/dashboards/sm-dashboards/#deleting-a-snapshot",
    "relUrl": "/dashboards/sm-dashboards/#deleting-a-snapshot"
  },"383": {
    "doc": "Snapshot management in Dashboards",
    "title": "Restoring a snapshot",
    "content": ". | On the top menu bar, go to OpenSearch Plugins &gt; Snapshot Management. | In the left panel, under Snapshot Management, select Snapshots. The Snapshots tab is selected by default. | Select the checkbox next to the snapshot you want to restore, as shown in the following image: . You can only restore snapshots with the status of Success or Partial. The status of the snapshot is displayed in the Snapshot status column. | In the Restore snapshot flyout, select the options for restoring the snapshot. The Restore snapshot flyout lists the snapshot name and status. To view the list of indexes in the snapshot, select the number under Indices (for example, 27 in the following image). This number represents the number of indexes in the snapshot. For more information about the options in the Restore snapshot flyout, see Restore snapshots. Ignoring missing indexes . If you specify which indexes you want to restore from the snapshot and select the Ignore unavailable indices option, the restore operation ignores the indexes that are missing from the snapshot. For example, if you want to restore the log1 and log2 indexes, but log2 is not in the snapshot, log1 is restored and log2 is ignored. If you don’t select Ignore unavailable indices, the entire restore operation fails if an index to be restored is missing from a snapshot. Custom index settings . You can choose to customize some settings for the indexes restored from a snapshot:  • Select the Customize index settings checkbox to provide new values for the specified index settings. All newly restored indexes will use these values instead of the ones in the snapshot.  • Select the Ignore index settings checkbox to specify the settings in the snapshot to ignore. All newly restored indexes will use the cluster defaults for these settings. The examples in the following image set index.number_of_replicas to 0, index.auto_expand_replicas to true, and index.refresh_interval and index.max_script_fields to the cluster default values for all newly restored indexes. For more information about index settings, see Index settings. For a list of settings that you cannot change or ignore, see Restore snapshots. After choosing the options, select the Restore snapshot button. | (Optional) To monitor the restore progress, select View restore activities in the confirmation dialog. You can also monitor the restore progress at any time by selecting the Restore activities in progress tab, as shown in the following image. You can view the percentage of the job that has been completed in the Status column. Once the snapshot restore is complete, the Status changes to Completed (100%). The Restore activities in progress panel is not persistent. It displays only the progress of the current restore operation. If multiple restore operations are running, the panel displays the most recent one. To view the status of each index being restored, select the link in the Indices being restored column (in the preceding image, the 27 Indices link). The Indices being restored flyout (shown in the following image) displays each index and its restore status. | . After the restore operation is complete, the restored indexes are listed in the Indices panel. To view the indexes, in the left panel, under Index Management, choose Indices. ",
    "url": "https://vagimeli.github.io/dashboards/sm-dashboards/#restoring-a-snapshot",
    "relUrl": "/dashboards/sm-dashboards/#restoring-a-snapshot"
  },"384": {
    "doc": "Snapshot management in Dashboards",
    "title": "Snapshot management in Dashboards",
    "content": " ",
    "url": "https://vagimeli.github.io/dashboards/sm-dashboards/",
    "relUrl": "/dashboards/sm-dashboards/"
  },"385": {
    "doc": "Using area charts",
    "title": "Using area charts",
    "content": "An area chart is a line chart with the area between the line and the axis shaded with a color, and is a primary visualization type used to display time series data. You can create area charts in Dashboards using the Area visualization type or using the Time Series Visual Builder (TSVB), Vega, or VisBuilder visualization tools. For this tutorial, you’ll use the Area visualization type. ",
    "url": "https://vagimeli.github.io/dashboards/visualize/area/",
    "relUrl": "/dashboards/visualize/area/"
  },"386": {
    "doc": "Using area charts",
    "title": "Try it: Create a simple aggregation-based area chart",
    "content": "In this tutorial you’ll create a simple area chart using sample data and aggregations in OpenSearch Dashboards by connecting to http://localhost:5601 from a browser. You have several aggregation options in Dashboards, and the choice influences your analysis. The use cases for aggregations vary from analyzing data in real time to using Dashboards to create a visualization dashboard. If you need an overview of aggregations in OpenSearch, see Aggregations before starting this tutorial. Make sure you have installed the latest version of Dashboards and added the sample data before continuing with this tutorial. This tutorial uses Dashboards version 2.4.1. ",
    "url": "https://vagimeli.github.io/dashboards/visualize/area/#try-it-create-a-simple-aggregation-based-area-chart",
    "relUrl": "/dashboards/visualize/area/#try-it-create-a-simple-aggregation-based-area-chart"
  },"387": {
    "doc": "Using area charts",
    "title": "Set up the area chart",
    "content": ". | Access Dashboards by connecting to http://localhost:5601 from a browser. | Select Visualize from the menu and then select Create visualization. | Select Area from the window. | Select opensearch_dashboards_sample_data_flights in the New Area/Choose a source window. | Select the calendar icon and set the time filter to Last 7 days. | Select Update. | . ",
    "url": "https://vagimeli.github.io/dashboards/visualize/area/#set-up-the-area-chart",
    "relUrl": "/dashboards/visualize/area/#set-up-the-area-chart"
  },"388": {
    "doc": "Using area charts",
    "title": "Add aggregations to the area chart",
    "content": "Continuing with the area chart created in the preceding steps, you’ll create a visualization that displays the top five logs for flights delayed for every three hours over the last seven days: . | Add a Metrics aggregation. | Under Metrics, select the Aggregation dropdown list and choose Average and then select the Field dropdown list and choose FlightDelayMin. | Under Metrics, select Add to add another Y-axis aggregation. | Select the Aggregation dropdown list and choose Max and then select the Field dropdown list and choose FlightDelayMin. | . | Add a Buckets aggregation. | Select Add to open the Add Bucket window and then select X-axis. | From the Aggregation dropdown list, select Date Histogram. | From the Field dropdown list, select timestamp. | Select Update. | . | Add a sub-aggregation. | Select Add to open the Add Sub-Buckets window and then select Split series. | From the Sub aggregation dropdown list, select Terms. | From the Field dropdown list, select FlightDelay. | Select Update to reflect these parameters in the graph. | . | . You’ve now created the following aggregation-based area chart. ",
    "url": "https://vagimeli.github.io/dashboards/visualize/area/#add-aggregations-to-the-area-chart",
    "relUrl": "/dashboards/visualize/area/#add-aggregations-to-the-area-chart"
  },"389": {
    "doc": "Using area charts",
    "title": "Related links",
    "content": ". | Visualize | Visualization types in OpenSearch Dashboards | Install and configure OpenSearch Dashboards | Aggregations | . ",
    "url": "https://vagimeli.github.io/dashboards/visualize/area/#related-links",
    "relUrl": "/dashboards/visualize/area/#related-links"
  },"390": {
    "doc": "Using Gantt charts",
    "title": "Gantt charts",
    "content": "OpenSearch Dashboards includes a Gantt chart visualization. Gantt charts show the start, end, and duration of unique events in a sequence. Gantt charts are useful in trace analytics, telemetry, and anomaly detection use cases, where you want to understand interactions and dependencies between various events in a schedule. For example, consider an index of log data. The fields in a typical set of log data, especially audit logs, contain a specific operation or event with a start time and duration. To create a Gantt chart, perform the following steps: . | In the visualizations menu, choose Create visualization and Gantt Chart. | Choose a source for the chart (e.g. some log data). | Under Metrics, choose Event. For log data, each log is an event. | Select the Start Time and Duration fields from your data set. The start time is the timestamp for the beginning of an event. The duration is the amount of time to add to the start time. | Under Results, choose the number of events to display on the chart. Gantt charts sequence events from earliest to latest based on start time. | Choose Panel settings to adjust axis labels, time format, and colors. | Choose Update. | . This Gantt chart displays the ID of each log on the y-axis. Each bar is a unique event that spans some amount of time. Hover over a bar to see the duration of that event. ",
    "url": "https://vagimeli.github.io/dashboards/visualize/gantt/#gantt-charts",
    "relUrl": "/dashboards/visualize/gantt/#gantt-charts"
  },"391": {
    "doc": "Using Gantt charts",
    "title": "Using Gantt charts",
    "content": " ",
    "url": "https://vagimeli.github.io/dashboards/visualize/gantt/",
    "relUrl": "/dashboards/visualize/gantt/"
  },"392": {
    "doc": "Using coordinate and region maps",
    "title": "Using coordinate and region maps",
    "content": "OpenSearch has a standard set of GeoJSON files that provide a vector map with each region map. OpenSearch Dashboards also provides basic map tiles with a standard vector map to create region maps. You can configure the base map tiles using Web Map Service (WMS). For more information, see Configuring WMS in OpenSearch Dashboards. For air gapped environments, OpenSearch Dashboards provides a self-host maps server. For more information, see Using the self-host maps server . While you can’t configure a server to support user-defined vector map layers, you can configure your own GeoJSON file and upload it for this purpose. ",
    "url": "https://vagimeli.github.io/dashboards/visualize/geojson-regionmaps/",
    "relUrl": "/dashboards/visualize/geojson-regionmaps/"
  },"393": {
    "doc": "Using coordinate and region maps",
    "title": "Customizing vector maps with GeoJSON",
    "content": "If you have a specific locale that is not provided by OpenSearch Dashboards vector maps, such as a US county or US ZIP Code, you can create your own custom vector map with a GeoJSON file. To create a custom region map you would define a geographic shape such as a polygon with multiple coordinates. To learn more about the various geographic shapes that support a custom region map location, see Geoshape field type. GeoJSON format allows you to encode geographic data structures. To learn more about the GeoJSON specification, go to geojson.org. You can use geojson.io to extract GeoJSON files. PREREQUISITE To use a custom vector map with GeoJSON, install these two required plugins: . | OpenSearch Dashboards Maps dashboards-maps front-end plugin | OpenSearch geospatial backend plugin | . Step 1: Creating a region map visualization . To create your own custom vector map, upload a JSON file that contains GEO data for your customized regional maps. The JSON file contains vector layers for visualization. | Prepare a JSON file to upload. Make sure the file has either a .geojson or .json extension. | On the top menu bar, go to OpenSearch Dashboards &gt; Visualize. | Select the Create Visualization button. | Select Region Map. | Choose a source. For example, [Flights] Flight Log. | In the right panel, select Import Vector Map. | In Upload map, select or drag and drop your JSON file and then enter Map name prefix (for example, usa-counties). Your map will have the prefix that you defined followed by the -map suffix (for example, usa-counties-map), as shown in the following image: . | Select the Import file button and then select the Refresh button in the pop-up window confirming successful upload, as shown in the following image. | . Step 2: Viewing the custom region map in OpenSearch Dashboards . After you upload a custom GeoJSON file, you need to set the vector map layer to custom, and select your vector map: . | From Layer Options &gt; Layer settings, select Custom vector map. | Under Vector map, select the name of the vector map that you just uploaded. | Optional: Under Style settings, increase Border thickness to see the borders more clearly. | Select the Update button. | View your region map in the Dashboards. For example, the following image shows the Los Angeles and San Diego county regions: | . Example GeoJSON file . The following example GeoJSON file provides coordinates for two US counties. { \"type\": \"FeatureCollection\", \"name\": \"usa counties\", \"features\": [ { \"type\": \"Feature\", \"properties\": { \"iso2\": \"US\", \"iso3\": \"LA-CA\", \"name\": \"Los Angeles County\", \"country\": \"US\", \"county\": \"LA\" }, \"geometry\": { \"type\": \"Polygon\", \"coordinates\":[[[-118.71826171875,34.07086232376631],[-118.69628906249999,34.03445260967645],[-118.56994628906249,34.02990029603907],[-118.487548828125,33.957030069982316],[-118.37219238281249,33.86129311351553],[-118.45458984375,33.75631505992707],[-118.33923339843749,33.715201644740844],[-118.22937011718749,33.75631505992707],[-118.1414794921875,33.678639851675555],[-117.9107666015625,33.578014746143985],[-117.75146484375,33.4955977448657],[-117.55920410156249,33.55512901742288],[-117.3065185546875,33.5963189611327],[-117.0703125,33.67406853374198],[-116.69677734375,34.06176136129718],[-116.9439697265625,34.28445325435288],[-117.18017578125,34.42956713470528],[-117.3779296875,34.542762387234845],[-117.62512207031251,34.56990638085636],[-118.048095703125,34.615126683462194],[-118.44909667968749,34.542762387234845],[-118.61938476562499,34.38877925439021],[-118.740234375,34.21180215769026],[-118.71826171875,34.07086232376631]]] } }, { \"type\": \"Feature\", \"properties\": { \"iso2\": \"US\", \"iso3\": \"SD-CA\", \"name\": \"San Diego County\", \"country\": \"US\", \"county\": \"SD\" }, \"geometry\": { \"type\": \"Polygon\", \"coordinates\":[[[-117.23510742187501,32.861132322810946],[-117.2406005859375,32.75494243654723],[-117.1636962890625,32.68099643258195],[-117.14172363281251,32.58384932565662],[-117.09228515624999,32.46342595776104],[-117.0538330078125,32.29177633471201],[-116.96044921875,32.194208672875384],[-116.85607910156249,32.16631295696736],[-116.6748046875,32.20350534542368],[-116.3671875,32.319633552035214],[-116.1474609375,32.55144352864431],[-116.1639404296875,32.80574473290688],[-116.4111328125,33.073130945006625],[-116.72973632812499,33.08233672856376],[-117.09228515624999,32.99484290420988],[-117.2515869140625,32.96258644191747], [-117.23510742187501,32.861132322810946]]] } } ] } . ",
    "url": "https://vagimeli.github.io/dashboards/visualize/geojson-regionmaps/#customizing-vector-maps-with-geojson",
    "relUrl": "/dashboards/visualize/geojson-regionmaps/#customizing-vector-maps-with-geojson"
  },"394": {
    "doc": "Maps Stats API",
    "title": "Maps Stats API",
    "content": "Introduced 2.7 . When you create and save a map in OpenSearch Dashboards, the map becomes a saved object of type map. The Maps Stats API provides information about such saved objects in OpenSearch Dashboards. Example request . You can access the Maps Stats API by providing its URL address in the following format: . &lt;opensearch-dashboards-endpoint-address&gt;/api/maps-dashboards/stats . The OpenSearch Dashboards endpoint address may contain a port number if it is specified in the OpenSearch configuration file. The specific URL format depends on the type of OpenSearch deployment and the network environment in which it is hosted. You can query the endpoint in two ways: . | By accessing the endpoint address (for example, http://localhost:5601/api/maps-dashboards/stats) in a browser . | By using the curl command in the terminal: . curl -X GET http://localhost:5601/api/maps-dashboards/stats . copy . | . Example response . The following is the response for the preceding request: . { \"maps_total\":4, \"layers_filters_total\":4, \"layers_total\":{ \"opensearch_vector_tile_map\":2, \"documents\":7, \"wms\":1, \"tms\":2 }, \"maps_list\":[ { \"id\":\"88a24e6c-0216-4f76-8bc7-c8db6c8705da\", \"layers_filters_total\":4, \"layers_total\":{ \"opensearch_vector_tile_map\":1, \"documents\":3, \"wms\":0, \"tms\":0 } }, { \"id\":\"4ce3fe50-d309-11ed-a958-770756e00bcd\", \"layers_filters_total\":0, \"layers_total\":{ \"opensearch_vector_tile_map\":0, \"documents\":2, \"wms\":0, \"tms\":1 } }, { \"id\":\"af5d3b90-d30a-11ed-a605-f7ad7bc98642\", \"layers_filters_total\":0, \"layers_total\":{ \"opensearch_vector_tile_map\":1, \"documents\":1, \"wms\":0, \"tms\":1 } }, { \"id\":\"5ca1ec10-d30b-11ed-a042-93d8ff0f09ee\", \"layers_filters_total\":0, \"layers_total\":{ \"opensearch_vector_tile_map\":0, \"documents\":1, \"wms\":1, \"tms\":0 } } ] } . ",
    "url": "https://vagimeli.github.io/dashboards/visualize/maps-stats-api/",
    "relUrl": "/dashboards/visualize/maps-stats-api/"
  },"395": {
    "doc": "Maps Stats API",
    "title": "Response fields",
    "content": "The response contains statistics for the following layer types: . | Basemaps: Either a default OpenSearch map or custom base layer maps. | WMS layers: Custom WMS base layer maps. | TMS layers: Custom TMS base layer maps. | Document layers: The map’s data layers. | . For more information about the layer types, see Adding layers. The following table lists all response fields. | Field | Data type | Description | . | maps_total | Integer | The total number of maps registered as saved objects with the Maps plugin. | . | layers_filters_total | Integer | The total number of filters for all layers in all maps. This includes layer-level filters but excludes global filters like shape filters. | . | layers_total | Object | Totals statistics for all layers in all maps. | . | layers_total.opensearch_vector_tile_map | Integer | The total number of OpenSearch basemaps in all maps. | . | layers_total.documents | Integer | The total number of document layers in all maps. | . | layers_total.wms | Integer | The total number of WMS layers in all maps. | . | layers_total.tms | Integer | The total number of TMS layers in all maps. | . | maps_list | Array | A list of all maps saved in OpenSearch Dashboards. | . Each map in the map_list contains the following fields. | Field | Data type | Description | . | id | String | The map’s saved object ID. | . | layers_filters_total | Integer | The total number of filters for all layers in the map. This includes layer-level filters but excludes global filters like shape filters . | . | layers_total | Object | Totals statistics for all layers in the map. | . | layers_total.opensearch_vector_tile_map | Integer | The total number of OpenSearch basemaps in the map. | . | layers_total.documents | Integer | The total number of document layers in the map. | . | layers_total.wms | Integer | The total number of WMS layers in the map. | . | layers_total.tms | Integer | The total number of TMS layers in the map. | . The saved object ID helps you navigate to a particular map because the ID is the last part of the map’s URL. For example, in OpenSearch Playground, the address of the [Flights] Flights Status on Maps Destination Location map is https://playground.opensearch.org/app/maps-dashboards/88a24e6c-0216-4f76-8bc7-c8db6c8705da, where 88a24e6c-0216-4f76-8bc7-c8db6c8705da is the saved object ID for this map. ",
    "url": "https://vagimeli.github.io/dashboards/visualize/maps-stats-api/#response-fields",
    "relUrl": "/dashboards/visualize/maps-stats-api/#response-fields"
  },"396": {
    "doc": "Using maps",
    "title": "Using maps",
    "content": "With OpenSearch Dashboards, you can create maps to visualize your geographical data. OpenSearch lets you construct map visualizations with multiple layers, combining data across different indexes. You can build each layer from a different index pattern. Additionally, you can configure maps to show specific data at different zoom levels. OpenSearch maps are powered by the OpenSearch maps service, which uses vector tiles to render maps. ",
    "url": "https://vagimeli.github.io/dashboards/visualize/maps/",
    "relUrl": "/dashboards/visualize/maps/"
  },"397": {
    "doc": "Using maps",
    "title": "Creating a new map",
    "content": "You can create a new map from the Maps or Visualize workflows by performing the following steps: . | To create a new map from the Maps workflow, perform the following steps: . | On the top menu bar, go to OpenSearch Plugins &gt; Maps. | Choose the Create map button. | . | To create a new map from the Visualize workflow, perform the following steps: . | On the top menu bar, go to OpenSearch Dashboards &gt; Visualize. | Choose the Create visualization button. | In the New Visualization dialog, choose Maps. | . | . You can now see the default OpenSearch basemap. To examine the Default map layer configuration, in the Layers panel on the upper left of the map, select Default map, as shown in the following image. To hide the Layers panel, select the collapse (arrow) icon in the panel’s upper-right corner. ",
    "url": "https://vagimeli.github.io/dashboards/visualize/maps/#creating-a-new-map",
    "relUrl": "/dashboards/visualize/maps/#creating-a-new-map"
  },"398": {
    "doc": "Using maps",
    "title": "Layer settings",
    "content": "To change the default map settings, select Default map in the Layers panel. Under Layer settings, you can change the layer name and description and configure zoom levels and opacity for your layer: . | Zoom levels: By default, a layer is visible at all zoom levels. If you want to make a layer visible only for a certain range of zoom levels, you can specify the zoom levels either by entering them in the text boxes or by sliding the range slider to the desired values. | Opacity: If your map contains multiple layers, one layer can obscure another one. In this case, you may want to reduce the opacity of the top layer so you can see both layers at the same time. | . ",
    "url": "https://vagimeli.github.io/dashboards/visualize/maps/#layer-settings",
    "relUrl": "/dashboards/visualize/maps/#layer-settings"
  },"399": {
    "doc": "Using maps",
    "title": "Adding layers",
    "content": "To add a layer to the map, in the Layers panel, select the Add layer button. The Add layer dialog is shown in the following image. You can add base layers or data layers to the map: . | A base layer serves as a basemap. To use your own or a third-party map as a base layer, add it as a Custom map. | Data layers let you visualize data from various data sources. | . ",
    "url": "https://vagimeli.github.io/dashboards/visualize/maps/#adding-layers",
    "relUrl": "/dashboards/visualize/maps/#adding-layers"
  },"400": {
    "doc": "Using maps",
    "title": "Adding a custom map",
    "content": "OpenSearch supports Web Map Service (WMS) or Tile Map Service (TMS) custom maps. To add a TMS custom map, perform the following steps: . | In the Layers panel, select the Add layer button. | From the Add layer dialog, select Base layer &gt; Custom map. Follow the next steps in the New layer dialog, which is shown in the following image. | In the Custom type dropdown list, select Tile Map Service (TMS). | Enter the TMS URL. | (Optional) In TMS attribution, enter a TMS attribution for the basemap. For example, if you’re using a custom basemap, enter the custom map name. This name will be displayed in the lower-right corner of the map. | Select the Settings tab to edit the layer settings. | Enter the layer name in Name. | (Optional) Enter the layer description in Description. | (Optional) Select the zoom levels and opacity for this layer. | Select the Update button. | . ",
    "url": "https://vagimeli.github.io/dashboards/visualize/maps/#adding-a-custom-map",
    "relUrl": "/dashboards/visualize/maps/#adding-a-custom-map"
  },"401": {
    "doc": "Using maps",
    "title": "Adding a document layer",
    "content": "Adding document layers lets you visualize your data. You can add one index pattern per document layer. To view multiple index patterns, create multiple layers. Document layers can display geopoint and geoshape document fields. The following example assumes that you have the opensearch_dashboards_sample_data_flights dataset installed. If you don’t have this dataset installed, perform the following steps: . | On the top left, select the home icon. | Select Add sample data. | In the Sample flight data panel, select the Add data button. | . Add a document layer as follows: . | In the Layers panel, select the Add layer button. | From the Add layer dialog, select Data layer &gt; Documents. | In Data source, select opensearch_dashboards_sample_data_flights. Alternatively, you can enter another index pattern to visualize. | In Geospatial field, select a geospatial field (geopoint or geoshape) to be displayed in the visualization. In this example, select DestLocation. | (Optional) Select the Style tab to change the fill color, border color, border thickness, or marker size. | Select the Settings tab to edit layer settings. | Enter Flight destination in Name. | Select the Update button. | To see more data, in the upper-right corner select the calendar icon dropdown list, then under Quick select, choose Last 15 days and select the Apply button. | . You should see the flight destination data, as in the following image. ",
    "url": "https://vagimeli.github.io/dashboards/visualize/maps/#adding-a-document-layer",
    "relUrl": "/dashboards/visualize/maps/#adding-a-document-layer"
  },"402": {
    "doc": "Using maps",
    "title": "Filtering data",
    "content": "To show a subset of the data in the index, filter the data. You can either filter data at the layer level or draw shapes on the map to filter all layer data globally. Filtering data at the layer level . To filter data at the layer level, select the layer and add a filter to it. The following example shows how to filter the flight destination data to display only United States destinations: . | In the Layers panel, select Flight destination. | Select Filters. | Select Add filter. | In Edit filter, select DestCountry in Field. | In Operator, select is. | In Value, select US. | Select the Save button. | Select the Update button. | . For large datasets, you may want to avoid loading data for the full map. To load data only for a specific geographic area, select Only request data around map extent. Drawing shapes to filter data . You can filter your data globally by drawing shapes on the map. To draw a rectangle or polygon on the map, perform the following steps: . | Select the Rectangle or Polygon icon on the right side of the map. | In the Filter label field, enter a name for the filter. | Choose a spatial relation type. By default, Intersects is selected. See Spatial relations for more information about spatial relationship types. | Select the Draw Rectangle or Draw Polygon button. | Draw the shape over the map area that you want to select: . | For a rectangle, select any starting point on the map (this point becomes a rectangle vertex). Then hover (do not drag) to another point on the map and select it (this point becomes the opposite vertex). | For a polygon, select any starting point on the map (this point becomes a polygon vertex) and hover (do not drag) to each subsequent vertex and select that point. Finally, make sure to select the starting point again to close the polygon, as shown in the following image. | . | . Disabling the shape filter for a map layer . By default, the shape filter is applied globally to all layers on the map. If you want to disable your shape filter for a map layer, perform the following steps: . | Select the layer from the Layers panel. | In the Filters section, deselect Apply global filters. | Select the Update button. | . Modifying an existing shape filter . To modify an existing shape filter, select your filter on the top left above the map. You can perform the following operations on an existing filter: . | Edit filter: Change the filter name or modify the shape’s coordinates. | Exclude results: Negate the filter, that is, show all data points except those to which the filter applies. | Temporarily disable: Disable the filter until you select Re-enable. | Delete: Remove your filter completely. | . ",
    "url": "https://vagimeli.github.io/dashboards/visualize/maps/#filtering-data",
    "relUrl": "/dashboards/visualize/maps/#filtering-data"
  },"403": {
    "doc": "Using maps",
    "title": "Using tooltips to visualize additional data",
    "content": "Document layers show geopoint and geoshape document fields as locations on the map. To add more information to the locations, you can use tooltips. For example, you may want to show flight delay, destination weather, and destination country information in the Flight destination layer. Perform the following steps to configure tooltips to show additional data: . | In the Layers panel, select Flight destination. | Select Tooltips. | Select the Show tooltips checkbox. | In the Tooltip fields dropdown list, select the fields that you’d like to display. In this example, select FlightDelay, DestWeather, and DestCountry. | Select the Update button. | . To view tooltips, hover over the geographical point you’re interested in. One tooltip can display many data points. For example, in the Flight destination layer there are multiple flights for a single destination city. To paginate over the flights, select the city you’re interested in and use the arrows in the tooltip, as shown in the following image. If a point on the map contains data from multiple layers, one tooltip can display data from multiple layers. To see all layers, select All layers. To choose a particular layer, select the layer name in the tooltip layer selection panel, as shown in the following image. ",
    "url": "https://vagimeli.github.io/dashboards/visualize/maps/#using-tooltips-to-visualize-additional-data",
    "relUrl": "/dashboards/visualize/maps/#using-tooltips-to-visualize-additional-data"
  },"404": {
    "doc": "Using maps",
    "title": "Adding labels to layers",
    "content": "Adding a label to a layer lets you visualize additional data on the map. For example, you may want to see the origin weather in the Flight destination layer. Perform the following steps to add a label to the Flight destination layer: . | In the Layers panel, select Flight destination. | In the Style tab, select the Add label checkbox. | You can choose to add a label based on fixed text to all data points in the layer or to use a field value as the label text. | To add a fixed-text label, under Label text, select Fixed and enter your desired label text. | To add a label based on a field value, under Label text, select Field value and select the field name. In this example, select OriginWeather. | . | (Optional) Change the label size, color, border color, or border width. | Select the Update button. | . The label with the origin weather is visible on the map and also added to the tooltips, as shown in the following image. ",
    "url": "https://vagimeli.github.io/dashboards/visualize/maps/#adding-labels-to-layers",
    "relUrl": "/dashboards/visualize/maps/#adding-labels-to-layers"
  },"405": {
    "doc": "Using maps",
    "title": "Reordering, hiding, and deleting layers",
    "content": "The Layers panel lets you reorder, hide, and delete layers: . | Layers on a map are stacked on top of each other. To reorder layers, use the handlebar (two horizontal lines) icon next to the layer name to drag the layer to the desired position. | If you’d like to hide a layer, select the show/hide (eye) icon next to the layer name. Toggle the show/hide icon to show the layer again. | To delete a layer, select the delete (trash can) icon next to the layer name. | . ",
    "url": "https://vagimeli.github.io/dashboards/visualize/maps/#reordering-hiding-and-deleting-layers",
    "relUrl": "/dashboards/visualize/maps/#reordering-hiding-and-deleting-layers"
  },"406": {
    "doc": "Using maps",
    "title": "Refreshing data for a real-time dataset",
    "content": "If you want to visualize a real-time dataset, after adding layers to the map, perform the following steps to set the refresh interval: . | Select the calendar icon in the upper-right corner. | Under Refresh every, select or enter the refresh interval (for example, 1 second). | Select the Start button. | . ",
    "url": "https://vagimeli.github.io/dashboards/visualize/maps/#refreshing-data-for-a-real-time-dataset",
    "relUrl": "/dashboards/visualize/maps/#refreshing-data-for-a-real-time-dataset"
  },"407": {
    "doc": "Using maps",
    "title": "Saving a map",
    "content": "To save a map with all the layers that you set up, perform the following steps: . | Select the Save button in the upper-right corner. | In the Save map dialog, enter the map name in the Title text box. | (Optional) In the Description text box, enter the map description. | Select the Save button. | . To open your saved map, choose Maps in the upper-left corner. The list of saved maps is displayed. ",
    "url": "https://vagimeli.github.io/dashboards/visualize/maps/#saving-a-map",
    "relUrl": "/dashboards/visualize/maps/#saving-a-map"
  },"408": {
    "doc": "Using maps",
    "title": "Adding a map to a dashboard",
    "content": "You can add a new or existing map to a new or existing dashboard by performing the following steps: . | To add a map to a new dashboard, first create the dashboard as follows: . | On the top menu bar, go to OpenSearch Dashboards &gt; Dashboard. | Choose the Create dashboard button. | Choose the Create new button. | . | To add a map to an existing dashboard, first open the dashboard as follows: . | On the top menu bar, go to OpenSearch Dashboards &gt; Dashboard. | Select the dashboard you want to open from the list. | In the upper-right corner, choose Edit. | . | . Once you’ve opened a dashboard, you can add a new or existing map to it. Adding an existing map . | From the top menu, choose Add. | In the Types dropdown list, select Maps. | Select the map you want to add from the list. | . Adding a new map . | From the top menu, choose the Create new button. | In the New Visualization dialog, choose Maps. | Edit the default map by adding a basemap, layers, or tooltips. | In the upper-right corner, choose the Save button. | In the Save map dialog, enter the Title and optional Description of the map. | Select Add to Dashboard after saving (this option is selected by default). | Choose the Save and return button. | . ",
    "url": "https://vagimeli.github.io/dashboards/visualize/maps/#adding-a-map-to-a-dashboard",
    "relUrl": "/dashboards/visualize/maps/#adding-a-map-to-a-dashboard"
  },"409": {
    "doc": "Using maps",
    "title": "Editing a map from a dashboard",
    "content": ". | In the dashboard, choose the gear icon in the upper-right corner of the map you want to edit. | Choose Edit maps. | Edit the map. | In the upper-right corner, choose the Save button. | In the Save map dialog, choose the Save and return button. | . ",
    "url": "https://vagimeli.github.io/dashboards/visualize/maps/#editing-a-map-from-a-dashboard",
    "relUrl": "/dashboards/visualize/maps/#editing-a-map-from-a-dashboard"
  },"410": {
    "doc": "Configuring a Web Map Service (WMS)",
    "title": "Configuring a Web Map Service (WMS)",
    "content": "The Open Geospatial Consortium (OGC) Web Map Service (WMS) specification is an international specification for requesting dynamic maps on the web. OpenSearch Dashboards includes default map tiles. For specialized maps, you can configure a WMS on OpenSearch Dashboards following these steps: . | Log in to OpenSearch Dashboards at https://&lt;host&gt;:&lt;port&gt;. For example, you can connect to OpenSearch Dashboards by connecting to https://localhost:5601. The default username and password are admin. | Choose Management &gt; Advanced Settings. | Locate visualization:tileMap:WMSdefaults. | Change enabled to true and add the URL of a valid WMS server, as shown in the following example: . { \"enabled\": true, \"url\": \"&lt;wms-map-server-url&gt;\", \"options\": { \"format\": \"image/png\", \"transparent\": true } } . | . Web map services may have licensing fees or restrictions, and you are responsible for complying with any such fees or restrictions. ",
    "url": "https://vagimeli.github.io/dashboards/visualize/maptiles/",
    "relUrl": "/dashboards/visualize/maptiles/"
  },"411": {
    "doc": "Using the self-host maps server",
    "title": "Using the self-host maps server",
    "content": "The self-host maps server for OpenSearch Dashboards allows users to access the default maps service in air-gapped environments. OpenSearch-compatible map URLs include a map manifest with map tiles and vectors, the map tiles, and the map vectors. The following sections provide steps for setting up and using the self-host maps server with OpenSearch Dashboards. You can access the maps-server image via the official OpenSearch Docker Hub repository. ",
    "url": "https://vagimeli.github.io/dashboards/visualize/selfhost-maps-server/",
    "relUrl": "/dashboards/visualize/selfhost-maps-server/"
  },"412": {
    "doc": "Using the self-host maps server",
    "title": "Pulling the Docker image",
    "content": "Open your terminal and run the following command: . docker pull opensearch/opensearch-maps-server . ",
    "url": "https://vagimeli.github.io/dashboards/visualize/selfhost-maps-server/#pulling-the-docker-image",
    "relUrl": "/dashboards/visualize/selfhost-maps-server/#pulling-the-docker-image"
  },"413": {
    "doc": "Using the self-host maps server",
    "title": "Setting up the server",
    "content": "You must set up the map tiles before running the server. You have two setup options: Use the OpenSearch-provided maps service tiles set, or generate the raster tiles set. Option 1: Use the OpenSearch-provided maps service tiles set . Create a Docker volume to hold the tiles set: . docker volume create tiles-data . Download the tiles set from the OpenSearch maps service. Two planet tiles sets are available based on the desired zoom level: . | Zoom Level 8 (https://maps.opensearch.org/offline/planet-osm-default-z0-z8.tar.gz) | Zoom level 10 (https://maps.opensearch.org/offline/planet-osm-default-z0-z10.tar.gz) | . The planet tiles set for zoom level 10 (2 GB compressed/6.8 GB uncompressed) is approximately 10 times larger than the set for zoom level 8 (225 MB compressed/519 MB uncompressed). docker run \\ -e DOWNLOAD_TILES=https://maps.opensearch.org/offline/planet-osm-default-z0-z8.tar.gz \\ -v tiles-data:/usr/src/app/public/tiles/data/ \\ opensearch/opensearch-maps-server \\ import . Option 2: Generate the raster tiles set . To generate the raster tiles set, use the raster tile generation pipeline and then use the tiles set absolute path to create a volume to start the server. ",
    "url": "https://vagimeli.github.io/dashboards/visualize/selfhost-maps-server/#setting-up-the-server",
    "relUrl": "/dashboards/visualize/selfhost-maps-server/#setting-up-the-server"
  },"414": {
    "doc": "Using the self-host maps server",
    "title": "Starting the server",
    "content": "Use the following command to start the server using the Docker volume tiles-data. The following command is an example using host URL “localhost” and port “8080”: . docker run \\ -v tiles-data:/usr/src/app/public/tiles/data/ \\ -e HOST_URL='http://localhost' \\ -p 8080:8080 \\ opensearch/opensearch-maps-server \\ run . Or, if you generated the raster tiles set, run the server using that tiles set: . docker run \\ -v /absolute/path/to/tiles/:/usr/src/app/dist/public/tiles/data/ \\ -p 8080:8080 \\ opensearch/opensearch-maps-server \\ run . To access the tiles set, open the URLs in a browser on the host or use the curl command curl http://localhost:8080/manifest.json. Confirm the server is running by opening each of the following links in a browser on your host or with a curl command (for example, curl http://localhost:8080/manifest.json). | Map manifest URL: http://localhost:8080/manifest.json | Map tiles URL: http://localhost:8080/tiles/data/{z}/{x}/{y}.png | Map tiles demo URL: http://localhost:8080/ | . ",
    "url": "https://vagimeli.github.io/dashboards/visualize/selfhost-maps-server/#starting-the-server",
    "relUrl": "/dashboards/visualize/selfhost-maps-server/#starting-the-server"
  },"415": {
    "doc": "Using the self-host maps server",
    "title": "Using the self-host maps server with OpenSearch Dashboards",
    "content": "You can use the self-host maps server with OpenSearch Dashboards by either adding the parameter to opensearch_dashboards.yml or configuring the default WMS properties in OpenSearch Dashboards. Option 1: Configure opensearch_dashboards.yml . Configure the manifest URL in opensearch_dashboards.yml: . map.opensearchManifestServiceUrl: \"http://localhost:8080/manifest.json\" . Option 2: Configure Default WMS properties in OpenSearch Dashboards . | On the OpenSearch Dashboards console, select Stack Management &gt; Advanced Settings. | Locate visualization:tileMap:WMSdefaults under Default WMS properties. | Change \"enabled\": false to \"enabled\": true and add the URL for the valid map server. | . ",
    "url": "https://vagimeli.github.io/dashboards/visualize/selfhost-maps-server/#using-the-self-host-maps-server-with-opensearch-dashboards",
    "relUrl": "/dashboards/visualize/selfhost-maps-server/#using-the-self-host-maps-server-with-opensearch-dashboards"
  },"416": {
    "doc": "Using the self-host maps server",
    "title": "Licenses",
    "content": "Tiles are generated per Terms of Use for Natural Earth vector map data and Copyright and License for OpenStreetMap. ",
    "url": "https://vagimeli.github.io/dashboards/visualize/selfhost-maps-server/#licenses",
    "relUrl": "/dashboards/visualize/selfhost-maps-server/#licenses"
  },"417": {
    "doc": "Using the self-host maps server",
    "title": "Related articles",
    "content": ". | Configuring a Web Map Service (WMS) | Using coordinate and region maps | . ",
    "url": "https://vagimeli.github.io/dashboards/visualize/selfhost-maps-server/#related-articles",
    "relUrl": "/dashboards/visualize/selfhost-maps-server/#related-articles"
  },"418": {
    "doc": "Using VisBuilder",
    "title": "Using VisBuilder",
    "content": "VisBuilder is an experimental feature and shouldn’t be used in a production environment. For updates on its progress, or if you want to leave feedback that helps improve the feature, see the GitHub issue. You can use the VisBuilder visualization type in OpenSearch Dashboards to create data visualizations by using a drag-and-drop gesture. With VisBuilder you have: . | An immediate view of your data without the need to preselect the visualization output. | The flexibility to change visualization types and index patterns quickly. | The ability to easily navigate between multiple screens. | . ",
    "url": "https://vagimeli.github.io/dashboards/visualize/visbuilder/",
    "relUrl": "/dashboards/visualize/visbuilder/"
  },"419": {
    "doc": "Using VisBuilder",
    "title": "Try VisBuilder in the OpenSearch Dashboards playground",
    "content": "If you’d like to try out VisBuilder without installing OpenSearch locally, you can do so in the Dashboards playground. VisBuilder is enabled by default. ",
    "url": "https://vagimeli.github.io/dashboards/visualize/visbuilder/#try-visbuilder-in-the-opensearch-dashboards-playground",
    "relUrl": "/dashboards/visualize/visbuilder/#try-visbuilder-in-the-opensearch-dashboards-playground"
  },"420": {
    "doc": "Using VisBuilder",
    "title": "Try VisBuilder locally",
    "content": "VisBuilder is enabled by default. If you want to disable it, set the feature flag vis_builder.enabled: to false in the opensearch_dashboards.yml file as follows: . # Set the value of this setting to false to disable VisBuilder # functionality in Visualization. vis_builder.enabled: false . Follow these steps to create a new visualization using VisBuilder in your environment: . | Open Dashboards: . | If you’re not running the Security plugin, go to http://localhost:5601. | If you’re running the Security plugin, go to https://localhost:5601 and log in with your username and password (default is admin/admin). | . | Confirm that the Enable experimental visualizations option is turned on. | From the top menu, select Management &gt; Stack Management &gt; Advanced Settings. | Select Visualization and verify that the option is turned on. | . | From the top menu, select Visualize &gt; Create visualization &gt; VisBuilder. | Drag and drop field names from the left column into the Configuration panel to generate a visualization. | . Here’s an example visualization. Your visualization will look different depending on your data and the fields you select. ",
    "url": "https://vagimeli.github.io/dashboards/visualize/visbuilder/#try-visbuilder-locally",
    "relUrl": "/dashboards/visualize/visbuilder/#try-visbuilder-locally"
  },"421": {
    "doc": "Building data visualizations",
    "title": "Building data visualizations",
    "content": "By visualizing your data, you translate complex, high-volume, or numerical data into a visual representation that is easier to process. OpenSearch Dashboards gives you data visualization tools to improve and automate the visual communication process. By using visual elements like charts, graphs, or maps to represent data, you can advance business intelligence and support data-driven decision-making and strategic planning. ",
    "url": "https://vagimeli.github.io/dashboards/visualize/viz-index/",
    "relUrl": "/dashboards/visualize/viz-index/"
  },"422": {
    "doc": "Building data visualizations",
    "title": "Understanding the visualization types in OpenSearch Dashboards",
    "content": "Dashboards has several visualization types to support your data analysis needs. The following sections provide an overview of the visualization types in Dashboards and their common use cases. Area charts . Area charts depict changes over time, and they are commonly used to show trends. Area charts more efficiently identify patterns in log data, such as sales data for a time range and trends over that time. See Using area charts to learn more about how to create and use them in Dashboards. Bar charts . Bar charts (vertical or horizontal) compare categorical data and depict changes of a variable over a period of time. | Vertical bar chart | Horizontal bar chart | . | | | . Controls . Controls is a panel, instead of a visualization type, added to a dashboard to filter data. Controls gives users the capability to add interactive inputs to a dashboard. You can create two types of controls in Dashboards: Options list and Range slider. Options list is a dropdown options list that allows filtering of data by a terms aggregation, such as machine.os.keyword. Range slider allows filtering within specified value ranges, such as hour_of_day. Data tables . Data tables, or tables, show your raw data in tabular form. Gantt charts . Gantt charts show the start, end, and duration of unique events in a sequence. Gantt charts are useful in trace analytics, telemetry, and anomaly detection use cases where you want to understand interactions and dependencies between various events in a schedule. Gantt chart is currently a plugin, instead of built-in, visualization type in Dashboards. See Gantt charts to learn how to create and use them in Dashboards. Gauge charts . Gauge charts look similar to an analog speedometer that reads left to right from zero. They display how much there is of the thing you are measuring, and this measurement can exist alone or in relation to another measurement, such as tracking performance against benchmarks or goals. Heat maps . A heat map is a view of a histogram (a graphical representation of the distribution of numerical data) over time. Instead of using bar height as a representation of frequency, as with a histogram, heat maps display data in a tabular form using colors to differentiate where values fall in a range. Line charts . Line charts compare changes in measured values over a period of time, such as gross sales by month or gross sales and net sales by month. Maps . You can create two types of maps in Dashboards: Coordinate maps and Region maps. Coordinate maps show the difference between data values for each location by size. Region maps show the difference between data values for each location by varying shades of color. See Using maps to learn more about maps capabilities in Dashboards. Coordinate maps . Coordinate maps show location-based data on a map. Use coordinate maps to visualize GPS data (latitude and longitude coordinates) on a map. For information about OpenSearch-supported coordinate field types, see Geographic field types and Cartesian field types. Region maps . Region maps show patterns and trends across geographic locations. A region map is one of the basemaps in Dashboards. For information about creating custom vector maps in Dashboards, see Using coordinate and region maps to learn how to create and use maps in Dashboards. Markdown . Markdown is a the markup language used in Dashboards to provide context to your data visualizations. Using Markdown, you can display information and instructions along with the visualization. Metric values . Metric values, or number charts, compare values in different measures. For example, you can create a metrics visualization to compare two values, such as actual sales compared to sales goals. Pie charts . Pie charts compare values for items in a dimension, such as a percentage of a total amount. TSVB . The time-series visual builder (TSVB) is a data visualization tool in Dashboards used to create detailed time-series visualizations. For example, you can use TSVB to build visualizations that show data over time, such as flights by status over time or flight delays by delay type over time. Currently, TSVB can be used to create the following Dashboards visualization types: Area, Line, Metric, Gauge, Markdown, and Data Table. Tag cloud . Tag (or word) clouds are a way to display how often a word is used in relation to other words in a dataset. The best use for this type of visual is to show word or phrase frequency. Timeline . Timeline is a data visualization tool in Dashboards that you can use to create time-series visualizations. Currently, Timeline can be used to create the following Dashboards visualization types: Area and Line. VisBuilder . VisBuilder is a drag-and-drop data visualization tool in Dashboards. It gives you an immediate view of your data without the need to preselect the data source or visualization type output. Currently, VisBuilder can be used to create the following Dashboards visualization types: Area, Bar, Line, Metric, and Data Table. See VisBuilder to learn how to create and use drag-and-drop visualizations in Dashboards. Vega . Vega and Vega-Lite are open-source, declarative language visualization grammars for creating, sharing, and saving interactive data visualizations. Vega visualizations give you the flexibility to visualize multidimensional data using a layered approach in order to build and manipulate visualizations in a structured manner. Vega can be used to create customized visualizations using any Dashboards visualization type. ",
    "url": "https://vagimeli.github.io/dashboards/visualize/viz-index/#understanding-the-visualization-types-in-opensearch-dashboards",
    "relUrl": "/dashboards/visualize/viz-index/#understanding-the-visualization-types-in-opensearch-dashboards"
  },"423": {
    "doc": "Availability and Recovery",
    "title": "Availability and Recovery",
    "content": "The following OpenSearch features help ensure consistent uptime so that your cluster can complete and scale based on your use case, as well as creating snapshots. ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/index/",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/index/"
  },"424": {
    "doc": "Remote-backed storage",
    "title": "Remote-backed storage",
    "content": "Remote-backed storage is an experimental feature. Therefore, we do not recommend the use of remote-backed storage in a production environment. For updates on the progress of remote-backed storage, or if you want leave feedback that could help improve the feature, refer to the issue on GitHub. Remote-backed storage offers OpenSearch users a new way to protect against data loss by automatically creating backups of all index transactions and sending them to remote storage. In order to expose this feature, segment replication must also be enabled. See Segment replication for additional information. ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/remote/",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/remote/"
  },"425": {
    "doc": "Remote-backed storage",
    "title": "Translog",
    "content": "Any index changes, such as indexing or deleting documents, are written to disk during a Lucene commit. However, Lucene commits are expensive operations, so they cannot be performed after every change to the index. Instead, each shard records every indexing operation in a transaction log called translog. When a document is indexed, it is added to the memory buffer and recorded in the translog. Frequent refresh operations write the documents in the memory buffer to a segment and then clear the memory buffer. Periodically, a flush performs a Lucene commit, which includes writing the segments to disk using fsync, purging the old translog, and starting a new translog. Thus, a translog contains all operations that have not yet been flushed. ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/remote/#translog",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/remote/#translog"
  },"426": {
    "doc": "Remote-backed storage",
    "title": "Segment replication and remote-backed storage",
    "content": "When neither segment replication nor remote-backed storage is enabled, OpenSearch uses document replication. In document replication, when a write request lands on the primary shard, the request is indexed to Lucene and stored in the translog. After this, the request is sent to the replicas, where, in turn, it is indexed to Lucene and stored in the translog for durability. With segment replication, segments are created on the primary shard only and then copied to all replicas. The replicas do not index requests to Lucene, but they do create and maintain a translog. With remote-backed storage, when a write request lands on the primary shard, the request is indexed to Lucene on the primary shard only. The corresponding translog is then uploaded to remote store. OpenSearch does not send the write request to the replicas, but rather performs a primary term validation to confirm that the request originator shard is still the primary shard. Primary term validation ensures that the acting primary shard fails if it becomes isolated and is unaware of the cluster manager electing a new primary. ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/remote/#segment-replication-and-remote-backed-storage",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/remote/#segment-replication-and-remote-backed-storage"
  },"427": {
    "doc": "Remote-backed storage",
    "title": "The index.translog.durability translog setting",
    "content": "Without remote-backed storage, indexing operations are only persisted to disk when the translog is fsynced. Therefore, any data that has not been written to disk can potentially be lost. The index.translog.durability setting controls how frequently OpenSearch fsyncs the translog to disk: . | By default, index.translog.durability is set to request. This means that fsync happens after every request, and all acknowledged write requests persist in case of failure. | If you set index.translog.durability to async, fsync happens periodically at the specified sync_interval (5 seconds by default). The fsync operation is asynchronous, so acknowledge is sent without waiting for fsync. Consequently, all acknowledged writes since the last commit are lost in case of failure. | . With remote-backed storage, the translog is uploaded to a remote store for durability. index.translog.durability is a dynamic setting. To update it, use the following query: . PUT my_index/_settings { \"index\" : { \"translog.durability\" : \"request\" } } . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/remote/#the-indextranslogdurability-translog-setting",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/remote/#the-indextranslogdurability-translog-setting"
  },"428": {
    "doc": "Remote-backed storage",
    "title": "Refresh-level and request-level durability",
    "content": "The remote store feature supports two levels of durability: . | Refresh-level durability: Segment files are uploaded to remote store after every refresh. Set the remote_store flag to true to achieve refresh-level durability. Commit-level durability is inherent, and uploads are asynchronous. If you need to refresh an index manually, you can use the _refresh API. For example, to refresh the my_index index, use the following request: . POST my_index/_refresh . | Request-level durability: Translogs are uploaded before acknowledging the request. Set the translog flag to true to achieve request-level durability. In this scenario, we recommend to batch as many requests as possible in a bulk request. Batching requests will improve indexing throughput and latency compared to sending individual write requests. | . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/remote/#refresh-level-and-request-level-durability",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/remote/#refresh-level-and-request-level-durability"
  },"429": {
    "doc": "Remote-backed storage",
    "title": "Enable the feature flag",
    "content": "There are several methods for enabling remote store feature, depending on the install type. You will also need to enable remote_store property when creating the index. Segment replication must also be enabled to use remote-backed storage. Enable on a node using a tarball install . The flag is toggled using a new jvm parameter that is set either in OPENSEARCH_JAVA_OPTS or in config/jvm.options. Option 1: Modify jvm.options . Add the following lines to config/jvm.options before starting the OpenSearch process to enable the feature and its dependency: . -Dopensearch.experimental.feature.replication_type.enabled=true -Dopensearch.experimental.feature.remote_store.enabled=true . Run OpenSearch ./bin/opensearch . Option 2: Enable from an environment variable . As an alternative to directly modifying config/jvm.options, you can define the properties by using an environment variable. This can be done in a single command when you start OpenSearch or by defining the variable with export. To add these flags in-line when starting OpenSearch: . OPENSEARCH_JAVA_OPTS=\"-Dopensearch.experimental.feature.replication_type.enabled=true -Dopensearch.experimental.feature.remote_store.enabled=true\" ./opensearch-2.7.0/bin/opensearch . If you want to define the environment variable separately, prior to running OpenSearch: . export OPENSEARCH_JAVA_OPTS=\"-Dopensearch.experimental.feature.replication_type.enabled=true -Dopensearch.experimental.feature.remote_store.enabled=true\" ./bin/opensearch . Enable with Docker containers . If you’re running Docker, add the following line to docker-compose.yml underneath the opensearch-node and environment section: . OPENSEARCH_JAVA_OPTS=\"-Dopensearch.experimental.feature.replication_type.enabled=true -Dopensearch.experimental.feature.remote_store.enabled=true\" . Enable for OpenSearch development . To create new indexes with remote-backed storage enabled, you must first enable these features by adding the correct properties to run.gradle before building OpenSearch. See the developer guide for information about to use how Gradle to build OpenSearch. Add the following properties to run.gradle to enable the feature: . testClusters { runTask { testDistribution = 'archive' if (numZones &gt; 1) numberOfZones = numZones if (numNodes &gt; 1) numberOfNodes = numNodes systemProperty 'opensearch.experimental.feature.replication_type.enabled', 'true' systemProperty 'opensearch.experimental.feature.remote_store.enabled', 'true' } } . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/remote/#enable-the-feature-flag",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/remote/#enable-the-feature-flag"
  },"430": {
    "doc": "Remote-backed storage",
    "title": "Register a remote repository",
    "content": "Now that your deployment is running with the feature flags enabled, the next step is to register a remote repository where backups will be stored. See Register repository for more information. ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/remote/#register-a-remote-repository",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/remote/#register-a-remote-repository"
  },"431": {
    "doc": "Remote-backed storage",
    "title": "Create an index",
    "content": "Remote-backed storage is enabled for an index when it is created. This feature cannot be enabled for indexes that already exist. For refresh-level durability, include the remote_store property to enable the feature and specify a segment repository: . curl -X PUT \"https://localhost:9200/my-index?pretty\" -ku admin:admin -H 'Content-Type: application/json' -d' { \"settings\": { \"index\": { \"number_of_shards\": 1, \"number_of_replicas\": 0, \"replication\": { \"type\": \"SEGMENT\" }, \"remote_store\": { \"enabled\": true, \"repository\": \"segment-repo\" } } } } ' . For request-level durability, in addition to the remote_store and segment repository, include the translog property and specify a translog repository: . curl -X PUT \"https://localhost:9200/my-index?pretty\" -ku admin:admin -H 'Content-Type: application/json' -d' { \"settings\": { \"index\": { \"number_of_shards\": 1, \"number_of_replicas\": 1, \"replication\": { \"type\": \"SEGMENT\" }, \"remote_store\": { \"enabled\": true, \"repository\": \"segment-repo\", \"translog\": { \"enabled\": true, \"repository\": \"translog-repo\", \"buffer_interval\": \"300ms\" } } } } } ' . You can have the same repository serve as both the segment repository and translog repository. As data is added to the index, it also will be continuously uploaded to remote storage in the form of segment and translog files because of refreshes, flushes, and translog fsyncs to disk. Along with data, other metadata files will be uploaded. The buffer_interval setting specifies the time interval during which translog operations are buffered. Instead of uploading individual translog files, OpenSearch creates a single translog file with all the write operations received during the configured interval. Bundling translog files leads to higher throughput but also increases latency. The default buffer_interval value is 100 ms. Setting translog.enabled to true is currently an irreversible operation. Restoring from a backup . To restore an index from a remote backup, such as in the event of a node failure, you must first close the index: . curl -X POST \"https://localhost:9200/my-index/_close\" -ku admin:admin . Restore the index from the backup stored on the remote repository: . curl -X POST \"https://localhost:9200/_remotestore/_restore\" -ku admin:admin -H 'Content-Type: application/json' -d' { \"indices\": [\"my-index\"] } ' . If the Security plugin is enabled, a user must have the cluster:admin/remotestore/restore permission. See Access control for information about configuring user permissions. ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/remote/#create-an-index",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/remote/#create-an-index"
  },"432": {
    "doc": "Remote-backed storage",
    "title": "Potential use cases",
    "content": "You can use remote-backed storage for the following purposes: . | To restore red clusters or indexes | To recover all data up to the last acknowledged write, regardless of replica count, if index.translog.durability is set to request | . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/remote/#potential-use-cases",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/remote/#potential-use-cases"
  },"433": {
    "doc": "Remote-backed storage",
    "title": "Known limitations",
    "content": "The following are known limitations of the remote-backed storage feature: . | Writing data to a remote store can be a high-latency operation when compared to writing data on the local file system. This may impact the indexing throughput and latency. For performance benchmarking results, see issue #6376. | . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/remote/#known-limitations",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/remote/#known-limitations"
  },"434": {
    "doc": "Search backpressure",
    "title": "Search backpressure",
    "content": "Search backpressure is a mechanism used to identify resource-intensive search requests and cancel them when the node is under duress. If a search request on a node or shard has breached the resource limits and does not recover within a certain threshold, it is rejected. These thresholds are dynamic and configurable through cluster settings. ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/search-backpressure/",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/search-backpressure/"
  },"435": {
    "doc": "Search backpressure",
    "title": "Measuring resource consumption",
    "content": "To decide whether to apply search backpressure, OpenSearch periodically measures the following resource consumption statistics for each search request: . | CPU usage | Heap usage | Elapsed time | . An observer thread periodically measures the resource usage of the node. If OpenSearch determines that the node is under duress, OpenSearch examines the resource usage of each search task and search shard task and compares it against configurable thresholds. OpenSearch considers CPU usage, heap usage, and elapsed time and assigns each task a cancellation score that is then used to cancel the most resource-intensive tasks. OpenSearch limits the number of cancellations to a fraction of successful task completions. Additionally, it limits the number of cancellations per unit time. OpenSearch continues to monitor and cancel tasks until the node is no longer under duress. ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/search-backpressure/#measuring-resource-consumption",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/search-backpressure/#measuring-resource-consumption"
  },"436": {
    "doc": "Search backpressure",
    "title": "Canceled queries",
    "content": "If a query is canceled, OpenSearch may return partial results if some shards failed. If all shards failed, OpenSearch returns an error from the server similar to the following error: . { \"error\": { \"root_cause\": [ { \"type\": \"task_cancelled_exception\", \"reason\": \"cancelled task with reason: cpu usage exceeded [17.9ms &gt;= 15ms], elapsed time exceeded [1.1s &gt;= 300ms]\" }, { \"type\": \"task_cancelled_exception\", \"reason\": \"cancelled task with reason: elapsed time exceeded [1.1s &gt;= 300ms]\" } ], \"type\": \"search_phase_execution_exception\", \"reason\": \"all shards failed\", \"phase\": \"query\", \"grouped\": true, \"failed_shards\": [ { \"shard\": 0, \"index\": \"foobar\", \"node\": \"7yIqOeMfRyWW1rHs2S4byw\", \"reason\": { \"type\": \"task_cancelled_exception\", \"reason\": \"cancelled task with reason: cpu usage exceeded [17.9ms &gt;= 15ms], elapsed time exceeded [1.1s &gt;= 300ms]\" } }, { \"shard\": 1, \"index\": \"foobar\", \"node\": \"7yIqOeMfRyWW1rHs2S4byw\", \"reason\": { \"type\": \"task_cancelled_exception\", \"reason\": \"cancelled task with reason: elapsed time exceeded [1.1s &gt;= 300ms]\" } } ] }, \"status\": 500 } . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/search-backpressure/#canceled-queries",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/search-backpressure/#canceled-queries"
  },"437": {
    "doc": "Search backpressure",
    "title": "Search backpressure modes",
    "content": "Search backpressure runs in monitor_only (default), enforced, or disabled mode. In the enforced mode, the server rejects search requests. In the monitor_only mode, the server does not actually cancel search requests but tracks statistics about them. You can specify the mode in the search_backpressure.mode parameter. ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/search-backpressure/#search-backpressure-modes",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/search-backpressure/#search-backpressure-modes"
  },"438": {
    "doc": "Search backpressure",
    "title": "Search backpressure settings",
    "content": "Search backpressure adds several settings to the standard OpenSearch cluster settings. These settings are dynamic, so you can change the default behavior of this feature without restarting your cluster. | Setting | Default | Description | . | search_backpressure.mode | monitor_only | The search backpressure mode. Valid values are monitor_only, enforced, or disabled. | . | search_backpressure.cancellation_ratio Deprecated in 2.6. Replaced by search_backpressure.search_shard_task.cancellation_ratio | 10% | The maximum number of tasks to cancel, as a percentage of successful task completions. | . | search_backpressure.cancellation_rate Deprecated in 2.6. Replaced by search_backpressure.search_shard_task.cancellation_rate | 0.003 | The maximum number of tasks to cancel per millisecond of elapsed time. | . | search_backpressure.cancellation_burst Deprecated in 2.6. Replaced by search_backpressure.search_shard_task.cancellation_burst | 10 | The maximum number of search shard tasks to cancel in a single iteration of the observer thread. | . | search_backpressure.node_duress.num_successive_breaches | 3 | The number of successive limit breaches after which the node is considered to be under duress. | . | search_backpressure.node_duress.cpu_threshold | 90% | The CPU usage threshold (as a percentage) required for a node to be considered to be under duress. | . | search_backpressure.node_duress.heap_threshold | 70% | The heap usage threshold (as a percentage) required for a node to be considered to be under duress. | . | search_backpressure.search_task.elapsed_time_millis_threshold | 45,000 | The elapsed time threshold (in milliseconds) required for an individual parent task before it is considered for cancellation. | . | search_backpressure.search_task.cancellation_ratio | 0.1 | The maximum number of search tasks to cancel, as a percentage of successful search task completions. | . | search_backpressure.search_task.cancellation_rate | 0.003 | The maximum number of search tasks to cancel per millisecond of elapsed time. | . | search_backpressure.search_task.cancellation_burst | 5 | The maximum number of search tasks to cancel in a single iteration of the observer thread. | . | search_backpressure.search_task.heap_percent_threshold | 2% | The heap usage threshold (as a percentage) required for an individual parent task before it is considered for cancellation. | . | search_backpressure.search_task.total_heap_percent_threshold | 5% | The heap usage threshold (as a percentage) required for the sum of heap usages of all search tasks before cancellation is applied. | . | search_backpressure.search_task.heap_variance | 2.0 | The heap usage variance required for an individual parent task before it is considered for cancellation. A task is considered for cancellation when taskHeapUsage is greater than or equal to heapUsageMovingAverage * variance. | . | search_backpressure.search_task.heap_moving_average_window_size | 10 | The window size used to calculate the rolling average of the heap usage for the completed parent tasks. | . | search_backpressure.search_task.cpu_time_millis_threshold | 30,000 | The CPU usage threshold (in milliseconds) required for an individual parent task before it is considered for cancellation. | . | search_backpressure.search_shard_task.elapsed_time_millis_threshold | 30,000 | The elapsed time threshold (in milliseconds) required for a single search shard task before it is considered for cancellation. | . | search_backpressure.search_shard_task.cancellation_ratio | 0.1 | The maximum number of search shard tasks to cancel, as a percentage of successful search shard task completions. | . | search_backpressure.search_shard_task.cancellation_rate | 0.003 | The maximum number of search shard tasks to cancel per millisecond of elapsed time. | . | search_backpressure.search_shard_task.cancellation_burst | 10 | The maximum number of search shard tasks to cancel in a single iteration of the observer thread. | . | search_backpressure.search_shard_task.heap_percent_threshold | 0.5% | The heap usage threshold (as a percentage) required for a single search shard task before it is considered for cancellation. | . | search_backpressure.search_shard_task.total_heap_percent_threshold | 5% | The heap usage threshold (as a percentage) required for the sum of heap usages of all search shard tasks before cancellation is applied. | . | search_backpressure.search_shard_task.heap_variance | 2.0 | The minimum variance required for a single search shard task’s heap usage compared to the rolling average of previously completed tasks before it is considered for cancellation. | . | search_backpressure.search_shard_task.heap_moving_average_window_size | 100 | The number of previously completed search shard tasks to consider when calculating the rolling average of heap usage. | . | search_backpressure.search_shard_task.cpu_time_millis_threshold | 15,000 | The CPU usage threshold (in milliseconds) required for a single search shard task before it is considered for cancellation. | . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/search-backpressure/#search-backpressure-settings",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/search-backpressure/#search-backpressure-settings"
  },"439": {
    "doc": "Search backpressure",
    "title": "Search Backpressure Stats API",
    "content": "Introduced 2.4 . You can use the nodes stats API operation to monitor server-side request cancellations. Example request . To retrieve the statistics, use the following request: . GET _nodes/stats/search_backpressure . Example response . The response contains server-side request cancellation statistics: . { \"_nodes\": { \"total\": 1, \"successful\": 1, \"failed\": 0 }, \"cluster_name\": \"runTask\", \"nodes\": { \"T7aqO6zaQX-lt8XBWBYLsA\": { \"timestamp\": 1667409521070, \"name\": \"runTask-0\", \"transport_address\": \"127.0.0.1:9300\", \"host\": \"127.0.0.1\", \"ip\": \"127.0.0.1:9300\", \"roles\": [ ], \"attributes\": { \"testattr\": \"test\", \"shard_indexing_pressure_enabled\": \"true\" }, \"search_backpressure\": { \"search_task\": { \"resource_tracker_stats\": { \"heap_usage_tracker\": { \"cancellation_count\": 57, \"current_max_bytes\": 5739204, \"current_avg_bytes\": 962465, \"rolling_avg_bytes\": 4009239 }, \"elapsed_time_tracker\": { \"cancellation_count\": 97, \"current_max_millis\": 15902, \"current_avg_millis\": 9705 }, \"cpu_usage_tracker\": { \"cancellation_count\": 64, \"current_max_millis\": 8483, \"current_avg_millis\": 7843 } }, \"cancellation_stats\": { \"cancellation_count\": 102, \"cancellation_limit_reached_count\": 25 } }, \"search_shard_task\": { \"resource_tracker_stats\": { \"heap_usage_tracker\": { \"cancellation_count\": 34, \"current_max_bytes\": 1203272, \"current_avg_bytes\": 700267, \"rolling_avg_bytes\": 1156270 }, \"cpu_usage_tracker\": { \"cancellation_count\": 318, \"current_max_millis\": 731, \"current_avg_millis\": 303 }, \"elapsed_time_tracker\": { \"cancellation_count\": 310, \"current_max_millis\": 1305, \"current_avg_millis\": 649 } }, \"cancellation_stats\": { \"cancellation_count\": 318, \"cancellation_limit_reached_count\": 97 } }, \"mode\": \"enforced\" } } } } . Response fields . The response contains the following fields. | Field Name | Data type | Description | . | search_backpressure | Object | Statistics about search backpressure. | . | search_backpressure.search_task | Object | Statistics specific to the search task. | . | search_backpressure.search_task.resource_tracker_stats | Object | Statistics about the current search tasks. | . | search_backpressure.search_task.cancellation_stats | Object | Statistics about the search tasks canceled since the node last restarted. | . | search_backpressure.search_shard_task | Object | Statistics specific to the search shard task. | . | search_backpressure.search_shard_task.resource_tracker_stats | Object | Statistics about the current search shard tasks. | . | search_backpressure.search_shard_task.cancellation_stats | Object | Statistics about the search shard tasks canceled since the node last restarted. | . | search_backpressure.mode | String | The mode for search backpressure. | . resource_tracker_stats . The resource_tracker_stats object contains the statistics for each resource tracker: elapsed_time_tracker, heap_usage_tracker, and cpu_usage_tracker. elapsed_time_tracker . The elapsed_time_tracker object contains the following statistics related to the elapsed time. | Field Name | Data type | Description | . | cancellation_count | Integer | The number of tasks marked for cancellation because of excessive elapsed time since the node last restarted. | . | current_max_millis | Integer | The maximum elapsed time for all tasks currently running on the node, in milliseconds. | . | current_avg_millis | Integer | The average elapsed time for all tasks currently running on the node, in milliseconds. | . heap_usage_tracker . The heap_usage_tracker object contains the following statistics related to the heap usage. | Field Name | Data type | Description | . | cancellation_count | Integer | The number of tasks marked for cancellation because of excessive heap usage since the node last restarted. | . | current_max_bytes | Integer | The maximum heap usage for all tasks currently running on the node, in bytes. | . | current_avg_bytes | Integer | The average heap usage for all tasks currently running on the node, in bytes. | . | rolling_avg_bytes | Integer | The rolling average heap usage for n most recent tasks, in bytes. n is configurable and defined by the search_backpressure.search_shard_task.heap_moving_average_window_size setting. The default value for this setting is 100. | . cpu_usage_tracker . The cpu_usage_tracker object contains the following statistics related to the CPU usage. | Field Name | Data type | Description | . | cancellation_count | Integer | The number of tasks marked for cancellation because of excessive CPU usage since the node last restarted. | . | current_max_millis | Integer | The maximum CPU time for all tasks currently running on the node, in milliseconds. | . | current_avg_millis | Integer | The average CPU time for all tasks currently running on the node, in milliseconds. | . cancellation_stats . The cancellation_stats object contains the following statistics for the tasks that are marked for cancellation. | Field Name | Data type | Description | . | cancellation_count | Integer | The total number of tasks marked for cancellation since the node last restarted. | . | cancellation_limit_reached_count | Integer | The number of times when the number of tasks eligible for cancellation exceeded the set cancellation threshold. | . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/search-backpressure/#search-backpressure-stats-api",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/search-backpressure/#search-backpressure-stats-api"
  },"440": {
    "doc": "Segment replication back-pressure",
    "title": "Segment replication backpressure",
    "content": "Segment replication backpressure is a shard-level rejection mechanism that dynamically rejects indexing requests as replica shards in your cluster fall behind primary shards. With segment replication backpressure, indexing requests are rejected when the percentage of stale shards in the replication group exceeds MAX_ALLOWED_STALE_SHARDS (50% by default). A replica is considered stale if it is behind the primary shard by the number of checkpoints that exceeds the MAX_INDEXING_CHECKPOINTS setting and its current replication lag is greater than the defined MAX_REPLICATION_TIME_SETTING field. Replica shards are also monitored to determine whether the shards are stuck or lagging for an extended period of time. When replica shards are stuck or lagging for more than double the amount of time defined by the MAX_REPLICATION_TIME_SETTING field, the shards are removed and replaced with new replica shards. ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/segment-replication/backpressure/#segment-replication-backpressure",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/segment-replication/backpressure/#segment-replication-backpressure"
  },"441": {
    "doc": "Segment replication back-pressure",
    "title": "Request fields",
    "content": "Segment replication backpressure is disabled by default. To enable it, set SEGMENT_REPLICATION_INDEXING_PRESSURE_ENABLED to true. You can update the following dynamic cluster settings using the cluster settings API endpoint. | Field | Data type | Description | . | SEGMENT_REPLICATION_INDEXING_PRESSURE_ENABLED | Boolean | Enables the segment replication backpressure mechanism. Default is false. | . | MAX_REPLICATION_TIME_SETTING | Time unit | The maximum amount of time that a replica shard can take to copy from the primary shard. Once MAX_REPLICATION_TIME_SETTING is breached along with MAX_INDEXING_CHECKPOINTS, the segment replication backpressure mechanism is initiated. Default is 5 minutes. | . | MAX_INDEXING_CHECKPOINTS | Integer | The maximum number of indexing checkpoints that a replica shard can fall behind when copying from primary. Once MAX_INDEXING_CHECKPOINTS is breached along with MAX_REPLICATION_TIME_SETTING, the segment replication backpressure mechanism is initiated. Default is 4 checkpoints. | . | MAX_ALLOWED_STALE_SHARDS | Floating point | The maximum number of stale replica shards that can exist in a replication group. Once MAX_ALLOWED_STALE_SHARDS is breached, the segment replication backpressure mechanism is initiated. Default is .5, which is 50% of a replication group. | . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/segment-replication/backpressure/#request-fields",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/segment-replication/backpressure/#request-fields"
  },"442": {
    "doc": "Segment replication back-pressure",
    "title": "Path and HTTP methods",
    "content": "You can use the segment replication API endpoint to retrieve segment replication backpressure metrics as follows: . GET _cat/segment_replication . copy . Example response . shardId target_node target_host checkpoints_behind bytes_behind current_lag last_completed_lag rejected_requests [index-1][0] runTask-1 127.0.0.1 0 0b 0s 7ms 0 . The checkpoints_behind and current_lag metrics are taken into consideration when initiating segment replication backpressure. They are checked against MAX_INDEXING_CHECKPOINTS and MAX_REPLICATION_TIME_SETTING, respectively. ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/segment-replication/backpressure/#path-and-http-methods",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/segment-replication/backpressure/#path-and-http-methods"
  },"443": {
    "doc": "Segment replication back-pressure",
    "title": "Segment replication back-pressure",
    "content": " ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/segment-replication/backpressure/",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/segment-replication/backpressure/"
  },"444": {
    "doc": "Segment replication",
    "title": "Segment replication",
    "content": "With segment replication, segment files are copied across shards instead of documents being indexed on each shard copy. This improves indexing throughput and lowers resource utilization at the expense of increased network utilization. When the primary shard sends a checkpoint to replica shards on a refresh, a new segment replication event is triggered on replica shards. This happens: . | When a new replica shard is added to a cluster. | When there are segment file changes on a primary shard refresh. | During peer recovery, such as replica shard recovery and shard relocation (explicit allocation using the move allocation command or automatic shard rebalancing). | . Segment replication is the first feature in a series of features designed to decouple reads and writes in order to lower compute costs. ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/segment-replication/index/",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/segment-replication/index/"
  },"445": {
    "doc": "Segment replication",
    "title": "Use cases",
    "content": ". | Users who have high write loads but do not have high search requirements and are comfortable with longer refresh times. | Users with very high loads who want to add new nodes, as you do not need to index all nodes when adding a new node to the cluster. | OpenSearch cluster deployments with low replica counts, such as those used for log analytics. | . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/segment-replication/index/#use-cases",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/segment-replication/index/#use-cases"
  },"446": {
    "doc": "Segment replication",
    "title": "Segment replication configuration",
    "content": "To set segment replication as the replication strategy, create an index with replication.type set to SEGMENT: . PUT /my-index1 { \"settings\": { \"index\": { \"replication.type\": \"SEGMENT\" } } } . copy . In segment replication, the primary shard is usually generating more network traffic than the replicas because it copies segment files to the replicas. Thus, it’s beneficial to distribute primary shards equally between the nodes. To ensure balanced primary shard distribution, set the dynamic cluster.routing.allocation.balance.prefer_primary setting to true. For more information, see Cluster settings. Segment replication currently does not support the wait_for value in the refresh query parameter. For the best performance, we recommend enabling both of the following settings: . | Segment replication backpressure. | Balanced primary shard allocation: | . curl -X PUT \"$host/_cluster/settings?pretty\" -H 'Content-Type: application/json' -d' { \"persistent\": { \"cluster.routing.allocation.balance.prefer_primary\": true, \"segrep.pressure.enabled\": true } } . copy . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/segment-replication/index/#segment-replication-configuration",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/segment-replication/index/#segment-replication-configuration"
  },"447": {
    "doc": "Segment replication",
    "title": "Considerations",
    "content": "When using segment replication, consider the following: . | Enabling segment replication for an existing index requires reindexing. | Rolling upgrades are not currently supported. Full cluster restarts are required when upgrading indexes using segment replication. See Issue 3881. | Cross-cluster replication does not currently use segment replication to copy between clusters. | Segment replication leads to increased network congestion on primary shards. See Issue - Optimize network bandwidth on primary shards. | Integration with remote-backed storage as the source of replication is currently not supported. | Read-after-write guarantees: The wait_until refresh policy is not compatible with segment replication. If you use the wait_until refresh policy while ingesting documents, you’ll get a response only after the primary node has refreshed and made those documents searchable. Replica shards will respond only after having written to their local translog. We are exploring other mechanisms for providing read-after-write guarantees. For more information, see the corresponding GitHub issue. | System indexes will continue to use document replication internally until read-after-write guarantees are available. In this case, document replication does not hinder the overall performance because there are few system indexes. | . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/segment-replication/index/#considerations",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/segment-replication/index/#considerations"
  },"448": {
    "doc": "Segment replication",
    "title": "Benchmarks",
    "content": "During initial benchmarks, segment replication users reported 40% higher throughput than when using document replication with the same cluster setup. The following benchmarks were collected with OpenSearch-benchmark using the stackoverflow and nyc_taxi datasets. The benchmarks demonstrate the effect of the following configurations on segment replication: . | The workload size . | The number of primary shards . | The number of replicas . | . Your results may vary based on the cluster topology, hardware used, shard count, and merge settings. Increasing the workload size . The following table lists benchmarking results for the nyc_taxi dataset with the following configuration: . | 10 m5.xlarge data nodes . | 40 primary shards, 1 replica each (80 shards total) . | 4 primary shards and 4 replica shards per node . | . | | 40 GB primary shard, 80 GB total | 240 GB primary shard, 480 GB total | | | Document Replication | Segment Replication | Percent difference | Document Replication | Segment Replication | Percent difference | . | Store size | | 85.2781 | 91.2268 | N/A | 515.726 | 558.039 | N/A | . | Index throughput (number of requests per second) | Minimum | 148,134 | 185,092 | 24.95% | 100,140 | 168,335 | 68.10% | . | Median | 160,110 | 189,799 | 18.54% | 106,642 | 170,573 | 59.95% | . | Maximum | 175,196 | 190,757 | 8.88% | 108,583 | 172,507 | 58.87% | . | Error rate | | 0.00% | 0.00% | 0.00% | 0.00% | 0.00% | 0.00% | . As the size of the workload increases, the benefits of segment replication are amplified because the replicas are not required to index the larger dataset. In general, segment replication leads to higher throughput at lower resource costs than document replication in all cluster configurations, not accounting for replication lag. Increasing the number of primary shards . The following table lists benchmarking results for the nyc_taxi dataset for 40 and 100 primary shards. | | 40 primary shards, 1 replica | 100 primary shards, 1 replica | | | Document Replication | Segment Replication | Percent difference | Document Replication | Segment Replication | Percent difference | . | Index throughput (number of requests per second) | Minimum | 148,134 | 185,092 | 24.95% | 151,404 | 167,391 | 9.55% | . | Median | 160,110 | 189,799 | 18.54% | 154,796 | 172,995 | 10.52% | . | Maximum | 175,196 | 190,757 | 8.88% | 166,173 | 174,655 | 4.86% | . | Error rate | | 0.00% | 0.00% | 0.00% | 0.00% | 0.00% | 0.00% | . As the number of primary shards increases, the benefits of segment replication over document replication decrease. While segment replication is still beneficial with a larger number of primary shards, the difference in performance becomes less pronounced because there are more primary shards per node that must copy segment files across the cluster. Increasing the number of replicas . The following table lists benchmarking results for the stackoverflow dataset for 1 and 9 replicas. | | 10 primary shards, 1 replica | 10 primary shards, 9 replicas | | | Document Replication | Segment Replication | Percent difference | Document Replication | Segment Replication | Percent difference | . | Index throughput (number of requests per second) | Median | 72,598.10 | 90,776.10 | 25.04% | 16,537.00 | 14,429.80 | &minus;12.74% | . | Maximum | 86,130.80 | 96,471.00 | 12.01% | 21,472.40 | 38,235.00 | 78.07% | . | CPU usage (%) | p50 | 17 | 18.857 | 10.92% | 69.857 | 8.833 | &minus;87.36% | . | p90 | 76 | 82.133 | 8.07% | 99 | 86.4 | &minus;12.73% | . | p99 | 100 | 100 | 0% | 100 | 100 | 0% | . | p100 | 100 | 100 | 0% | 100 | 100 | 0% | . | Memory usage (%) | p50 | 35 | 23 | &minus;34.29% | 42 | 40 | &minus;4.76% | . | p90 | 59 | 57 | &minus;3.39% | 59 | 63 | 6.78% | . | p99 | 69 | 61 | &minus;11.59% | 66 | 70 | 6.06% | . | p100 | 72 | 62 | &minus;13.89% | 69 | 72 | 4.35% | . | Error rate | | 0.00% | 0.00% | 0.00% | 0.00% | 2.30% | 2.30% | . As the number of replicas increases, the amount of time required for primary shards to keep replicas up to date (known as the replication lag) also increases. This is because segment replication copies the segment files directly from primary shards to replicas. The benchmarking results show a non-zero error rate as the number of replicas increases. The error rate indicates that the segment replication backpressure mechanism is initiated when replicas cannot keep up with the primary shard. However, the error rate is offset by the significant CPU and memory gains that segment replication provides. ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/segment-replication/index/#benchmarks",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/segment-replication/index/#benchmarks"
  },"449": {
    "doc": "Segment replication",
    "title": "Next steps",
    "content": ". | Track future enhancements to segment replication. | Read this blog post about segment replication. | . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/segment-replication/index/#next-steps",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/segment-replication/index/#next-steps"
  },"450": {
    "doc": "Shard indexing backpressure",
    "title": "Shard indexing backpressure",
    "content": "Shard indexing backpressure is a smart rejection mechanism at a per-shard level that dynamically rejects indexing requests when your cluster is under strain. It propagates a backpressure that transfers requests from an overwhelmed node or shard to other nodes or shards that are still healthy. With shard indexing backpressure, you can prevent nodes in your cluster from running into cascading failures due to performance degradation caused by slow nodes, stuck tasks, resource-intensive requests, traffic surges, skewed shard allocations, and so on. Shard indexing backpressure comes into effect only when one primary and one secondary parameter is breached. ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/shard-indexing-backpressure/",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/shard-indexing-backpressure/"
  },"451": {
    "doc": "Shard indexing backpressure",
    "title": "Primary parameters",
    "content": "Primary parameters are early indicators that a cluster is under strain: . | Shard memory limit breach: If the memory usage of a shard exceeds 95% of its allocated memory, this limit is breached. | Node memory limit breach: If the memory usage of a node exceeds 70% of its allocated memory, this limit is breached. | . The breach of primary parameters doesn’t cause any actual request rejections, it just triggers an evaluation of the secondary parameters. ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/shard-indexing-backpressure/#primary-parameters",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/shard-indexing-backpressure/#primary-parameters"
  },"452": {
    "doc": "Shard indexing backpressure",
    "title": "Secondary parameters",
    "content": "Secondary parameters check the performance at the shard level to confirm that the cluster is under strain: . | Throughput: If the throughput at the shard level decreases significantly in its historic view, this limit is breached. | Successful Request: If the number of pending requests increases significantly in its historic view, this limit is breached. | . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/shard-indexing-backpressure/#secondary-parameters",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/shard-indexing-backpressure/#secondary-parameters"
  },"453": {
    "doc": "Settings",
    "title": "Settings",
    "content": "Shard indexing backpressure adds several settings to the standard OpenSearch cluster settings. They are dynamic, so you can change the default behavior of this feature without restarting your cluster. ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/shard-indexing-settings/",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/shard-indexing-settings/"
  },"454": {
    "doc": "Settings",
    "title": "High-level controls",
    "content": "The high-level controls allow you to turn the shard indexing backpressure feature on or off. | Setting | Default | Description | . | shard_indexing_pressure.enabled | False | Change to true to enable shard indexing backpressure. | . | shard_indexing_pressure.enforced | False | Run shard indexing backpressure in shadow mode or enforced mode. In shadow mode (value set as false), shard indexing backpressure tracks all granular-level metrics, but it doesn’t actually reject any indexing requests. In enforced mode (value set as true), shard indexing backpressure rejects any requests to the cluster that might cause a dip in its performance. | . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/shard-indexing-settings/#high-level-controls",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/shard-indexing-settings/#high-level-controls"
  },"455": {
    "doc": "Settings",
    "title": "Node-level limits",
    "content": "Node-level limits allow you to control memory usage on a node. | Setting | Default | Description | . | shard_indexing_pressure.primary_parameter.node.soft_limit | 70% | Define the percentage of the node-level memory threshold that acts as a soft indicator for strain on a node. | . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/shard-indexing-settings/#node-level-limits",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/shard-indexing-settings/#node-level-limits"
  },"456": {
    "doc": "Settings",
    "title": "Shard-level limits",
    "content": "Shard-level limits allow you to control memory usage on a shard. | Setting | Default | Description | . | shard_indexing_pressure.primary_parameter.shard.min_limit | 0.001d | Specify the minimum assigned quota for a new shard in any role (coordinator, primary, or replica). Shard indexing backpressure increases or decreases this allocated quota based on the inflow of traffic for the shard. | . | shard_indexing_pressure.operating_factor.lower | 75% | Specify the lower occupancy limit of the allocated quota of memory for the shard. If the total memory usage of a shard is below this limit, shard indexing backpressure decreases the current allocated memory for that shard. | . | shard_indexing_pressure.operating_factor.optimal | 85% | Specify the optimal occupancy of the allocated quota of memory for the shard. If the total memory usage of a shard is at this level, shard indexing backpressure doesn’t change the current allocated memory for that shard. | . | shard_indexing_pressure.operating_factor.upper | 95% | Specify the upper occupancy limit of the allocated quota of memory for the shard. If the total memory usage of a shard is above this limit, shard indexing backpressure increases the current allocated memory for that shard. | . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/shard-indexing-settings/#shard-level-limits",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/shard-indexing-settings/#shard-level-limits"
  },"457": {
    "doc": "Settings",
    "title": "Performance degradation factors",
    "content": "The performance degradation factors allow you to control the dynamic performance thresholds for a shard. | Setting | Default | Description | . | shard_indexing_pressure.secondary_parameter.throughput.request_size_window | 2,000 | The number of requests in the sampling window size on a shard. Shard indexing backpressure compares the overall performance of requests with the requests in the sample window to detect any performance degradation. | . | shard_indexing_pressure.secondary_parameter.throughput.degradation_factor | 5x | The degradation factor per unit byte for a request. This parameter determines the threshold for any latency spikes. The default value is 5x, which implies that if the latency shoots up 5 times in the historic view, shard indexing backpressure marks it as a performance degradation. | . | shard_indexing_pressure.secondary_parameter.successful_request.elapsed_timeout | 300000 ms | The amount of time a request is pending in a cluster. This parameter helps identify any stuck-request scenarios. | . | shard_indexing_pressure.secondary_parameter.successful_request.max_outstanding_requests | 100 | The maximum number of pending requests in a cluster. | . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/shard-indexing-settings/#performance-degradation-factors",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/shard-indexing-settings/#performance-degradation-factors"
  },"458": {
    "doc": "Snapshots",
    "title": "Snapshots",
    "content": "Snapshots are backups of a cluster’s indexes and state. State includes cluster settings, node information, index metadata (mappings, settings, templates, etc.), and shard allocation. Snapshots have two main uses: . | Recovering from failure . For example, if cluster health goes red, you might restore the red indexes from a snapshot. | Migrating from one cluster to another . For example, if you’re moving from a proof-of-concept to a production cluster, you might take a snapshot of the former and restore it on the latter. | . You can take and restore snapshots using the snapshot API. If you need to automate taking snapshots, you can use the snapshot management feature. ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/snapshots/index/",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/snapshots/index/"
  },"459": {
    "doc": "Searchable snapshots",
    "title": "Searchable snapshots",
    "content": "A searchable snapshot index reads data from a snapshot repository on demand in real time (at search time) rather than downloading all index data to cluster storage at restore time. Because the index data remains in the snapshot format in the repository, searchable snapshot indexes are inherently read-only. Any attempt to write to a searchable snapshot index results in an error. The searchable snapshot feature incorporates techniques like caching frequently used data segments in cluster nodes and removing the least used data segment from the cluster nodes to make space for frequently used data segments. The data segments downloaded from snapshots on block storage reside alongside the general indexes of the cluster nodes. As such, the computing capacity of cluster nodes is shared between indexing, local search, and data segments on a snapshot residing on lower-cost object storage like Amazon Simple Storage Service (Amazon S3). While cluster node resources are utilized much more efficiently, the high number of tasks results in slower and longer snapshot searches. The local storage of the node is also used for caching the snapshot data. ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/snapshots/searchable_snapshot/",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/snapshots/searchable_snapshot/"
  },"460": {
    "doc": "Searchable snapshots",
    "title": "Configuring a node to use searchable snapshots",
    "content": "To configure the searchable snapshots feature, create a node in your opensearch.yml file and define the node role as search: . ```bash node.name: snapshots-node node.roles: [ search ] ``` . If you’re running Docker, you can create a node with the search node role by adding the line - node.roles: [ search ] to your docker-compose.yml file: . version: '3' services: opensearch-node1: image: opensearchproject/opensearch:2.7.0 container_name: opensearch-node1 environment: - cluster.name=opensearch-cluster - node.name=opensearch-node1 - node.roles: [ search ] . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/snapshots/searchable_snapshot/#configuring-a-node-to-use-searchable-snapshots",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/snapshots/searchable_snapshot/#configuring-a-node-to-use-searchable-snapshots"
  },"461": {
    "doc": "Searchable snapshots",
    "title": "Create a searchable snapshot index",
    "content": "A searchable snapshot index is created by specifying the remote_snapshot storage type using the restore snapshots API. | Request Field | Description | . | storage_type | local indicates that all snapshot metadata and index data will be downloaded to local storage. remote_snapshot indicates that snapshot metadata will be downloaded to the cluster, but the remote repository will remain the authoritative store of the index data. Data will be downloaded and cached as necessary to service queries. At least one node in the cluster must be configured with the search node role in order to restore a snapshot using the remote_snapshot type. Defaults to local. | . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/snapshots/searchable_snapshot/#create-a-searchable-snapshot-index",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/snapshots/searchable_snapshot/#create-a-searchable-snapshot-index"
  },"462": {
    "doc": "Searchable snapshots",
    "title": "Listing indexes",
    "content": "To determine whether an index is a searchable snapshot index, look for a store type with the value of remote_snapshot: . GET /my-index/_settings?pretty . { \"my-index\": { \"settings\": { \"index\": { \"store\": { \"type\": \"remote_snapshot\" } } } } } . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/snapshots/searchable_snapshot/#listing-indexes",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/snapshots/searchable_snapshot/#listing-indexes"
  },"463": {
    "doc": "Searchable snapshots",
    "title": "Potential use cases",
    "content": "The following are potential use cases for the searchable snapshots feature: . | The ability to offload indexes from cluster-based storage but retain the ability to search them. | The ability to have a large number of searchable indexes in lower-cost media. | . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/snapshots/searchable_snapshot/#potential-use-cases",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/snapshots/searchable_snapshot/#potential-use-cases"
  },"464": {
    "doc": "Searchable snapshots",
    "title": "Known limitations",
    "content": "The following are known limitations of the searchable snapshots feature: . | Accessing data from a remote repository is slower than local disk reads, so higher latencies on search queries are expected. | Many remote object stores charge on a per-request basis for retrieval, so users should closely monitor any costs incurred. | Searching remote data can impact the performance of other queries running on the same node. We recommend that users provision dedicated nodes with the search role for performance-critical applications. | . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/snapshots/searchable_snapshot/#known-limitations",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/snapshots/searchable_snapshot/#known-limitations"
  },"465": {
    "doc": "Snapshot management API",
    "title": "Snapshot Management API",
    "content": "Use the snapshot management (SM) API to automate taking snapshots. . | Create or update a policy . | Example | Response | Parameters | . | Get policies . | Example | Response | . | Explain . | Example | Response | . | Start a policy . | Example | Response | . | Stop a policy . | Example | Response | . | Delete a policy . | Example | Response | . | . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/snapshots/sm-api/#snapshot-management-api",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/snapshots/sm-api/#snapshot-management-api"
  },"466": {
    "doc": "Snapshot management API",
    "title": "Create or update a policy",
    "content": "Introduced 2.1 . Creates or updates an SM policy. Request . Create: . POST _plugins/_sm/policies/&lt;policy_name&gt; . Update: . PUT _plugins/_sm/policies/&lt;policy_name&gt;?if_seq_no=0&amp;if_primary_term=1 . You must provide the seq_no and primary_term parameters for an update request. Example . POST _plugins/_sm/policies/daily-policy { \"description\": \"Daily snapshot policy\", \"creation\": { \"schedule\": { \"cron\": { \"expression\": \"0 8 * * *\", \"timezone\": \"UTC\" } }, \"time_limit\": \"1h\" }, \"deletion\": { \"schedule\": { \"cron\": { \"expression\": \"0 1 * * *\", \"timezone\": \"America/Los_Angeles\" } }, \"condition\": { \"max_age\": \"7d\", \"max_count\": 21, \"min_count\": 7 }, \"time_limit\": \"1h\" }, \"snapshot_config\": { \"date_format\": \"yyyy-MM-dd-HH:mm\", \"timezone\": \"America/Los_Angeles\", \"indices\": \"*\", \"repository\": \"s3-repo\", \"ignore_unavailable\": \"true\", \"include_global_state\": \"false\", \"partial\": \"true\", \"metadata\": { \"any_key\": \"any_value\" } }, \"notification\": { \"channel\": { \"id\": \"NC3OpoEBzEoHMX183R3f\" }, \"conditions\": { \"creation\": true, \"deletion\": false, \"failure\": false, \"time_limit_exceeded\": false } } } . Response . { \"_id\" : \"daily-policy-sm-policy\", \"_version\" : 5, \"_seq_no\" : 54983, \"_primary_term\" : 21, \"sm_policy\" : { \"name\" : \"daily-policy\", \"description\" : \"Daily snapshot policy\", \"schema_version\" : 15, \"creation\" : { \"schedule\" : { \"cron\" : { \"expression\" : \"0 8 * * *\", \"timezone\" : \"UTC\" } }, \"time_limit\" : \"1h\" }, \"deletion\" : { \"schedule\" : { \"cron\" : { \"expression\" : \"0 1 * * *\", \"timezone\" : \"America/Los_Angeles\" } }, \"condition\" : { \"max_age\" : \"7d\", \"min_count\" : 7, \"max_count\" : 21 }, \"time_limit\" : \"1h\" }, \"snapshot_config\" : { \"indices\" : \"*\", \"metadata\" : { \"any_key\" : \"any_value\" }, \"ignore_unavailable\" : \"true\", \"timezone\" : \"America/Los_Angeles\", \"include_global_state\" : \"false\", \"date_format\" : \"yyyy-MM-dd-HH:mm\", \"repository\" : \"s3-repo\", \"partial\" : \"true\" }, \"schedule\" : { \"interval\" : { \"start_time\" : 1656425122909, \"period\" : 1, \"unit\" : \"Minutes\" } }, \"enabled\" : true, \"last_updated_time\" : 1656425122909, \"enabled_time\" : 1656425122909, \"notification\" : { \"channel\" : { \"id\" : \"NC3OpoEBzEoHMX183R3f\" }, \"conditions\" : { \"creation\" : true, \"deletion\" : false, \"failure\" : false, \"time_limit_exceeded\" : false } } } } . Parameters . You can specify the following parameters to create/update an SM policy. | Parameter | Type | Description | . | description | String | The description of the SM policy. Optional. | . | enabled | Boolean | Should this SM policy be enabled at creation? Optional. | . | snapshot_config | Object | The configuration options for snapshot creation. Required. | . | snapshot_config.date_format | String | Snapshot names have the format &lt;policy_name&gt;-&lt;date&gt;-&lt;random number&gt;. date_format specifies the format for the date in the snapshot name. Supports all date formats supported by OpenSearch. Optional. Default is “yyyy-MM-dd’T’HH:mm:ss”. | . | snapshot_config.date_format_timezone | String | Snapshot names have the format &lt;policy_name&gt;-&lt;date&gt;-&lt;random number&gt;. date_format_timezone specifies the time zone for the date in the snapshot name. Optional. Default is UTC. | . | snapshot_config.indices | String | The names of the indexes in the snapshot. Multiple index names are separated by ,. Supports wildcards (*). Optional. Default is * (all indexes). | . | snapshot_config.repository | String | The repository in which to store snapshots. Required. | . | snapshot_config.ignore_unavailable | Boolean | Do you want to ignore unavailable indexes? Optional. Default is false. | . | snapshot_config.include_global_state | Boolean | Do you want to include cluster state? Optional. Default is true because of Security plugin considerations. | . | snapshot_config.partial | Boolean | Do you want to allow partial snapshots? Optional. Default is false. | . | snapshot_config.metadata | Object | Metadata in the form of key/value pairs. Optional. | . | creation | Object | Configuration for snapshot creation. Required. | . | creation.schedule | String | The cron schedule used to create snapshots. Required. | . | creation.time_limit | String | Sets the maximum time to wait for snapshot creation to finish. If time_limit is longer than the scheduled time interval for taking snapshots, no scheduled snapshots are taken until time_limit elapses. For example, if time_limit is set to 35 minutes and snapshots are taken every 30 minutes starting at midnight, the snapshots at 00:00 and 01:00 are taken, but the snapshot at 00:30 is skipped. Optional. | . | deletion | Object | Configuration for snapshot deletion. Optional. Default is to retain all snapshots. | . | deletion.schedule | String | The cron schedule used to delete snapshots. Optional. Default is to use creation.schedule, which is required. | . | deletion.time_limit | String | Sets the maximum time to wait for snapshot deletion to finish. Optional. | . | deletion.delete_condition | Object | Conditions for snapshot deletion. Optional. | . | deletion.delete_condition.max_count | Integer | The maximum number of snapshots to be retained. Optional. | . | deletion.delete_condition.max_age | String | The maximum time a snapshot is retained. Optional. | . | deletion.delete_condition.min_count | Integer | The minimum number of snapshots to be retained. Optional. Default is one. | . | notification | Object | Defines notifications for SM events. Optional. | . | notification.channel | Object | Defines a channel for notifications. You must create and configure a notification channel before setting up SM notifications. Required. | . | notification.channel.id | String | The channel ID of the channel used for notifications. To get the channel IDs of all created channels, use GET _plugins/_notifications/configs. Required. | . | notification.conditions | Object | SM events you want to be notified about. Set the ones you are interested in to true. | . | notification.conditions.creation | Boolean | Do you want notifications about snapshot creation? Optional. Default is true. | . | notification.conditions.deletion | Boolean | Do you want notifications about snapshot deletion? Optional. Default is false. | . | notification.conditions.failure | Boolean | Do you want notifications about creation or deletion failure? Optional. Default is false. | . | notification.conditions.time_limit_exceeded | Boolean | Do you want notifications when snapshot operations take longer than time_limit? Optional. Default is false. | . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/snapshots/sm-api/#create-or-update-a-policy",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/snapshots/sm-api/#create-or-update-a-policy"
  },"467": {
    "doc": "Snapshot management API",
    "title": "Get policies",
    "content": "Introduced 2.1 . Gets SM policies. Request . Get all SM policies: . GET _plugins/_sm/policies . You can use a query string and specify pagination, the field to be sorted by, and sort order: . GET _plugins/_sm/policies?from=0&amp;size=20&amp;sortField=sm_policy.name&amp;sortOrder=desc&amp;queryString=* . Get a specific SM policy: . GET _plugins/_sm/policies/&lt;policy_name&gt; . Example . GET _plugins/_sm/policies/daily-policy . Response . { \"_id\" : \"daily-policy-sm-policy\", \"_version\" : 6, \"_seq_no\" : 44696, \"_primary_term\" : 19, \"sm_policy\" : { \"name\" : \"daily-policy\", \"description\" : \"Daily snapshot policy\", \"schema_version\" : 15, \"creation\" : { \"schedule\" : { \"cron\" : { \"expression\" : \"0 8 * * *\", \"timezone\" : \"UTC\" } }, \"time_limit\" : \"1h\" }, \"deletion\" : { \"schedule\" : { \"cron\" : { \"expression\" : \"0 1 * * *\", \"timezone\" : \"America/Los_Angeles\" } }, \"condition\" : { \"max_age\" : \"7d\", \"min_count\" : 7, \"max_count\" : 21 }, \"time_limit\" : \"1h\" }, \"snapshot_config\" : { \"metadata\" : { \"any_key\" : \"any_value\" }, \"ignore_unavailable\" : \"true\", \"include_global_state\" : \"false\", \"date_format\" : \"yyyy-MM-dd-HH:mm\", \"repository\" : \"s3-repo\", \"partial\" : \"true\" }, \"schedule\" : { \"interval\" : { \"start_time\" : 1656341042874, \"period\" : 1, \"unit\" : \"Minutes\" } }, \"enabled\" : true, \"last_updated_time\" : 1656341042874, \"enabled_time\" : 1656341042874 } } . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/snapshots/sm-api/#get-policies",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/snapshots/sm-api/#get-policies"
  },"468": {
    "doc": "Snapshot management API",
    "title": "Explain",
    "content": "Introduced 2.1 . Provides the enabled/disabled status and the metadata for all policies specified. Multiple policy names are separated with ,. You can also specify desired policies with a wildcard pattern. SM uses a state machine for snapshot creation and deletion. The image on the left shows one execution period of the creation workflow, from the CREATION_START state to the CREATION_FINISHED state. Deletion workflow follows the same pattern as creation workflow. The creation workflow starts in the CREATION_START state and continuously checks if the conditions in the creation cron schedule are met. After the conditions are met, the creation workflow switches to the CREATION_CONDITION_MET state and continues to the CREATING state. The CREATING state calls the create snapshot API asynchronously and then waits for snapshot creation to end in the CREATION_FINISHED state. Once snapshot creation ends, the creation workflow goes back to the CREATION_START state, and the cycle continues. The current_state field of metadata.creation and metadata.deletion returns the current state of the state machine. Request . GET _plugins/_sm/policies/&lt;policy_names&gt;/_explain . Example . GET _plugins/_sm/policies/daily*/_explain . Response . { \"policies\" : [ { \"name\" : \"daily-policy\", \"creation\" : { \"current_state\" : \"CREATION_START\", \"trigger\" : { \"time\" : 1656403200000 } }, \"deletion\" : { \"current_state\" : \"DELETION_START\", \"trigger\" : { \"time\" : 1656403200000 } }, \"policy_seq_no\" : 44696, \"policy_primary_term\" : 19, \"enabled\" : true } ] } . The following table lists all fields for each policy in the response. | Field | Description | . | name | The name of the SM policy. | . | creation | Information about the latest creation operation. See subfields below. | . | deletion | Information about the latest deletion operation. See subfields below. | . | policy_seq_no policy_primary_term | The version of the SM policy. | . | enabled | Is the policy running? | . The following table lists all fields in the creation and deletion objects of each policy. | Field | Description | . | current_state | The current state of the state machine that runs snapshot creation/deletion as described above. | . | trigger.time | The next creation/deletion execution time in milliseconds since the epoch. | . | latest_execution | Describes the latest creation/deletion execution. | . | latest_execution.status | The execution status of the latest creation/deletion. Possible values are: IN_PROGRESS: Snapshot creation/deletion has started. SUCCESS: Snapshot creation/deletion has finished successfully. RETRYING: The creation/deletion attempt has failed. It will be retried three times. FAILED: The creation/deletion attempt failed after three retries. End the current execution period and go to the next execution period. TIME_LIMIT_EXCEEDED: The creation/deletion time exceeded the time_limit set in the policy. End the current execution period and go to the next execution period. | . | latest_execution.start_time | The start time of the latest execution in milliseconds since the epoch. | . | latest_execution.end_time | The end time of the latest execution in milliseconds since the epoch. | . | latest_execution.info.message | A user-friendly message describing the status of the latest execution. | . | latest_execution.info.cause | Contains the failure reason if the latest execution fails. | . | retry.count | The number of remaining execution retry attempts. | . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/snapshots/sm-api/#explain",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/snapshots/sm-api/#explain"
  },"469": {
    "doc": "Snapshot management API",
    "title": "Start a policy",
    "content": "Introduced 2.1 . Starts the policy by setting its enabled flag to true. Request . POST _plugins/_sm/policies/&lt;policy_name&gt;/_start . Example . POST _plugins/_sm/policies/daily-policy/_start . Response . { \"acknowledged\" : true } . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/snapshots/sm-api/#start-a-policy",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/snapshots/sm-api/#start-a-policy"
  },"470": {
    "doc": "Snapshot management API",
    "title": "Stop a policy",
    "content": "Introduced 2.1 . Sets the enabled flag to false for an SM policy. The policy will not run until you start it. Request . POST _plugins/_sm/policies/&lt;policy_name&gt;/_stop . Example . POST _plugins/_sm/policies/daily-policy/_stop . Response . { \"acknowledged\" : true } . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/snapshots/sm-api/#stop-a-policy",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/snapshots/sm-api/#stop-a-policy"
  },"471": {
    "doc": "Snapshot management API",
    "title": "Delete a policy",
    "content": "Introduced 2.1 . Deletes the specified SM policy. Request . DELETE _plugins/_sm/policies/&lt;policy_name&gt; . Example . DELETE _plugins/_sm/policies/daily-policy . Response . { \"_index\" : \".opendistro-ism-config\", \"_id\" : \"daily-policy-sm-policy\", \"_version\" : 8, \"result\" : \"deleted\", \"forced_refresh\" : true, \"_shards\" : { \"total\" : 2, \"successful\" : 2, \"failed\" : 0 }, \"_seq_no\" : 45366, \"_primary_term\" : 20 } . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/snapshots/sm-api/#delete-a-policy",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/snapshots/sm-api/#delete-a-policy"
  },"472": {
    "doc": "Snapshot management API",
    "title": "Snapshot management API",
    "content": " ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/snapshots/sm-api/",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/snapshots/sm-api/"
  },"473": {
    "doc": "Snapshot management",
    "title": "Snapshot management",
    "content": "Snapshot management (SM) lets you automate taking snapshots. To use this feature, you need to install the Index Management (IM) Plugin. Snapshots store only incremental changes since the last snapshot. Thus, while taking an initial snapshot may be a heavy operation, subsequent snapshots have minimal overhead. To set up automatic snapshots, you have to create an SM policy with a desired SM schedule and configuration. When you create an SM policy, its document ID is given the name &lt;policy_name&gt;-sm-policy. Because of this, SM policies have to obey the following rules: . | SM policies must have unique names. | You cannot update the policy name after its creation. | . SM-created snapshots have names in the format &lt;policy_name&gt;-&lt;date&gt;-&lt;random number&gt;. Two snapshots created by different policies at the same time always have different names because of the &lt;policy_name&gt; prefix. To avoid name collisions within the same policy, each snapshot’s name contains a random string suffix. Each policy has associated metadata that stores the policy status. Snapshot management saves SM policies and metadata in the system index and reads them from the system index. Thus, Snapshot Management depends on the OpenSearch cluster’s indexing and searching functions. The policy’s metadata keeps information about the latest creation and deletion only. The metadata is read before running every scheduled job so that SM can continue execution from the previous job’s state. You can view the metadata using the explain API. An SM schedule is a custom cron expression. It consists of two parts: a creation schedule and a deletion schedule. You must set up a creation schedule that specifies the frequency and timing of snapshot creation. Optionally, you can set up a separate schedule for deleting snapshots. An SM configuration includes the indexes and repository for the snapshots and supports all parameters you can define when creating a snapshot using the API. Additionally, you can specify the format and time zone for the date used in the snapshot’s name. ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/snapshots/snapshot-management/",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/snapshots/snapshot-management/"
  },"474": {
    "doc": "Snapshot management",
    "title": "Performance",
    "content": "One snapshot can contain as many indexes as there are in the cluster. We expect at most dozens of SM policies in one cluster, but a snapshot repository can safely scale to thousands of snapshots. However, to manage its metadata, a large repository requires more memory on the cluster manager node. Snapshot Management depends on the Job Scheduler plugin to schedule a job that is run periodically. Each SM policy corresponds to one SM-scheduled job. The scheduled job is lightweight, so the burden of SM depends on the snapshot creation frequency and the burden of running the snapshot operation itself. ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/snapshots/snapshot-management/#performance",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/snapshots/snapshot-management/#performance"
  },"475": {
    "doc": "Snapshot management",
    "title": "Concurrency",
    "content": "An SM policy does not support concurrent snapshot operations, since too many such operations may degrade the cluster. Snapshot operations (creation or deletion) are performed asynchronously. SM does not start a new operation until the previous asynchronous operation finishes. We don’t recommend creating several SM policies with the same schedule and overlapping indexes in one cluster because it leads to concurrent snapshot creation on the same indexes and hinders performance. We don’t recommend setting up the same repository for multiple SM policies with same schedule in different clusters, since it may cause a sudden spike of burden in this repository. ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/snapshots/snapshot-management/#concurrency",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/snapshots/snapshot-management/#concurrency"
  },"476": {
    "doc": "Snapshot management",
    "title": "Failure management",
    "content": "If a snapshot operation fails, it is retried a maximum of three times. The failure message is saved in metadata.latest_execution and is overwritten when a subsequent snapshot operation starts. You can view the failure message using the explain API. When using OpenSearch Dashboards, you can view the failure message on the policy details page. Possible reasons for failure include red index status and shard reallocation. ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/snapshots/snapshot-management/#failure-management",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/snapshots/snapshot-management/#failure-management"
  },"477": {
    "doc": "Snapshot management",
    "title": "Security",
    "content": "The Security plugin has two built-in roles for Snapshot Management actions: snapshot_management_full_access and snapshot_management_read_access. For descriptions of each, see Predefined roles. The following table lists the required permissions for each Snapshot Management API. | Function | API | Permission | . | Get policy | GET _plugins/_sm/policiesGET _plugins/_sm/policies/policy_name | cluster:admin/opensearch/snapshot_management/policy/getcluster:admin/opensearch/snapshot_management/policy/search | . | Create/update policy | POST _plugins/_sm/policies/policy_name PUT _plugins/_sm/policies/policy_name?if_seq_no=1&amp;if_primary_term=1 | cluster:admin/opensearch/snapshot_management/policy/write | . | Delete policy | DELETE _plugins/_sm/policies/policy_name | cluster:admin/opensearch/snapshot_management/policy/delete | . | Explain | GET _plugins/_sm/policies/policy_names/_explain | cluster:admin/opensearch/snapshot_management/policy/explain | . | Start | POST _plugins/_sm/policies/policy_name/_start | cluster:admin/opensearch/snapshot_management/policy/start | . | Stop | POST _plugins/_sm/policies/policy_name/_stop | cluster:admin/opensearch/snapshot_management/policy/stop | . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/snapshots/snapshot-management/#security",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/snapshots/snapshot-management/#security"
  },"478": {
    "doc": "Snapshot management",
    "title": "API",
    "content": "The following table lists all Snapshot Management API functions. | Function | API | Description | . | Create policy | POST _plugins/_sm/policies/policy_name | Creates an SM policy. | . | Update policy | PUT _plugins/_sm/policies/policy_name?if_seq_no=sequence_number&amp;if_primary_term=primary_term | Modifies the policy_name policy. | . | Get all policies | GET _plugins/_sm/policies | Returns all SM policies. | . | Get the policy policy_name | GET _plugins/_sm/policies/policy_name | Returns the policy_name SM policy. | . | Delete policy | DELETE _plugins/_sm/policies/policy_name | Deletes the policy_name policy. | . | Explain | GET _plugins/_sm/policies/policy_names/_explain | Provides the enabled/disabled status and the metadata for all policies specified by policy_names. | . | Start policy | POST _plugins/_sm/policies/policy_name/_start | Starts the policy_name policy. | . | Stop policy | POST _plugins/_sm/policies/policy_name/_stop | Stops the policy_name policy. | . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/snapshots/snapshot-management/#api",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/snapshots/snapshot-management/#api"
  },"479": {
    "doc": "Take and restore snapshots",
    "title": "Take and restore snapshots",
    "content": "Snapshots aren’t instantaneous. They take time to complete and do not represent perfect point-in-time views of the cluster. While a snapshot is in progress, you can still index documents and send other requests to the cluster, but new documents and updates to existing documents generally aren’t included in the snapshot. The snapshot includes primary shards as they existed when OpenSearch initiated the snapshot. Depending on the size of your snapshot thread pool, different shards might be included in the snapshot at slightly different times. OpenSearch snapshots are incremental, meaning that they only store data that has changed since the last successful snapshot. The difference in disk usage between frequent and infrequent snapshots is often minimal. In other words, taking hourly snapshots for a week (for a total of 168 snapshots) might not use much more disk space than taking a single snapshot at the end of the week. Also, the more frequently you take snapshots, the less time they take to complete. Some OpenSearch users take snapshots as often as every 30 minutes. If you need to delete a snapshot, be sure to use the OpenSearch API rather than navigating to the storage location and purging files. Incremental snapshots from a cluster often share a lot of the same data; when you use the API, OpenSearch only removes data that no other snapshot is using. . | Register repository . | Shared file system | Amazon S3 | . | Take snapshots | Restore snapshots . | Conflicts and compatibility | . | Security considerations | . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/snapshots/snapshot-restore/",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/snapshots/snapshot-restore/"
  },"480": {
    "doc": "Take and restore snapshots",
    "title": "Register repository",
    "content": "Before you can take a snapshot, you have to “register” a snapshot repository. A snapshot repository is just a storage location: a shared file system, Amazon S3, Hadoop Distributed File System (HDFS), Azure Storage, etc. Shared file system . | To use a shared file system as a snapshot repository, add it to opensearch.yml: . path.repo: [\"/mnt/snapshots\"] . On the RPM and Debian installs, you can then mount the file system. If you’re using the Docker install, add the file system to each node in docker-compose.yml before starting the cluster: . volumes: - /Users/jdoe/snapshots:/mnt/snapshots . | Then register the repository using the REST API: . PUT /_snapshot/my-fs-repository { \"type\": \"fs\", \"settings\": { \"location\": \"/mnt/snapshots\" } } . copy . | . You will most likely not need to specify any parameters except for location. For allowed request parameters, see Register or update snapshot repository API. Amazon S3 . | To use an Amazon S3 bucket as a snapshot repository, install the repository-s3 plugin on all nodes: . sudo ./bin/opensearch-plugin install repository-s3 . If you’re using the Docker installation, see Working with plugins. Your Dockerfile should look something like this: . FROM opensearchproject/opensearch:2.7.0 ENV AWS_ACCESS_KEY_ID &lt;access-key&gt; ENV AWS_SECRET_ACCESS_KEY &lt;secret-key&gt; # Optional ENV AWS_SESSION_TOKEN &lt;optional-session-token&gt; RUN /usr/share/opensearch/bin/opensearch-plugin install --batch repository-s3 RUN /usr/share/opensearch/bin/opensearch-keystore create RUN echo $AWS_ACCESS_KEY_ID | /usr/share/opensearch/bin/opensearch-keystore add --stdin s3.client.default.access_key RUN echo $AWS_SECRET_ACCESS_KEY | /usr/share/opensearch/bin/opensearch-keystore add --stdin s3.client.default.secret_key # Optional RUN echo $AWS_SESSION_TOKEN | /usr/share/opensearch/bin/opensearch-keystore add --stdin s3.client.default.session_token . After the Docker cluster starts, skip to step 7. | Add your AWS access and secret keys to the OpenSearch keystore: . sudo ./bin/opensearch-keystore add s3.client.default.access_key sudo ./bin/opensearch-keystore add s3.client.default.secret_key . | (Optional) If you’re using temporary credentials, add your session token: . sudo ./bin/opensearch-keystore add s3.client.default.session_token . | (Optional) If you connect to the internet through a proxy, add those credentials: . sudo ./bin/opensearch-keystore add s3.client.default.proxy.username sudo ./bin/opensearch-keystore add s3.client.default.proxy.password . | (Optional) Add other settings to opensearch.yml: . s3.client.default.disable_chunked_encoding: false # Disables chunked encoding for compatibility with some storage services, but you probably don't need to change this value. s3.client.default.endpoint: s3.amazonaws.com # S3 has alternate endpoints, but you probably don't need to change this value. s3.client.default.max_retries: 3 # number of retries if a request fails s3.client.default.path_style_access: false # whether to use the deprecated path-style bucket URLs. # You probably don't need to change this value, but for more information, see https://docs.aws.amazon.com/AmazonS3/latest/dev/VirtualHosting.html#path-style-access. s3.client.default.protocol: https # http or https s3.client.default.proxy.host: my-proxy-host # the hostname for your proxy server s3.client.default.proxy.port: 8080 # port for your proxy server s3.client.default.read_timeout: 50s # the S3 connection timeout s3.client.default.use_throttle_retries: true # whether the client should wait a progressively longer amount of time (exponential backoff) between each successive retry s3.client.default.region: us-east-2 # AWS region to use . | (Optional) If you don’t want to use AWS access and secret keys, you could configure the S3 plugin to use AWS Identity and Access Management (IAM) roles for service accounts: . sudo ./bin/opensearch-keystore add s3.client.default.role_arn sudo ./bin/opensearch-keystore add s3.client.default.role_session_name . If you don’t want to configure AWS access and secret keys, modify the following opensearch.yml setting. Make sure the file is accessible by the repository-s3 plugin: . s3.client.default.identity_token_file: /usr/share/opensearch/plugins/repository-s3/token . If copying is not an option, you can create a symlink to the web identity token file in the ${OPENSEARCH_PATH_CONFIG} folder: . ln -s $AWS_WEB_IDENTITY_TOKEN_FILE \"${OPENSEARCH_PATH_CONFIG}/aws-web-identity-token-file\" . You can reference the web identity token file in the following opensearch.yml setting by specifying the relative path that is resolved against ${OPENSEARCH_PATH_CONFIG}: . s3.client.default.identity_token_file: aws-web-identity-token-file . IAM roles require at least one of the above settings. Other settings will be taken from environment variables (if available): AWS_ROLE_ARN, AWS_WEB_IDENTITY_TOKEN_FILE, AWS_ROLE_SESSION_NAME. | If you changed opensearch.yml, you must restart each node in the cluster. Otherwise, you only need to reload secure cluster settings: . POST /_nodes/reload_secure_settings . copy . | Create an S3 bucket if you don’t already have one. To take snapshots, you need permissions to access the bucket. The following IAM policy is an example of those permissions: . { \"Version\": \"2012-10-17\", \"Statement\": [{ \"Action\": [ \"s3:*\" ], \"Effect\": \"Allow\", \"Resource\": [ \"arn:aws:s3:::your-bucket\", \"arn:aws:s3:::your-bucket/*\" ] }] } . | Register the repository using the REST API: . PUT /_snapshot/my-s3-repository { \"type\": \"s3\", \"settings\": { \"bucket\": \"my-s3-bucket\", \"base_path\": \"my/snapshot/directory\" } } . copy . | . You will most likely not need to specify any parameters except for bucket and base_path. For allowed request parameters, see Register or update snapshot repository API. ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/snapshots/snapshot-restore/#register-repository",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/snapshots/snapshot-restore/#register-repository"
  },"481": {
    "doc": "Take and restore snapshots",
    "title": "Take snapshots",
    "content": "You specify two pieces of information when you create a snapshot: . | Name of your snapshot repository | Name for the snapshot | . The following snapshot includes all indices and the cluster state: . PUT /_snapshot/my-repository/1 . copy . You can also add a request body to include or exclude certain indices or specify other settings: . PUT /_snapshot/my-repository/2 { \"indices\": \"opensearch-dashboards*,my-index*,-my-index-2016\", \"ignore_unavailable\": true, \"include_global_state\": false, \"partial\": false } . copy . | Request fields | Description | . | indices | The indices you want to include in the snapshot. You can use , to create a list of indices, * to specify an index pattern, and - to exclude certain indices. Don’t put spaces between items. Default is all indices. | . | ignore_unavailable | If an index from the indices list doesn’t exist, whether to ignore it rather than fail the snapshot. Default is false. | . | include_global_state | Whether to include cluster state in the snapshot. Default is true. | . | partial | Whether to allow partial snapshots. Default is false, which fails the entire snapshot if one or more shards fails to store. | . If you request the snapshot immediately after taking it, you might see something like this: . GET /_snapshot/my-repository/2 { \"snapshots\": [{ \"snapshot\": \"2\", \"version\": \"6.5.4\", \"indices\": [ \"opensearch_dashboards_sample_data_ecommerce\", \"my-index\", \"opensearch_dashboards_sample_data_logs\", \"opensearch_dashboards_sample_data_flights\" ], \"include_global_state\": true, \"state\": \"IN_PROGRESS\", ... }] } . copy . Note that the snapshot is still in progress. If you want to wait for the snapshot to finish before continuing, add the wait_for_completion parameter to your request. Snapshots can take a while to complete, so consider whether or not this option fits your use case: . PUT _snapshot/my-repository/3?wait_for_completion=true . copy . Snapshots have the following states: . | State | Description | . | SUCCESS | The snapshot successfully stored all shards. | . | IN_PROGRESS | The snapshot is currently running. | . | PARTIAL | At least one shard failed to store successfully. Can only occur if you set partial to true when taking the snapshot. | . | FAILED | The snapshot encountered an error and stored no data. | . | INCOMPATIBLE | The snapshot is incompatible with the version of OpenSearch running on this cluster. See Conflicts and compatibility. | . You can’t take a snapshot if one is currently in progress. To check the status: . GET /_snapshot/_status . copy . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/snapshots/snapshot-restore/#take-snapshots",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/snapshots/snapshot-restore/#take-snapshots"
  },"482": {
    "doc": "Take and restore snapshots",
    "title": "Restore snapshots",
    "content": "The first step in restoring a snapshot is retrieving existing snapshots. To see all snapshot repositories: . GET /_snapshot/_all . copy . To see all snapshots in a repository: . GET /_snapshot/my-repository/_all . copy . Then restore a snapshot: . POST /_snapshot/my-repository/2/_restore . copy . Just like when taking a snapshot, you can add a request body to include or exclude certain indices or specify some other settings: . POST /_snapshot/my-repository/2/_restore { \"indices\": \"opensearch-dashboards*,my-index*\", \"ignore_unavailable\": true, \"include_global_state\": false, \"include_aliases\": false, \"partial\": false, \"rename_pattern\": \"opensearch-dashboards(.+)\", \"rename_replacement\": \"restored-opensearch-dashboards$1\", \"index_settings\": { \"index.blocks.read_only\": false }, \"ignore_index_settings\": [ \"index.refresh_interval\" ] } . copy . | Request parameters | Description | . | indices | The indices you want to restore. You can use , to create a list of indices, * to specify an index pattern, and - to exclude certain indices. Don’t put spaces between items. Default is all indices. | . | ignore_unavailable | If an index from the indices list doesn’t exist, whether to ignore it rather than fail the restore operation. Default is false. | . | include_global_state | Whether to restore the cluster state. Default is false. | . | include_aliases | Whether to restore aliases alongside their associated indices. Default is true. | . | partial | Whether to allow the restoration of partial snapshots. Default is false. | . | rename_pattern | If you want to rename indices as you restore them, use this option to specify a regular expression that matches all indices you want to restore. Use capture groups (()) to reuse portions of the index name. | . | rename_replacement | If you want to rename indices as you restore them, use this option to specify the replacement pattern. Use $0 to include the entire matching index name, $1 to include the content of the first capture group, etc. | . | index_settings | If you want to change index settings applied during restore, specify them here. You cannot change index.number_of_shards. | . | ignore_index_settings | Rather than explicitly specifying new settings with index_settings, you can ignore certain index settings in the snapshot and use the cluster defaults applied during restore. You cannot ignore index.number_of_shards, index.number_of_replicas, or index.auto_expand_replicas. | . | storage_type | local indicates that all snapshot metadata and index data will be downloaded to local storage. remote_snapshot indicates that snapshot metadata will be downloaded to the cluster, but the remote repository will remain the authoritative store of the index data. Data will be downloaded and cached as necessary to service queries. At least one node in the cluster must be configured with the search role in order to restore a snapshot using the type remote_snapshot. Defaults to local. | . Conflicts and compatibility . One way to avoid naming conflicts when restoring indices is to use the rename_pattern and rename_replacement options. Then, if necessary, you can use the _reindex API to combine the two. The simpler way is to delete existing indices prior to restoring from a snapshot. You can use the _close API to close existing indices prior to restoring from a snapshot, but the index in the snapshot has to have the same number of shards as the existing index. We recommend ceasing write requests to a cluster before restoring from a snapshot, which helps avoid scenarios such as: . | You delete an index, which also deletes its alias. | A write request to the now-deleted alias creates a new index with the same name as the alias. | The alias from the snapshot fails to restore due to a naming conflict with the new index. | . Snapshots are only forward-compatible by one major version. If you have an old snapshot, you can sometimes restore it into an intermediate cluster, reindex all indices, take a new snapshot, and repeat until you arrive at your desired version, but you might find it easier to just manually index your data on the new cluster. ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/snapshots/snapshot-restore/#restore-snapshots",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/snapshots/snapshot-restore/#restore-snapshots"
  },"483": {
    "doc": "Take and restore snapshots",
    "title": "Security considerations",
    "content": "If you’re using the Security plugin, snapshots have some additional restrictions: . | To perform snapshot and restore operations, users must have the built-in manage_snapshots role. | You can’t restore snapshots that contain global state or the .opendistro_security index. | . If a snapshot contains global state, you must exclude it when performing the restore. If your snapshot also contains the .opendistro_security index, either exclude it or list all the other indices you want to include: . POST /_snapshot/my-repository/3/_restore { \"indices\": \"-.opendistro_security\", \"include_global_state\": false } . copy . The .opendistro_security index contains sensitive data, so we recommend excluding it when you take a snapshot. If you do need to restore the index from a snapshot, you must include an admin certificate in the request: . curl -k --cert ./kirk.pem --key ./kirk-key.pem -XPOST 'https://localhost:9200/_snapshot/my-repository/3/_restore?pretty' . copy . We strongly recommend against restoring .opendistro_security using an admin certificate because doing so can alter the security posture of the entire cluster. See A word of caution for a recommended process to back up and restore your Security plugin configuration. ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/snapshots/snapshot-restore/#security-considerations",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/snapshots/snapshot-restore/#security-considerations"
  },"484": {
    "doc": "Stats API",
    "title": "Stats API",
    "content": "Use the stats operation to monitor shard indexing backpressure. ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/stats-api/",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/stats-api/"
  },"485": {
    "doc": "Stats API",
    "title": "Stats",
    "content": "Introduced 1.2 . Returns node-level and shard-level stats for indexing request rejections. Request . GET _nodes/_local/stats/shard_indexing_pressure . If enforced is true: . Example response . { \"_nodes\": { \"total\": 1, \"successful\": 1, \"failed\": 0 }, \"cluster_name\": \"runTask\", \"nodes\": { \"q3e1dQjFSqyPSLAgpyQlfw\": { \"timestamp\": 1613072111162, \"name\": \"runTask-0\", \"transport_address\": \"127.0.0.1:9300\", \"host\": \"127.0.0.1\", \"ip\": \"127.0.0.1:9300\", \"roles\": [ \"data\", \"ingest\", \"cluster_manager\", \"remote_cluster_client\" ], \"attributes\": { \"testattr\": \"test\" }, \"shard_indexing_pressure\": { \"stats\": { \"[index_name][0]\": { \"memory\": { \"current\": { \"coordinating_in_bytes\": 0, \"primary_in_bytes\": 0, \"replica_in_bytes\": 0 }, \"total\": { \"coordinating_in_bytes\": 299, \"primary_in_bytes\": 299, \"replica_in_bytes\": 0 } }, \"rejection\": { \"coordinating\": { \"coordinating_rejections\": 0, \"breakup\": { \"node_limits\": 0, \"no_successful_request_limits\": 0, \"throughput_degradation_limits\": 0 } }, \"primary\": { \"primary_rejections\": 0, \"breakup\": { \"node_limits\": 0, \"no_successful_request_limits\": 0, \"throughput_degradation_limits\": 0 } }, \"replica\": { \"replica_rejections\": 0, \"breakup\": { \"node_limits\": 0, \"no_successful_request_limits\": 0, \"throughput_degradation_limits\": 0 } } }, \"last_successful_timestamp\": { \"coordinating_last_successful_request_timestamp_in_millis\": 1613072107990, \"primary_last_successful_request_timestamp_in_millis\": 0, \"replica_last_successful_request_timestamp_in_millis\": 0 }, \"indexing\": { \"coordinating_time_in_millis\": 96, \"coordinating_count\": 1, \"primary_time_in_millis\": 0, \"primary_count\": 0, \"replica_time_in_millis\": 0, \"replica_count\": 0 }, \"memory_allocation\": { \"current\": { \"current_coordinating_and_primary_bytes\": 0, \"current_replica_bytes\": 0 }, \"limit\": { \"current_coordinating_and_primary_limits_in_bytes\": 51897, \"current_replica_limits_in_bytes\": 77845 } } } }, \"total_rejections_breakup\": { \"node_limits\": 0, \"no_successful_request_limits\": 0, \"throughput_degradation_limits\": 0 }, \"enabled\": true, \"enforced\" : true } } } } . If enforced is false: . Example response . { \"_nodes\": { \"total\": 1, \"successful\": 1, \"failed\": 0 }, \"cluster_name\": \"runTask\", \"nodes\": { \"q3e1dQjFSqyPSLAgpyQlfw\": { \"timestamp\": 1613072111162, \"name\": \"runTask-0\", \"transport_address\": \"127.0.0.1:9300\", \"host\": \"127.0.0.1\", \"ip\": \"127.0.0.1:9300\", \"roles\": [ \"data\", \"ingest\", \"cluster_manager\", \"remote_cluster_client\" ], \"attributes\": { \"testattr\": \"test\" }, \"shard_indexing_pressure\": { \"stats\": { \"[index_name][0]\": { \"memory\": { \"current\": { \"coordinating_in_bytes\": 0, \"primary_in_bytes\": 0, \"replica_in_bytes\": 0 }, \"total\": { \"coordinating_in_bytes\": 299, \"primary_in_bytes\": 299, \"replica_in_bytes\": 0 } }, \"rejection\": { \"coordinating\": { \"coordinating_rejections\": 0, \"breakup_shadow_mode\": { \"node_limits\": 0, \"no_successful_request_limits\": 0, \"throughput_degradation_limits\": 0 } }, \"primary\": { \"primary_rejections\": 0, \"breakup_shadow_mode\": { \"node_limits\": 0, \"no_successful_request_limits\": 0, \"throughput_degradation_limits\": 0 } }, \"replica\": { \"replica_rejections\": 0, \"breakup_shadow_mode\": { \"node_limits\": 0, \"no_successful_request_limits\": 0, \"throughput_degradation_limits\": 0 } } }, \"last_successful_timestamp\": { \"coordinating_last_successful_request_timestamp_in_millis\": 1613072107990, \"primary_last_successful_request_timestamp_in_millis\": 0, \"replica_last_successful_request_timestamp_in_millis\": 0 }, \"indexing\": { \"coordinating_time_in_millis\": 96, \"coordinating_count\": 1, \"primary_time_in_millis\": 0, \"primary_count\": 0, \"replica_time_in_millis\": 0, \"replica_count\": 0 }, \"memory_allocation\": { \"current\": { \"current_coordinating_and_primary_bytes\": 0, \"current_replica_bytes\": 0 }, \"limit\": { \"current_coordinating_and_primary_limits_in_bytes\": 51897, \"current_replica_limits_in_bytes\": 77845 } } } }, \"total_rejections_breakup_shadow_mode\": { \"node_limits\": 0, \"no_successful_request_limits\": 0, \"throughput_degradation_limits\": 0 }, \"enabled\": true, \"enforced\" : false } } } } . To include all the shards with both active and previous write operations performed on them, specify the include_all parameter: . Request . GET _nodes/_local/stats/shard_indexing_pressure?include_all . Example response . { \"_nodes\": { \"total\": 1, \"successful\": 1, \"failed\": 0 }, \"cluster_name\": \"runTask\", \"nodes\": { \"q3e1dQjFSqyPSLAgpyQlfw\": { \"timestamp\": 1613072198171, \"name\": \"runTask-0\", \"transport_address\": \"127.0.0.1:9300\", \"host\": \"127.0.0.1\", \"ip\": \"127.0.0.1:9300\", \"roles\": [ \"data\", \"ingest\", \"cluster_manager\", \"remote_cluster_client\" ], \"attributes\": { \"testattr\": \"test\" }, \"shard_indexing_pressure\": { \"stats\": { \"[index_name][0]\": { \"memory\": { \"current\": { \"coordinating_in_bytes\": 0, \"primary_in_bytes\": 0, \"replica_in_bytes\": 0 }, \"total\": { \"coordinating_in_bytes\": 604, \"primary_in_bytes\": 604, \"replica_in_bytes\": 0 } }, \"rejection\": { \"coordinating\": { \"coordinating_rejections\": 0, \"breakup\": { \"node_limits\": 0, \"no_successful_request_limits\": 0, \"throughput_degradation_limits\": 0 } }, \"primary\": { \"primary_rejections\": 0, \"breakup\": { \"node_limits\": 0, \"no_successful_request_limits\": 0, \"throughput_degradation_limits\": 0 } }, \"replica\": { \"replica_rejections\": 0, \"breakup\": { \"node_limits\": 0, \"no_successful_request_limits\": 0, \"throughput_degradation_limits\": 0 } } }, \"last_successful_timestamp\": { \"coordinating_last_successful_request_timestamp_in_millis\": 1613072194656, \"primary_last_successful_request_timestamp_in_millis\": 0, \"replica_last_successful_request_timestamp_in_millis\": 0 }, \"indexing\": { \"coordinating_time_in_millis\": 145, \"coordinating_count\": 2, \"primary_time_in_millis\": 0, \"primary_count\": 0, \"replica_time_in_millis\": 0, \"replica_count\": 0 }, \"memory_allocation\": { \"current\": { \"current_coordinating_and_primary_bytes\": 0, \"current_replica_bytes\": 0 }, \"limit\": { \"current_coordinating_and_primary_limits_in_bytes\": 51897, \"current_replica_limits_in_bytes\": 77845 } } } }, \"total_rejections_breakup\": { \"node_limits\": 0, \"no_successful_request_limits\": 0, \"throughput_degradation_limits\": 0 }, \"enabled\": true, \"enforced\": true } } } } . To get only all the top-level aggregated stats, specify the top parameter (skips the per-shard stats). Request . GET _nodes/_local/stats/shard_indexing_pressure?top . If enforced is true: . Example response . { \"_nodes\": { \"total\": 1, \"successful\": 1, \"failed\": 0 }, \"cluster_name\": \"runTask\", \"nodes\": { \"q3e1dQjFSqyPSLAgpyQlfw\": { \"timestamp\": 1613072382719, \"name\": \"runTask-0\", \"transport_address\": \"127.0.0.1:9300\", \"host\": \"127.0.0.1\", \"ip\": \"127.0.0.1:9300\", \"roles\": [ \"data\", \"ingest\", \"cluster_manager\", \"remote_cluster_client\" ], \"attributes\": { \"testattr\": \"test\" }, \"shard_indexing_pressure\": { \"stats\": {}, \"total_rejections_breakup\": { \"node_limits\": 0, \"no_successful_request_limits\": 0, \"throughput_degradation_limits\": 0 }, \"enabled\": true, \"enforced\": true } } } } . If enforced is false: . Example response . { \"_nodes\": { \"total\": 1, \"successful\": 1, \"failed\": 0 }, \"cluster_name\": \"runTask\", \"nodes\": { \"q3e1dQjFSqyPSLAgpyQlfw\": { \"timestamp\": 1613072382719, \"name\": \"runTask-0\", \"transport_address\": \"127.0.0.1:9300\", \"host\": \"127.0.0.1\", \"ip\": \"127.0.0.1:9300\", \"roles\": [ \"data\", \"ingest\", \"cluster_manager\", \"remote_cluster_client\" ], \"attributes\": { \"testattr\": \"test\" }, \"shard_indexing_pressure\": { \"stats\": {}, \"total_rejections_breakup_shadow_mode\": { \"node_limits\": 0, \"no_successful_request_limits\": 0, \"throughput_degradation_limits\": 0 }, \"enabled\": true, \"enforced\" : false } } } } . To get the shard-level breakup of rejections for every node (only includes shards with active write operations): . Request . GET _nodes/stats/shard_indexing_pressure . Example response . { \"_nodes\": { \"total\": 1, \"successful\": 1, \"failed\": 0 }, \"cluster_name\": \"runTask\", \"nodes\": { \"q3e1dQjFSqyPSLAgpyQlfw\": { \"timestamp\": 1613072382719, \"name\": \"runTask-0\", \"transport_address\": \"127.0.0.1:9300\", \"host\": \"127.0.0.1\", \"ip\": \"127.0.0.1:9300\", \"roles\": [ \"data\", \"ingest\", \"cluster_manager\", \"remote_cluster_client\" ], \"attributes\": { \"testattr\": \"test\" }, \"shard_indexing_pressure\": { \"stats\": {}, \"total_rejections_breakup\": { \"node_limits\": 0, \"no_successful_request_limits\": 0, \"throughput_degradation_limits\": 0 }, \"enabled\": true, \"enforced\": true } } } } . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/availability-and-recovery/stats-api/#stats",
    "relUrl": "/tuning-your-cluster/availability-and-recovery/stats-api/#stats"
  },"486": {
    "doc": "Cluster manager task throttling",
    "title": "Cluster manager task throttling",
    "content": "For many cluster state updates, such as defining a mapping or creating an index, nodes submit tasks to the cluster manager. The cluster manager maintains a pending task queue for these tasks and runs them in a single-threaded environment. When nodes send tens of thousands of resource-intensive tasks, like put-mapping or snapshot tasks, these tasks can pile up in the queue and flood the cluster manager. This affects the cluster manager’s performance and may in turn affect the availability of the whole cluster. The first line of defense is to implement mechanisms in the caller nodes to avoid task overload on the cluster manager. However, even with those mechanisms in place, the cluster manager needs a built-in way to protect itself: cluster manager task throttling. To turn on cluster manager task throttling, you need to set throttling limits. The cluster manager uses the throttling limits to determine whether to reject a task. The cluster manager rejects a task based on its type. For any incoming task, the cluster manager evaluates the total number of tasks of the same type in the pending task queue. If this number exceeds the threshold for this task type, the cluster manager rejects the incoming task. Rejecting a task does not affect tasks of a different type. For example, if the cluster manager rejects a put-mapping task, it can still accept a subsequent create-index task. When the cluster manager rejects a task, the node performs retries with exponential backoff to resubmit the task to the cluster manager. If retries are unsuccessful within the timeout period, OpenSearch returns a cluster timeout error. ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/cluster-manager-task-throttling/",
    "relUrl": "/tuning-your-cluster/cluster-manager-task-throttling/"
  },"487": {
    "doc": "Cluster manager task throttling",
    "title": "Setting throttling limits",
    "content": "You can set throttling limits by specifying them in the cluster_manager.throttling.thresholds object and updating the OpenSearch cluster settings. The setting is dynamic, so you can change the behavior of this feature without restarting your cluster. By default, throttling is disabled for all task types. The request has the following format: . PUT _cluster/settings { \"persistent\": { \"cluster_manager.throttling.thresholds\" : { \"&lt;task-type&gt;\" : { \"value\" : &lt;threshold limit&gt; } } } } . The following table describes the cluster_manager.throttling.thresholds object. | Field Name | Description | . | task-type | The task type. See supported task types for a list of valid values. | . | value | The maximum number of tasks of the task-type type in the cluster manager’s pending task queue. Default is -1 (no task throttling). | . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/cluster-manager-task-throttling/#setting-throttling-limits",
    "relUrl": "/tuning-your-cluster/cluster-manager-task-throttling/#setting-throttling-limits"
  },"488": {
    "doc": "Cluster manager task throttling",
    "title": "Supported task types",
    "content": "The following task types are supported: . | create-index | update-settings | cluster-update-settings | auto-create | delete-index | delete-dangling-index | create-data-stream | remove-data-stream | rollover-index | index-aliases | put-mapping | create-index-template | remove-index-template | create-component-template | remove-component-template | create-index-template-v2 | remove-index-template-v2 | put-pipeline | delete-pipeline | create-persistent-task | finish-persistent-task | remove-persistent-task | update-task-state | put-script | delete-script | put-repository | delete-repository | create-snapshot | delete-snapshot | update-snapshot-state | restore-snapshot | cluster-reroute-api | . Example request . The following request sets the throttling threshold for the put-mapping task type to 100: . PUT _cluster/settings { \"persistent\": { \"cluster_manager.throttling.thresholds\": { \"put-mapping\": { \"value\": 100 } } } } . Set the threshold to -1 to disable throttling for a task type. ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/cluster-manager-task-throttling/#supported-task-types",
    "relUrl": "/tuning-your-cluster/cluster-manager-task-throttling/#supported-task-types"
  },"489": {
    "doc": "Creating a cluster",
    "title": "Creating a cluster",
    "content": "Before diving into OpenSearch and searching and aggregating data, you first need to create an OpenSearch cluster. OpenSearch can operate as a single-node or multi-node cluster. The steps to configure both are, in general, quite similar. This page demonstrates how to create and configure a multi-node cluster, but with only a few minor adjustments, you can follow the same steps to create a single-node cluster. To create and deploy an OpenSearch cluster according to your requirements, it’s important to understand how node discovery and cluster formation work and what settings govern them. There are many ways to design a cluster. The following illustration shows a basic architecture that includes a four-node cluster that has one dedicated cluster manager node, one dedicated coordinating node, and two data nodes that are cluster manager eligible and also used for ingesting data. The nomenclature recently changed for the master node; it is now called the cluster manager node. Nodes . The following table provides brief descriptions of the node types: . | Node type | Description | Best practices for production | . | Cluster manager | Manages the overall operation of a cluster and keeps track of the cluster state. This includes creating and deleting indexes, keeping track of the nodes that join and leave the cluster, checking the health of each node in the cluster (by running ping requests), and allocating shards to nodes. | Three dedicated cluster manager nodes in three different zones is the right approach for almost all production use cases. This configuration ensures your cluster never loses quorum. Two nodes will be idle for most of the time except when one node goes down or needs some maintenance. | . | Cluster manager eligible | Elects one node among them as the cluster manager node through a voting process. | For production clusters, make sure you have dedicated cluster manager nodes. The way to achieve a dedicated node type is to mark all other node types as false. In this case, you have to mark all the other nodes as not cluster manager eligible. | . | Data | Stores and searches data. Performs all data-related operations (indexing, searching, aggregating) on local shards. These are the worker nodes of your cluster and need more disk space than any other node type. | As you add data nodes, keep them balanced between zones. For example, if you have three zones, add data nodes in multiples of three, one for each zone. We recommend using storage and RAM-heavy nodes. | . | Ingest | Pre-processes data before storing it in the cluster. Runs an ingest pipeline that transforms your data before adding it to an index. | If you plan to ingest a lot of data and run complex ingest pipelines, we recommend you use dedicated ingest nodes. You can also optionally offload your indexing from the data nodes so that your data nodes are used exclusively for searching and aggregating. | . | Coordinating | Delegates client requests to the shards on the data nodes, collects and aggregates the results into one final result, and sends this result back to the client. | A couple of dedicated coordinating-only nodes is appropriate to prevent bottlenecks for search-heavy workloads. We recommend using CPUs with as many cores as you can. | . | Dynamic | Delegates a specific node for custom work, such as machine learning (ML) tasks, preventing the consumption of resources from data nodes and therefore not affecting any OpenSearch functionality. |   | . By default, each node is a cluster-manager-eligible, data, ingest, and coordinating node. Deciding on the number of nodes, assigning node types, and choosing the hardware for each node type depends on your use case. You must take into account factors like the amount of time you want to hold on to your data, the average size of your documents, your typical workload (indexing, searches, aggregations), your expected price-performance ratio, your risk tolerance, and so on. After you assess all these requirements, we recommend you use a benchmark testing tool like OpenSearch Benchmark to provision a small sample cluster and run tests with varying workloads and configurations. Compare and analyze the system and query metrics for these tests to design an optimum architecture. This page demonstrates how to work with the different node types. It assumes that you have a four-node cluster similar to the preceding illustration. ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/index/",
    "relUrl": "/tuning-your-cluster/index/"
  },"490": {
    "doc": "Creating a cluster",
    "title": "Prerequisites",
    "content": "Before you get started, you must install and configure OpenSearch on all of your nodes. For information about the available options, see Install and configure OpenSearch. After you’re done, use SSH to connect to each node, then open the config/opensearch.yml file. You can set all configurations for your cluster in this file. ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/index/#prerequisites",
    "relUrl": "/tuning-your-cluster/index/#prerequisites"
  },"491": {
    "doc": "Creating a cluster",
    "title": "Step 1: Name a cluster",
    "content": "Specify a unique name for the cluster. If you don’t specify a cluster name, it’s set to opensearch by default. Setting a descriptive cluster name is important, especially if you want to run multiple clusters inside a single network. To specify the cluster name, change the following line: . #cluster.name: my-application . to . cluster.name: opensearch-cluster . Make the same change on all the nodes to make sure that they’ll join to form a cluster. ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/index/#step-1-name-a-cluster",
    "relUrl": "/tuning-your-cluster/index/#step-1-name-a-cluster"
  },"492": {
    "doc": "Creating a cluster",
    "title": "Step 2: Set node attributes for each node in a cluster",
    "content": "After you name the cluster, set node attributes for each node in your cluster. Cluster manager node . Give your cluster manager node a name. If you don’t specify a name, OpenSearch assigns a machine-generated name that makes the node difficult to monitor and troubleshoot. node.name: opensearch-cluster_manager . You can also explicitly specify that this node is a cluster manager node, even though it is already set to true by default. Set the node role to cluster_manager to make it easier to identify the cluster manager node. node.roles: [ cluster_manager ] . Data nodes . Change the name of two nodes to opensearch-d1 and opensearch-d2, respectively: . node.name: opensearch-d1 . node.name: opensearch-d2 . You can make them cluster-manager-eligible data nodes that will also be used for ingesting data: . node.roles: [ data, ingest ] . You can also specify any other attributes that you’d like to set for the data nodes. Coordinating node . Change the name of the coordinating node to opensearch-c1: . node.name: opensearch-c1 . Every node is a coordinating node by default, so to make this node a dedicated coordinating node, set node.roles to an empty list: . node.roles: [] . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/index/#step-2-set-node-attributes-for-each-node-in-a-cluster",
    "relUrl": "/tuning-your-cluster/index/#step-2-set-node-attributes-for-each-node-in-a-cluster"
  },"493": {
    "doc": "Creating a cluster",
    "title": "Step 3: Bind a cluster to specific IP addresses",
    "content": "network_host defines the IP address used to bind the node. By default, OpenSearch listens on a local host, which limits the cluster to a single node. You can also use _local_ and _site_ to bind to any loopback or site-local address, whether IPv4 or IPv6: . network.host: [_local_, _site_] . To form a multi-node cluster, specify the IP address of the node: . network.host: &lt;IP address of the node&gt; . Make sure to configure these settings on all of your nodes. ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/index/#step-3-bind-a-cluster-to-specific-ip-addresses",
    "relUrl": "/tuning-your-cluster/index/#step-3-bind-a-cluster-to-specific-ip-addresses"
  },"494": {
    "doc": "Creating a cluster",
    "title": "Step 4: Configure discovery hosts for a cluster",
    "content": "Now that you’ve configured the network hosts, you need to configure the discovery hosts. Zen Discovery is the built-in, default mechanism that uses unicast to find other nodes in the cluster. You can generally just add all of your cluster-manager-eligible nodes to the discovery.seed_hosts array. When a node starts up, it finds the other cluster-manager-eligible nodes, determines which one is the cluster manager, and asks to join the cluster. For example, for opensearch-cluster_manager the line looks something like this: . discovery.seed_hosts: [\"&lt;private IP of opensearch-d1&gt;\", \"&lt;private IP of opensearch-d2&gt;\", \"&lt;private IP of opensearch-c1&gt;\"] . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/index/#step-4-configure-discovery-hosts-for-a-cluster",
    "relUrl": "/tuning-your-cluster/index/#step-4-configure-discovery-hosts-for-a-cluster"
  },"495": {
    "doc": "Creating a cluster",
    "title": "Step 5: Start the cluster",
    "content": "After you set the configurations, start OpenSearch on all nodes: . sudo systemctl start opensearch.service . Installing OpenSearch from a tar archive will not automatically create a service with systemd. See Run OpenSearch as a service with systemd for instructions on how to create and start the service if you receive an error like Failed to start opensearch.service: Unit not found. Then go to the logs file to see the formation of the cluster: . less /var/log/opensearch/opensearch-cluster.log . Perform the following _cat query on any node to see all the nodes formed as a cluster: . curl -XGET https://&lt;private-ip&gt;:9200/_cat/nodes?v -u 'admin:admin' --insecure . ip heap.percent ram.percent cpu load_1m load_5m load_15m node.role cluster_manager name x.x.x.x 13 61 0 0.02 0.04 0.05 mi * opensearch-cluster_manager x.x.x.x 16 60 0 0.06 0.05 0.05 md - opensearch-d1 x.x.x.x 34 38 0 0.12 0.07 0.06 md - opensearch-d2 x.x.x.x 23 38 0 0.12 0.07 0.06 md - opensearch-c1 . To better understand and monitor your cluster, use the CAT API. ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/index/#step-5-start-the-cluster",
    "relUrl": "/tuning-your-cluster/index/#step-5-start-the-cluster"
  },"496": {
    "doc": "Creating a cluster",
    "title": "(Advanced) Step 6: Configure shard allocation awareness or forced awareness",
    "content": "Shard allocation awareness . If your nodes are spread across several geographical zones, you can configure shard allocation awareness to allocate all replica shards to a zone that’s different from their primary shard. With shard allocation awareness, if the nodes in one of your zones fail, you can be assured that your replica shards are spread across your other zones. It adds a layer of fault tolerance to ensure your data survives a zone failure beyond just individual node failures. To configure shard allocation awareness, add zone attributes to opensearch-d1 and opensearch-d2, respectively: . node.attr.zone: zoneA . node.attr.zone: zoneB . Update the cluster settings: . PUT _cluster/settings { \"persistent\": { \"cluster.routing.allocation.awareness.attributes\": \"zone\" } } . You can either use persistent or transient settings. We recommend the persistent setting because it persists through a cluster reboot. Transient settings don’t persist through a cluster reboot. Shard allocation awareness attempts to separate primary and replica shards across multiple zones. However, if only one zone is available (such as after a zone failure), OpenSearch allocates replica shards to the only remaining zone. Forced awareness . Another option is to require that primary and replica shards are never allocated to the same zone. This is called forced awareness. To configure forced awareness, specify all the possible values for your zone attributes: . PUT _cluster/settings { \"persistent\": { \"cluster.routing.allocation.awareness.attributes\": \"zone\", \"cluster.routing.allocation.awareness.force.zone.values\":[\"zoneA\", \"zoneB\"] } } . Now, if a data node fails, forced awareness doesn’t allocate the replicas to a node in the same zone. Instead, the cluster enters a yellow state and only allocates the replicas when nodes in another zone come online. In our two-zone architecture, we can use allocation awareness if opensearch-d1 and opensearch-d2 are less than 50% utilized, so that each of them have the storage capacity to allocate replicas in the same zone. If that is not the case, and opensearch-d1 and opensearch-d2 do not have the capacity to contain all primary and replica shards, we can use forced awareness. This approach helps to make sure that, in the event of a failure, OpenSearch doesn’t overload your last remaining zone and lock up your cluster due to lack of storage. Choosing allocation awareness or forced awareness depends on how much space you might need in each zone to balance your primary and replica shards. Replica count enforcement . To enforce an even distribution of shards across all zones and avoid hotspots, you can set the routing.allocation.awareness.balance attribute to true. This setting can be configured in the opensearch.yml file and dynamically updated using the cluster update settings API: . PUT _cluster/settings { \"persistent\": { \"cluster\": { \"routing.allocation.awareness.balance\": \"true\" } } } . The routing.allocation.awareness.balance setting is false by default. When it is set to true, the total number of shards for the index must be a multiple of the highest count for any awareness attribute. For example, consider a configuration with two awareness attributes—zones and rack IDs. Let’s say there are two zones and three rack IDs. The highest count of either the number of zones or the number of rack IDs is three. Therefore, the number of shards must be a multiple of three. If it is not, OpenSearch throws a validation exception. routing.allocation.awareness.balance takes effect only if cluster.routing.allocation.awareness.attributes and cluster.routing.allocation.awareness.force.zone.values are set. routing.allocation.awareness.balance applies to all operations that create or update indices. For example, let’s say you’re running a cluster with three nodes and three zones in a zone-aware setting. If you try to create an index with one replica or update an index’s settings to one replica, the attempt will fail with a validation exception because the number of shards must be a multiple of three. Similarly, if you try to create an index template with one shard and no replicas, the attempt will fail for the same reason. However, in all of those operations, if you set the number of shards to one and the number of replicas to two, the total number of shards is three and the attempt will succeed. ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/index/#advanced-step-6-configure-shard-allocation-awareness-or-forced-awareness",
    "relUrl": "/tuning-your-cluster/index/#advanced-step-6-configure-shard-allocation-awareness-or-forced-awareness"
  },"497": {
    "doc": "Creating a cluster",
    "title": "(Advanced) Step 7: Set up a hot-warm architecture",
    "content": "You can design a hot-warm architecture where you first index your data to hot nodes—fast and expensive—and after a certain period of time move them to warm nodes—slow and cheap. If you analyze time series data that you rarely update and want the older data to go onto cheaper storage, this architecture can be a good fit. This architecture helps save money on storage costs. Rather than increasing the number of hot nodes and using fast, expensive storage, you can add warm nodes for data that you don’t access as frequently. To configure a hot-warm storage architecture, add temp attributes to opensearch-d1 and opensearch-d2, respectively: . node.attr.temp: hot . node.attr.temp: warm . You can set the attribute name and value to whatever you want as long as it’s consistent for all your hot and warm nodes. To add an index newindex to the hot node: . PUT newindex { \"settings\": { \"index.routing.allocation.require.temp\": \"hot\" } } . Take a look at the following shard allocation for newindex: . GET _cat/shards/newindex?v index shard prirep state docs store ip node new_index 2 p STARTED 0 230b 10.0.0.225 opensearch-d1 new_index 2 r UNASSIGNED new_index 3 p STARTED 0 230b 10.0.0.225 opensearch-d1 new_index 3 r UNASSIGNED new_index 4 p STARTED 0 230b 10.0.0.225 opensearch-d1 new_index 4 r UNASSIGNED new_index 1 p STARTED 0 230b 10.0.0.225 opensearch-d1 new_index 1 r UNASSIGNED new_index 0 p STARTED 0 230b 10.0.0.225 opensearch-d1 new_index 0 r UNASSIGNED . In this example, all primary shards are allocated to opensearch-d1, which is our hot node. All replica shards are unassigned, because we’re forcing this index to allocate only to hot nodes. To add an index oldindex to the warm node: . PUT oldindex { \"settings\": { \"index.routing.allocation.require.temp\": \"warm\" } } . The shard allocation for oldindex: . GET _cat/shards/oldindex?v index shard prirep state docs store ip node old_index 2 p STARTED 0 230b 10.0.0.74 opensearch-d2 old_index 2 r UNASSIGNED old_index 3 p STARTED 0 230b 10.0.0.74 opensearch-d2 old_index 3 r UNASSIGNED old_index 4 p STARTED 0 230b 10.0.0.74 opensearch-d2 old_index 4 r UNASSIGNED old_index 1 p STARTED 0 230b 10.0.0.74 opensearch-d2 old_index 1 r UNASSIGNED old_index 0 p STARTED 0 230b 10.0.0.74 opensearch-d2 old_index 0 r UNASSIGNED . In this case, all primary shards are allocated to opensearch-d2. Again, all replica shards are unassigned because we only have one warm node. A popular approach is to configure your index templates to set the index.routing.allocation.require.temp value to hot. This way, OpenSearch stores your most recent data on your hot nodes. You can then use the Index State Management (ISM) plugin to periodically check the age of an index and specify actions to take on it. For example, when the index reaches a specific age, change the index.routing.allocation.require.temp setting to warm to automatically move your data from hot nodes to warm nodes. ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/index/#advanced-step-7-set-up-a-hot-warm-architecture",
    "relUrl": "/tuning-your-cluster/index/#advanced-step-7-set-up-a-hot-warm-architecture"
  },"498": {
    "doc": "Creating a cluster",
    "title": "Next steps",
    "content": "If you are using the Security plugin, the previous request to _cat/nodes?v might have failed with an initialization error. For full guidance around using the Security plugin, see Security configuration. ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/index/#next-steps",
    "relUrl": "/tuning-your-cluster/index/#next-steps"
  },"499": {
    "doc": "API",
    "title": "Cross-cluster replication API",
    "content": "Use these replication operations to programmatically manage cross-cluster replication. | Start replication | Stop replication | Pause replication | Resume replication | Get replication status | Get leader cluster stats | Get follower cluster stats | Get auto-follow stats | Update settings | Create replication rule | Delete replication rule | . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/replication-plugin/api/#cross-cluster-replication-api",
    "relUrl": "/tuning-your-cluster/replication-plugin/api/#cross-cluster-replication-api"
  },"500": {
    "doc": "API",
    "title": "Start replication",
    "content": "Introduced 1.1 . Initiate replication of an index from the leader cluster to the follower cluster. Send this request to the follower cluster. Request . PUT /_plugins/_replication/&lt;follower-index&gt;/_start { \"leader_alias\":\"&lt;connection-alias-name&gt;\", \"leader_index\":\"&lt;index-name&gt;\", \"use_roles\":{ \"leader_cluster_role\":\"&lt;role-name&gt;\", \"follower_cluster_role\":\"&lt;role-name&gt;\" } } . Specify the following options: . | Options | Description | Type | Required | . | leader_alias | The name of the cross-cluster connection. You define this alias when you set up a cross-cluster connection. | string | Yes | . | leader_index | The index on the leader cluster that you want to replicate. | string | Yes | . | use_roles | The roles to use for all subsequent backend replication tasks between the indexes. Specify a leader_cluster_role and follower_cluster_role. See Map the leader and follower cluster roles. | string | If Security plugin is enabled | . Example response . { \"acknowledged\": true } . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/replication-plugin/api/#start-replication",
    "relUrl": "/tuning-your-cluster/replication-plugin/api/#start-replication"
  },"501": {
    "doc": "API",
    "title": "Stop replication",
    "content": "Introduced 1.1 . Terminates replication and converts the follower index to a standard index. Send this request to the follower cluster. Request . POST /_plugins/_replication/&lt;follower-index&gt;/_stop {} . Example response . { \"acknowledged\": true } . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/replication-plugin/api/#stop-replication",
    "relUrl": "/tuning-your-cluster/replication-plugin/api/#stop-replication"
  },"502": {
    "doc": "API",
    "title": "Pause replication",
    "content": "Introduced 1.1 . Pauses replication of the leader index. Send this request to the follower cluster. Request . POST /_plugins/_replication/&lt;follower-index&gt;/_pause {} . You can’t resume replication after it’s been paused for more than 12 hours. You must stop replication, delete the follower index, and restart replication of the leader. Example response . { \"acknowledged\": true } . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/replication-plugin/api/#pause-replication",
    "relUrl": "/tuning-your-cluster/replication-plugin/api/#pause-replication"
  },"503": {
    "doc": "API",
    "title": "Resume replication",
    "content": "Introduced 1.1 . Resumes replication of the leader index. Send this request to the follower cluster. Request . POST /_plugins/_replication/&lt;follower-index&gt;/_resume {} . Example response . { \"acknowledged\": true } . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/replication-plugin/api/#resume-replication",
    "relUrl": "/tuning-your-cluster/replication-plugin/api/#resume-replication"
  },"504": {
    "doc": "API",
    "title": "Get replication status",
    "content": "Introduced 1.1 . Gets the status of index replication. Possible statuses are SYNCING, BOOTSTRAPING, PAUSED, and REPLICATION NOT IN PROGRESS. Use the syncing details to measure replication lag. Send this request to the follower cluster. Request . GET /_plugins/_replication/&lt;follower-index&gt;/_status . Example response . { \"status\" : \"SYNCING\", \"reason\" : \"User initiated\", \"leader_alias\" : \"my-connection-name\", \"leader_index\" : \"leader-01\", \"follower_index\" : \"follower-01\", \"syncing_details\" : { \"leader_checkpoint\" : 19, \"follower_checkpoint\" : 19, \"seq_no\" : 0 } } . To include shard replication details in the response, add the &amp;verbose=true parameter. The leader and follower checkpoint values begin as negative integers and reflect the shard count (-1 for one shard, -5 for five shards, and so on). The values increment toward positive integers with each change that you make. For example, when you make a change on the leader index, the leader_checkpoint becomes 0. The follower_checkpoint is initially still -1 until the follower index pulls the change from the leader, at which point it increments to 0. If the values are the same, it means the indexes are fully synced. ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/replication-plugin/api/#get-replication-status",
    "relUrl": "/tuning-your-cluster/replication-plugin/api/#get-replication-status"
  },"505": {
    "doc": "API",
    "title": "Get leader cluster stats",
    "content": "Introduced 1.1 . Gets information about replicated leader indexes on a specified cluster. Request . GET /_plugins/_replication/leader_stats . Example response . { \"num_replicated_indices\": 2, \"operations_read\": 15, \"translog_size_bytes\": 1355, \"operations_read_lucene\": 0, \"operations_read_translog\": 15, \"total_read_time_lucene_millis\": 0, \"total_read_time_translog_millis\": 659, \"bytes_read\": 1000, \"index_stats\":{ \"leader-index-1\":{ \"operations_read\": 7, \"translog_size_bytes\": 639, \"operations_read_lucene\": 0, \"operations_read_translog\": 7, \"total_read_time_lucene_millis\": 0, \"total_read_time_translog_millis\": 353, \"bytes_read\":466 }, \"leader-index-2\":{ \"operations_read\": 8, \"translog_size_bytes\": 716, \"operations_read_lucene\": 0, \"operations_read_translog\": 8, \"total_read_time_lucene_millis\": 0, \"total_read_time_translog_millis\": 306, \"bytes_read\": 534 } } } . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/replication-plugin/api/#get-leader-cluster-stats",
    "relUrl": "/tuning-your-cluster/replication-plugin/api/#get-leader-cluster-stats"
  },"506": {
    "doc": "API",
    "title": "Get follower cluster stats",
    "content": "Introduced 1.1 . Gets information about follower (syncing) indexes on a specified cluster. Request . GET /_plugins/_replication/follower_stats . Example response . { \"num_syncing_indices\": 2, \"num_bootstrapping_indices\": 0, \"num_paused_indices\": 0, \"num_failed_indices\": 0, \"num_shard_tasks\": 2, \"num_index_tasks\": 2, \"operations_written\": 3, \"operations_read\": 3, \"failed_read_requests\": 0, \"throttled_read_requests\": 0, \"failed_write_requests\": 0, \"throttled_write_requests\": 0, \"follower_checkpoint\": 1, \"leader_checkpoint\": 1, \"total_write_time_millis\": 2290, \"index_stats\":{ \"follower-index-1\":{ \"operations_written\": 2, \"operations_read\": 2, \"failed_read_requests\": 0, \"throttled_read_requests\": 0, \"failed_write_requests\": 0, \"throttled_write_requests\": 0, \"follower_checkpoint\": 1, \"leader_checkpoint\": 1, \"total_write_time_millis\": 1355 }, \"follower-index-2\":{ \"operations_written\": 1, \"operations_read\": 1, \"failed_read_requests\": 0, \"throttled_read_requests\": 0, \"failed_write_requests\": 0, \"throttled_write_requests\": 0, \"follower_checkpoint\": 0, \"leader_checkpoint\": 0, \"total_write_time_millis\": 935 } } } . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/replication-plugin/api/#get-follower-cluster-stats",
    "relUrl": "/tuning-your-cluster/replication-plugin/api/#get-follower-cluster-stats"
  },"507": {
    "doc": "API",
    "title": "Get auto-follow stats",
    "content": "Introduced 1.1 . Gets information about auto-follow activity and any replication rules configured on the specified cluster. Request . GET /_plugins/_replication/autofollow_stats . Example response . { \"num_success_start_replication\": 2, \"num_failed_start_replication\": 0, \"num_failed_leader_calls\": 0, \"failed_indices\":[ ], \"autofollow_stats\":[ { \"name\":\"my-replication-rule\", \"pattern\":\"movies*\", \"num_success_start_replication\": 2, \"num_failed_start_replication\": 0, \"num_failed_leader_calls\": 0, \"failed_indices\":[ ] } ] } . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/replication-plugin/api/#get-auto-follow-stats",
    "relUrl": "/tuning-your-cluster/replication-plugin/api/#get-auto-follow-stats"
  },"508": {
    "doc": "API",
    "title": "Update settings",
    "content": "Introduced 1.1 . Updates settings on the follower index. Request . PUT /_plugins/_replication/&lt;follower-index&gt;/_update { \"settings\":{ \"index.number_of_shards\": 4, \"index.number_of_replicas\": 2 } } . Example response . { \"acknowledged\": true } . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/replication-plugin/api/#update-settings",
    "relUrl": "/tuning-your-cluster/replication-plugin/api/#update-settings"
  },"509": {
    "doc": "API",
    "title": "Create replication rule",
    "content": "Introduced 1.1 . Automatically starts replication on indexes matching a specified pattern. If a new index on the leader cluster matches the pattern, OpenSearch automatically creates a follower index and begins replication. You can also use this API to update existing replication rules. Send this request to the follower cluster. Make sure to note the names of all auto-follow patterns after you create them. The replication plugin currently does not include an API operation to retrieve a list of existing patterns. Request . POST /_plugins/_replication/_autofollow { \"leader_alias\" : \"&lt;connection-alias-name&gt;\", \"name\": \"&lt;auto-follow-pattern-name&gt;\", \"pattern\": \"&lt;pattern&gt;\", \"use_roles\":{ \"leader_cluster_role\": \"&lt;role-name&gt;\", \"follower_cluster_role\": \"&lt;role-name&gt;\" } } . Specify the following options: . | Options | Description | Type | Required | . | leader_alias | The name of the cross-cluster connection. You define this alias when you set up a cross-cluster connection. | string | Yes | . | name | A name for the auto-follow pattern. | string | Yes | . | pattern | An array of index patterns to match against indexes in the specified leader cluster. Supports wildcard characters. For example, leader-*. | string | Yes | . | use_roles | The roles to use for all subsequent backend replication tasks between the indexes. Specify a leader_cluster_role and follower_cluster_role. See Map the leader and follower cluster roles. | string | If Security plugin is enabled | . Example response . { \"acknowledged\": true } . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/replication-plugin/api/#create-replication-rule",
    "relUrl": "/tuning-your-cluster/replication-plugin/api/#create-replication-rule"
  },"510": {
    "doc": "API",
    "title": "Delete replication rule",
    "content": "Introduced 1.1 . Deletes the specified replication rule. This operation prevents any new indexes from being replicated but does not stop existing replication that the rule has already initiated. Replicated indexes remain read-only until you stop replication. Send this request to the follower cluster. Request . DELETE /_plugins/_replication/_autofollow { \"leader_alias\" : \"&lt;connection-alias-name&gt;\", \"name\": \"&lt;auto-follow-pattern-name&gt;\", } . Specify the following options: . | Options | Description | Type | Required | . | leader_alias | The name of the cross-cluster connection. You define this alias when you set up a cross-cluster connection. | string | Yes | . | name | The name of the pattern. | string | Yes | . Example response . { \"acknowledged\": true } . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/replication-plugin/api/#delete-replication-rule",
    "relUrl": "/tuning-your-cluster/replication-plugin/api/#delete-replication-rule"
  },"511": {
    "doc": "API",
    "title": "API",
    "content": " ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/replication-plugin/api/",
    "relUrl": "/tuning-your-cluster/replication-plugin/api/"
  },"512": {
    "doc": "Auto-follow",
    "title": "Auto-follow for cross-cluster replication",
    "content": "Auto-follow lets you automatically replicate indexes created on the leader cluster based on matching patterns. When you create an index on the leader cluster with a name that matches a specified pattern (for example, index-01*), a corresponding follower index is automatically created on the follower cluster. You can configure multiple replication rules for a single cluster. The patterns currently only support wildcard matching. ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/replication-plugin/auto-follow/#auto-follow-for-cross-cluster-replication",
    "relUrl": "/tuning-your-cluster/replication-plugin/auto-follow/#auto-follow-for-cross-cluster-replication"
  },"513": {
    "doc": "Auto-follow",
    "title": "Prerequisites",
    "content": "You need to set up a cross-cluster connection between two clusters before you can enable auto-follow. ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/replication-plugin/auto-follow/#prerequisites",
    "relUrl": "/tuning-your-cluster/replication-plugin/auto-follow/#prerequisites"
  },"514": {
    "doc": "Auto-follow",
    "title": "Permissions",
    "content": "If the Security plugin is enabled, make sure that non-admin users are mapped to the appropriate permissions so they can perform replication actions. For index and cluster-level permissions requirements, see Cross-cluster replication permissions. ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/replication-plugin/auto-follow/#permissions",
    "relUrl": "/tuning-your-cluster/replication-plugin/auto-follow/#permissions"
  },"515": {
    "doc": "Auto-follow",
    "title": "Get started with auto-follow",
    "content": "Replication rules are a collection of patterns that you create against a single follower cluster. When you create a replication rule, it starts by automatically replicating any existing indexes that match the pattern. It will then continue to replicate any new indexes that you create that match the pattern. Create a replication rule on the follower cluster: . curl -XPOST -k -H 'Content-Type: application/json' -u 'admin:admin' 'https://localhost:9200/_plugins/_replication/_autofollow?pretty' -d ' { \"leader_alias\" : \"my-connection-alias\", \"name\": \"my-replication-rule\", \"pattern\": \"movies*\", \"use_roles\":{ \"leader_cluster_role\": \"all_access\", \"follower_cluster_role\": \"all_access\" } }' . If the Security plugin is disabled, you can leave out the use_roles parameter. If it’s enabled, however, you need to specify the leader and follower cluster roles that OpenSearch uses to authenticate requests. This example uses all_access for simplicity, but we recommend creating a replication user on each cluster and mapping it accordingly. To test the rule, create a matching index on the leader cluster: . curl -XPUT -k -H 'Content-Type: application/json' -u 'admin:admin' 'https://localhost:9201/movies-0001?pretty' . And confirm its replica shows up on the follower cluster: . curl -XGET -u 'admin:admin' -k 'https://localhost:9200/_cat/indices?v' . It might take several seconds for the index to appear. health status index uuid pri rep docs.count docs.deleted store.size pri.store.size yellow open movies-0001 kHOxYYHxRMeszLjTD9rvSQ 1 1 0 0 208b 208b . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/replication-plugin/auto-follow/#get-started-with-auto-follow",
    "relUrl": "/tuning-your-cluster/replication-plugin/auto-follow/#get-started-with-auto-follow"
  },"516": {
    "doc": "Auto-follow",
    "title": "Retrieve replication rules",
    "content": "To retrieve a list of existing replication rules that are configured on a cluster, send the following request: . curl -XGET -u 'admin:admin' -k 'https://localhost:9200/_plugins/_replication/autofollow_stats' { \"num_success_start_replication\": 1, \"num_failed_start_replication\": 0, \"num_failed_leader_calls\": 0, \"failed_indices\":[ ], \"autofollow_stats\":[ { \"name\":\"my-replication-rule\", \"pattern\":\"movies*\", \"num_success_start_replication\": 1, \"num_failed_start_replication\": 0, \"num_failed_leader_calls\": 0, \"failed_indices\":[ ] } ] } . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/replication-plugin/auto-follow/#retrieve-replication-rules",
    "relUrl": "/tuning-your-cluster/replication-plugin/auto-follow/#retrieve-replication-rules"
  },"517": {
    "doc": "Auto-follow",
    "title": "Delete a replication rule",
    "content": "To delete a replication rule, send the following request to the follower cluster: . curl -XDELETE -k -H 'Content-Type: application/json' -u 'admin:admin' 'https://localhost:9200/_plugins/_replication/_autofollow?pretty' -d ' { \"leader_alias\" : \"my-conection-alias\", \"name\": \"my-replication-rule\" }' . When you delete a replication rule, OpenSearch stops replicating new indexes that match the pattern, but existing indexes that the rule previously created remain read-only and continue to replicate. If you need to stop existing replication activity and open the indexes up for writes, use the stop replication API operation. ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/replication-plugin/auto-follow/#delete-a-replication-rule",
    "relUrl": "/tuning-your-cluster/replication-plugin/auto-follow/#delete-a-replication-rule"
  },"518": {
    "doc": "Auto-follow",
    "title": "Auto-follow",
    "content": " ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/replication-plugin/auto-follow/",
    "relUrl": "/tuning-your-cluster/replication-plugin/auto-follow/"
  },"519": {
    "doc": "Getting started",
    "title": "Getting started with cross-cluster replication",
    "content": "With cross-cluster replication, you index data to a leader index, and OpenSearch replicates that data to one or more read-only follower indexes. All subsequent operations on the leader are replicated on the follower, such as creating, updating, or deleting documents. ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/replication-plugin/getting-started/#getting-started-with-cross-cluster-replication",
    "relUrl": "/tuning-your-cluster/replication-plugin/getting-started/#getting-started-with-cross-cluster-replication"
  },"520": {
    "doc": "Getting started",
    "title": "Prerequisites",
    "content": "Cross-cluster replication has the following prerequisites: . | Both the leader and follower cluster must have the replication plugin installed. | If you’ve overridden node.roles in opensearch.yml on the follower cluster, make sure it also includes the remote_cluster_client role: . node.roles: [&lt;other_roles&gt;, remote_cluster_client] . | . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/replication-plugin/getting-started/#prerequisites",
    "relUrl": "/tuning-your-cluster/replication-plugin/getting-started/#prerequisites"
  },"521": {
    "doc": "Getting started",
    "title": "Permissions",
    "content": "Make sure the Security plugin is either enabled on both clusters or disabled on both clusters. If you disabled the Security plugin, you can skip this section. However, we strongly recommend enabling the Security plugin in production scenarios. If the Security plugin is enabled, make sure that non-admin users are mapped to the appropriate permissions so they can perform replication actions. For index and cluster-level permissions requirements, see Cross-cluster replication permissions. In addition, verify and add the distinguished names (DNs) of each follower cluster node on the leader cluster to allow connections from the followers to the leader. First, get the node’s DN from each follower cluster: . curl -XGET -k -u 'admin:admin' 'https://localhost:9200/_opendistro/_security/api/ssl/certs?pretty' { \"transport_certificates_list\": [ { \"issuer_dn\" : \"CN=Test,OU=Server CA 1B,O=Test,C=US\", \"subject_dn\" : \"CN=follower.test.com\", # To be added under leader's nodes_dn configuration \"not_before\" : \"2021-11-12T00:00:00Z\", \"not_after\" : \"2022-12-11T23:59:59Z\" } ] } . Then verify that it’s part of the leader cluster configuration in opensearch.yml. Otherwise, add it under the following setting: . plugins.security.nodes_dn: - \"CN=*.leader.com, OU=SSL, O=Test, L=Test, C=DE\" # Already part of the configuration - \"CN=follower.test.com\" # From the above response from follower . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/replication-plugin/getting-started/#permissions",
    "relUrl": "/tuning-your-cluster/replication-plugin/getting-started/#permissions"
  },"522": {
    "doc": "Getting started",
    "title": "Example setup",
    "content": "To start two single-node clusters on the same network, save this sample file as docker-compose.yml and run docker-compose up: . version: '3' services: replication-node1: image: opensearchproject/opensearch:2.7.0 container_name: replication-node1 environment: - cluster.name=leader-cluster - discovery.type=single-node - bootstrap.memory_lock=true - \"OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m\" ulimits: memlock: soft: -1 hard: -1 volumes: - opensearch-data2:/usr/share/opensearch/data ports: - 9201:9200 - 9700:9600 # required for Performance Analyzer networks: - opensearch-net replication-node2: image: opensearchproject/opensearch:2.7.0 container_name: replication-node2 environment: - cluster.name=follower-cluster - discovery.type=single-node - bootstrap.memory_lock=true - \"OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m\" ulimits: memlock: soft: -1 hard: -1 volumes: - opensearch-data1:/usr/share/opensearch/data ports: - 9200:9200 - 9600:9600 # required for Performance Analyzer networks: - opensearch-net volumes: opensearch-data1: opensearch-data2: networks: opensearch-net: . After the clusters start, verify the names of each: . curl -XGET -u 'admin:admin' -k 'https://localhost:9201' { \"cluster_name\" : \"leader-cluster\", ... } curl -XGET -u 'admin:admin' -k 'https://localhost:9200' { \"cluster_name\" : \"follower-cluster\", ... } . For this example, use port 9201 (replication-node1) as the leader and port 9200 (replication-node2) as the follower cluster. To get the IP address for the leader cluster, first identify its container ID: . docker ps CONTAINER ID IMAGE PORTS NAMES 3b8cdc698be5 opensearchproject/opensearch:2.7.0 0.0.0.0:9200-&gt;9200/tcp, 0.0.0.0:9600-&gt;9600/tcp, 9300/tcp replication-node2 731f5e8b0f4b opensearchproject/opensearch:2.7.0 9300/tcp, 0.0.0.0:9201-&gt;9200/tcp, 0.0.0.0:9700-&gt;9600/tcp replication-node1 . Then get that container’s IP address: . docker inspect --format='{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' 731f5e8b0f4b 172.22.0.3 . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/replication-plugin/getting-started/#example-setup",
    "relUrl": "/tuning-your-cluster/replication-plugin/getting-started/#example-setup"
  },"523": {
    "doc": "Getting started",
    "title": "Set up a cross-cluster connection",
    "content": "Cross-cluster replication follows a “pull” model, so most changes occur on the follower cluster, not the leader cluster. On the follower cluster, add the IP address (with port 9300) for each seed node. Because this is a single-node cluster, you only have one seed node. Provide a descriptive name for the connection, which you’ll use in the request to start replication: . curl -XPUT -k -H 'Content-Type: application/json' -u 'admin:admin' 'https://localhost:9200/_cluster/settings?pretty' -d ' { \"persistent\": { \"cluster\": { \"remote\": { \"my-connection-alias\": { \"seeds\": [\"172.22.0.3:9300\"] } } } } }' . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/replication-plugin/getting-started/#set-up-a-cross-cluster-connection",
    "relUrl": "/tuning-your-cluster/replication-plugin/getting-started/#set-up-a-cross-cluster-connection"
  },"524": {
    "doc": "Getting started",
    "title": "Start replication",
    "content": "To get started, create an index called leader-01 on the leader cluster: . curl -XPUT -k -H 'Content-Type: application/json' -u 'admin:admin' 'https://localhost:9201/leader-01?pretty' . Then start replication from the follower cluster. In the request body, provide the connection name and leader index that you want to replicate, along with the security roles you want to use: . curl -XPUT -k -H 'Content-Type: application/json' -u 'admin:admin' 'https://localhost:9200/_plugins/_replication/follower-01/_start?pretty' -d ' { \"leader_alias\": \"my-connection-alias\", \"leader_index\": \"leader-01\", \"use_roles\":{ \"leader_cluster_role\": \"all_access\", \"follower_cluster_role\": \"all_access\" } }' . If the Security plugin is disabled, omit the use_roles parameter. If it’s enabled, however, you must specify the leader and follower cluster roles that OpenSearch will use to authenticate the request. This example uses all_access for simplicity, but we recommend creating a replication user on each cluster and mapping it accordingly. This command creates an identical read-only index named follower-01 on the follower cluster that continuously stays updated with changes to the leader-01 index on the leader cluster. Starting replication creates a follower index from scratch – you can’t convert an existing index to a follower index. ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/replication-plugin/getting-started/#start-replication",
    "relUrl": "/tuning-your-cluster/replication-plugin/getting-started/#start-replication"
  },"525": {
    "doc": "Getting started",
    "title": "Confirm replication",
    "content": "After replication starts, get the status: . curl -XGET -k -u 'admin:admin' 'https://localhost:9200/_plugins/_replication/follower-01/_status?pretty' { \"status\" : \"SYNCING\", \"reason\" : \"User initiated\", \"leader_alias\" : \"my-connection-alias\", \"leader_index\" : \"leader-01\", \"follower_index\" : \"follower-01\", \"syncing_details\" : { \"leader_checkpoint\" : -1, \"follower_checkpoint\" : -1, \"seq_no\" : 0 } } . Possible statuses are SYNCING, BOOTSTRAPPING, PAUSED, and REPLICATION NOT IN PROGRESS. The leader and follower checkpoint values begin as negative numbers and reflect the shard count (-1 for one shard, -5 for five shards, and so on). The values increment with each change and illustrate how many updates the follower is behind the leader. If the indexes are fully synced, the values are the same. To confirm that replication is actually happening, add a document to the leader index: . curl -XPUT -k -H 'Content-Type: application/json' -u 'admin:admin' 'https://localhost:9201/leader-01/_doc/1?pretty' -d '{\"The Shining\": \"Stephen King\"}' . Then validate the replicated content on the follower index: . curl -XGET -k -u 'admin:admin' 'https://localhost:9200/follower-01/_search?pretty' { ... \"hits\": [{ \"_index\": \"follower-01\", \"_id\": \"1\", \"_score\": 1.0, \"_source\": { \"The Shining\": \"Stephen King\" } }] } . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/replication-plugin/getting-started/#confirm-replication",
    "relUrl": "/tuning-your-cluster/replication-plugin/getting-started/#confirm-replication"
  },"526": {
    "doc": "Getting started",
    "title": "Pause and resume replication",
    "content": "You can temporarily pause replication of an index if you need to remediate issues or reduce load on the leader cluster: . curl -XPOST -k -H 'Content-Type: application/json' -u 'admin:admin' 'https://localhost:9200/_plugins/_replication/follower-01/_pause?pretty' -d '{}' . To confirm that replication is paused, get the status: . curl -XGET -k -u 'admin:admin' 'https://localhost:9200/_plugins/_replication/follower-01/_status?pretty' { \"status\" : \"PAUSED\", \"reason\" : \"User initiated\", \"leader_alias\" : \"my-connection-alias\", \"leader_index\" : \"leader-01\", \"follower_index\" : \"follower-01\" } . When you’re done making changes, resume replication: . curl -XPOST -k -H 'Content-Type: application/json' -u 'admin:admin' 'https://localhost:9200/_plugins/_replication/follower-01/_resume?pretty' -d '{}' . When replication resumes, the follower index picks up any changes that were made to the leader index while replication was paused. Note that you can’t resume replication after it’s been paused for more than 12 hours. You must stop replication, delete the follower index, and restart replication of the leader. ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/replication-plugin/getting-started/#pause-and-resume-replication",
    "relUrl": "/tuning-your-cluster/replication-plugin/getting-started/#pause-and-resume-replication"
  },"527": {
    "doc": "Getting started",
    "title": "Stop replication",
    "content": "When you no longer need to replicate an index, terminate replication from the follower cluster: . curl -XPOST -k -H 'Content-Type: application/json' -u 'admin:admin' 'https://localhost:9200/_plugins/_replication/follower-01/_stop?pretty' -d '{}' . When you stop replication, the follower index un-follows the leader and becomes a standard index that you can write to. You can’t restart replication after stopping it. Get the status to confirm that the index is no longer being replicated: . curl -XGET -k -u 'admin:admin' 'https://localhost:9200/_plugins/_replication/follower-01/_status?pretty' { \"status\" : \"REPLICATION NOT IN PROGRESS\" } . You can further confirm that replication is stopped by making modifications to the leader index and confirming they don’t show up on the follower index. ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/replication-plugin/getting-started/#stop-replication",
    "relUrl": "/tuning-your-cluster/replication-plugin/getting-started/#stop-replication"
  },"528": {
    "doc": "Getting started",
    "title": "Getting started",
    "content": " ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/replication-plugin/getting-started/",
    "relUrl": "/tuning-your-cluster/replication-plugin/getting-started/"
  },"529": {
    "doc": "Cross-cluster replication",
    "title": "Cross-cluster replication",
    "content": "The cross-cluster replication plugin lets you replicate indexes, mappings, and metadata from one OpenSearch cluster to another. Cross-cluster replication has the following benefits: . | By replicating your indexes, you ensure that you can continue to handle search requests if there’s an outage. | Replicating data across geographically distant data centers minimizes the distance between the data and the application server. This reduces expensive latencies. | You can replicate data from multiple smaller clusters to a centralized reporting cluster, which is useful when it’s inefficient to query across a large network. | . Replication follows an active-passive model where the follower index (where the data is replicated) pulls data from the leader (remote) index. The replication plugin supports replication of indexes using wildcard pattern matching and provides commands to pause, resume, and stop replication. Once replication starts on an index, it initiates persistent background tasks on all primary shards on the follower cluster, which continuously poll corresponding shards from the leader cluster for updates. You can use the replication plugin with the Security plugin to encrypt cross-cluster traffic with node-to-node encryption and control access to replication activities. To start, see Get started with cross-cluster replication. ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/replication-plugin/index/",
    "relUrl": "/tuning-your-cluster/replication-plugin/index/"
  },"530": {
    "doc": "Replication security",
    "title": "Cross-cluster replication security",
    "content": "You can use the Security plugin with cross-cluster replication to limit users to certain actions. For example, you might want certain users to only perform replication activity on the leader or follower cluster. Because cross-cluster replication involves multiple clusters, it’s possible that clusters might have different security configurations. The following configurations are supported: . | Security plugin fully enabled on both clusters | Security plugin enabled only for TLS on both clusters (plugins.security.ssl_only) | Security plugin absent or disabled on both clusters (not recommended) | . Enable node-to-node encryption on both the leader and the follower cluster to ensure that replication traffic between the clusters is encrypted. ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/replication-plugin/permissions/#cross-cluster-replication-security",
    "relUrl": "/tuning-your-cluster/replication-plugin/permissions/#cross-cluster-replication-security"
  },"531": {
    "doc": "Replication security",
    "title": "Basic permissions",
    "content": "In order for non-admin users to perform replication activities, they must be mapped to the appropriate permissions. The Security plugin has two built-in roles that cover most replication use cases: cross_cluster_replication_leader_full_access, which provides replication permissions on the leader cluster, and cross_cluster_replication_follower_full_access, which provides replication permissions on the follower cluster. For descriptions of each, see Predefined roles. If you don’t want to use the default roles, you can combine individual replication permissions to meet your needs. Most permissions correspond to specific REST API operations. For example, the indices:admin/plugins/replication/index/pause permission lets you pause replication. ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/replication-plugin/permissions/#basic-permissions",
    "relUrl": "/tuning-your-cluster/replication-plugin/permissions/#basic-permissions"
  },"532": {
    "doc": "Replication security",
    "title": "Map the leader and follower cluster roles",
    "content": "The start replication and create replication rule operations are special cases. They involve background processes on the leader and follower clusters that must be associated with roles. When you perform one of these actions, you must explicitly pass the leader_cluster_role and follower_cluster_role in the request, which OpenSearch then uses in all backend replication tasks. To enable non-admins to start replication and create replication rules, create an identical user on each cluster (for example, replication_user) and map them to the cross_cluster_replication_leader_full_access role on the remote cluster and cross_cluster_replication_follower_full_access on the follower cluster. For instructions, see Map users to roles. Then add those roles to the request, and sign it with the appropriate credentials: . curl -XPUT -k -H 'Content-Type: application/json' -u 'replication_user:password' 'https://localhost:9200/_plugins/_replication/follower-01/_start?pretty' -d ' { \"leader_alias\": \"leader-cluster\", \"leader_index\": \"leader-01\", \"use_roles\":{ \"leader_cluster_role\": \"cross_cluster_replication_leader_full_access\", \"follower_cluster_role\": \"cross_cluster_replication_follower_full_access\" } }' . You can create your own, custom leader and follower cluster roles using individual permissions, but we recommend using the default roles, which are a good fit for most use cases. ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/replication-plugin/permissions/#map-the-leader-and-follower-cluster-roles",
    "relUrl": "/tuning-your-cluster/replication-plugin/permissions/#map-the-leader-and-follower-cluster-roles"
  },"533": {
    "doc": "Replication security",
    "title": "Replication permissions",
    "content": "The following sections list the available index and cluster-level permissions for cross-cluster replication. Follower cluster . The Security plugin supports these permissions for the follower cluster: . indices:admin/plugins/replication/index/setup/validate indices:admin/plugins/replication/index/start indices:admin/plugins/replication/index/pause indices:admin/plugins/replication/index/resume indices:admin/plugins/replication/index/stop indices:admin/plugins/replication/index/update indices:admin/plugins/replication/index/status_check indices:data/write/plugins/replication/changes cluster:admin/plugins/replication/autofollow/update . Leader cluster . The Security plugin supports these permissions for the leader cluster: . indices:admin/plugins/replication/validate indices:data/read/plugins/replication/file_chunk indices:data/read/plugins/replication/changes . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/replication-plugin/permissions/#replication-permissions",
    "relUrl": "/tuning-your-cluster/replication-plugin/permissions/#replication-permissions"
  },"534": {
    "doc": "Replication security",
    "title": "Replication security",
    "content": " ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/replication-plugin/permissions/",
    "relUrl": "/tuning-your-cluster/replication-plugin/permissions/"
  },"535": {
    "doc": "Replication settings",
    "title": "Replication settings",
    "content": "The replication plugin adds several settings to the standard OpenSearch cluster settings. The settings are dynamic, so you can change the default behavior of the plugin without restarting your cluster. You can mark settings as persistent or transient. For example, to update how often the follower cluster polls the leader cluster for updates: . PUT _cluster/settings { \"persistent\": { \"plugins.replication.follower.metadata_sync_interval\": \"30s\" } } . These settings manage the resources consumed by remote recoveries. We don’t recommend changing these settings; the defaults should work well for most use cases. | Setting | Default | Description | . | plugins.replication.follower.index.recovery.chunk_size | 10 MB | The chunk size requested by the follower cluster during file transfer. Specify the chunk size as a value and unit, for example, 10 MB, 5 KB. See Supported units. | . | plugins.replication.follower.index.recovery.max_concurrent_file_chunks | 4 | The number of file chunk requests that can be sent in parallel for each recovery. | . | plugins.replication.follower.index.ops_batch_size | 5000 | The number of operations that can be fetched at a time during the syncing phase of replication. | . | plugins.replication.follower.concurrent_readers_per_shard | 2 | The number of concurrent requests from the follower cluster per shard during the syncing phase of replication. | . | plugins.replication.autofollow.fetch_poll_interval | 30s | How often auto-follow tasks poll the leader cluster for new matching indexes. | . | plugins.replication.follower.metadata_sync_interval | 60s | How often the follower cluster polls the leader cluster for updated index metadata. | . | plugins.replication.translog.retention_lease.pruning.enabled | true | If enabled, prunes the translog based on retention leases on the leader index. | . | plugins.replication.translog.retention_size | 512 MB | Controls the size of the translog on the leader index. | . ",
    "url": "https://vagimeli.github.io/tuning-your-cluster/replication-plugin/settings/",
    "relUrl": "/tuning-your-cluster/replication-plugin/settings/"
  },"536": {
    "doc": "API",
    "title": "API",
    "content": "The Security plugin REST API lets you programmatically create and manage users, roles, role mappings, action groups, and tenants. . | Access control for the API | Reserved and hidden resources | Account . | Get account details | Change password | . | Action groups . | Get action group | Get action groups | Delete action group | Create action group | Patch action group | Patch action groups | . | Users . | Get user | Get users | Delete user | Create user | Patch user | Patch users | . | Roles . | Get role | Get roles | Delete role | Create role | Patch role | Patch roles | . | Role mappings . | Get role mapping | Get role mappings | Delete role mapping | Create role mapping | Patch role mapping | Patch role mappings | . | Tenants . | Get tenant | Get tenants | Delete tenant | Create tenant | Patch tenant | Patch tenants | . | Configuration . | Get configuration | Update configuration | Patch configuration | . | Distinguished names . | Get distinguished names | Update distinguished names | Delete distinguished names | . | Certificates . | Get certificates | Reload certificates | . | Cache . | Flush cache | . | Health . | Health check | . | Audit logs . | Enable Audit Logs | . | . ",
    "url": "https://vagimeli.github.io/security/access-control/api/",
    "relUrl": "/security/access-control/api/"
  },"537": {
    "doc": "API",
    "title": "Access control for the API",
    "content": "Just like OpenSearch permissions, you control access to the Security plugin REST API using roles. Specify roles in opensearch.yml: . plugins.security.restapi.roles_enabled: [\"&lt;role&gt;\", ...] . copy . These roles can now access all APIs. To prevent access to certain APIs: . plugins.security.restapi.endpoints_disabled.&lt;role&gt;.&lt;endpoint&gt;: [\"&lt;method&gt;\", ...] . copy . Possible values for endpoint are: . | ACTIONGROUPS | ROLES | ROLESMAPPING | INTERNALUSERS | CONFIG | CACHE | SYSTEMINFO | . Possible values for method are: . | GET | PUT | POST | DELETE | PATCH | . For example, the following configuration grants three roles access to the REST API, but then prevents test-role from making PUT, POST, DELETE, or PATCH requests to _plugins/_security/api/roles or _plugins/_security/api/internalusers: . plugins.security.restapi.roles_enabled: [\"all_access\", \"security_rest_api_access\", \"test-role\"] plugins.security.restapi.endpoints_disabled.test-role.ROLES: [\"PUT\", \"POST\", \"DELETE\", \"PATCH\"] plugins.security.restapi.endpoints_disabled.test-role.INTERNALUSERS: [\"PUT\", \"POST\", \"DELETE\", \"PATCH\"] . copy . To use the PUT and PATCH methods for the configuration APIs, add the following line to opensearch.yml: . plugins.security.unsupported.restapi.allow_securityconfig_modification: true . copy . ",
    "url": "https://vagimeli.github.io/security/access-control/api/#access-control-for-the-api",
    "relUrl": "/security/access-control/api/#access-control-for-the-api"
  },"538": {
    "doc": "API",
    "title": "Reserved and hidden resources",
    "content": "You can mark users, role, role mappings, and action groups as reserved. Resources that have this flag set to true can’t be changed using the REST API or OpenSearch Dashboards. To mark a resource as reserved, add the following flag: . kibana_user: reserved: true . copy . Likewise, you can mark users, role, role mappings, and action groups as hidden. Resources that have this flag set to true are not returned by the REST API and not visible in OpenSearch Dashboards: . kibana_user: hidden: true . copy . Hidden resources are automatically reserved. To add or remove these flags, modify config/opensearch-security/internal_users.yml and run plugins/opensearch-security/tools/securityadmin.sh. ",
    "url": "https://vagimeli.github.io/security/access-control/api/#reserved-and-hidden-resources",
    "relUrl": "/security/access-control/api/#reserved-and-hidden-resources"
  },"539": {
    "doc": "API",
    "title": "Account",
    "content": "Get account details . Introduced 1.0 . Returns account details for the current user. For example, if you sign the request as the admin user, the response includes details for that user. Request . GET _plugins/_security/api/account . copy . Example response . { \"user_name\": \"admin\", \"is_reserved\": true, \"is_hidden\": false, \"is_internal_user\": true, \"user_requested_tenant\": null, \"backend_roles\": [ \"admin\" ], \"custom_attribute_names\": [], \"tenants\": { \"global_tenant\": true, \"admin_tenant\": true, \"admin\": true }, \"roles\": [ \"all_access\", \"own_index\" ] } . Change password . Introduced 1.0 . Changes the password for the current user. Request . PUT _plugins/_security/api/account { \"current_password\" : \"old-password\", \"password\" : \"new-password\" } . copy . Example response . { \"status\": \"OK\", \"message\": \"'test-user' updated.\" } . ",
    "url": "https://vagimeli.github.io/security/access-control/api/#account",
    "relUrl": "/security/access-control/api/#account"
  },"540": {
    "doc": "API",
    "title": "Action groups",
    "content": "Get action group . Introduced 1.0 . Retrieves one action group. GET _plugins/_security/api/actiongroups/&lt;action-group&gt; . copy . Request . GET _plugins/_security/api/actiongroups/custom_action_group . copy . Example response . { \"custom_action_group\": { \"reserved\": false, \"hidden\": false, \"allowed_actions\": [ \"kibana_all_read\", \"indices:admin/aliases/get\", \"indices:admin/aliases/exists\" ], \"description\": \"My custom action group\", \"static\": false } } . Get action groups . Introduced 1.0 . Retrieves all action groups. Request . GET _plugins/_security/api/actiongroups/ . copy . Example response . { \"read\": { \"reserved\": true, \"hidden\": false, \"allowed_actions\": [ \"indices:data/read*\", \"indices:admin/mappings/fields/get*\", \"indices:admin/resolve/index\" ], \"type\": \"index\", \"description\": \"Allow all read operations\", \"static\": true }, \"cluster_all\": { \"reserved\": true, \"hidden\": false, \"allowed_actions\": [ \"cluster:*\" ], \"type\": \"cluster\", \"description\": \"Allow everything on cluster level\", \"static\": true }, ... } . Delete action group . Introduced 1.0 . Request . DELETE _plugins/_security/api/actiongroups/&lt;action-group&gt; . copy . Example response . { \"status\":\"OK\", \"message\":\"actiongroup SEARCH deleted.\" } . Create action group . Introduced 1.0 . Creates or replaces the specified action group. Request . PUT _plugins/_security/api/actiongroups/&lt;action-group&gt; { \"allowed_actions\": [ \"indices:data/write/index*\", \"indices:data/write/update*\", \"indices:admin/mapping/put\", \"indices:data/write/bulk*\", \"read\", \"write\" ] } . copy . Example response . { \"status\": \"CREATED\", \"message\": \"'my-action-group' created.\" } . Patch action group . Introduced 1.0 . Updates individual attributes of an action group. Request . PATCH _plugins/_security/api/actiongroups/&lt;action-group&gt; [ { \"op\": \"replace\", \"path\": \"/allowed_actions\", \"value\": [\"indices:admin/create\", \"indices:admin/mapping/put\"] } ] . copy . Example response . { \"status\":\"OK\", \"message\":\"actiongroup SEARCH deleted.\" } . Patch action groups . Introduced 1.0 . Creates, updates, or deletes multiple action groups in a single call. Request . PATCH _plugins/_security/api/actiongroups [ { \"op\": \"add\", \"path\": \"/CREATE_INDEX\", \"value\": { \"allowed_actions\": [\"indices:admin/create\", \"indices:admin/mapping/put\"] } }, { \"op\": \"remove\", \"path\": \"/CRUD\" } ] . copy . Example response . { \"status\":\"OK\", \"message\":\"actiongroup SEARCH deleted.\" } . ",
    "url": "https://vagimeli.github.io/security/access-control/api/#action-groups",
    "relUrl": "/security/access-control/api/#action-groups"
  },"541": {
    "doc": "API",
    "title": "Users",
    "content": "These calls let you create, update, and delete internal users. If you use an external authentication backend, you probably don’t need to worry about internal users. Get user . Introduced 1.0 . Request . GET _plugins/_security/api/internalusers/&lt;username&gt; . copy . Example response . { \"kirk\": { \"hash\": \"\", \"roles\": [ \"captains\", \"starfleet\" ], \"attributes\": { \"attribute1\": \"value1\", \"attribute2\": \"value2\", } } } . Get users . Introduced 1.0 . Request . GET _plugins/_security/api/internalusers/ . copy . Example response . { \"kirk\": { \"hash\": \"\", \"roles\": [ \"captains\", \"starfleet\" ], \"attributes\": { \"attribute1\": \"value1\", \"attribute2\": \"value2\", } } } . Delete user . Introduced 1.0 . Request . DELETE _plugins/_security/api/internalusers/&lt;username&gt; . copy . Example response . { \"status\":\"OK\", \"message\":\"user kirk deleted.\" } . Create user . Introduced 1.0 . Creates or replaces the specified user. You must specify either password (plain text) or hash (the hashed user password). If you specify password, the Security plugin automatically hashes the password before storing it. Note that any role you supply in the opendistro_security_roles array must already exist for the Security plugin to map the user to that role. To see predefined roles, refer to the list of predefined roles. For instructions on how to create a role, refer to creating a role. Request . PUT _plugins/_security/api/internalusers/&lt;username&gt; { \"password\": \"kirkpass\", \"opendistro_security_roles\": [\"maintenance_staff\", \"weapons\"], \"backend_roles\": [\"role 1\", \"role 2\"], \"attributes\": { \"attribute1\": \"value1\", \"attribute2\": \"value2\" } } . copy . Example response . { \"status\":\"CREATED\", \"message\":\"User kirk created\" } . Patch user . Introduced 1.0 . Updates individual attributes of an internal user. Request . PATCH _plugins/_security/api/internalusers/&lt;username&gt; [ { \"op\": \"replace\", \"path\": \"/backend_roles\", \"value\": [\"klingons\"] }, { \"op\": \"replace\", \"path\": \"/opendistro_security_roles\", \"value\": [\"ship_manager\"] }, { \"op\": \"replace\", \"path\": \"/attributes\", \"value\": { \"newattribute\": \"newvalue\" } } ] . copy . Example response . { \"status\": \"OK\", \"message\": \"'kirk' updated.\" } . Patch users . Introduced 1.0 . Creates, updates, or deletes multiple internal users in a single call. Request . PATCH _plugins/_security/api/internalusers [ { \"op\": \"add\", \"path\": \"/spock\", \"value\": { \"password\": \"testpassword1\", \"backend_roles\": [\"testrole1\"] } }, { \"op\": \"add\", \"path\": \"/worf\", \"value\": { \"password\": \"testpassword2\", \"backend_roles\": [\"testrole2\"] } }, { \"op\": \"remove\", \"path\": \"/riker\" } ] . copy . Example response . { \"status\": \"OK\", \"message\": \"Resource updated.\" } . ",
    "url": "https://vagimeli.github.io/security/access-control/api/#users",
    "relUrl": "/security/access-control/api/#users"
  },"542": {
    "doc": "API",
    "title": "Roles",
    "content": "Get role . Introduced 1.0 . Retrieves one role. Request . GET _plugins/_security/api/roles/&lt;role&gt; . copy . Example response . { \"test-role\": { \"reserved\": false, \"hidden\": false, \"cluster_permissions\": [ \"cluster_composite_ops\", \"indices_monitor\" ], \"index_permissions\": [{ \"index_patterns\": [ \"movies*\" ], \"dls\": \"\", \"fls\": [], \"masked_fields\": [], \"allowed_actions\": [ \"read\" ] }], \"tenant_permissions\": [{ \"tenant_patterns\": [ \"human_resources\" ], \"allowed_actions\": [ \"kibana_all_read\" ] }], \"static\": false } } . Get roles . Introduced 1.0 . Retrieves all roles. Request . GET _plugins/_security/api/roles/ . copy . Example response . { \"manage_snapshots\": { \"reserved\": true, \"hidden\": false, \"description\": \"Provide the minimum permissions for managing snapshots\", \"cluster_permissions\": [ \"manage_snapshots\" ], \"index_permissions\": [{ \"index_patterns\": [ \"*\" ], \"fls\": [], \"masked_fields\": [], \"allowed_actions\": [ \"indices:data/write/index\", \"indices:admin/create\" ] }], \"tenant_permissions\": [], \"static\": true }, ... } . Delete role . Introduced 1.0 . Request . DELETE _plugins/_security/api/roles/&lt;role&gt; . copy . Example response . { \"status\":\"OK\", \"message\":\"role test-role deleted.\" } . Create role . Introduced 1.0 . Creates or replaces the specified role. Request . PUT _plugins/_security/api/roles/&lt;role&gt; { \"cluster_permissions\": [ \"cluster_composite_ops\", \"indices_monitor\" ], \"index_permissions\": [{ \"index_patterns\": [ \"movies*\" ], \"dls\": \"\", \"fls\": [], \"masked_fields\": [], \"allowed_actions\": [ \"read\" ] }], \"tenant_permissions\": [{ \"tenant_patterns\": [ \"human_resources\" ], \"allowed_actions\": [ \"kibana_all_read\" ] }] } . copy . Example response . { \"status\": \"OK\", \"message\": \"'test-role' updated.\" } . Due to word boundaries associated with Unicode special characters, the Unicode standard analyzer cannot index a text field type value as a whole value when it includes one of these special characters. As a result, a text field value that includes a special character is parsed by the standard analyzer as multiple values separated by the special character, effectively tokenizing the different elements on either side of it. For example, since the values in the fields \"user.id\": \"User-1\" and \"user.id\": \"User-2\" contain the hyphen/minus sign, this special character will prevent the analyzer from distinguishing between the two different users for user.id and interpret them as one and the same. This can lead to unintentional filtering of documents and potentially compromise control over their access. To avoid this circumstance, you can use a custom analyzer or map the field as keyword, which performs an exact-match search. See Keyword field type for the latter option. For a list of characters that should be avoided when field type is text, see Word Boundaries. Patch role . Introduced 1.0 . Updates individual attributes of a role. Request . PATCH _plugins/_security/api/roles/&lt;role&gt; [ { \"op\": \"replace\", \"path\": \"/index_permissions/0/fls\", \"value\": [\"myfield1\", \"myfield2\"] }, { \"op\": \"remove\", \"path\": \"/index_permissions/0/dls\" } ] . copy . Example response . { \"status\": \"OK\", \"message\": \"'&lt;role&gt;' updated.\" } . Patch roles . Introduced 1.0 . Creates, updates, or deletes multiple roles in a single call. Request . PATCH _plugins/_security/api/roles [ { \"op\": \"replace\", \"path\": \"/role1/index_permissions/0/fls\", \"value\": [\"test1\", \"test2\"] }, { \"op\": \"remove\", \"path\": \"/role1/index_permissions/0/dls\" }, { \"op\": \"add\", \"path\": \"/role2/cluster_permissions\", \"value\": [\"manage_snapshots\"] } ] . copy . Example response . { \"status\": \"OK\", \"message\": \"Resource updated.\" } . ",
    "url": "https://vagimeli.github.io/security/access-control/api/#roles",
    "relUrl": "/security/access-control/api/#roles"
  },"543": {
    "doc": "API",
    "title": "Role mappings",
    "content": "Get role mapping . Introduced 1.0 . Retrieves one role mapping. Request . GET _plugins/_security/api/rolesmapping/&lt;role&gt; . copy . Example response . { \"role_starfleet\" : { \"backend_roles\" : [ \"starfleet\", \"captains\", \"defectors\", \"cn=ldaprole,ou=groups,dc=example,dc=com\" ], \"hosts\" : [ \"*.starfleetintranet.com\" ], \"users\" : [ \"worf\" ] } } . Get role mappings . Introduced 1.0 . Retrieves all role mappings. Request . GET _plugins/_security/api/rolesmapping . copy . Example response . { \"role_starfleet\" : { \"backend_roles\" : [ \"starfleet\", \"captains\", \"defectors\", \"cn=ldaprole,ou=groups,dc=example,dc=com\" ], \"hosts\" : [ \"*.starfleetintranet.com\" ], \"users\" : [ \"worf\" ] } } . Delete role mapping . Introduced 1.0 . Deletes the specified role mapping. Request . DELETE _plugins/_security/api/rolesmapping/&lt;role&gt; . copy . Example response . { \"status\": \"OK\", \"message\": \"'my-role' deleted.\" } . Create role mapping . Introduced 1.0 . Creates or replaces the specified role mapping. Request . PUT _plugins/_security/api/rolesmapping/&lt;role&gt; { \"backend_roles\" : [ \"starfleet\", \"captains\", \"defectors\", \"cn=ldaprole,ou=groups,dc=example,dc=com\" ], \"hosts\" : [ \"*.starfleetintranet.com\" ], \"users\" : [ \"worf\" ] } . copy . Example response . { \"status\": \"CREATED\", \"message\": \"'my-role' created.\" } . Patch role mapping . Introduced 1.0 . Updates individual attributes of a role mapping. Request . PATCH _plugins/_security/api/rolesmapping/&lt;role&gt; [ { \"op\": \"replace\", \"path\": \"/users\", \"value\": [\"myuser\"] }, { \"op\": \"replace\", \"path\": \"/backend_roles\", \"value\": [\"mybackendrole\"] } ] . copy . Example response . { \"status\": \"OK\", \"message\": \"'my-role' updated.\" } . Patch role mappings . Introduced 1.0 . Creates or updates multiple role mappings in a single call. Request . PATCH _plugins/_security/api/rolesmapping [ { \"op\": \"add\", \"path\": \"/human_resources\", \"value\": { \"users\": [\"user1\"], \"backend_roles\": [\"backendrole2\"] } }, { \"op\": \"add\", \"path\": \"/finance\", \"value\": { \"users\": [\"user2\"], \"backend_roles\": [\"backendrole2\"] } } ] . copy . Example response . { \"status\": \"OK\", \"message\": \"Resource updated.\" } . ",
    "url": "https://vagimeli.github.io/security/access-control/api/#role-mappings",
    "relUrl": "/security/access-control/api/#role-mappings"
  },"544": {
    "doc": "API",
    "title": "Tenants",
    "content": "Get tenant . Introduced 1.0 . Retrieves one tenant. Request . GET _plugins/_security/api/tenants/&lt;tenant&gt; . copy . Example response . { \"human_resources\": { \"reserved\": false, \"hidden\": false, \"description\": \"A tenant for the human resources team.\", \"static\": false } } . Get tenants . Introduced 1.0 . Retrieves all tenants. Request . GET _plugins/_security/api/tenants/ . copy . Example response . { \"global_tenant\": { \"reserved\": true, \"hidden\": false, \"description\": \"Global tenant\", \"static\": true }, \"human_resources\": { \"reserved\": false, \"hidden\": false, \"description\": \"A tenant for the human resources team.\", \"static\": false } } . Delete tenant . Introduced 1.0 . Deletes the specified tenant. Request . DELETE _plugins/_security/api/tenants/&lt;tenant&gt; . copy . Example response . { \"status\":\"OK\", \"message\":\"tenant human_resources deleted.\" } . Create tenant . Introduced 1.0 . Creates or replaces the specified tenant. Request . PUT _plugins/_security/api/tenants/&lt;tenant&gt; { \"description\": \"A tenant for the human resources team.\" } . copy . Example response . { \"status\":\"CREATED\", \"message\":\"tenant human_resources created\" } . Patch tenant . Introduced 1.0 . Add, delete, or modify a single tenant. Request . PATCH _plugins/_security/api/tenants/&lt;tenant&gt; [ { \"op\": \"replace\", \"path\": \"/description\", \"value\": \"An updated description\" } ] . copy . Example response . { \"status\": \"OK\", \"message\": \"Resource updated.\" } . Patch tenants . Introduced 1.0 . Add, delete, or modify multiple tenants in a single call. Request . PATCH _plugins/_security/api/tenants/ [ { \"op\": \"replace\", \"path\": \"/human_resources/description\", \"value\": \"An updated description\" }, { \"op\": \"add\", \"path\": \"/another_tenant\", \"value\": { \"description\": \"Another description.\" } } ] . copy . Example response . { \"status\": \"OK\", \"message\": \"Resource updated.\" } . ",
    "url": "https://vagimeli.github.io/security/access-control/api/#tenants",
    "relUrl": "/security/access-control/api/#tenants"
  },"545": {
    "doc": "API",
    "title": "Configuration",
    "content": "Get configuration . Introduced 1.0 . Retrieves the current Security plugin configuration in JSON format. Request . GET _plugins/_security/api/securityconfig . copy . Update configuration . Introduced 1.0 . Creates or updates the existing configuration using the REST API. This operation can easily break your existing configuration, so we recommend using securityadmin.sh instead, which is far safer. See Access control for the API for how to enable this operation. Request . PUT _plugins/_security/api/securityconfig/config { \"dynamic\": { \"filtered_alias_mode\": \"warn\", \"disable_rest_auth\": false, \"disable_intertransport_auth\": false, \"respect_request_indices_options\": false, \"opensearch-dashboards\": { \"multitenancy_enabled\": true, \"server_username\": \"kibanaserver\", \"index\": \".opensearch-dashboards\" }, \"http\": { \"anonymous_auth_enabled\": false }, \"authc\": { \"basic_internal_auth_domain\": { \"http_enabled\": true, \"transport_enabled\": true, \"order\": 0, \"http_authenticator\": { \"challenge\": true, \"type\": \"basic\", \"config\": {} }, \"authentication_backend\": { \"type\": \"intern\", \"config\": {} }, \"description\": \"Authenticate via HTTP Basic against internal users database\" } }, \"auth_failure_listeners\": {}, \"do_not_fail_on_forbidden\": false, \"multi_rolespan_enabled\": true, \"hosts_resolver_mode\": \"ip-only\", \"do_not_fail_on_forbidden_empty\": false } } . copy . Example response . { \"status\": \"OK\", \"message\": \"'config' updated.\" } . Patch configuration . Introduced 1.0 . Updates the existing configuration using the REST API. This operation can easily break your existing configuration, so we recommend using securityadmin.sh instead, which is far safer. See Access control for the API for how to enable this operation. Before you can execute the operation, you must first add the following line to opensearch.yml: . plugins.security.unsupported.restapi.allow_securityconfig_modification: true . copy . Request . PATCH _plugins/_security/api/securityconfig [ { \"op\": \"replace\", \"path\": \"/config/dynamic/authc/basic_internal_auth_domain/transport_enabled\", \"value\": \"true\" } ] . copy . Example response . { \"status\": \"OK\", \"message\": \"Resource updated.\" } . ",
    "url": "https://vagimeli.github.io/security/access-control/api/#configuration",
    "relUrl": "/security/access-control/api/#configuration"
  },"546": {
    "doc": "API",
    "title": "Distinguished names",
    "content": "These REST APIs let a super admin add, retrieve, update, or delete any distinguished names from an allow list to enable communication between clusters and/or nodes. Before you can use the REST API to configure the allow list, you must first add the following line to opensearch.yml: . plugins.security.nodes_dn_dynamic_config_enabled: true . copy . Get distinguished names . Retrieves all distinguished names in the allow list. Request . GET _plugins/_security/api/nodesdn . copy . Example response . { \"cluster1\": { \"nodes_dn\": [ \"CN=cluster1.example.com\" ] } } . To get the distinguished names from a specific cluster’s or node’s allow list, include the cluster’s name in the request path. Request . GET _plugins/_security/api/nodesdn/&lt;cluster-name&gt; . copy . Example response . { \"cluster3\": { \"nodes_dn\": [ \"CN=cluster3.example.com\" ] } } . Update distinguished names . Adds or updates the specified distinguished names in the cluster’s or node’s allow list. Request . PUT _plugins/_security/api/nodesdn/&lt;cluster-name&gt; { \"nodes_dn\": [ \"CN=cluster3.example.com\" ] } . copy . Example response . { \"status\": \"CREATED\", \"message\": \"'cluster3' created.\" } . Delete distinguished names . Deletes all distinguished names in the specified cluster’s or node’s allow list. Request . DELETE _plugins/_security/api/nodesdn/&lt;cluster-name&gt; . copy . Example response . { \"status\": \"OK\", \"message\": \"'cluster3' deleted.\" } . ",
    "url": "https://vagimeli.github.io/security/access-control/api/#distinguished-names",
    "relUrl": "/security/access-control/api/#distinguished-names"
  },"547": {
    "doc": "API",
    "title": "Certificates",
    "content": "Get certificates . Introduced 1.0 . Retrieves the cluster’s security certificates. Request . GET _plugins/_security/api/ssl/certs . copy . Example response . { \"http_certificates_list\": [ { \"issuer_dn\": \"CN=Example Com Inc. Root CA,OU=Example Com Inc. Root CA,O=Example Com Inc.,DC=example,DC=com\", \"subject_dn\": \"CN=node-0.example.com,OU=node,O=node,L=test,DC=de\", \"san\": \"[[8, 1.2.3.4.5.5], [2, node-0.example.com]\", \"not_before\": \"2018-04-22T03:43:47Z\", \"not_after\": \"2028-04-19T03:43:47Z\" } ], \"transport_certificates_list\": [ { \"issuer_dn\": \"CN=Example Com Inc. Root CA,OU=Example Com Inc. Root CA,O=Example Com Inc.,DC=example,DC=com\", \"subject_dn\": \"CN=node-0.example.com,OU=node,O=node,L=test,DC=de\", \"san\": \"[[8, 1.2.3.4.5.5], [2, node-0.example.com]\", \"not_before\": \"2018-04-22T03:43:47Z\", \"not_after\": \"2028-04-19T03:43:47Z\" } ] } . Reload certificates . Introduced 1.0 . Reloads SSL certificates that are about to expire without restarting the OpenSearch node. This call assumes that new certificates are in the same location specified by the security configurations in opensearch.yml. To keep sensitive certificate reloads secure, this call only allows hot reload with certificates issued by the same issuer and subject DN and SAN with expiry dates after the current certificate. Request . PUT _opendistro/_security/api/ssl/transport/reloadcerts . copy . PUT _opendistro/_security/api/ssl/http/reloadcerts . copy . Example response . { \"message\": \"updated http certs\" } . ",
    "url": "https://vagimeli.github.io/security/access-control/api/#certificates",
    "relUrl": "/security/access-control/api/#certificates"
  },"548": {
    "doc": "API",
    "title": "Cache",
    "content": "Flush cache . Introduced 1.0 . Flushes the Security plugin user, authentication, and authorization cache. Request . DELETE _plugins/_security/api/cache . copy . Example response . { \"status\": \"OK\", \"message\": \"Cache flushed successfully.\" } . ",
    "url": "https://vagimeli.github.io/security/access-control/api/#cache",
    "relUrl": "/security/access-control/api/#cache"
  },"549": {
    "doc": "API",
    "title": "Health",
    "content": "Health check . Introduced 1.0 . Checks to see if the Security plugin is up and running. If you operate your cluster behind a load balancer, this operation is useful for determining node health and doesn’t require a signed request. Request . GET _plugins/_security/health . copy . Example response . { \"message\": null, \"mode\": \"strict\", \"status\": \"UP\" } . ",
    "url": "https://vagimeli.github.io/security/access-control/api/#health",
    "relUrl": "/security/access-control/api/#health"
  },"550": {
    "doc": "API",
    "title": "Audit logs",
    "content": "The following API is available for audit logging in the Security plugin. Enable Audit Logs . This API allows you to enable or disable audit logging, define the configuration for audit logging and compliance, and make updates to settings. For details on using audit logging to track access to OpenSearch clusters, as well as information on further configurations, see Audit logs. You can do an initial configuration of audit logging in the audit.yml file, found in the opensearch-project/security/config directory. Thereafter, you can use the REST API or Dashboards for further changes to the configuration. Request fields . | Field | Data Type | Description | . | enabled | Boolean | Enables or disables audit logging. Default is true. | . | audit | Object | Contains fields for audit logging configuration. | . | audit.ignore_users | Array | Users to be excluded from auditing. Wildcard patterns are supportedExample: ignore_users: [\"test-user\", employee-*\"] | . | audit.ignore_requests | Array | Requests to be excluded from auditing. Wildcard patterns are supported.Example: ignore_requests: [\"indices:data/read/*\", \"SearchRequest\"] | . | audit.disabled_rest_categories | Array | Categories to exclude from REST API auditing. Default categories are AUTHENTICATED, GRANTED_PRIVILEGES. | . | audit.disabled_transport_categories | Array | Categories to exclude from Transport API auditing. Default categories are AUTHENTICATED, GRANTED_PRIVILEGES. | . | audit.log_request_body | Boolean | Includes the body of the request (if available) for both REST and the transport layer. Default is true. | . | audit.resolve_indices | Boolean | Logs all indexes affected by a request. Resolves aliases and wildcards/date patterns. Default is true. | . | audit.resolve_bulk_requests | Boolean | Logs individual operations in a bulk request. Default is false. | . | audit.exclude_sensitive_headers | Boolean | Excludes sensitive headers from being included in the logs. Default is true. | . | audit.enable_transport | Boolean | Enables/disables Transport API auditing. Default is true. | . | audit.enable_rest | Boolean | Enables/disables REST API auditing. Default is true. | . | compliance | Object | Contains fields for compliance configuration. | . | compliance.enabled | Boolean | Enables or disables compliance. Default is true. | . | compliance.write_log_diffs | Boolean | Logs only diffs for document updates. Default is false. | . | compliance.read_watched_fields | Object | Map of indexes and fields to monitor for read events. Wildcard patterns are supported for both index names and fields. | . | compliance.read_ignore_users | Array | List of users to ignore for read events. Wildcard patterns are supported.Example: read_ignore_users: [\"test-user\", \"employee-*\"] | . | compliance.write_watched_indices | Array | List of indexes to watch for write events. Wildcard patterns are supported.Example: write_watched_indices: [\"twitter\", \"logs-*\"] | . | compliance.write_ignore_users | Array | List of users to ignore for write events. Wildcard patterns are supported.Example: write_ignore_users: [\"test-user\", \"employee-*\"] | . | compliance.read_metadata_only | Boolean | Logs only metadata of the document for read events. Default is true. | . | compliance.write_metadata_only | Boolean | Log only metadata of the document for write events. Default is true. | . | compliance.external_config | Boolean | Logs external configuration files for the node. Default is false. | . | compliance.internal_config | Boolean | Logs updates to internal security changes. Default is true. | . Changes to the _readonly property result in a 409 error, as indicated in the response below. { \"status\" : \"error\", \"reason\" : \"Invalid configuration\", \"invalid_keys\" : { \"keys\" : \"_readonly,config\" } } . Example request . GET . A GET call retrieves the audit configuration. GET /_opendistro/_security/api/audit . copy . PUT . A PUT call updates the audit configuration. PUT /_opendistro/_security/api/audit/config { \"enabled\": true, \"audit\": { \"ignore_users\": [], \"ignore_requests\": [], \"disabled_rest_categories\": [ \"AUTHENTICATED\", \"GRANTED_PRIVILEGES\" ], \"disabled_transport_categories\": [ \"AUTHENTICATED\", \"GRANTED_PRIVILEGES\" ], \"log_request_body\": false, \"resolve_indices\": false, \"resolve_bulk_requests\": false, \"exclude_sensitive_headers\": true, \"enable_transport\": false, \"enable_rest\": true }, \"compliance\": { \"enabled\": true, \"write_log_diffs\": false, \"read_watched_fields\": {}, \"read_ignore_users\": [], \"write_watched_indices\": [], \"write_ignore_users\": [], \"read_metadata_only\": true, \"write_metadata_only\": true, \"external_config\": false, \"internal_config\": true } } . copy . PATCH . A PATCH call is used to update specified fields in the audit configuration. The PATCH method requires an operation, a path, and a value to complete a valid request. For details on using the PATCH method, see the following Patching resources description at Wikipedia. Using the PATCH method also requires a user to have a security configuration that includes admin certificates for encryption. To find out more about these certificates, see Configuring admin certificates. curl -X PATCH -k -i --cert &lt;admin_cert file name&gt; --key &lt;admin_cert_key file name&gt; &lt;domain&gt;/_opendistro/_security/api/audit -H 'Content-Type: application/json' -d'[{\"op\":\"add\",\"path\":\"/config/enabled\",\"value\":\"true\"}]' . copy . OpenSearch Dashboards Dev Tools do not currently support the PATCH method. You can use curl, Postman, or another alternative process to update the configuration using this method. To follow the GitHub issue for support of the PATCH method in Dashboards, see issue #2343. Example response . The GET call produces a response that appears similar to the following: . { \"_readonly\" : [ \"/audit/exclude_sensitive_headers\", \"/compliance/internal_config\", \"/compliance/external_config\" ], \"config\" : { \"compliance\" : { \"enabled\" : true, \"write_log_diffs\" : false, \"read_watched_fields\" : { }, \"read_ignore_users\" : [ ], \"write_watched_indices\" : [ ], \"write_ignore_users\" : [ ], \"read_metadata_only\" : true, \"write_metadata_only\" : true, \"external_config\" : false, \"internal_config\" : true }, \"enabled\" : true, \"audit\" : { \"ignore_users\" : [ ], \"ignore_requests\" : [ ], \"disabled_rest_categories\" : [ \"AUTHENTICATED\", \"GRANTED_PRIVILEGES\" ], \"disabled_transport_categories\" : [ \"AUTHENTICATED\", \"GRANTED_PRIVILEGES\" ], \"log_request_body\" : true, \"resolve_indices\" : true, \"resolve_bulk_requests\" : true, \"exclude_sensitive_headers\" : true, \"enable_transport\" : true, \"enable_rest\" : true } } } . The PUT request produces a response that appears similar to the following: . { \"status\" : \"OK\", \"message\" : \"'config' updated.\" } . The PATCH request produces a response similar to the following: . HTTP/1.1 200 OK content-type: application/json; charset=UTF-8 content-length: 45 . ",
    "url": "https://vagimeli.github.io/security/access-control/api/#audit-logs",
    "relUrl": "/security/access-control/api/#audit-logs"
  },"551": {
    "doc": "Cross-cluster search",
    "title": "Cross-cluster search",
    "content": "Cross-cluster search is exactly what it sounds like: it lets any node in a cluster execute search requests against other clusters. The Security plugin supports cross-cluster search out of the box. . | Authentication flow | Permissions | Walkthrough | . ",
    "url": "https://vagimeli.github.io/security/access-control/cross-cluster-search/",
    "relUrl": "/security/access-control/cross-cluster-search/"
  },"552": {
    "doc": "Cross-cluster search",
    "title": "Authentication flow",
    "content": "When accessing a remote cluster from a coordinating cluster using cross-cluster search: . | The Security plugin authenticates the user on the coordinating cluster. | The Security plugin fetches the user’s backend roles on the coordinating cluster. | The call, including the authenticated user, is forwarded to the remote cluster. | The user’s permissions are evaluated on the remote cluster. | . You can have different authentication and authorization configurations on the remote and coordinating cluster, but we recommend using the same settings on both. ",
    "url": "https://vagimeli.github.io/security/access-control/cross-cluster-search/#authentication-flow",
    "relUrl": "/security/access-control/cross-cluster-search/#authentication-flow"
  },"553": {
    "doc": "Cross-cluster search",
    "title": "Permissions",
    "content": "To query indexes on remote clusters, users need to have READ or SEARCH permissions. Furthermore, when the search request includes the query parameter ccs_minimize_roundtrips=false – which tells OpenSearch not to minimize outgoing and ingoing requests to remote clusters – users need to have the following additional permission for the index: . indices:admin/shards/search_shards . For more information about the ccs_minimize_roundtrips parameter, see the list of URL Parameters for the Search API. Sample roles.yml configuration . humanresources: cluster: - CLUSTER_COMPOSITE_OPS_RO indices: 'humanresources': '*': - READ - indices:admin/shards/search_shards # needed when the search request includes parameter setting 'ccs_minimize_roundtrips=false'. Sample role in OpenSearch Dashboards . ",
    "url": "https://vagimeli.github.io/security/access-control/cross-cluster-search/#permissions",
    "relUrl": "/security/access-control/cross-cluster-search/#permissions"
  },"554": {
    "doc": "Cross-cluster search",
    "title": "Walkthrough",
    "content": "Save this file as docker-compose.yml and run docker-compose up to start two single-node clusters on the same network: . version: '3' services: opensearch-ccs-node1: image: opensearchproject/opensearch:2.7.0 container_name: opensearch-ccs-node1 environment: - cluster.name=opensearch-ccs-cluster1 - discovery.type=single-node - bootstrap.memory_lock=true # along with the memlock settings below, disables swapping - \"OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m\" # minimum and maximum Java heap size, recommend setting both to 50% of system RAM ulimits: memlock: soft: -1 hard: -1 volumes: - opensearch-data1:/usr/share/opensearch/data ports: - 9200:9200 - 9600:9600 # required for Performance Analyzer networks: - opensearch-net opensearch-ccs-node2: image: opensearchproject/opensearch:2.7.0 container_name: opensearch-ccs-node2 environment: - cluster.name=opensearch-ccs-cluster2 - discovery.type=single-node - bootstrap.memory_lock=true # along with the memlock settings below, disables swapping - \"OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m\" # minimum and maximum Java heap size, recommend setting both to 50% of system RAM ulimits: memlock: soft: -1 hard: -1 volumes: - opensearch-data2:/usr/share/opensearch/data ports: - 9250:9200 - 9700:9600 # required for Performance Analyzer networks: - opensearch-net volumes: opensearch-data1: opensearch-data2: networks: opensearch-net: . After the clusters start, verify the names of each: . curl -XGET -u 'admin:admin' -k 'https://localhost:9200' { \"cluster_name\" : \"opensearch-ccs-cluster1\", ... } curl -XGET -u 'admin:admin' -k 'https://localhost:9250' { \"cluster_name\" : \"opensearch-ccs-cluster2\", ... } . Both clusters run on localhost, so the important identifier is the port number. In this case, use port 9200 (opensearch-ccs-node1) as the remote cluster, and port 9250 (opensearch-ccs-node2) as the coordinating cluster. To get the IP address for the remote cluster, first identify its container ID: . docker ps CONTAINER ID IMAGE PORTS NAMES 6fe89ebc5a8e opensearchproject/opensearch:2.7.0 0.0.0.0:9200-&gt;9200/tcp, 0.0.0.0:9600-&gt;9600/tcp, 9300/tcp opensearch-ccs-node1 2da08b6c54d8 opensearchproject/opensearch:2.7.0 9300/tcp, 0.0.0.0:9250-&gt;9200/tcp, 0.0.0.0:9700-&gt;9600/tcp opensearch-ccs-node2 . Then get that container’s IP address: . docker inspect --format='{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' 6fe89ebc5a8e 172.31.0.3 . On the coordinating cluster, add the remote cluster name and the IP address (with port 9300) for each “seed node.” In this case, you only have one seed node: . curl -k -XPUT -H 'Content-Type: application/json' -u 'admin:admin' 'https://localhost:9250/_cluster/settings' -d ' { \"persistent\": { \"cluster.remote\": { \"opensearch-ccs-cluster1\": { \"seeds\": [\"172.31.0.3:9300\"] } } } }' . On the remote cluster, index a document: . curl -XPUT -k -H 'Content-Type: application/json' -u 'admin:admin' 'https://localhost:9200/books/_doc/1' -d '{\"Dracula\": \"Bram Stoker\"}' . At this point, cross-cluster search works. You can test it using the admin user: . curl -XGET -k -u 'admin:admin' 'https://localhost:9250/opensearch-ccs-cluster1:books/_search?pretty' { ... \"hits\": [{ \"_index\": \"opensearch-ccs-cluster1:books\", \"_id\": \"1\", \"_score\": 1.0, \"_source\": { \"Dracula\": \"Bram Stoker\" } }] } . To continue testing, create a new user on both clusters: . curl -XPUT -k -u 'admin:admin' 'https://localhost:9200/_plugins/_security/api/internalusers/booksuser' -H 'Content-Type: application/json' -d '{\"password\":\"password\"}' curl -XPUT -k -u 'admin:admin' 'https://localhost:9250/_plugins/_security/api/internalusers/booksuser' -H 'Content-Type: application/json' -d '{\"password\":\"password\"}' . Then run the same search as before with booksuser: . curl -XGET -k -u booksuser:password 'https://localhost:9250/opensearch-ccs-cluster1:books/_search?pretty' { \"error\" : { \"root_cause\" : [ { \"type\" : \"security_exception\", \"reason\" : \"no permissions for [indices:admin/shards/search_shards, indices:data/read/search] and User [name=booksuser, roles=[], requestedTenant=null]\" } ], \"type\" : \"security_exception\", \"reason\" : \"no permissions for [indices:admin/shards/search_shards, indices:data/read/search] and User [name=booksuser, roles=[], requestedTenant=null]\" }, \"status\" : 403 } . Note the permissions error. On the remote cluster, create a role with the appropriate permissions, and map booksuser to that role: . curl -XPUT -k -u 'admin:admin' -H 'Content-Type: application/json' 'https://localhost:9200/_plugins/_security/api/roles/booksrole' -d '{\"index_permissions\":[{\"index_patterns\":[\"books\"],\"allowed_actions\":[\"indices:admin/shards/search_shards\",\"indices:data/read/search\"]}]}' curl -XPUT -k -u 'admin:admin' -H 'Content-Type: application/json' 'https://localhost:9200/_plugins/_security/api/rolesmapping/booksrole' -d '{\"users\" : [\"booksuser\"]}' . Both clusters must have the user, but only the remote cluster needs the role and mapping; in this case, the coordinating cluster handles authentication (i.e. “Does this request include valid user credentials?”), and the remote cluster handles authorization (i.e. “Can this user access this data?”). Finally, repeat the search: . curl -XGET -k -u booksuser:password 'https://localhost:9250/opensearch-ccs-cluster1:books/_search?pretty' { ... \"hits\": [{ \"_index\": \"opensearch-ccs-cluster1:books\", \"_id\": \"1\", \"_score\": 1.0, \"_source\": { \"Dracula\": \"Bram Stoker\" } }] } . ",
    "url": "https://vagimeli.github.io/security/access-control/cross-cluster-search/#walkthrough",
    "relUrl": "/security/access-control/cross-cluster-search/#walkthrough"
  },"555": {
    "doc": "Default action groups",
    "title": "Default action groups",
    "content": "This page catalogs all default action groups. Often, the most coherent way to create new action groups is to use a combination of these default groups and individual permissions. ",
    "url": "https://vagimeli.github.io/security/access-control/default-action-groups/",
    "relUrl": "/security/access-control/default-action-groups/"
  },"556": {
    "doc": "Default action groups",
    "title": "General",
    "content": "| Name | Description | . | unlimited | Grants complete access. Can be used on an cluster- or index-level. Equates to \"*\". | . ",
    "url": "https://vagimeli.github.io/security/access-control/default-action-groups/#general",
    "relUrl": "/security/access-control/default-action-groups/#general"
  },"557": {
    "doc": "Default action groups",
    "title": "Cluster-level",
    "content": "| Name | Description | . | cluster_all | Grants all cluster permissions. Equates to cluster:*. | . | cluster_monitor | Grants all cluster monitoring permissions. Equates to cluster:monitor/*. | . | cluster_composite_ops_ro | Grants read-only permissions to execute requests like mget, msearch, or mtv, plus permissions to query for aliases. | . | cluster_composite_ops | Same as CLUSTER_COMPOSITE_OPS_RO, but also grants bulk permissions and all aliases permissions. | . | manage_snapshots | Grants permissions to manage snapshots and repositories. | . | cluster_manage_pipelines | Grants permissions to manage ingest pipelines. | . | cluster_manage_index_templates | Grants permissions to manage index templates. | . ",
    "url": "https://vagimeli.github.io/security/access-control/default-action-groups/#cluster-level",
    "relUrl": "/security/access-control/default-action-groups/#cluster-level"
  },"558": {
    "doc": "Default action groups",
    "title": "Index-level",
    "content": "| Name | Description | . | indices_all | Grants all permissions on the index. Equates to indices:*. | . | get | Grants permissions to use get and mget actions only. | . | read | Grants read permissions such as search, get field mappings, get, and mget. | . | write | Grants permissions to create and update documents within existing indices. To create new indices, see create_index. | . | delete | Grants permissions to delete documents. | . | crud | Combines the read, write, and delete action groups. Included in the data_access action group. | . | search | Grants permissions to search documents. Includes suggest. | . | suggest | Grants permissions to use the suggest API. Included in the read action group. | . | create_index | Grants permissions to create indices and mappings. | . | indices_monitor | Grants permissions to execute all index monitoring actions (e.g. recovery, segments info, index stats, and status). | . | index | A more limited version of the write action group. | . | data_access | Combines the crud action group with indices:data/*. | . | manage_aliases | Grants permissions to manage aliases. | . | manage | Grants all monitoring and administration permissions for indices. | . ",
    "url": "https://vagimeli.github.io/security/access-control/default-action-groups/#index-level",
    "relUrl": "/security/access-control/default-action-groups/#index-level"
  },"559": {
    "doc": "Document-level security",
    "title": "Document-level security (DLS)",
    "content": "Document-level security lets you restrict a role to a subset of documents in an index. The easiest way to get started with document- and field-level security is to open OpenSearch Dashboards and choose Security. Then choose Roles, create a new role, and review the Index permissions section. ",
    "url": "https://vagimeli.github.io/security/access-control/document-level-security/#document-level-security-dls",
    "relUrl": "/security/access-control/document-level-security/#document-level-security-dls"
  },"560": {
    "doc": "Document-level security",
    "title": "Simple roles",
    "content": "Document-level security uses the OpenSearch query DSL to define which documents a role grants access to. In OpenSearch Dashboards, choose an index pattern and provide a query in the Document level security section: . { \"bool\": { \"must\": { \"match\": { \"genres\": \"Comedy\" } } } } . This query specifies that for the role to have access to a document, its genres field must include Comedy. A typical request to the _search API includes { \"query\": { ... } } around the query, but in this case, you only need to specify the query itself. In the REST API, you provide the query as a string, so you must escape your quotes. This role allows a user to read any document in any index with the field public set to true: . PUT _plugins/_security/api/roles/public_data { \"cluster_permissions\": [ \"*\" ], \"index_permissions\": [{ \"index_patterns\": [ \"pub*\" ], \"dls\": \"{\\\"term\\\": { \\\"public\\\": true}}\", \"allowed_actions\": [ \"read\" ] }] } . These queries can be as complex as you want, but we recommend keeping them simple to minimize the performance impact that the document-level security feature has on the cluster. A note on Unicode special characters in text fields . Due to word boundaries associated with Unicode special characters, the Unicode standard analyzer cannot index a text field type value as a whole value when it includes one of these special characters. As a result, a text field value that includes a special character is parsed by the standard analyzer as multiple values separated by the special character, effectively tokenizing the different elements on either side of it. This can lead to unintentional filtering of documents and potentially compromise control over their access. The examples below illustrate values containing special characters that will be parsed improperly by the standard analyzer. In this example, the existence of the hyphen/minus sign in the value prevents the analyzer from distinguishing between the two different users for user.id and interprets them as one and the same: . { \"bool\": { \"must\": { \"match\": { \"user.id\": \"User-1\" } } } } . { \"bool\": { \"must\": { \"match\": { \"user.id\": \"User-2\" } } } } . To avoid this circumstance when using either Query DSL or the REST API, you can use a custom analyzer or map the field as keyword, which performs an exact-match search. See Keyword field type for the latter option. For a list of characters that should be avoided when field type is text, see Word Boundaries. ",
    "url": "https://vagimeli.github.io/security/access-control/document-level-security/#simple-roles",
    "relUrl": "/security/access-control/document-level-security/#simple-roles"
  },"561": {
    "doc": "Document-level security",
    "title": "Parameter substitution",
    "content": "A number of variables exist that you can use to enforce rules based on the properties of a user. For example, ${user.name} is replaced with the name of the current user. This rule allows a user to read any document where the username is a value of the readable_by field: . PUT _plugins/_security/api/roles/user_data { \"cluster_permissions\": [ \"*\" ], \"index_permissions\": [{ \"index_patterns\": [ \"pub*\" ], \"dls\": \"{\\\"term\\\": { \\\"readable_by\\\": \\\"${user.name}\\\"}}\", \"allowed_actions\": [ \"read\" ] }] } . This table lists substitutions. | Term | Replaced with | . | ${user.name} | Username. | . | ${user.roles} | A comma-separated, quoted list of user backend roles. | . | ${user.securityRoles} | A comma-separated, quoted list of user security roles. | . | ${attr.&lt;TYPE&gt;.&lt;NAME&gt;} | An attribute with name &lt;NAME&gt; defined for a user. &lt;TYPE&gt; is internal, jwt, proxy or ldap | . ",
    "url": "https://vagimeli.github.io/security/access-control/document-level-security/#parameter-substitution",
    "relUrl": "/security/access-control/document-level-security/#parameter-substitution"
  },"562": {
    "doc": "Document-level security",
    "title": "Attribute-based security",
    "content": "You can use roles and parameter substitution with the terms_set query to enable attribute-based security. Note that the security_attributes of the index need to be of type keyword. User definition . PUT _plugins/_security/api/internalusers/user1 { \"password\": \"asdf\", \"backend_roles\": [\"abac\"], \"attributes\": { \"permissions\": \"\\\"att1\\\", \\\"att2\\\", \\\"att3\\\"\" } } . Role definition . PUT _plugins/_security/api/roles/abac { \"index_permissions\": [{ \"index_patterns\": [ \"*\" ], \"dls\": \"{\\\"terms_set\\\": {\\\"security_attributes\\\": {\\\"terms\\\": [${attr.internal.permissions}], \\\"minimum_should_match_script\\\": {\\\"source\\\": \\\"doc['security_attributes'].length\\\"}}}}\", \"allowed_actions\": [ \"read\" ] }] } . ",
    "url": "https://vagimeli.github.io/security/access-control/document-level-security/#attribute-based-security",
    "relUrl": "/security/access-control/document-level-security/#attribute-based-security"
  },"563": {
    "doc": "Document-level security",
    "title": "Use term-level lookup queries (TLQs) with DLS",
    "content": "You can perform term-level lookup queries (TLQs) with document-level security (DLS) using either of two modes: adaptive or filter level. The default mode is adaptive, where OpenSearch automatically switches between Lucene-level or filter-level mode depending on whether or not there is a TLQ. DLS queries without TLQs are executed in Lucene-level mode, whereas DLS queries with TLQs are executed in filter-level mode. By default, the Security plugin detects if a DLS query contains a TLQ or not and chooses the appropriate mode automatically at runtime. To learn more about OpenSearch queries, see Term-level queries. How to set the DLS evaluation mode in opensearch.yml . By default, the DLS evaluation mode is set to adaptive. You can also explicitly set the mode in opensearch.yml with the plugins.security.dls.mode setting. Add a line to opensearch.yml with the desired evaluation mode. For example, to set it to filter level, add this line: . plugins.security.dls.mode: filter-level . DLS evaluation modes . | Evaluation mode | Parameter | Description | Usage | . | Lucene-level DLS | lucene-level | This setting makes all DLS queries apply to the Lucene level. | Lucene-level DLS modifies Lucene queries and data structures directly. This is the most efficient mode but does not allow certain advanced constructs in DLS queries, including TLQs. | . | Filter-level DLS | filter-level | This setting makes all DLS queries apply to the filter level. | In this mode, OpenSearch applies DLS by modifying queries that OpenSearch receives. This allows for term-level lookup queries in DLS queries, but you can only use the get, search, mget, and msearch operations to retrieve data from the protected index. Additionally, cross-cluster searches are limited with this mode. | . | Adaptive | adaptive-level | The default setting that allows OpenSearch to automatically choose the mode. | DLS queries without TLQs are executed in Lucene-level mode, while DLS queries that contain TLQ are executed in filter- level mode. | . ",
    "url": "https://vagimeli.github.io/security/access-control/document-level-security/#use-term-level-lookup-queries-tlqs-with-dls",
    "relUrl": "/security/access-control/document-level-security/#use-term-level-lookup-queries-tlqs-with-dls"
  },"564": {
    "doc": "Document-level security",
    "title": "Document-level security",
    "content": " ",
    "url": "https://vagimeli.github.io/security/access-control/document-level-security/",
    "relUrl": "/security/access-control/document-level-security/"
  },"565": {
    "doc": "Field-level security",
    "title": "Field-level security",
    "content": "Field-level security lets you control which document fields a user can see. Just like document-level security, you control access by index within a role. The easiest way to get started with document- and field-level security is open OpenSearch Dashboards and choose Security. Then choose Roles, create a new role, and review the Index permissions section. . | Include or exclude fields . | OpenSearch Dashboards | roles.yml | REST API | . | Interaction with multiple roles | Interaction with document-level security | . ",
    "url": "https://vagimeli.github.io/security/access-control/field-level-security/",
    "relUrl": "/security/access-control/field-level-security/"
  },"566": {
    "doc": "Field-level security",
    "title": "Include or exclude fields",
    "content": "You have two options when you configure field-level security: include or exclude fields. If you include fields, users see only those fields when they retrieve a document. For example, if you include the actors, title, and year fields, a search result might look like this: . { \"_index\": \"movies\", \"_source\": { \"year\": 2013, \"title\": \"Rush\", \"actors\": [ \"Daniel Brühl\", \"Chris Hemsworth\", \"Olivia Wilde\" ] } } . If you exclude fields, users see everything but those fields when they retrieve a document. For example, if you exclude those same fields, the same search result might look like this: . { \"_index\": \"movies\", \"_source\": { \"directors\": [ \"Ron Howard\" ], \"plot\": \"A re-creation of the merciless 1970s rivalry between Formula One rivals James Hunt and Niki Lauda.\", \"genres\": [ \"Action\", \"Biography\", \"Drama\", \"Sport\" ] } } . You can achieve the same outcomes using inclusion or exclusion, so choose whichever makes sense for your use case. Mixing the two doesn’t make sense and is not supported. You can specify field-level security settings using OpenSearch Dashboards, roles.yml, and the REST API. | To exclude fields in roles.yml or the REST API, add ~ before the field name. | Field names support wildcards (*). Wildcards are especially useful for excluding subfields. For example, if you index a document that has a string (e.g. {\"title\": \"Thor\"}), OpenSearch creates a title field of type text, but it also creates a title.keyword subfield of type keyword. In this example, to prevent unauthorized access to data in the title field, you must also exclude the title.keyword subfield. Use title* to match all fields that begin with title. | . OpenSearch Dashboards . | Choose a role and Add index permission. | Choose an index pattern. | Under Field level security, use the drop-down to select your preferred option. Then specify one or more fields and press Enter. | . roles.yml . someonerole: cluster: [] indices: movies: '*': - \"READ\" _fls_: - \"~actors\" - \"~title\" - \"~year\" . REST API . See Create role. ",
    "url": "https://vagimeli.github.io/security/access-control/field-level-security/#include-or-exclude-fields",
    "relUrl": "/security/access-control/field-level-security/#include-or-exclude-fields"
  },"567": {
    "doc": "Field-level security",
    "title": "Interaction with multiple roles",
    "content": "If you map a user to multiple roles, we recommend that those roles use either include or exclude statements for each index. The Security plugin evaluates field-level security settings using the AND operator, so combining include and exclude statements can lead to neither behavior working properly. For example, in the movies index, if you include actors, title, and year in one role, exclude actors, title, and genres in another role, and then map both roles to the same user, a search result might look like this: . { \"_index\": \"movies\", \"_source\": { \"year\": 2013, \"directors\": [ \"Ron Howard\" ], \"plot\": \"A re-creation of the merciless 1970s rivalry between Formula One rivals James Hunt and Niki Lauda.\" } } . ",
    "url": "https://vagimeli.github.io/security/access-control/field-level-security/#interaction-with-multiple-roles",
    "relUrl": "/security/access-control/field-level-security/#interaction-with-multiple-roles"
  },"568": {
    "doc": "Field-level security",
    "title": "Interaction with document-level security",
    "content": "Document-level security relies on OpenSearch queries, which means that all fields in the query must be visible in order for it to work properly. If you use field-level security in conjunction with document-level security, make sure you don’t restrict access to the fields that document-level security uses. ",
    "url": "https://vagimeli.github.io/security/access-control/field-level-security/#interaction-with-document-level-security",
    "relUrl": "/security/access-control/field-level-security/#interaction-with-document-level-security"
  },"569": {
    "doc": "Field masking",
    "title": "Field masking",
    "content": "If you don’t want to remove fields from a document using field-level security, you can mask their values. Currently, field masking is only available for string-based fields and replaces the field’s value with a cryptographic hash. Field masking works alongside field-level security on the same per-role, per-index basis. You can allow certain roles to see sensitive fields in plain text and mask them for others. A search result with a masked field might look like this: . { \"_index\": \"movies\", \"_source\": { \"year\": 2013, \"directors\": [ \"Ron Howard\" ], \"title\": \"ca998e768dd2e6cdd84c77015feb29975f9f498a472743f159bec6f1f1db109e\" } } . ",
    "url": "https://vagimeli.github.io/security/access-control/field-masking/",
    "relUrl": "/security/access-control/field-masking/"
  },"570": {
    "doc": "Field masking",
    "title": "Set the salt",
    "content": "You set the salt (a random string used to hash your data) in opensearch.yml: . plugins.security.compliance.salt: abcdefghijklmnopqrstuvqxyz1234567890 . | Property | Description | . | plugins.security.compliance.salt | The salt to use when generating the hash value. Must be at least 32 characters. Only ASCII characters are allowed. Optional. | . Setting the salt is optional, but we highly recommend it. ",
    "url": "https://vagimeli.github.io/security/access-control/field-masking/#set-the-salt",
    "relUrl": "/security/access-control/field-masking/#set-the-salt"
  },"571": {
    "doc": "Field masking",
    "title": "Configure field masking",
    "content": "You configure field masking using OpenSearch Dashboards, roles.yml, or the REST API. OpenSearch Dashboards . | Choose a role. | Choose an index permission. | For Anonymization, specify one or more fields and press Enter. | . roles.yml . someonerole: cluster: [] indices: movies: _masked_fields_: - \"title\" - \"genres\" '*': - \"READ\" . REST API . See Create role. ",
    "url": "https://vagimeli.github.io/security/access-control/field-masking/#configure-field-masking",
    "relUrl": "/security/access-control/field-masking/#configure-field-masking"
  },"572": {
    "doc": "Field masking",
    "title": "(Advanced) Use an alternative hash algorithm",
    "content": "By default, the Security plugin uses the BLAKE2b algorithm, but you can use any hashing algorithm that your JVM provides. This list typically includes MD5, SHA-1, SHA-384, and SHA-512. To specify a different algorithm, add it after the masked field: . someonerole: cluster: [] indices: movies: _masked_fields_: - \"title::SHA-512\" - \"genres\" '*': - \"READ\" . ",
    "url": "https://vagimeli.github.io/security/access-control/field-masking/#advanced-use-an-alternative-hash-algorithm",
    "relUrl": "/security/access-control/field-masking/#advanced-use-an-alternative-hash-algorithm"
  },"573": {
    "doc": "Field masking",
    "title": "(Advanced) Pattern-based field masking",
    "content": "Rather than creating a hash, you can use one or more regular expressions and replacement strings to mask a field. The syntax is &lt;field&gt;::/&lt;regular-expression&gt;/::&lt;replacement-string&gt;. If you use multiple regular expressions, the results are passed from left to right, like piping in a shell: . hr_employee: index_permissions: - index_patterns: - 'humanresources' allowed_actions: - ... masked_fields: - 'lastname::/.*/::*' - '*ip_source::/[0-9]{1,3}$/::XXX::/^[0-9]{1,3}/::***' someonerole: cluster: [] indices: movies: _masked_fields_: - \"title::/./::*\" - \"genres::/^[a-zA-Z]{1,3}/::XXX::/[a-zA-Z]{1,3}$/::YYY\" '*': - \"READ\" . The title statement changes each character in the field to *, so you can still discern the length of the masked string. The genres statement changes the first three characters of the string to XXX and the last three characters to YYY. ",
    "url": "https://vagimeli.github.io/security/access-control/field-masking/#advanced-pattern-based-field-masking",
    "relUrl": "/security/access-control/field-masking/#advanced-pattern-based-field-masking"
  },"574": {
    "doc": "Field masking",
    "title": "Effect on audit logging",
    "content": "The read history feature lets you track read access to sensitive fields in your documents. For example, you might track access to the email field of your customer records. Access to masked fields are excluded from read history, because the user only saw the hash value, not the clear text value of the field. ",
    "url": "https://vagimeli.github.io/security/access-control/field-masking/#effect-on-audit-logging",
    "relUrl": "/security/access-control/field-masking/#effect-on-audit-logging"
  },"575": {
    "doc": "User impersonation",
    "title": "User impersonation",
    "content": "User impersonation allows specially privileged users to act as another user without knowledge of nor access to the impersonated user’s credentials. Impersonation can be useful for testing and troubleshooting, or for allowing system services to safely act as a user. Impersonation can occur on either the REST interface or at the transport layer. ",
    "url": "https://vagimeli.github.io/security/access-control/impersonation/",
    "relUrl": "/security/access-control/impersonation/"
  },"576": {
    "doc": "User impersonation",
    "title": "REST interface",
    "content": "To allow one user to impersonate another, add the following to opensearch.yml: . plugins.security.authcz.rest_impersonation_user: &lt;AUTHENTICATED_USER&gt;: - &lt;IMPERSONATED_USER_1&gt; - &lt;IMPERSONATED_USER_2&gt; . The impersonated user field supports wildcards. Setting it to * allows AUTHENTICATED_USER to impersonate any user. ",
    "url": "https://vagimeli.github.io/security/access-control/impersonation/#rest-interface",
    "relUrl": "/security/access-control/impersonation/#rest-interface"
  },"577": {
    "doc": "User impersonation",
    "title": "Transport interface",
    "content": "In a similar fashion, add the following to enable transport layer impersonation: . plugins.security.authcz.impersonation_dn: \"CN=spock,OU=client,O=client,L=Test,C=DE\": - worf . ",
    "url": "https://vagimeli.github.io/security/access-control/impersonation/#transport-interface",
    "relUrl": "/security/access-control/impersonation/#transport-interface"
  },"578": {
    "doc": "User impersonation",
    "title": "Impersonating Users",
    "content": "To impersonate another user, submit a request to the system with the HTTP header opendistro_security_impersonate_as set to the name of the user to be impersonated. A good test is to make a GET request to the _plugins/_security/authinfo URI: . curl -XGET -u 'admin:admin' -k -H \"opendistro_security_impersonate_as: user_1\" https://localhost:9200/_plugins/_security/authinfo?pretty . ",
    "url": "https://vagimeli.github.io/security/access-control/impersonation/#impersonating-users",
    "relUrl": "/security/access-control/impersonation/#impersonating-users"
  },"579": {
    "doc": "Access control",
    "title": "Access control",
    "content": "After you configure the Security plugin to use your own certificates and preferred authentication backend, you can start adding users, creating roles, and mapping roles to users. This section of the documentation covers what a user is allowed to see and do after successfully authenticating. ",
    "url": "https://vagimeli.github.io/security/access-control/index/",
    "relUrl": "/security/access-control/index/"
  },"580": {
    "doc": "Access control",
    "title": "Concepts",
    "content": "| Term | Description | . | Permission | An individual action, such as creating an index (e.g. indices:admin/create). For a complete list, see Permissions. | . | Action group | A set of permissions. For example, the predefined SEARCH action group authorizes roles to use the _search and _msearch APIs. | . | Role | Security roles define the scope of a permission or action group: cluster, index, document, or field. For example, a role named delivery_analyst might have no cluster permissions, the READ action group for all indexes that match the delivery-data-* pattern, access to all document types within those indexes, and access to all fields except delivery_driver_name. | . | Backend role | (Optional) Arbitrary strings that you specify or that come from an external authentication system (for example, LDAP/Active Directory). Backend roles can help simplify the role mapping process. Rather than mapping a role to 100 individual users, you can map the role to a single backend role that all 100 users share. | . | User | Users make requests to OpenSearch clusters. A user has credentials (e.g. a username and password), zero or more backend roles, and zero or more custom attributes. | . | Role mapping | Users assume roles after they successfully authenticate. Role mappings map roles to users (or backend roles). For example, a mapping of kibana_user (role) to jdoe (user) means that John Doe gains all the permissions of kibana_user after authenticating. Likewise, a mapping of all_access (role) to admin (backend role) means that any user with the backend role of admin gains all the permissions of all_access after authenticating. You can map each role to multiple users and/or backend roles. | . The Security plugin comes with a number of predefined action groups, roles, mappings, and users. These entities serve as sensible defaults and are good examples of how to use the plugin. ",
    "url": "https://vagimeli.github.io/security/access-control/index/#concepts",
    "relUrl": "/security/access-control/index/#concepts"
  },"581": {
    "doc": "Permissions",
    "title": "Permissions",
    "content": "Each permission in the Security plugin controls access to some action that the OpenSearch cluster can perform, such as indexing a document or checking cluster health. Most permissions are self-describing. For example, cluster:admin/ingest/pipeline/get lets you retrieve information about ingest pipelines. In many cases, a permission correlates to a specific REST API operation, such as GET _ingest/pipeline. Despite this correlation, permissions do not directly map to REST API operations. Operations such as POST _bulk and GET _msearch can access many indexes and perform many actions in a single request. Even a simple request, such as GET _cat/nodes, performs several actions in order to generate its response. In short, controlling access to the REST API is insufficient. Instead, the Security plugin controls access to the underlying OpenSearch actions. For example, consider the following _bulk request: . POST _bulk { \"delete\": { \"_index\": \"test-index\", \"_id\": \"tt2229499\" } } { \"index\": { \"_index\": \"test-index\", \"_id\": \"tt1979320\" } } { \"title\": \"Rush\", \"year\": 2013 } { \"create\": { \"_index\": \"test-index\", \"_id\": \"tt1392214\" } } { \"title\": \"Prisoners\", \"year\": 2013 } { \"update\": { \"_index\": \"test-index\", \"_id\": \"tt0816711\" } } { \"doc\" : { \"title\": \"World War Z\" } } . For this request to succeed, you must have the following permissions for test-index: . | indices:data/write/bulk* | indices:data/write/delete | indices:data/write/index | indices:data/write/update | . These permissions also allow you add, update, or delete documents (e.g. PUT test-index/_doc/tt0816711), because they govern the underlying OpenSearch actions of indexing and deleting documents rather than a specific API path and HTTP method. ",
    "url": "https://vagimeli.github.io/security/access-control/permissions/",
    "relUrl": "/security/access-control/permissions/"
  },"582": {
    "doc": "Permissions",
    "title": "Test permissions",
    "content": "If you want a user to have the absolute minimum set of permissions necessary to perform some function—the principle of least privilege—the best way is to send representative requests to your cluster as a new test user. In the case of a permissions error, the Security plugin is very explicit about which permissions are missing. Consider this request and response: . GET _cat/shards?v { \"error\": { \"root_cause\": [{ \"type\": \"security_exception\", \"reason\": \"no permissions for [indices:monitor/stats] and User [name=test-user, backend_roles=[], requestedTenant=null]\" }] }, \"status\": 403 } . Create a user and a role, map the role to the user, and start sending signed requests using curl, Postman, or any other client. Then gradually add permissions to the role as you encounter errors. Even after you resolve one permissions error, the same request might generate new errors; the plugin only returns the first error it encounters, so keep trying until the request succeeds. Rather than individual permissions, you can often achieve your desired security posture using a combination of the default action groups. See Default action groups for descriptions of the permissions that each group grants. ",
    "url": "https://vagimeli.github.io/security/access-control/permissions/#test-permissions",
    "relUrl": "/security/access-control/permissions/#test-permissions"
  },"583": {
    "doc": "Permissions",
    "title": "Cluster permissions",
    "content": "These permissions are for the cluster and can’t be applied granularly. For example, you either have permissions to take snapshots (cluster:admin/snapshot/create) or you don’t. The cluster permission, therefore, cannot grant a user privileges to take snapshots of a select set of indexes while preventing the user from taking snapshots of others. Cross-references to API documentation in the permissions that follow are only intended to provide an understanding of the permissions. As stated at the beginning of this section, permissions often correlate to APIs but do not map directly to them. Ingest API permissions . See Ingest APIs. | cluster:admin/ingest/pipeline/delete | cluster:admin/ingest/pipeline/get | cluster:admin/ingest/pipeline/put | cluster:admin/ingest/pipeline/simulate | cluster:admin/ingest/processor/grok/get | . Anomaly Detection permissions . See Anomaly detection API. | cluster:admin/opendistro/ad/detector/delete | cluster:admin/opendistro/ad/detector/info | cluster:admin/opendistro/ad/detector/jobmanagement | cluster:admin/opendistro/ad/detector/preview | cluster:admin/opendistro/ad/detector/run | cluster:admin/opendistro/ad/detector/search | cluster:admin/opendistro/ad/detector/stats | cluster:admin/opendistro/ad/detector/write | cluster:admin/opendistro/ad/detector/validate | cluster:admin/opendistro/ad/detectors/get | cluster:admin/opendistro/ad/result/search | cluster:admin/opendistro/ad/result/topAnomalies | cluster:admin/opendistro/ad/tasks/search | . Alerting permissions . See Alerting API. | cluster:admin/opendistro/alerting/alerts/ack | cluster:admin/opendistro/alerting/alerts/get | cluster:admin/opendistro/alerting/destination/delete | cluster:admin/opendistro/alerting/destination/email_account/delete | cluster:admin/opendistro/alerting/destination/email_account/get | cluster:admin/opendistro/alerting/destination/email_account/search | cluster:admin/opendistro/alerting/destination/email_account/write | cluster:admin/opendistro/alerting/destination/email_group/delete | cluster:admin/opendistro/alerting/destination/email_group/get | cluster:admin/opendistro/alerting/destination/email_group/search | cluster:admin/opendistro/alerting/destination/email_group/write | cluster:admin/opendistro/alerting/destination/get | cluster:admin/opendistro/alerting/destination/write | cluster:admin/opendistro/alerting/monitor/delete | cluster:admin/opendistro/alerting/monitor/execute | cluster:admin/opendistro/alerting/monitor/get | cluster:admin/opendistro/alerting/monitor/search | cluster:admin/opendistro/alerting/monitor/write | . Asynchronous Search permissions . See Asynchronous search. | cluster:admin/opendistro/asynchronous_search/stats | cluster:admin/opendistro/asynchronous_search/delete | cluster:admin/opendistro/asynchronous_search/get | cluster:admin/opendistro/asynchronous_search/submit | . Index State Management permissions . See ISM API. | cluster:admin/opendistro/ism/managedindex/add | cluster:admin/opendistro/ism/managedindex/change | cluster:admin/opendistro/ism/managedindex/remove | cluster:admin/opendistro/ism/managedindex/explain | cluster:admin/opendistro/ism/managedindex/retry | cluster:admin/opendistro/ism/policy/write | cluster:admin/opendistro/ism/policy/get | cluster:admin/opendistro/ism/policy/search | cluster:admin/opendistro/ism/policy/delete | . Index rollups permissions . See Index rollups API. | cluster:admin/opendistro/rollup/index | cluster:admin/opendistro/rollup/get | cluster:admin/opendistro/rollup/search | cluster:admin/opendistro/rollup/delete | cluster:admin/opendistro/rollup/start | cluster:admin/opendistro/rollup/stop | cluster:admin/opendistro/rollup/explain | . Reporting permissions . See Creating reports with the Dashboards interface. | cluster:admin/opendistro/reports/definition/create | cluster:admin/opendistro/reports/definition/update | cluster:admin/opendistro/reports/definition/on_demand | cluster:admin/opendistro/reports/definition/delete | cluster:admin/opendistro/reports/definition/get | cluster:admin/opendistro/reports/definition/list | cluster:admin/opendistro/reports/instance/list | cluster:admin/opendistro/reports/instance/get | cluster:admin/opendistro/reports/menu/download | . Transform job permissions . See Transforms APIs . | cluster:admin/opendistro/transform/index | cluster:admin/opendistro/transform/get | cluster:admin/opendistro/transform/preview | cluster:admin/opendistro/transform/delete | cluster:admin/opendistro/transform/start | cluster:admin/opendistro/transform/stop | cluster:admin/opendistro/transform/explain | . Observability permissions . See Observability security. | cluster:admin/opensearch/observability/create | cluster:admin/opensearch/observability/update | cluster:admin/opensearch/observability/delete | cluster:admin/opensearch/observability/get | . Cross-cluster replication . See Cross-cluster replication security. | cluster:admin/plugins/replication/autofollow/update | . Reindex . See Reindex document. | cluster:admin/reindex/rethrottle | . Snapshot repository permissions . See Snapshot APIs. | cluster:admin/repository/delete | cluster:admin/repository/get | cluster:admin/repository/put | cluster:admin/repository/verify | . Reroute . See Cluster manager task throttling. | cluster:admin/reroute | . Script permissions . See Script APIs. | cluster:admin/script/delete | cluster:admin/script/get | cluster:admin/script/put | . Update settings permission . See Update settings on the Index APIs page. | cluster:admin/settings/update | . Snapshot permissions . See Snapshot APIs. | cluster:admin/snapshot/create | cluster:admin/snapshot/delete | cluster:admin/snapshot/get | cluster:admin/snapshot/restore | cluster:admin/snapshot/status | cluster:admin/snapshot/status* | . Task permissions . See Tasks in the API Reference section. | cluster:admin/tasks/cancel | cluster:admin/tasks/test | cluster:admin/tasks/testunblock | . Security Analytics permissions . See API tools. | Permission | Description | . | cluster:admin/opensearch/securityanalytics/alerts/get | Permission to get alerts | . | cluster:admin/opensearch/securityanalytics/alerts/ack | Permission to acknowledge alerts | . | cluster:admin/opensearch/securityanalytics/detector/get | Permission to get detectors | . | cluster:admin/opensearch/securityanalytics/detector/search | Permission to search detectors | . | cluster:admin/opensearch/securityanalytics/detector/write | Permission to create and update detectors | . | cluster:admin/opensearch/securityanalytics/detector/delete | Permission to delete detectors | . | cluster:admin/opensearch/securityanalytics/findings/get | Permission to get findings | . | cluster:admin/opensearch/securityanalytics/mapping/get | Permission to get field mappings by index | . | cluster:admin/opensearch/securityanalytics/mapping/view/get | Permission to get field mappings by index and view mapped and unmapped fields | . | cluster:admin/opensearch/securityanalytics/mapping/create | Permission to create field mappings | . | cluster:admin/opensearch/securityanalytics/mapping/update | Permission to update field mappings | . | cluster:admin/opensearch/securityanalytics/rules/categories | Permission to get all rule categories | . | cluster:admin/opensearch/securityanalytics/rule/write | Permission to create and update rules | . | cluster:admin/opensearch/securityanalytics/rule/search | Permission to search for rules | . | cluster:admin/opensearch/securityanalytics/rules/validate | Permission to validate rules | . | cluster:admin/opensearch/securityanalytics/rule/delete | Permission to delete rules | . Monitoring permissions . Cluster permissions for monitoring the cluster apply to read-only operations, such as checking cluster health and getting information about usage on nodes or tasks running in the cluster. See REST API reference. | cluster:monitor/allocation/explain | cluster:monitor/health | cluster:monitor/main | cluster:monitor/nodes/hot_threads | cluster:monitor/nodes/info | cluster:monitor/nodes/liveness | cluster:monitor/nodes/stats | cluster:monitor/nodes/usage | cluster:monitor/remote/info | cluster:monitor/state | cluster:monitor/stats | cluster:monitor/task | cluster:monitor/task/get | cluster:monitor/tasks/list | . Index templates . The index template permissions are for indexes but apply globally to the cluster. See Index templates. | indices:admin/index_template/delete | indices:admin/index_template/get | indices:admin/index_template/put | indices:admin/index_template/simulate | indices:admin/index_template/simulate_index | . ",
    "url": "https://vagimeli.github.io/security/access-control/permissions/#cluster-permissions",
    "relUrl": "/security/access-control/permissions/#cluster-permissions"
  },"584": {
    "doc": "Permissions",
    "title": "Index permissions",
    "content": "These permissions apply to an index or index pattern. You might want a user to have read access to all indexes (that is, *), but write access to only a few (for example, web-logs and product-catalog). | indices:admin/aliases | indices:admin/aliases/exists | indices:admin/aliases/get | indices:admin/analyze | indices:admin/cache/clear | indices:admin/close | indices:admin/close* | indices:admin/create (create indexes) | indices:admin/data_stream/create | indices:admin/data_stream/delete | indices:admin/data_stream/get | indices:admin/delete (delete indexes) | indices:admin/exists | indices:admin/flush | indices:admin/flush* | indices:admin/forcemerge | indices:admin/get (retrieve index and mapping) | indices:admin/mapping/put | indices:admin/mappings/fields/get | indices:admin/mappings/fields/get* | indices:admin/mappings/get | indices:admin/open | indices:admin/plugins/replication/index/setup/validate | indices:admin/plugins/replication/index/start | indices:admin/plugins/replication/index/pause | indices:admin/plugins/replication/index/resume | indices:admin/plugins/replication/index/stop | indices:admin/plugins/replication/index/update | indices:admin/plugins/replication/index/status_check | indices:admin/refresh | indices:admin/refresh* | indices:admin/resolve/index | indices:admin/rollover | indices:admin/seq_no/global_checkpoint_sync | indices:admin/settings/update | indices:admin/shards/search_shards | indices:admin/shrink | indices:admin/synced_flush | indices:admin/template/delete | indices:admin/template/get | indices:admin/template/put | indices:admin/types/exists | indices:admin/upgrade | indices:admin/validate/query | indices:data/read/explain | indices:data/read/field_caps | indices:data/read/field_caps* | indices:data/read/get | indices:data/read/mget | indices:data/read/mget* | indices:data/read/msearch | indices:data/read/msearch/template | indices:data/read/mtv (multi-term vectors) | indices:data/read/mtv* | indices:data/read/plugins/replication/file_chunk | indices:data/read/plugins/replication/changes | indices:data/read/scroll | indices:data/read/scroll/clear | indices:data/read/search | indices:data/read/search* | indices:data/read/search/template | indices:data/read/tv (term vectors) | indices:data/write/bulk | indices:data/write/bulk* | indices:data/write/delete (delete documents) | indices:data/write/delete/byquery | indices:data/write/plugins/replication/changes | indices:data/write/index (add documents to existing indexes) | indices:data/write/reindex | indices:data/write/update | indices:data/write/update/byquery | indices:monitor/data_stream/stats | indices:monitor/recovery | indices:monitor/segments | indices:monitor/settings/get | indices:monitor/shard_stores | indices:monitor/stats | indices:monitor/upgrade | . ",
    "url": "https://vagimeli.github.io/security/access-control/permissions/#index-permissions",
    "relUrl": "/security/access-control/permissions/#index-permissions"
  },"585": {
    "doc": "Users and roles",
    "title": "Users and roles",
    "content": "The Security plugin includes an internal user database. Use this database in place of or in addition to an external authentication system such as LDAP or Active Directory. Roles are the core way of controlling access to your cluster. Roles contain any combination of cluster-wide permissions, index-specific permissions, document- and field-level security, and tenants. Then you map users to these roles so that users gain those permissions. Unless you need to create new reserved or hidden users, we highly recommend using OpenSearch Dashboards or the REST API to create new users, roles, and role mappings. The .yml files are for initial setup, not ongoing use. . | Create users . | OpenSearch Dashboards | internal_users.yml | REST API | . | Create roles . | OpenSearch Dashboards | roles.yml | REST API | . | Map users to roles . | OpenSearch Dashboards | roles_mapping.yml | REST API | . | Predefined roles | Sample roles . | Set up a read-only user in OpenSearch Dashboards | Set up a bulk access role in OpenSearch Dashboards | . | . ",
    "url": "https://vagimeli.github.io/security/access-control/users-roles/",
    "relUrl": "/security/access-control/users-roles/"
  },"586": {
    "doc": "Users and roles",
    "title": "Create users",
    "content": "You can create users using OpenSearch Dashboards, internal_users.yml, or the REST API. When creating a user, you can map users to roles using internal_users.yml or the REST API, but that feature is not currently available in OpenSearch Dashboards. OpenSearch Dashboards . | Choose Security, Internal Users, and Create internal user. | Provide a username and password. The Security plugin automatically hashes the password and stores it in the .opendistro_security index. | If desired, specify user attributes. Attributes are optional user properties that you can use for variable substitution in index permissions or document-level security. | Choose Submit. | . internal_users.yml . See YAML files. REST API . See Create user. ",
    "url": "https://vagimeli.github.io/security/access-control/users-roles/#create-users",
    "relUrl": "/security/access-control/users-roles/#create-users"
  },"587": {
    "doc": "Users and roles",
    "title": "Create roles",
    "content": "Just like users, you can create roles using OpenSearch Dashboards, roles.yml, or the REST API. OpenSearch Dashboards . | Choose Security, Roles, and Create role. | Provide a name for the role. | Add permissions as desired. For example, you might give a role no cluster permissions, read permissions to two indexes, unlimited permissions to a third index, and read permissions to the analysts tenant. | Choose Submit. | . roles.yml . See YAML files. REST API . See Create role. ",
    "url": "https://vagimeli.github.io/security/access-control/users-roles/#create-roles",
    "relUrl": "/security/access-control/users-roles/#create-roles"
  },"588": {
    "doc": "Users and roles",
    "title": "Map users to roles",
    "content": "If you didn’t specify roles when you created your user, you can map roles to it afterwards. Just like users and roles, you create role mappings using OpenSearch Dashboards, roles_mapping.yml, or the REST API. OpenSearch Dashboards . | Choose Security, Roles, and a role. | Choose the Mapped users tab and Manage mapping. | Specify users or external identities (also known as backend roles). | Choose Map. | . roles_mapping.yml . See YAML files. REST API . See Create role mapping. ",
    "url": "https://vagimeli.github.io/security/access-control/users-roles/#map-users-to-roles",
    "relUrl": "/security/access-control/users-roles/#map-users-to-roles"
  },"589": {
    "doc": "Users and roles",
    "title": "Predefined roles",
    "content": "The Security plugin includes several predefined roles that serve as useful defaults. | Role | Description | . | alerting_ack_alerts | Grants permissions to view and acknowledge alerts, but not to modify destinations or monitors. | . | alerting_full_access | Grants full permissions to all alerting actions. | . | alerting_read_access | Grants permissions to view alerts, destinations, and monitors, but not to acknowledge alerts or modify destinations or monitors. | . | anomaly_full_access | Grants full permissions to all anomaly detection actions. | . | anomaly_read_access | Grants permissions to view detectors, but not to create, modify, or delete detectors. | . | all_access | Grants full access to the cluster, including all cluster-wide operations, permission to write to all cluster indexes, and permission to write to all tenants. For more information on access using the REST API, see Access control for the API. | . | cross_cluster_replication_follower_full_access | Grants full access to perform cross-cluster replication actions on the follower cluster. | . | cross_cluster_replication_leader_full_access | Grants full access to perform cross-cluster replication actions on the leader cluster. | . | observability_full_access | Grants full access to perform actions on Observability objects such as visualizations, notebooks, and operational panels. | . | observability_read_access | Grants permission to view Observability objects such as visualizations, notebooks, and operational panels, but not to create, modify, or delete them. | . | kibana_read_only | A special role that prevents users from making changes to visualizations, dashboards, and other OpenSearch Dashboards objects. To enable read-only mode in Dashboards, add the opensearch_security.readonly_mode.roles setting to the opensearch_dashboards.yml file and include the role as a setting value. See the example configuration in Dashboards documentation. | . | kibana_user | Grants permissions to use OpenSearch Dashboards: cluster-wide searches, index monitoring, and write to various OpenSearch Dashboards indexes. | . | logstash | Grants permissions for Logstash to interact with the cluster: cluster-wide searches, cluster monitoring, and write to the various Logstash indexes. | . | manage_snapshots | Grants permissions to manage snapshot repositories, take snapshots, and restore snapshots. | . | readall | Grants permissions for cluster-wide searches like msearch and search permissions for all indexes. | . | readall_and_monitor | Same as readall but with added cluster permissions for monitoring. | . | security_rest_api_access | A special role that allows access to the REST API. See plugins.security.restapi.roles_enabled in opensearch.yml and Access control for the API. | . | reports_read_access | Grants permissions to generate on-demand reports, download existing reports, and view report definitions but not to create report definitions. | . | reports_instances_read_access | Grants permissions to generate on-demand reports and download existing reports but not to view or create report definitions. | . | reports_full_access | Grants full permissions to reports. | . | asynchronous_search_full_access | Grants full permissions to all asynchronous search actions. | . | asynchronous_search_read_access | Grants permissions to view asynchronous searches but not to submit, modify, or delete them. | . | index_management_full_access | Grants full permissions to all index management actions, including Index State Management (ISM), transforms, and rollups. | . | snapshot_management_full_access | Grants full permissions to all snapshot management actions. | . | snapshot_management_read_access | Grants permissions to view policies but not to create, modify, start, stop, or delete them. | . | point_in_time_full_access | Grants full permissions to all Point in Time operations. | . | security_analytics_full_access | Grants full permissions to all Security Analytics functionality. | . | security_analytics_read_access | Grants permissions to view the various components in Security Analytics, such as detectors, alerts, and findings. It also includes permissions that allow users to search for detectors and rules. This role does not allow a user to perform actions such as modifying or deleting a detector. | . | security_analytics_ack_alerts | Grants permissions to view and acknowledge alerts. | . For more detailed summaries of the permissions for each role, reference their action groups against the descriptions in Default action groups. ",
    "url": "https://vagimeli.github.io/security/access-control/users-roles/#predefined-roles",
    "relUrl": "/security/access-control/users-roles/#predefined-roles"
  },"590": {
    "doc": "Users and roles",
    "title": "Sample roles",
    "content": "The following examples demonstrate how you might set up a read-only role and a bulk access role. Set up a read-only user in OpenSearch Dashboards . Create a new read_only_index role: . | Open OpenSearch Dashboards. | Choose Security, Roles. | Create a new role named read_only_index. | For Cluster permissions, add the cluster_composite_ops_ro action group. | For Index Permissions, add an index pattern. For example, you might specify my-index-*. | For index permissions, add the read action group. | Choose Create. | . Map three roles to the read-only user: . | Choose the Mapped users tab and Manage mapping. | For Internal users, add your read-only user. | Choose Map. | Repeat these steps for the opensearch_dashboards_user and opensearch_dashboards_read_only roles. | . Set up a bulk access role in OpenSearch Dashboards . Create a new bulk_access role: . | Open OpenSearch Dashboards. | Choose Security, Roles. | Create a new role named bulk_access. | For Cluster permissions, add the cluster_composite_ops action group. | For Index Permissions, add an index pattern. For example, you might specify my-index-*. | For index permissions, add the write action group. | Choose Create. | . Map the role to your user: . | Choose the Mapped users tab and Manage mapping. | For Internal users, add your bulk access user. | Choose Map. | . ",
    "url": "https://vagimeli.github.io/security/access-control/users-roles/#sample-roles",
    "relUrl": "/security/access-control/users-roles/#sample-roles"
  },"591": {
    "doc": "Audit log field reference",
    "title": "Audit log field reference",
    "content": "This page contains descriptions for all audit log fields. ",
    "url": "https://vagimeli.github.io/security/audit-logs/field-reference/",
    "relUrl": "/security/audit-logs/field-reference/"
  },"592": {
    "doc": "Audit log field reference",
    "title": "Common attributes",
    "content": "The following attributes are logged for all event categories, independent of the layer. | Name | Description | . | audit_format_version | The audit log message format version. | . | audit_category | The audit log category. FAILED_LOGIN, MISSING_PRIVILEGES, BAD_HEADERS, SSL_EXCEPTION, OPENSEARCH_SECURITY_INDEX_ATTEMPT, AUTHENTICATED, or GRANTED_PRIVILEGES. | . | audit_node_id | The ID of the node where the event was generated. | . | audit_node_name | The name of the node where the event was generated. | . | audit_node_host_address | The host address of the node where the event was generated. | . | audit_node_host_name | The host name of the node where the event was generated. | . | audit_request_layer | The layer on which the event has been generated, either TRANSPORT or REST. | . | audit_request_origin | The layer from which the event originated, either TRANSPORT or REST. | . | audit_request_effective_user_is_admin | True if the request was made with a TLS admin certificate, otherwise false. | . ",
    "url": "https://vagimeli.github.io/security/audit-logs/field-reference/#common-attributes",
    "relUrl": "/security/audit-logs/field-reference/#common-attributes"
  },"593": {
    "doc": "Audit log field reference",
    "title": "REST FAILED_LOGIN attributes",
    "content": "| Name | Description | . | audit_request_effective_user | The username that failed to authenticate. | . | audit_rest_request_path | The REST endpoint URI. | . | audit_rest_request_params | The HTTP request parameters, if any. | . | audit_rest_request_headers | The HTTP headers, if any. | . | audit_request_initiating_user | The user that initiated the request. Only logged if it differs from the effective user. | . | audit_request_body | The HTTP request body, if any (and if request body logging is enabled). | . ",
    "url": "https://vagimeli.github.io/security/audit-logs/field-reference/#rest-failed_login-attributes",
    "relUrl": "/security/audit-logs/field-reference/#rest-failed_login-attributes"
  },"594": {
    "doc": "Audit log field reference",
    "title": "REST AUTHENTICATED attributes",
    "content": "| Name | Description | . | audit_request_effective_user | The username that failed to authenticate. | . | audit_request_initiating_user | The user that initiated the request. Only logged if it differs from the effective user. | . | audit_rest_request_path | The REST endpoint URI. | . | audit_rest_request_params | The HTTP request parameters, if any. | . | audit_rest_request_headers | The HTTP headers, if any. | . | audit_request_body | The HTTP request body, if any (and if request body logging is enabled). | . ",
    "url": "https://vagimeli.github.io/security/audit-logs/field-reference/#rest-authenticated-attributes",
    "relUrl": "/security/audit-logs/field-reference/#rest-authenticated-attributes"
  },"595": {
    "doc": "Audit log field reference",
    "title": "REST SSL_EXCEPTION attributes",
    "content": "| Name | Description | . | audit_request_exception_stacktrace | The stack trace of the SSL exception. | . ",
    "url": "https://vagimeli.github.io/security/audit-logs/field-reference/#rest-ssl_exception-attributes",
    "relUrl": "/security/audit-logs/field-reference/#rest-ssl_exception-attributes"
  },"596": {
    "doc": "Audit log field reference",
    "title": "REST BAD_HEADERS attributes",
    "content": "| Name | Description | . | audit_rest_request_path | The REST endpoint URI. | . | audit_rest_request_params | The HTTP request parameters, if any. | . | audit_rest_request_headers | The HTTP headers, if any. | . | audit_request_body | The HTTP request body, if any (and if request body logging is enabled). | . ",
    "url": "https://vagimeli.github.io/security/audit-logs/field-reference/#rest-bad_headers-attributes",
    "relUrl": "/security/audit-logs/field-reference/#rest-bad_headers-attributes"
  },"597": {
    "doc": "Audit log field reference",
    "title": "Transport FAILED_LOGIN attributes",
    "content": "| Name | Description | . | audit_trace_task_id | The ID of the request. | . | audit_transport_headers | The headers of the request, if any. | . | audit_request_effective_user | The username that failed to authenticate. | . | audit_request_initiating_user | The user that initiated the request. Only logged if it differs from the effective user. | . | audit_transport_request_type | The type of request (e.g. IndexRequest). | . | audit_request_body | The HTTP request body, if any (and if request body logging is enabled). | . | audit_trace_indices | The index name(s) included in the request. Can contain wildcards, date patterns, and aliases. Only logged if resolve_indices is true. | . | audit_trace_resolved_indices | The resolved index name(s) affected by the request. Only logged if resolve_indices is true. | . | audit_trace_doc_types | The document types affected by the request. Only logged if resolve_indices is true. | . ",
    "url": "https://vagimeli.github.io/security/audit-logs/field-reference/#transport-failed_login-attributes",
    "relUrl": "/security/audit-logs/field-reference/#transport-failed_login-attributes"
  },"598": {
    "doc": "Audit log field reference",
    "title": "Transport AUTHENTICATED attributes",
    "content": "| Name | Description | . | audit_trace_task_id | The ID of the request. | . | audit_transport_headers | The headers of the request, if any. | . | audit_request_effective_user | The username that failed to authenticate. | . | audit_request_initiating_user | The user that initiated the request. Only logged if it differs from the effective user. | . | audit_transport_request_type | The type of request (e.g. IndexRequest). | . | audit_request_body | The HTTP request body, if any (and if request body logging is enabled). | . | audit_trace_indices | The index name(s) included in the request. Can contain wildcards, date patterns, and aliases. Only logged if resolve_indices is true. | . | audit_trace_resolved_indices | The resolved index name(s) affected by the request. Only logged if resolve_indices is true. | . | audit_trace_doc_types | The document types affected by the request. Only logged if resolve_indices is true. | . ",
    "url": "https://vagimeli.github.io/security/audit-logs/field-reference/#transport-authenticated-attributes",
    "relUrl": "/security/audit-logs/field-reference/#transport-authenticated-attributes"
  },"599": {
    "doc": "Audit log field reference",
    "title": "Transport MISSING_PRIVILEGES attributes",
    "content": "| Name | Description | . | audit_trace_task_id | The ID of the request. | . | audit_trace_task_parent_id | The parent ID of this request, if any. | . | audit_transport_headers | The headers of the request, if any. | . | audit_request_effective_user | The username that failed to authenticate. | . | audit_request_initiating_user | The user that initiated the request. Only logged if it differs from the effective user. | . | audit_transport_request_type | The type of request (e.g. IndexRequest). | . | audit_request_privilege | The required privilege of the request (e.g. indices:data/read/search). | . | audit_request_body | The HTTP request body, if any (and if request body logging is enabled). | . | audit_trace_indices | The index name(s) included in the request. Can contain wildcards, date patterns, and aliases. Only logged if resolve_indices is true. | . | audit_trace_resolved_indices | The resolved index name(s) affected by the request. Only logged if resolve_indices is true. | . | audit_trace_doc_types | The document types affected by the request. Only logged if resolve_indices is true. | . ",
    "url": "https://vagimeli.github.io/security/audit-logs/field-reference/#transport-missing_privileges-attributes",
    "relUrl": "/security/audit-logs/field-reference/#transport-missing_privileges-attributes"
  },"600": {
    "doc": "Audit log field reference",
    "title": "Transport GRANTED_PRIVILEGES attributes",
    "content": "| Name | Description | . | audit_trace_task_id | The ID of the request. | . | audit_trace_task_parent_id | The parent ID of this request, if any. | . | audit_transport_headers | The headers of the request, if any. | . | audit_request_effective_user | The username that failed to authenticate. | . | audit_request_initiating_user | The user that initiated the request. Only logged if it differs from the effective user. | . | audit_transport_request_type | The type of request (e.g. IndexRequest). | . | audit_request_privilege | The required privilege of the request (e.g. indices:data/read/search). | . | audit_request_body | The HTTP request body, if any (and if request body logging is enabled). | . | audit_trace_indices | The index name(s) included in the request. Can contain wildcards, date patterns, and aliases. Only logged if resolve_indices is true. | . | audit_trace_resolved_indices | The resolved index name(s) affected by the request. Only logged if resolve_indices is true. | . | audit_trace_doc_types | The document types affected by the request. Only logged if resolve_indices is true. | . ",
    "url": "https://vagimeli.github.io/security/audit-logs/field-reference/#transport-granted_privileges-attributes",
    "relUrl": "/security/audit-logs/field-reference/#transport-granted_privileges-attributes"
  },"601": {
    "doc": "Audit log field reference",
    "title": "Transport SSL_EXCEPTION attributes",
    "content": "| Name | Description | . | audit_request_exception_stacktrace | The stack trace of the SSL exception. | . ",
    "url": "https://vagimeli.github.io/security/audit-logs/field-reference/#transport-ssl_exception-attributes",
    "relUrl": "/security/audit-logs/field-reference/#transport-ssl_exception-attributes"
  },"602": {
    "doc": "Audit log field reference",
    "title": "Transport BAD_HEADERS attributes",
    "content": "| Name | Description | . | audit_trace_task_id | The ID of the request. | . | audit_trace_task_parent_id | The parent ID of this request, if any. | . | audit_transport_headers | The headers of the request, if any. | . | audit_request_effective_user | The username that failed to authenticate. | . | audit_request_initiating_user | The user that initiated the request. Only logged if it differs from the effective user. | . | audit_transport_request_type | The type of request (e.g. IndexRequest). | . | audit_request_body | The HTTP request body, if any (and if request body logging is enabled). | . | audit_trace_indices | The index name(s) included in the request. Can contain wildcards, date patterns, and aliases. Only logged if resolve_indices is true. | . | audit_trace_resolved_indices | The resolved index name(s) affected by the request. Only logged if resolve_indices is true. | . | audit_trace_doc_types | The document types affected by the request. Only logged if resolve_indices is true. | . ",
    "url": "https://vagimeli.github.io/security/audit-logs/field-reference/#transport-bad_headers-attributes",
    "relUrl": "/security/audit-logs/field-reference/#transport-bad_headers-attributes"
  },"603": {
    "doc": "Audit log field reference",
    "title": "Transport opensearch_SECURITY_INDEX_ATTEMPT attributes",
    "content": "| Name | Description | . | audit_trace_task_id | The ID of the request. | . | audit_transport_headers | The headers of the request, if any. | . | audit_request_effective_user | The username that failed to authenticate. | . | audit_request_initiating_user | The user that initiated the request. Only logged if it differs from the effective user. | . | audit_transport_request_type | The type of request (e.g. IndexRequest). | . | audit_request_body | The HTTP request body, if any (and if request body logging is enabled). | . | audit_trace_indices | The index name(s) included in the request. Can contain wildcards, date patterns, and aliases. Only logged if resolve_indices is true. | . | audit_trace_resolved_indices | The resolved index name(s) affected by the request. Only logged if resolve_indices is true. | . | audit_trace_doc_types | The document types affected by the request. Only logged if resolve_indices is true. | . ",
    "url": "https://vagimeli.github.io/security/audit-logs/field-reference/#transport-opensearch_security_index_attempt-attributes",
    "relUrl": "/security/audit-logs/field-reference/#transport-opensearch_security_index_attempt-attributes"
  },"604": {
    "doc": "Audit logs",
    "title": "Audit logs",
    "content": "Audit logs let you track access to your OpenSearch cluster and are useful for compliance purposes or in the aftermath of a security breach. You can configure the categories to be logged, the detail level of the logged messages, and where to store the logs. To enable audit logging: . | Add the following line to opensearch.yml on each node: . plugins.security.audit.type: internal_opensearch . This setting stores audit logs on the current cluster. For other storage options, see Audit Log Storage Types. | Restart each node. | . After this initial setup, you can use OpenSearch Dashboards to manage your audit log categories and other settings. In OpenSearch Dashboards, choose Security, Audit logs. . | Tracked events | Exclude categories | Disable REST or the transport layer | Disable request body logging | Log index names | Configure bulk request handling | Exclude requests | Exclude users | Configure the audit log index name | (Advanced) Tune the thread pool | . ",
    "url": "https://vagimeli.github.io/security/audit-logs/index/",
    "relUrl": "/security/audit-logs/index/"
  },"605": {
    "doc": "Audit logs",
    "title": "Tracked events",
    "content": "Audit logging records events in two ways: HTTP requests (REST) and the transport layer. | Event | Logged on REST | Logged on transport | Description | . | FAILED_LOGIN | Yes | Yes | The credentials of a request could not be validated, most likely because the user does not exist or the password is incorrect. | . | AUTHENTICATED | Yes | Yes | A user successfully authenticated. | . | MISSING_PRIVILEGES | No | Yes | The user does not have the required permissions to execute the request. | . | GRANTED_PRIVILEGES | No | Yes | A user made a successful request to OpenSearch. | . | SSL_EXCEPTION | Yes | Yes | An attempt was made to access OpenSearch without a valid SSL/TLS certificate. | . | opensearch_SECURITY_INDEX_ATTEMPT | No | Yes | An attempt was made to modify the Security plugin internal user and privileges index without the required permissions or TLS admin certificate. | . | BAD_HEADERS | Yes | Yes | An attempt was made to spoof a request to OpenSearch with the Security plugin internal headers. | . These default log settings work well for most use cases, but you can change settings to save storage space or adapt the information to your exact needs. ",
    "url": "https://vagimeli.github.io/security/audit-logs/index/#tracked-events",
    "relUrl": "/security/audit-logs/index/#tracked-events"
  },"606": {
    "doc": "Audit logs",
    "title": "Exclude categories",
    "content": "To exclude categories, set: . plugins.security.audit.config.disabled_rest_categories: &lt;disabled categories&gt; plugins.security.audit.config.disabled_transport_categories: &lt;disabled categories&gt; . For example: . plugins.security.audit.config.disabled_rest_categories: AUTHENTICATED, opensearch_SECURITY_INDEX_ATTEMPT plugins.security.audit.config.disabled_transport_categories: GRANTED_PRIVILEGES . If you want to log events in all categories, use NONE: . plugins.security.audit.config.disabled_rest_categories: NONE plugins.security.audit.config.disabled_transport_categories: NONE . ",
    "url": "https://vagimeli.github.io/security/audit-logs/index/#exclude-categories",
    "relUrl": "/security/audit-logs/index/#exclude-categories"
  },"607": {
    "doc": "Audit logs",
    "title": "Disable REST or the transport layer",
    "content": "By default, the Security plugin logs events on both REST and the transport layer. You can disable either type: . plugins.security.audit.enable_rest: false plugins.security.audit.enable_transport: false . ",
    "url": "https://vagimeli.github.io/security/audit-logs/index/#disable-rest-or-the-transport-layer",
    "relUrl": "/security/audit-logs/index/#disable-rest-or-the-transport-layer"
  },"608": {
    "doc": "Audit logs",
    "title": "Disable request body logging",
    "content": "By default, the Security plugin includes the body of the request (if available) for both REST and the transport layer. If you do not want or need the request body, you can disable it: . plugins.security.audit.log_request_body: false . ",
    "url": "https://vagimeli.github.io/security/audit-logs/index/#disable-request-body-logging",
    "relUrl": "/security/audit-logs/index/#disable-request-body-logging"
  },"609": {
    "doc": "Audit logs",
    "title": "Log index names",
    "content": "By default, the Security plugin logs all indices affected by a request. Because index names can be aliases and contain wildcards/date patterns, the Security plugin logs the index name that the user submitted and the actual index name to which it resolves. For example, if you use an alias or a wildcard, the audit event might look like: . audit_trace_indices: [ \"human*\" ], audit_trace_resolved_indices: [ \"humanresources\" ] . You can disable this feature by setting: . plugins.security.audit.resolve_indices: false . Disabling this feature only takes effect if plugins.security.audit.log_request_body is also set to false. ",
    "url": "https://vagimeli.github.io/security/audit-logs/index/#log-index-names",
    "relUrl": "/security/audit-logs/index/#log-index-names"
  },"610": {
    "doc": "Audit logs",
    "title": "Configure bulk request handling",
    "content": "Bulk requests can contain many indexing operations. By default, the Security plugin only logs the single bulk request, not each individual operation. The Security plugin can be configured to log each indexing operation as a separate event: . plugins.security.audit.resolve_bulk_requests: true . This change can create a massive number of events in the audit logs, so we don’t recommend enabling this setting if you make heavy use of the _bulk API. ",
    "url": "https://vagimeli.github.io/security/audit-logs/index/#configure-bulk-request-handling",
    "relUrl": "/security/audit-logs/index/#configure-bulk-request-handling"
  },"611": {
    "doc": "Audit logs",
    "title": "Exclude requests",
    "content": "You can exclude certain requests from being logged completely, by either configuring actions (for transport requests) and/or HTTP request paths (REST): . plugins.security.audit.ignore_requests: [\"indices:data/read/*\", \"SearchRequest\"] . ",
    "url": "https://vagimeli.github.io/security/audit-logs/index/#exclude-requests",
    "relUrl": "/security/audit-logs/index/#exclude-requests"
  },"612": {
    "doc": "Audit logs",
    "title": "Exclude users",
    "content": "By default, the Security plugin logs events from all users, but excludes the internal OpenSearch Dashboards server user kibanaserver. You can exclude other users: . plugins.security.audit.ignore_users: - kibanaserver - admin . If requests from all users should be logged, use NONE: . plugins.security.audit.ignore_users: NONE . ",
    "url": "https://vagimeli.github.io/security/audit-logs/index/#exclude-users",
    "relUrl": "/security/audit-logs/index/#exclude-users"
  },"613": {
    "doc": "Audit logs",
    "title": "Configure the audit log index name",
    "content": "By default, the Security plugin stores audit events in a daily rolling index named auditlog-YYYY.MM.dd. You can configure the name of the index in opensearch.yml: . plugins.security.audit.config.index: myauditlogindex . Use a date pattern in the index name to configure daily, weekly, or monthly rolling indices: . plugins.security.audit.config.index: \"'auditlog-'YYYY.MM.dd\" . For a reference on the date pattern format, see the Joda DateTimeFormat documentation. ",
    "url": "https://vagimeli.github.io/security/audit-logs/index/#configure-the-audit-log-index-name",
    "relUrl": "/security/audit-logs/index/#configure-the-audit-log-index-name"
  },"614": {
    "doc": "Audit logs",
    "title": "(Advanced) Tune the thread pool",
    "content": "The Search plugin logs events asynchronously, which keeps performance impact on your cluster minimal. The plugin uses a fixed thread pool to log events. You can define the number of threads in the pool in opensearch.yml: . plugins.security.audit.threadpool.size: &lt;integer&gt; . The default setting is 10. Setting this value to 0 disables the thread pool, which means the plugin logs events synchronously. To set the maximum queue length per thread: . plugins.security.audit.threadpool.max_queue_len: 100000 . ",
    "url": "https://vagimeli.github.io/security/audit-logs/index/#advanced-tune-the-thread-pool",
    "relUrl": "/security/audit-logs/index/#advanced-tune-the-thread-pool"
  },"615": {
    "doc": "Audit log storage types",
    "title": "Audit log storage types",
    "content": "Audit logs can take up quite a bit of space, so the Security plugin offers several options for storage locations. | Setting | Description | . | debug | Outputs to stdout. Useful for testing and debugging. | . | internal_opensearch | Writes to an audit index on the current OpenSearch cluster. | . | external_opensearch | Writes to an audit index on a remote OpenSearch cluster. | . | webhook | Sends events to an arbitrary HTTP endpoint. | . | log4j | Writes the events to a Log4j logger. You can use any Log4j appender, such as SNMP, JDBC, Cassandra, and Kafka. | . You configure the output location in opensearch.yml: . plugins.security.audit.type: &lt;debug|internal_opensearch|external_opensearch|webhook|log4j&gt; . external_opensearch, webhook, and log4j all have additional configuration options. Details follow. ",
    "url": "https://vagimeli.github.io/security/audit-logs/storage-types/",
    "relUrl": "/security/audit-logs/storage-types/"
  },"616": {
    "doc": "Audit log storage types",
    "title": "External OpenSearch",
    "content": "The external_opensearch storage type requires one or more OpenSearch endpoints with a host/IP address and port. Optionally, provide the index name and a document type. plugins.security.audit.type: external_opensearch plugins.security.audit.config.http_endpoints: [&lt;endpoints&gt;] plugins.security.audit.config.index: &lt;indexname&gt; plugins.security.audit.config.type: _doc . The Security plugin uses the OpenSearch REST API to send events, just like any other indexing request. For plugins.security.audit.config.http_endpoints, use a comma-separated list of hosts/IP addresses and the REST port (default 9200). plugins.security.audit.config.http_endpoints: [192.168.178.1:9200,192.168.178.2:9200] . If you use external_opensearch and the remote cluster also uses the Security plugin, you must supply some additional parameters for authentication. These parameters depend on which authentication type you configured for the remote cluster. TLS settings . | Name | Data type | Description | . | plugins.security.audit.config.enable_ssl | Boolean | If you enabled SSL/TLS on the receiving cluster, set to true. The default is false. | . | plugins.security.audit.config.verify_hostnames | Boolean | Whether to verify the hostname of the SSL/TLS certificate of the receiving cluster. Default is true. | . | plugins.security.audit.config.pemtrustedcas_filepath | String | The trusted root certificate of the external OpenSearch cluster, relative to the config directory. | . | plugins.security.audit.config.pemtrustedcas_content | String | Instead of specifying the path (plugins.security.audit.config.pemtrustedcas_filepath), you can configure the Base64-encoded certificate content directly. | . | plugins.security.audit.config.enable_ssl_client_auth | Boolean | Whether to enable SSL/TLS client authentication. If you set this to true, the audit log module sends the node’s certificate along with the request. The receiving cluster can use this certificate to verify the identity of the caller. | . | plugins.security.audit.config.pemcert_filepath | String | The path to the TLS certificate to send to the external OpenSearch cluster, relative to the config directory. | . | plugins.security.audit.config.pemcert_content | String | Instead of specifying the path (plugins.security.audit.config.pemcert_filepath), you can configure the Base64-encoded certificate content directly. | . | plugins.security.audit.config.pemkey_filepath | String | The path to the private key of the TLS certificate to send to the external OpenSearch cluster, relative to the config directory. | . | plugins.security.audit.config.pemkey_content | String | Instead of specifying the path (plugins.security.audit.config.pemkey_filepath), you can configure the Base64-encoded certificate content directly. | . | plugins.security.audit.config.pemkey_password | String | The password of the private key. | . Basic auth settings . If you enabled HTTP basic authentication on the receiving cluster, use these settings to specify the username and password: . plugins.security.audit.config.username: &lt;username&gt; plugins.security.audit.config.password: &lt;password&gt; . ",
    "url": "https://vagimeli.github.io/security/audit-logs/storage-types/#external-opensearch",
    "relUrl": "/security/audit-logs/storage-types/#external-opensearch"
  },"617": {
    "doc": "Audit log storage types",
    "title": "Webhook",
    "content": "Use the following keys to configure the webhook storage type. | Name | Data type | Description | . | plugins.security.audit.config.webhook.url | String | The HTTP or HTTPS URL to send the logs to. | . | plugins.security.audit.config.webhook.ssl.verify | Boolean | If true, the TLS certificate provided by the endpoint (if any) will be verified. If set to false, no verification is performed. You can disable this check if you use self-signed certificates. | . | plugins.security.audit.config.webhook.ssl.pemtrustedcas_filepath | String | The path to the trusted certificate against which the webhook’s TLS certificate is validated. | . | plugins.security.audit.config.webhook.ssl.pemtrustedcas_content | String | Same as plugins.security.audit.config.webhook.ssl.pemtrustedcas_content, but you can configure the base 64 encoded certificate content directly. | . | plugins.security.audit.config.webhook.format | String | The format in which the audit log message is logged, can be one of URL_PARAMETER_GET, URL_PARAMETER_POST, TEXT, JSON, SLACK. See Formats. | . Formats . | Format | Description | . | URL_PARAMETER_GET | Uses HTTP GET to send logs to the webhook URL. All logged information is appended to the URL as request parameters. | . | URL_PARAMETER_POST | Uses HTTP POST to send logs to the webhook URL. All logged information is appended to the URL as request parameters. | . | TEXT | Uses HTTP POST to send logs to the webhook URL. The request body contains the audit log message in plain text format. | . | JSON | Uses HTTP POST to send logs to the webhook URL. The request body contains the audit log message in JSON format. | . | SLACK | Uses HTTP POST to send logs to the webhook URL. The request body contains the audit log message in JSON format suitable for consumption by Slack. The default implementation returns \"text\": \"&lt;AuditMessage#toText&gt;\". | . ",
    "url": "https://vagimeli.github.io/security/audit-logs/storage-types/#webhook",
    "relUrl": "/security/audit-logs/storage-types/#webhook"
  },"618": {
    "doc": "Audit log storage types",
    "title": "Log4j",
    "content": "The log4j storage type lets you specify the name of the logger and log level. plugins.security.audit.config.log4j.logger_name: audit plugins.security.audit.config.log4j.level: INFO . By default, the Security plugin uses the logger name audit and logs the events on INFO level. Audit events are stored in JSON format. ",
    "url": "https://vagimeli.github.io/security/audit-logs/storage-types/#log4j",
    "relUrl": "/security/audit-logs/storage-types/#log4j"
  },"619": {
    "doc": "Authentication backends",
    "title": "Authentication backends",
    "content": "Authentication backend configurations determine the method or methods you use for authenticating users and the way users pass their credentials and sign in to OpenSearch. Having an understanding of the basic authentication flow before getting started can help with the configuration process for whichever backend you choose. Consider the high-level sequence of events in the description that follows, and then refer to the detailed steps for configuring the authentication type you choose to use with OpenSearch. ",
    "url": "https://vagimeli.github.io/security/authentication-backends/authc-index/",
    "relUrl": "/security/authentication-backends/authc-index/"
  },"620": {
    "doc": "Authentication backends",
    "title": "Authentication flow",
    "content": ". | To identify a user who wants to access the cluster, the Security plugin needs the user’s credentials. These credentials differ depending on how you’ve configured the plugin. For example, if you use basic authentication, the credentials are a username and password. If you use a JSON web token, the credentials (username and roles) are stored within the token itself. If you use TLS certificates, the credentials are the distinguished name (DN) of the certificate. No matter which backend you use, these credentials are included in the request for authentication. | The Security plugin authenticates a request against a backend configured for an authentication provider. Some examples of authentication providers used with OpenSearch include Basic Auth (which uses the internal user database), LDAP/Active Directory, JSON web tokens, SAML, or another authentication protocol. The plugin supports chaining backends in config/opensearch-security/config.yml. If more than one backend is present, the plugin tries to authenticate the user sequentially against each until one succeeds. A common use case is to combine the internal user database of the Security plugin with LDAP/Active Directory. | After a backend verifies the user’s credentials, the plugin collects any backend roles. The authentication provider determines the way these roles are retrieved. For example, LDAP extracts backend roles from its directory service based on their mappings to roles in OpenSearch, while SAML stores the roles as attributes. When basic authentication is used, the internal user database refers to role mappings configured in OpenSearch. | After the user is authenticated and any backend roles are retrieved, the Security plugin uses the role mapping to assign security roles to the user. If the role mapping doesn’t include the user (or the user’s backend roles), the user is successfully authenticated, but has no permissions. | The user can now perform actions as defined by the mapped security roles. For example, a user might map to the kibana_user role and thus have permissions to access OpenSearch Dashboards. | . ",
    "url": "https://vagimeli.github.io/security/authentication-backends/authc-index/#authentication-flow",
    "relUrl": "/security/authentication-backends/authc-index/#authentication-flow"
  },"621": {
    "doc": "Client certificate authentication",
    "title": "Client certificate authentication",
    "content": "After obtaining your own certificates either from a certificate authority (CA) or by generating your own certificates using OpenSSL, you can start configuring OpenSearch to authenticate a user using a client certificate. Client certificate authentication offers more security advantages than just using basic authentication (username and password). Because client certificate authentication requires both a client certificate and its private key, which are often in the user’s possession, it is less vulnerable to brute force attacks in which malicious individuals try to guess a user’s password. Another benefit of client certificate authentication is you can use it along with basic authentication, providing two layers of security. ",
    "url": "https://vagimeli.github.io/security/authentication-backends/client-auth/",
    "relUrl": "/security/authentication-backends/client-auth/"
  },"622": {
    "doc": "Client certificate authentication",
    "title": "Enabling client certificate authentication",
    "content": "To enable client certificate authentication, you must first set clientauth_mode in opensearch.yml to either OPTIONAL or REQUIRE: . plugins.security.ssl.http.clientauth_mode: OPTIONAL . Next, enable client certificate authentication in the client_auth_domain section of config.yml. clientcert_auth_domain: description: \"Authenticate via SSL client certificates\" http_enabled: true transport_enabled: true order: 1 http_authenticator: type: clientcert config: username_attribute: cn #optional, if omitted DN becomes username challenge: false authentication_backend: type: noop . ",
    "url": "https://vagimeli.github.io/security/authentication-backends/client-auth/#enabling-client-certificate-authentication",
    "relUrl": "/security/authentication-backends/client-auth/#enabling-client-certificate-authentication"
  },"623": {
    "doc": "Client certificate authentication",
    "title": "Assigning roles to your common name",
    "content": "You can now assign your certificate’s common name (CN) to a role. For this step, you must know your certificate’s CN and the role you want to assign to. To get a list of all predefined roles in OpenSearch, refer to our list of predefined roles. If you want to first create a role, refer to how to create a role, and then map your certificate’s CN to that role. After deciding which role you want to map your certificate’s CN to, you can use OpenSearch Dashboards, roles_mapping.yml, or the REST API to map your certificate’s CN to the role. The following example uses the REST API to map the common name CLIENT1 to the role readall. Example request . PUT _plugins/_security/api/rolesmapping/readall { \"backend_roles\" : [\"sample_role\" ], \"hosts\" : [ \"example.host.com\" ], \"users\" : [ \"CLIENT1\" ] } . Example response . { \"status\": \"OK\", \"message\": \"'readall' updated.\" } . After mapping a role to your client certificate’s CN, you’re ready to connect to your cluster using those credentials. The code example below uses the Python requests library to connect to a local OpenSearch cluster and sends a GET request to the movies index. import requests import json base_url = 'https://localhost:9200/' headers = { 'Content-Type': 'application/json' } cert_file_path = \"/full/path/to/client-cert.pem\" key_file_path = \"/full/path/to/client-cert-key.pem\" root_ca_path = \"/full/path/to/root-ca.pem\" # Send the request. path = 'movies/_doc/3' url = base_url + path response = requests.get(url, cert = (cert_file_path, key_file_path), verify=root_ca_path) print(response.text) . ",
    "url": "https://vagimeli.github.io/security/authentication-backends/client-auth/#assigning-roles-to-your-common-name",
    "relUrl": "/security/authentication-backends/client-auth/#assigning-roles-to-your-common-name"
  },"624": {
    "doc": "Client certificate authentication",
    "title": "Using certificates with Docker",
    "content": "While we recommend using the tarball installation of ODFE to test client certificate authentication configurations, you can also use any of the other install types. For instructions on using Docker security, see Configuring basic security settings. ",
    "url": "https://vagimeli.github.io/security/authentication-backends/client-auth/#using-certificates-with-docker",
    "relUrl": "/security/authentication-backends/client-auth/#using-certificates-with-docker"
  },"625": {
    "doc": "Active Directory and LDAP",
    "title": "Active Directory and LDAP",
    "content": "Active Directory and LDAP can be used for both authentication and authorization (the authc and authz sections of the configuration, respectively). Authentication checks whether the user has entered valid credentials. Authorization retrieves any backend roles for the user. In most cases, you want to configure both authentication and authorization. You can also use authentication only and map the users retrieved from LDAP directly to Security plugin roles. ",
    "url": "https://vagimeli.github.io/security/authentication-backends/ldap/",
    "relUrl": "/security/authentication-backends/ldap/"
  },"626": {
    "doc": "Active Directory and LDAP",
    "title": "Docker example",
    "content": "We provide a fully functional example that can help you understand how to use an LDAP server for both authentication and authorization. | Download and unzip the example zip file. | At the command line, run docker-compose up. | Review the files: . | docker-compose.yml defines a single OpenSearch node, an LDAP server, and a PHP administration tool for the LDAP server. You can access the administration tool at https://localhost:6443. Acknowledge the security warning and log in using cn=admin,dc=example,dc=org and changethis. | directory.ldif seeds the LDAP server with three users and two groups. psantos is in the Administrator and Developers groups. jroe and jdoe are in the Developers group. The Security plugin loads these groups as backend roles. | roles_mapping.yml maps the Administrator and Developers LDAP groups (as backend roles) to security roles so that users gain the appropriate permissions after authenticating. | internal_users.yml removes all default users except administrator and kibanaserver. | config.yml includes all necessary LDAP settings. | . | Index a document as psantos: . curl -XPUT 'https://localhost:9200/new-index/_doc/1' -H 'Content-Type: application/json' -d '{\"title\": \"Spirited Away\"}' -u 'psantos:password' -k . If you try the same request as jroe, it fails. The Developers group is mapped to the readall, manage_snapshots, and kibana_user roles and has no write permissions. | Search for the document as jroe: . curl -XGET 'https://localhost:9200/new-index/_search?pretty' -u 'jroe:password' -k . This request succeeds, because the Developers group is mapped to the readall role. | If you want to examine the contents of the various containers, run docker ps to find the container ID and then docker exec -it &lt;container-id&gt; /bin/bash. | . ",
    "url": "https://vagimeli.github.io/security/authentication-backends/ldap/#docker-example",
    "relUrl": "/security/authentication-backends/ldap/#docker-example"
  },"627": {
    "doc": "Active Directory and LDAP",
    "title": "Connection settings",
    "content": "To enable LDAP authentication and authorization, add the following lines to config/opensearch-security/config.yml: . authc: ldap: http_enabled: true transport_enabled: true order: 1 http_authenticator: type: basic challenge: false authentication_backend: type: ldap config: ... authz: ldap: http_enabled: true transport_enabled: true authorization_backend: type: ldap config: ... The connection settings are identical for authentication and authorization and are added to the config sections. Hostname and port . To configure the hostname and port of your Active Directory servers, use the following: . config: hosts: - primary.ldap.example.com:389 - secondary.ldap.example.com:389 . You can configure more than one server here. If the Security plugin cannot connect to the first server, it tries to connect to the remaining servers sequentially. Timeouts . To configure connection and response timeouts to your Active Directory server, use the following (values are in milliseconds): . config: connect_timeout: 5000 response_timeout: 0 . If your server supports two-factor authentication (2FA), the default timeout settings might result in login errors. You can increase connect_timeout to accommodate the 2FA process. Setting response_timeout to 0 (the default) indicates an indefinite waiting period. Bind DN and password . To configure the bind_dn and password that the Security plugin uses when issuing queries to your server, use the following: . config: bind_dn: cn=admin,dc=example,dc=com password: password . If your server supports anonymous authentication, both bind_dn and password can be set to null. TLS settings . Use the following parameters to configure TLS for connecting to your server: . config: enable_ssl: &lt;true|false&gt; enable_start_tls: &lt;true|false&gt; enable_ssl_client_auth: &lt;true|false&gt; verify_hostnames: &lt;true|false&gt; . | Name | Description | . | enable_ssl | Whether to use LDAP over SSL (LDAPS). | . | enable_start_tls | Whether to use STARTTLS. Can’t be used in combination with LDAPS. | . | enable_ssl_client_auth | Whether to send the client certificate to the LDAP server. | . | verify_hostnames | Whether to verify the hostnames of the server’s TLS certificate. | . Certificate validation . By default, the Security plugin validates the TLS certificate of the LDAP servers against the root CA configured in opensearch.yml, either as a PEM certificate or a truststore: . plugins.security.ssl.transport.pemtrustedcas_filepath: ... plugins.security.ssl.http.truststore_filepath: ... If your server uses a certificate signed by a different CA, import this CA into your truststore or add it to your trusted CA file on each node. You can also use a separate root CA in PEM format by setting one of the following configuration options: . config: pemtrustedcas_filepath: /full/path/to/trusted_cas.pem . config: pemtrustedcas_content: |- MIID/jCCAuagAwIBAgIBATANBgkqhkiG9w0BAQUFADCBjzETMBEGCgmSJomT8ixk ARkWA2NvbTEXMBUGCgmSJomT8ixkARkWB2V4YW1wbGUxGTAXBgNVBAoMEEV4YW1w bGUgQ29tIEluYy4xITAfBgNVBAsMGEV4YW1wbGUgQ29tIEluYy4gUm9vdCBDQTEh ... | Name | Description | . | pemtrustedcas_filepath | Absolute path to the PEM file containing the root CAs of your Active Directory/LDAP server. | . | pemtrustedcas_content | The root CA content of your Active Directory/LDAP server. Cannot be used when pemtrustedcas_filepath is set. | . Client authentication . If you use TLS client authentication, the Security plugin sends the PEM certificate of the node, as configured in opensearch.yml. Set one of the following configuration options: . config: pemkey_filepath: /full/path/to/private.key.pem pemkey_password: private_key_password pemcert_filepath: /full/path/to/certificate.pem . or . config: pemkey_content: |- MIID2jCCAsKgAwIBAgIBBTANBgkqhkiG9w0BAQUFADCBlTETMBEGCgmSJomT8ixk ARkWA2NvbTEXMBUGCgmSJomT8ixkARkWB2V4YW1wbGUxGTAXBgNVBAoMEEV4YW1w bGUgQ29tIEluYy4xJDAiBgNVBAsMG0V4YW1wbGUgQ29tIEluYy4gU2lnbmluZyBD ... pemkey_password: private_key_password pemcert_content: |- MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCHRZwzwGlP2FvL oEzNeDu2XnOF+ram7rWPT6fxI+JJr3SDz1mSzixTeHq82P5A7RLdMULfQFMfQPfr WXgB4qfisuDSt+CPocZRfUqqhGlMG2l8LgJMr58tn0AHvauvNTeiGlyXy0ShxHbD ... | Name | Description | . | pemkey_filepath | Absolute path to the file containing the private key of your certificate. | . | pemkey_content | The content of the private key of your certificate. Cannot be used when pemkey_filepath is set. | . | pemkey_password | The password of your private key, if any. | . | pemcert_filepath | Absolute path to the client certificate. | . | pemcert_content | The content of the client certificate. Cannot be used when pemcert_filepath is set. | . Enabled ciphers and protocols . You can limit the allowed ciphers and TLS protocols for the LDAP connection. For example, you can allow only strong ciphers and limit the TLS versions to the most recent ones: . ldap: http_enabled: true transport_enabled: true ... authentication_backend: type: ldap config: enabled_ssl_ciphers: - \"TLS_DHE_RSA_WITH_AES_256_CBC_SHA\" - \"TLS_DHE_DSS_WITH_AES_128_CBC_SHA256\" enabled_ssl_protocols: - \"TLSv1.1\" - \"TLSv1.2\" . | Name | Description | . | enabled_ssl_ciphers | Array, enabled TLS ciphers. Only the Java format is supported. | . | enabled_ssl_protocols | Array, enabled TLS protocols. Only the Java format is supported. | . ",
    "url": "https://vagimeli.github.io/security/authentication-backends/ldap/#connection-settings",
    "relUrl": "/security/authentication-backends/ldap/#connection-settings"
  },"628": {
    "doc": "Active Directory and LDAP",
    "title": "Use Active Directory and LDAP for authentication",
    "content": "To use Active Directory/LDAP for authentication, first configure a respective authentication domain in the authc section of config/opensearch-security/config.yml: . authc: ldap: http_enabled: true transport_enabled: true order: 1 http_authenticator: type: basic challenge: true authentication_backend: type: ldap config: ... Next, add the connection settings for your Active Directory/LDAP server to the config section of the authentication domain: . config: enable_ssl: true enable_start_tls: false enable_ssl_client_auth: false verify_hostnames: true hosts: - ldap.example.com:8389 bind_dn: cn=admin,dc=example,dc=com password: passw0rd . Authentication works by issuing an LDAP query containing the user name against the user subtree of the LDAP tree. The Security plugin first takes the configured LDAP query and replaces the placeholder {0} with the user name from the user’s credentials. usersearch: '(sAMAccountName={0})' . Then it issues this query against the user subtree. Currently, the entire subtree under the configured userbase is searched: . userbase: 'ou=people,dc=example,dc=com' . If the query is successful, the Security plugin retrieves the user name from the LDAP entry. You can specify which attribute from the LDAP entry the Security plugin should use as the user name: . username_attribute: uid . If this key is not set or null, then the distinguished name (DN) of the LDAP entry is used. Configuration summary . | Name | Description | . | userbase | Specifies the subtree in the directory where user information is stored. | . | usersearch | The actual LDAP query that the Security plugin executes when trying to authenticate a user. The variable {0} is substituted with the user name. | . | username_attribute | The Security plugin uses this attribute of the directory entry to look for the user name. If set to null, the DN is used (default). | . Complete authentication example . ldap: http_enabled: true transport_enabled: true order: 1 http_authenticator: type: basic challenge: true authentication_backend: type: ldap config: enable_ssl: true enable_start_tls: false enable_ssl_client_auth: false verify_hostnames: true hosts: - ldap.example.com:636 bind_dn: cn=admin,dc=example,dc=com password: password userbase: 'ou=people,dc=example,dc=com' usersearch: '(sAMAccountName={0})' username_attribute: uid . ",
    "url": "https://vagimeli.github.io/security/authentication-backends/ldap/#use-active-directory-and-ldap-for-authentication",
    "relUrl": "/security/authentication-backends/ldap/#use-active-directory-and-ldap-for-authentication"
  },"629": {
    "doc": "Active Directory and LDAP",
    "title": "Use Active Directory and LDAP for authorization",
    "content": "To use Active Directory/LDAP for authorization, first configure a respective authorization domain in the authz section of config.yml: . authz: ldap: http_enabled: true transport_enabled: true authorization_backend: type: ldap config: ... Authorization is the process of retrieving backend roles for an authenticated user from an LDAP server. This is typically the same servers that you use for authentication, but you can also use a different server. The only requirement is that the user you use to fetch the roles actually exists on the LDAP server. Because the Security plugin always checks if a user exists in the LDAP server, you must also configure userbase, usersearch and username_attribute in the authz section. Authorization works similarly to authentication. The Security plugin issues an LDAP query containing the user name against the role subtree of the LDAP tree. As an alternative, the Security plugin can also fetch roles that are defined as a direct attribute of the user entry in the user subtree. Approach 1: Query the role subtree . The Security plugin first takes the LDAP query for fetching roles (“rolesearch”) and substitutes any variables found in the query. For example, for a standard Active Directory installation, you would use the following role search: . rolesearch: '(member={0})' . You can use the following variables: . | {0} is substituted with the DN of the user. | {1} is substituted with the user name, as defined by the username_attribute setting. | {2} is substituted with an arbitrary attribute value from the authenticated user’s directory entry. | . The variable {2} refers to an attribute from the user’s directory entry. The attribute that you should use is specified by the userroleattribute setting: . userroleattribute: myattribute . The Security plugin then issues the substituted query against the configured role subtree. The entire subtree under rolebase is searched: . rolebase: 'ou=groups,dc=example,dc=com' . If you use nested roles (roles that are members of other roles), you can configure the Security plugin to resolve them: . resolve_nested_roles: false . After all roles have been fetched, the Security plugin extracts the final role names from a configurable attribute of the role entries: . rolename: cn . If this is not set, the DN of the role entry is used. You can now use this role name for mapping it to one or more of the Security plugin roles, as defined in roles_mapping.yml. Approach 2: Use a user’s attribute as the role name . If you store the roles as a direct attribute of the user entries in the user subtree, you need to configure only the attribute name: . userrolename: roles . You can configure multiple attribute names: . userrolename: roles, otherroles . This approach can be combined with querying the role subtree. The Security plugin fetches the roles from the user’s role attribute and then executes the role search. If you don’t use or have a role subtree, you can disable the role search completely: . rolesearch_enabled: false . (Advanced) Control LDAP user attributes . By default, the Security plugin reads all LDAP user attributes and makes them available for index name variable substitution and DLS query variable substitution. If your LDAP entries have a lot of attributes, you might want to control which attributes should be made available. The fewer the attributes, the better the performance. Note that this setting is made in the authentication authc section of the config.yml file. | Name | Description | . | custom_attr_allowlist | String array. Specifies the LDAP attributes that should be made available for variable substitution. | . | custom_attr_maxval_len | Integer. Specifies the maximum allowed length of each attribute. All attributes longer than this value are discarded. A value of 0 disables custom attributes altogether. Default is 36. | . Example: . authc: ldap: http_enabled: true transport_enabled: true authentication_backend: type: ldap config: custom_attr_allowlist: - attribute1 - attribute2 custom_attr_maxval_len: 36 ... (Advanced) Exclude certain users from role lookup . If you are using multiple authentication methods, it can make sense to exclude certain users from the LDAP role lookup. Consider the following scenario for a typical OpenSearch Dashboards setup: All OpenSearch Dashboards users are stored in an LDAP/Active Directory server. However, you also have an OpenSearch Dashboards server user. OpenSearch Dashboards uses this user to manage stored objects and perform monitoring and maintenance tasks. You do not want to add this user to your Active Directory installation, but rather store it in the Security plugin internal user database. In this case, it makes sense to exclude the OpenSearch Dashboards server user from the LDAP authorization because we already know that there is no corresponding entry. You can use the skip_users configuration setting to define which users should be skipped. Wildcards and regular expressions are supported: . skip_users: - kibanaserver - 'cn=Jane Doe,ou*people,o=TEST' - '/\\S*/' . (Advanced) Exclude roles from nested role lookups . If the users in your LDAP installation have a large number of roles, and you have the requirement to resolve nested roles as well, you might run into performance issues. In most cases, however, not all user roles are related to OpenSearch and OpenSearch Dashboards. You might need only a couple of roles. In this case, you can use the nested role filter feature to define a list of roles that are filtered out from the list of the user’s roles. Wildcards and regular expressions are supported. This has an effect only if resolve_nested_roles is true: . nested_role_filter: - 'cn=Jane Doe,ou*people,o=TEST' - ... Configuration summary . | Name | Description | . | rolebase | Specifies the subtree in the directory where role/group information is stored. | . | rolesearch | The actual LDAP query that the Security plugin executes when trying to determine the roles of a user. You can use three variables here (see below). | . | userroleattribute | The attribute in a user entry to use for {2} variable substitution. | . | userrolename | If the roles/groups of a user are not stored in the groups subtree, but as an attribute of the user’s directory entry, define this attribute name here. | . | rolename | The attribute of the role entry that should be used as the role name. | . | resolve_nested_roles | Boolean. Whether or not to resolve nested roles. Default is false. | . | max_nested_depth | Integer. When resolve_nested_roles is true, this defines the maximum number of nested roles to traverse. Setting smaller values can reduce the amount of data retrieved from LDAP and improve authentication times at the cost of failing to discover deeply nested roles. Default is 30. | . | skip_users | Array of users that should be skipped when retrieving roles. Wildcards and regular expressions are supported. | . | nested_role_filter | Array of role DNs that should be filtered before resolving nested roles. Wildcards and regular expressions are supported. | . | rolesearch_enabled | Boolean. Enable or disable the role search. Default is true. | . | custom_attr_allowlist | String array. Specifies the LDAP attributes that should be made available for variable substitution. | . | custom_attr_maxval_len | Integer. Specifies the maximum allowed length of each attribute. All attributes longer than this value are discarded. A value of 0 disables custom attributes altogether. Default is 36. | . Complete authorization example . authz: ldap: http_enabled: true transport_enabled: true authorization_backend: type: ldap config: enable_ssl: true enable_start_tls: false enable_ssl_client_auth: false verify_hostnames: true hosts: - ldap.example.com:636 bind_dn: cn=admin,dc=example,dc=com password: password userbase: 'ou=people,dc=example,dc=com' usersearch: '(uid={0})' username_attribute: uid rolebase: 'ou=groups,dc=example,dc=com' rolesearch: '(member={0})' userroleattribute: null userrolename: none rolename: cn resolve_nested_roles: true skip_users: - kibanaserver - 'cn=Jane Doe,ou*people,o=TEST' - '/\\S*/' . (Advanced) Configuring multiple user and role bases . To configure multiple user bases in the authc and/or authz section, use the following syntax: ... bind_dn: cn=admin,dc=example,dc=com password: password users: primary-userbase: base: 'ou=people,dc=example,dc=com' search: '(uid={0})' secondary-userbase: base: 'cn=users,dc=example,dc=com' search: '(uid={0})' username_attribute: uid ... Similarly, use the following setup to configure multiple role bases in the authz section: ... username_attribute: uid roles: primary-rolebase: base: 'ou=groups,dc=example,dc=com' search: '(uniqueMember={0})' secondary-rolebase: base: 'ou=othergroups,dc=example,dc=com' search: '(member={0})' userroleattribute: null ... Complete authentication and authorization with multiple user and role bases example: . authc: ... ldap: http_enabled: true transport_enabled: true order: 1 http_authenticator: type: basic challenge: true authentication_backend: type: ldap config: enable_ssl: true enable_start_tls: false enable_ssl_client_auth: false verify_hostnames: true hosts: - ldap.example.com:636 bind_dn: cn=admin,dc=example,dc=com password: password users: primary-userbase: base: 'ou=people,dc=example,dc=com' search: '(uid={0})' secondary-userbase: base: 'cn=users,dc=example,dc=com' search: '(uid={0})' username_attribute: uid authz: ldap: http_enabled: true transport_enabled: true authorization_backend: type: ldap config: enable_ssl: true enable_start_tls: false enable_ssl_client_auth: false verify_hostnames: true hosts: - ldap.example.com:636 bind_dn: cn=admin,dc=example,dc=com password: password users: primary-userbase: base: 'ou=people,dc=example,dc=com' search: '(uid={0})' secondary-userbase: base: 'cn=users,dc=example,dc=com' search: '(uid={0})' username_attribute: uid roles: primary-rolebase: base: 'ou=groups,dc=example,dc=com' search: '(uniqueMember={0})' secondary-rolebase: base: 'ou=othergroups,dc=example,dc=com' search: '(member={0})' userroleattribute: null userrolename: none rolename: cn resolve_nested_roles: true . ",
    "url": "https://vagimeli.github.io/security/authentication-backends/ldap/#use-active-directory-and-ldap-for-authorization",
    "relUrl": "/security/authentication-backends/ldap/#use-active-directory-and-ldap-for-authorization"
  },"630": {
    "doc": "OpenID Connect",
    "title": "OpenID Connect",
    "content": "The Security plugin can integrate with identify providers that use the OpenID Connect standard. This feature enables the following: . | Automatic configuration . Point the Security plugin to the metadata of your identity provider (IdP), and the Security plugin uses that data for configuration. | Automatic key fetching . The Security plugin automatically retrieves the public key for validating the JSON Web Tokens (JWTs) from the JSON Web Key Set (JWKS) endpoint of your IdP. You don’t have to configure keys or shared secrets in config.yml. | Key rollover . You can change the keys used for signing the JWTs directly in your IdP. If the Security plugin detects an unknown key, it tries to retrieve it from the IdP. This rollover is transparent to the user. | OpenSearch Dashboards as single sign-on or as one option among multiple authentication types in the Dashboards sign-in window. | . ",
    "url": "https://vagimeli.github.io/security/authentication-backends/openid-connect/",
    "relUrl": "/security/authentication-backends/openid-connect/"
  },"631": {
    "doc": "OpenID Connect",
    "title": "Configure OpenID Connect integration",
    "content": "To integrate with an OpenID IdP, set up an authentication domain and choose openid as the HTTP authentication type. JWTs already contain all of the information required to verify the request, so set challenge to false and authentication_backend to noop. This is the minimal configuration: . openid_auth_domain: http_enabled: true transport_enabled: true order: 0 http_authenticator: type: openid challenge: false config: subject_key: preferred_username roles_key: roles openid_connect_url: https://keycloak.example.com:8080/auth/realms/master/.well-known/openid-configuration authentication_backend: type: noop . The following table shows the configuration parameters. | Name | Description | . | openid_connect_url | The URL of your IdP where the Security plugin can find the OpenID Connect metadata/configuration settings. This URL differs between IdPs. Required. | . | jwt_header | The HTTP header that stores the token. Typically the Authorization header with the Bearer schema: Authorization: Bearer &lt;token&gt;. Optional. Default is Authorization. | . | jwt_url_parameter | If the token is not transmitted in the HTTP header, but as an URL parameter, define the name of the parameter here. Optional. | . | subject_key | The key in the JSON payload that stores the user’s name. If not defined, the subject registered claim is used. Most IdP providers use the preferred_username claim. Optional. | . | roles_key | The key in the JSON payload that stores the user’s roles. The value of this key must be a comma-separated list of roles. Required only if you want to use roles in the JWT. | . ",
    "url": "https://vagimeli.github.io/security/authentication-backends/openid-connect/#configure-openid-connect-integration",
    "relUrl": "/security/authentication-backends/openid-connect/#configure-openid-connect-integration"
  },"632": {
    "doc": "OpenID Connect",
    "title": "OpenID Connect URL",
    "content": "OpenID Connect specifies various endpoints for integration purposes. The most important endpoint is well-known, which lists endpoints and other configuration options for the Security plugin. The URL differs between IdPs, but usually ends in /.well-known/openid-configuration. Keycloak example: . http(s)://&lt;server&gt;:&lt;port&gt;/auth/realms/&lt;realm&gt;/.well-known/openid-configuration . The main information that the Security plugin needs is jwks_uri. This URI specifies where the IdP’s public keys in JWKS format can be found. For example: . jwks_uri: \"https://keycloak.example.com:8080/auth/realms/master/protocol/openid-connect/certs\" . { keys:[ { kid:\"V-diposfUJIk5jDBFi_QRouiVinG5PowskcSWy5EuCo\", kty:\"RSA\", alg:\"RS256\", use:\"sig\", n:\"rI8aUrAcI_auAdF10KUopDOmEFa4qlUUaNoTER90XXWADtKne6VsYoD3ZnHGFXvPkRAQLM5d65ScBzWungcbLwZGWtWf5T2NzQj0wDyquMRwwIAsFDFtAZWkXRfXeXrFY0irYUS9rIJDafyMRvBbSz1FwWG7RTQkILkwiC4B8W1KdS5d9EZ8JPhrXvPMvW509g0GhLlkBSbPBeRSUlAS2Kk6nY5i3m6fi1H9CP3Y_X-TzOjOTsxQA_1pdP5uubXPUh5YfJihXcgewO9XXiqGDuQn6wZ3hrF6HTlhNWGcSyQPKh1gEcmXWQlRENZMvYET-BuJEE7eKyM5vRhjNoYR3w\", e:\"AQAB\" } ] } . For more information about IdP endpoints, see the following: . | Okta | Keycloak | Auth0 | Connect2ID | Salesforce | IBM OpenID Connect | . ",
    "url": "https://vagimeli.github.io/security/authentication-backends/openid-connect/#openid-connect-url",
    "relUrl": "/security/authentication-backends/openid-connect/#openid-connect-url"
  },"633": {
    "doc": "OpenID Connect",
    "title": "Time disparity compensation for JWT validation",
    "content": "Occasionally you may find that the clock times between the authentication server and the OpenSearch node are not perfectly synchronized. When this is the case, even by a few seconds, the system that either issues or receives a JWT may try to validate nbf (not before) and exp (expiration) claims and fail to authenticate the user due to the time disparity. By default, Security allows for a window of 30 seconds to compensate for possible misalignment between server clock times. To set a custom value for this feature and override the default, you can add the jwt_clock_skew_tolerance_seconds setting to the config.yml: . http_authenticator: type: openid challenge: false config: subject_key: preferred_username roles_key: roles openid_connect_url: https://keycloak.example.com:8080/auth/realms/master/.well-known/openid-configuration jwt_clock_skew_tolerance_seconds: 20 . ",
    "url": "https://vagimeli.github.io/security/authentication-backends/openid-connect/#time-disparity-compensation-for-jwt-validation",
    "relUrl": "/security/authentication-backends/openid-connect/#time-disparity-compensation-for-jwt-validation"
  },"634": {
    "doc": "OpenID Connect",
    "title": "Fetching public keys",
    "content": "When an IdP generates and signs a JWT, it must add the ID of the key to the JWT header. For example: . { \"alg\": \"RS256\", \"typ\": \"JWT\", \"kid\": \"V-diposfUJIk5jDBFi_QRouiVinG5PowskcSWy5EuCo\" } . As per the OpenID Connect specification, the kid (key ID) is mandatory. Token verification does not work if an IdP fails to add the kid field to the JWT. If the Security plugin receives a JWT with an unknown kid, it visits the IdP’s jwks_uri and retrieves all available, valid keys. These keys are used and cached until a refresh is triggered by retrieving another unknown key ID. ",
    "url": "https://vagimeli.github.io/security/authentication-backends/openid-connect/#fetching-public-keys",
    "relUrl": "/security/authentication-backends/openid-connect/#fetching-public-keys"
  },"635": {
    "doc": "OpenID Connect",
    "title": "Key rollover and multiple public keys",
    "content": "The Security plugin can maintain multiple valid public keys at once. The OpenID specification does not allow for a validity period of public keys, so a key is valid until it has been removed from the list of valid keys in your IdP and the list of valid keys has been refreshed. If you want to roll over a key in your IdP, follow these best practices: . | Create a new key pair in your IdP, and give the new key a higher priority than the currently used key. Your IdP uses this new key over the old key. | Upon first appearance of the new kid in a JWT, the Security plugin refreshes the key list. At this point, both the old key and the new key are valid. Tokens signed with the old key are also still valid. | The old key can be removed from your IdP when the last JWT signed with this key has timed out. | . If you have to immediately change your public key, you can also delete the old key first and then create a new one. In this case, all JWTs signed with the old key become invalid immediately. ",
    "url": "https://vagimeli.github.io/security/authentication-backends/openid-connect/#key-rollover-and-multiple-public-keys",
    "relUrl": "/security/authentication-backends/openid-connect/#key-rollover-and-multiple-public-keys"
  },"636": {
    "doc": "OpenID Connect",
    "title": "TLS settings",
    "content": "To prevent man-in-the-middle attacks, you should secure the connection between the Security plugin and your IdP with TLS. Enabling TLS . Use the following parameters to enable TLS for connecting to your IdP: . config: openid_connect_idp: enable_ssl: &lt;true|false&gt; verify_hostnames: &lt;true|false&gt; . | Name | Description | . | enable_ssl | Whether to use TLS. Default is false. | . | verify_hostnames | Whether to verify the hostnames of the IdP’s TLS certificate. Default is true. | . Certificate validation . To validate the TLS certificate of your IdP, configure either the path to the IdP’s root CA or the root certificate’s content: . config: openid_connect_idp: enable_ssl: true pemtrustedcas_filepath: /full/path/to/trusted_cas.pem . config: openid_connect_idp: enable_ssl: true pemtrustedcas_content: |- MIID/jCCAuagAwIBAgIBATANBgkqhkiG9w0BAQUFADCBjzETMBEGCgmSJomT8ixk ARkWA2NvbTEXMBUGCgmSJomT8ixkARkWB2V4YW1wbGUxGTAXBgNVBAoMEEV4YW1w bGUgQ29tIEluYy4xITAfBgNVBAsMGEV4YW1wbGUgQ29tIEluYy4gUm9vdCBDQTEh ... | Name | Description | . | pemtrustedcas_filepath | Absolute path to the PEM file containing the root CAs of your IdP. | . | pemtrustedcas_content | The root CA content of your IdP. Cannot be used if pemtrustedcas_filepath is set. | . TLS client authentication . To use TLS client authentication, configure the PEM certificate and private key the Security plugin should send for TLS client authentication (or its content): . config: openid_connect_idp: enable_ssl: true pemkey_filepath: /full/path/to/private.key.pem pemkey_password: private_key_password pemcert_filepath: /full/path/to/certificate.pem . config: openid_connect_idp: enable_ssl: true pemkey_content: |- MIID2jCCAsKgAwIBAgIBBTANBgkqhkiG9w0BAQUFADCBlTETMBEGCgmSJomT8ixk ARkWA2NvbTEXMBUGCgmSJomT8ixkARkWB2V4YW1wbGUxGTAXBgNVBAoMEEV4YW1w bGUgQ29tIEluYy4xJDAiBgNVBAsMG0V4YW1wbGUgQ29tIEluYy4gU2lnbmluZyBD ... pemkey_password: private_key_password pemcert_content: |- MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCHRZwzwGlP2FvL oEzNeDu2XnOF+ram7rWPT6fxI+JJr3SDz1mSzixTeHq82P5A7RLdMULfQFMfQPfr WXgB4qfisuDSt+CPocZRfUqqhGlMG2l8LgJMr58tn0AHvauvNTeiGlyXy0ShxHbD ... | Name | Description | . | enable_ssl_client_auth | Whether to send the client certificate to the IdP server. Default is false. | . | pemcert_filepath | Absolute path to the client certificate. | . | pemcert_content | The content of the client certificate. Cannot be used when pemcert_filepath is set. | . | pemkey_filepath | Absolute path to the file containing the private key of the client certificate. | . | pemkey_content | The content of the private key of your client certificate. Cannot be used when pemkey_filepath is set. | . | pemkey_password | The password of your private key, if any. | . Enabled ciphers and protocols . You can limit the allowed ciphers and TLS protocols by using the following keys. | Name | Description | . | enabled_ssl_ciphers | Array. Enabled TLS cipher suites. Only Java format is supported. | . | enabled_ssl_protocols | Array. Enabled TLS protocols. Only Java format is supported. | . ",
    "url": "https://vagimeli.github.io/security/authentication-backends/openid-connect/#tls-settings",
    "relUrl": "/security/authentication-backends/openid-connect/#tls-settings"
  },"637": {
    "doc": "OpenID Connect",
    "title": "(Advanced) DoS protection",
    "content": "To help protect against denial-of-service (DoS) attacks, the Security plugin only allows a maximum number of new key IDs in a certain span of time. If the number of new key IDs exceeds this threshold, the Security plugin returns HTTP status code 503 (Service Unavailable) and refuses to query the IdP. By default, the Security plugin does not allow for more than 10 unknown key IDs within 10 seconds. The following table shows how to modify these settings. | Name | Description | . | refresh_rate_limit_count | The maximum number of unknown key IDs in the time frame. Default is 10. | . | refresh_rate_limit_time_window_ms | The time frame to use when checking the maximum number of unknown key IDs, in milliseconds. Default is 10000 (10 seconds). | . ",
    "url": "https://vagimeli.github.io/security/authentication-backends/openid-connect/#advanced-dos-protection",
    "relUrl": "/security/authentication-backends/openid-connect/#advanced-dos-protection"
  },"638": {
    "doc": "OpenID Connect",
    "title": "OpenSearch Dashboards single sign-on",
    "content": "Activate OpenID Connect by adding the following to opensearch_dashboards.yml: . opensearch_security.auth.type: \"openid\" . Configuration . OpenID Connect providers usually publish their configuration in JSON format under the metadata url. Therefore, most settings can be pulled in automatically, so the OpenSearch Dashboards configuration becomes minimal. The most important settings are the following: . | Connect URL | Client ID . Every IdP can host multiple clients (sometimes called applications) with different settings and authentication protocols. When enabling OpenID Connect, you should create a new client for OpenSearch Dashboards in your IdP. The client ID uniquely identifies OpenSearch Dashboards. | Client secret . Beyond the ID, each client also has a client secret assigned. The client secret is usually generated when the client is created. Applications can obtain an identity token only when they provide a client secret. You can find this secret in the settings of the client on your IdP. | . Configuration settings . | Name | Description | . | opensearch_security.openid.connect_url | The URL where the IdP publishes the OpenID metadata. Required. | . | opensearch_security.openid.client_id | The ID of the OpenID Connect client configured in your IdP. Required. | . | opensearch_security.openid.client_secret | The client secret of the OpenID Connect client configured in your IdP. Required. | . | opensearch_security.openid.scope | The scope of the identity token issued by the IdP. Optional. Default is openid profile email address phone. | . | opensearch_security.openid.header | HTTP header name of the JWT token. Optional. Default is Authorization. | . | opensearch_security.openid.logout_url | The logout URL of your IdP. Optional. Only necessary if your IdP does not publish the logout URL in its metadata. | . | opensearch_security.openid.base_redirect_url | The base of the redirect URL that will be sent to your IdP. Optional. Only necessary when OpenSearch Dashboards is behind a reverse proxy, in which case it should be different than server.host and server.port in opensearch_dashboards.yml. | . | opensearch_security.openid.trust_dynamic_headers | Compute base_redirect_url from the reverse proxy HTTP headers (X-Forwarded-Host / X-Forwarded-Proto). Optional. Default is false. | . Configuration example . # Enable OpenID authentication opensearch_security.auth.type: \"openid\" # The IdP metadata endpoint opensearch_security.openid.connect_url: \"http://keycloak.example.com:8080/auth/realms/master/.well-known/openid-configuration\" # The ID of the OpenID Connect client in your IdP opensearch_security.openid.client_id: \"opensearch-dashboards-sso\" # The client secret of the OpenID Connect client opensearch_security.openid.client_secret: \"a59c51f5-f052-4740-a3b0-e14ba355b520\" # Use HTTPS instead of HTTP opensearch.url: \"https://&lt;hostname&gt;.com:&lt;http port&gt;\" # Configure the OpenSearch Dashboards internal server user opensearch.username: \"kibanaserver\" opensearch.password: \"kibanaserver\" # Disable SSL verification when using self-signed demo certificates opensearch.ssl.verificationMode: none # allowlist basic headers and multi-tenancy header opensearch.requestHeadersAllowlist: [\"Authorization\", \"security_tenant\"] . To include OpenID Connect with other authentication types in the Dashboards sign-in window, see Configuring sign-in options. Session management with additional cookies . To improve session management—especially for users who have multiple roles assigned to them—Dashboards provides an option to split cookie payloads into multiple cookies and then recombine the payloads when receiving them. This can help prevent larger OpenID Connect assertions from exceeding size limits for each cookie. The two settings in the following example allow you to set a prefix name for additional cookies and specify the number of them. They are added to the opensearch_dashboards.yml file. The default number of additional cookies is three: . opensearch_security.openid.extra_storage.cookie_prefix: security_authentication_oidc opensearch_security.openid.extra_storage.additional_cookies: 3 . Note that reducing the number of additional cookies can cause some of the cookies that were in use before the change to stop working. We recommend establishing a fixed number of additional cookies and not changing the configuration after that. If the ID token from the IdP is especially large, OpenSearch may throw a server log authentication error indicating that the HTTP header is too large. In this case, you can increase the value for the http.max_header_size setting in the opensearch.yml file. OpenSearch security configuration . Because OpenSearch Dashboards requires that the internal OpenSearch Dashboards server user can authenticate through HTTP basic authentication, you must configure two authentication domains. For OpenID Connect, the HTTP basic domain has to be placed first in the chain. Make sure you set the challenge flag to false. Modify and apply the following example settings in config.yml: . basic_internal_auth_domain: http_enabled: true transport_enabled: true order: 0 http_authenticator: type: basic challenge: false authentication_backend: type: internal openid_auth_domain: http_enabled: true transport_enabled: true order: 1 http_authenticator: type: openid challenge: false config: subject_key: preferred_username roles_key: roles openid_connect_url: https://keycloak.example.com:8080/auth/realms/master/.well-known/openid-configuration authentication_backend: type: noop . ",
    "url": "https://vagimeli.github.io/security/authentication-backends/openid-connect/#opensearch-dashboards-single-sign-on",
    "relUrl": "/security/authentication-backends/openid-connect/#opensearch-dashboards-single-sign-on"
  },"639": {
    "doc": "Proxy-based authentication",
    "title": "Proxy-based authentication",
    "content": "If you already have a single sign-on (SSO) solution in place, you might want to use it as an authentication backend. Most solutions work as a proxy in front of OpenSearch and the Security plugin. If proxy authentication succeeds, the proxy adds the (verified) username and its (verified) roles in HTTP header fields. The names of these fields depend on the SSO solution you have in place. The Security plugin then extracts these HTTP header fields from the request and uses the values to determine the user’s permissions. ",
    "url": "https://vagimeli.github.io/security/authentication-backends/proxy/",
    "relUrl": "/security/authentication-backends/proxy/"
  },"640": {
    "doc": "Proxy-based authentication",
    "title": "Enable proxy detection",
    "content": "To enable proxy detection for OpenSearch, configure it in the xff section of config.yml: . --- _meta: type: \"config\" config_version: 2 config: dynamic: http: anonymous_auth_enabled: false xff: enabled: true internalProxies: '192\\.168\\.0\\.10|192\\.168\\.0\\.11' remoteIpHeader: 'x-forwarded-for' . You can configure the following settings: . | Name | Description | . | enabled | Enables or disables proxy support. Default is false. | . | internalProxies | A regular expression containing the IP addresses of all trusted proxies. The pattern .* trusts all internal proxies. | . | remoteIpHeader | Name of the HTTP header field that has the hostname chain. Default is x-forwarded-for. | . To determine whether a request comes from a trusted internal proxy, the Security plugin compares the remote address of the HTTP request with the list of configured internal proxies. If the remote address is not in the list, the plugin treats the request like a client request. ",
    "url": "https://vagimeli.github.io/security/authentication-backends/proxy/#enable-proxy-detection",
    "relUrl": "/security/authentication-backends/proxy/#enable-proxy-detection"
  },"641": {
    "doc": "Proxy-based authentication",
    "title": "Enable proxy authentication",
    "content": "Configure the names of the HTTP header fields that carry the authenticated username and role(s) in in the proxy HTTP authenticator section: . proxy_auth_domain: http_enabled: true transport_enabled: true order: 0 http_authenticator: type: proxy challenge: false config: user_header: \"x-proxy-user\" roles_header: \"x-proxy-roles\" authentication_backend: type: noop . | Name | Description | . | user_header | The HTTP header field containing the authenticated username. Default is x-proxy-user. | . | roles_header | The HTTP header field containing the comma-separated list of authenticated role names. The Security plugin uses the roles found in this header field as backend roles. Default is x-proxy-roles. | . | roles_separator | The separator for roles. Default is ,. | . ",
    "url": "https://vagimeli.github.io/security/authentication-backends/proxy/#enable-proxy-authentication",
    "relUrl": "/security/authentication-backends/proxy/#enable-proxy-authentication"
  },"642": {
    "doc": "Proxy-based authentication",
    "title": "Enable extended proxy authentication",
    "content": "The Security plugin has an extended version of the proxy type that lets you pass additional user attributes for use with document-level security. Aside from type: extended-proxy and attr_header_prefix, configuration is identical: . proxy_auth_domain: http_enabled: true transport_enabled: true order: 0 http_authenticator: type: extended-proxy challenge: false config: user_header: \"x-proxy-user\" roles_header: \"x-proxy-roles\" attr_header_prefix: \"x-proxy-ext-\" authentication_backend: type: noop . | Name | Description | . | attr_header_prefix | The header prefix that the proxy uses to provide user attributes. For example, if the proxy provides x-proxy-ext-namespace: my-namespace, use ${attr.proxy.namespace} in document-level security queries. | . ",
    "url": "https://vagimeli.github.io/security/authentication-backends/proxy/#enable-extended-proxy-authentication",
    "relUrl": "/security/authentication-backends/proxy/#enable-extended-proxy-authentication"
  },"643": {
    "doc": "Proxy-based authentication",
    "title": "Example",
    "content": "The following example uses an nginx proxy in front of a three-node OpenSearch cluster. For simplicity, we use hardcoded values for x-proxy-user and x-proxy-roles. In a real world example you would set these headers dynamically. The example also includes a commented header for use with the extended proxy. events { worker_connections 1024; } http { upstream opensearch { server node1.example.com:9200; server node2.example.com:9200; server node3.example.com:9200; keepalive 15; } server { listen 8090; server_name nginx.example.com; location / { proxy_pass https://opensearch; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header x-proxy-user test; proxy_set_header x-proxy-roles test; #proxy_set_header x-proxy-ext-namespace my-namespace; } } } . The corresponding minimal config.yml looks like: . --- _meta: type: \"config\" config_version: 2 config: dynamic: http: xff: enabled: true internalProxies: '172.16.0.203' # the nginx proxy authc: proxy_auth_domain: http_enabled: true transport_enabled: true order: 0 http_authenticator: type: proxy #type: extended-proxy challenge: false config: user_header: \"x-proxy-user\" roles_header: \"x-proxy-roles\" #attr_header_prefix: \"x-proxy-ext-\" authentication_backend: type: noop . The important part is to enable the X-Forwarded-For (XFF) resolution and set the IP(s) of the internal proxies correctly: . enabled: true internalProxies: '172.16.0.203' # nginx proxy . In this case, nginx.example.com runs on 172.16.0.203, so add this IP to the list of internal proxies. Be sure to set internalProxies to the minimum number of IP addresses so that the Security plugin only accepts requests from trusted IPs. ",
    "url": "https://vagimeli.github.io/security/authentication-backends/proxy/#example",
    "relUrl": "/security/authentication-backends/proxy/#example"
  },"644": {
    "doc": "Proxy-based authentication",
    "title": "OpenSearch Dashboards proxy authentication",
    "content": "To use proxy authentication with OpenSearch Dashboards, the most common configuration is to place the proxy in front of OpenSearch Dashboards and let OpenSearch Dashboards pass the user and role headers to the Security plugin. In this case, the remote address of the HTTP call is the IP of OpenSearch Dashboards, because it sits directly in front of OpenSearch. Add the IP of OpenSearch Dashboards to the list of internal proxies: . --- _meta: type: \"config\" config_version: 2 config: dynamic: http: xff: enabled: true remoteIpHeader: \"x-forwarded-for\" internalProxies: '&lt;opensearch-dashboards-ip-address&gt;' . To pass the user and role headers that the authenticating proxy adds from OpenSearch Dashboards to the Security plugin, add them to the HTTP header allow list in opensearch_dashboards.yml: . opensearch.requestHeadersAllowlist: [\"securitytenant\",\"Authorization\",\"x-forwarded-for\",\"x-proxy-user\",\"x-proxy-roles\"] . You must also enable the authentication type in opensearch_dashboards.yml: . opensearch_security.auth.type: \"proxy\" opensearch_security.proxycache.user_header: \"x-proxy-user\" opensearch_security.proxycache.roles_header: \"x-proxy-roles\" . ",
    "url": "https://vagimeli.github.io/security/authentication-backends/proxy/#opensearch-dashboards-proxy-authentication",
    "relUrl": "/security/authentication-backends/proxy/#opensearch-dashboards-proxy-authentication"
  },"645": {
    "doc": "SAML",
    "title": "SAML",
    "content": "The Security plugin supports user authentication through SAML single sign-on. The Security plugin implements the web browser SSO profile of the SAML 2.0 protocol. This profile is meant for use with web browsers. It is not a general-purpose way of authenticating users against the Security plugin, so its primary use case is to support OpenSearch Dashboards single sign-on. ",
    "url": "https://vagimeli.github.io/security/authentication-backends/saml/",
    "relUrl": "/security/authentication-backends/saml/"
  },"646": {
    "doc": "SAML",
    "title": "Docker example",
    "content": "We provide a fully functional example that can help you understand how to use SAML with OpenSearch Dashboards. | Download and unzip the example zip file. | At the command line, run docker-compose up. | Review the files: . | docker-compose.yml defines two OpenSearch nodes, an OpenSearch Dashboards server, and a SAML server. | custom-opensearch_dashboards.yml add a few SAML settings to the default opensearch_dashboards.yml file. | config.yml configures SAML for authentication. | . | Access OpenSearch Dashboards at http://localhost:5601. Note that OpenSearch Dashboards immediately redirects you to the SAML login page. | Log in as admin with a password of admin. | After logging in, note that your user in the upper-right is SAMLAdmin, as defined in /var/www/simplesamlphp/config/authsources.php of the SAML server. | If you want to examine the SAML server, run docker ps to find its container ID and then docker exec -it &lt;container-id&gt; /bin/bash. In particular, you might find it helpful to review the contents of the /var/www/simplesamlphp/config/ and /var/www/simplesamlphp/metadata/ directories. | . ",
    "url": "https://vagimeli.github.io/security/authentication-backends/saml/#docker-example",
    "relUrl": "/security/authentication-backends/saml/#docker-example"
  },"647": {
    "doc": "SAML",
    "title": "Activating SAML",
    "content": "To use SAML for authentication, you need to configure a respective authentication domain in the authc section of config/opensearch-security/config.yml. Because SAML works solely on the HTTP layer, you do not need any authentication_backend and can set it to noop. Place all SAML-specific configuration options in this chapter in the config section of the SAML HTTP authenticator: . authc: saml_auth_domain: http_enabled: true transport_enabled: false order: 1 http_authenticator: type: saml challenge: true config: idp: metadata_file: okta.xml ... authentication_backend: type: noop . After you have configured SAML in config.yml, you must also activate it in OpenSearch Dashboards. ",
    "url": "https://vagimeli.github.io/security/authentication-backends/saml/#activating-saml",
    "relUrl": "/security/authentication-backends/saml/#activating-saml"
  },"648": {
    "doc": "SAML",
    "title": "Running multiple authentication domains",
    "content": "We recommend adding at least one other authentication domain, such as LDAP or the internal user database, to support API access to OpenSearch without SAML. For OpenSearch Dashboards and the internal OpenSearch Dashboards server user, you also must add another authentication domain that supports basic authentication. This authentication domain should be placed first in the chain, and the challenge flag must be set to false: . authc: basic_internal_auth_domain: http_enabled: true transport_enabled: true order: 0 http_authenticator: type: basic challenge: false authentication_backend: type: internal saml_auth_domain: http_enabled: true transport_enabled: false order: 1 http_authenticator: type: saml challenge: true config: ... authentication_backend: type: noop . ",
    "url": "https://vagimeli.github.io/security/authentication-backends/saml/#running-multiple-authentication-domains",
    "relUrl": "/security/authentication-backends/saml/#running-multiple-authentication-domains"
  },"649": {
    "doc": "SAML",
    "title": "Identity provider metadata",
    "content": "A SAML identity provider (IdP) provides a SAML 2.0 metadata file describing the IdP’s capabilities and configuration. The Security plugin can read IdP metadata either from a URL or a file. The choice that you make depends on your IdP and your preferences. The SAML 2.0 metadata file is required. | Name | Description | . | idp.metadata_file | The path to the SAML 2.0 metadata file of your IdP. Place the metadata file in the config directory of OpenSearch. The path has to be specified relative to the config directory. Required if idp.metadata_url is not set. | . | idp.metadata_url | The SAML 2.0 metadata URL of your IdP. Required if idp.metadata_file is not set. | . ",
    "url": "https://vagimeli.github.io/security/authentication-backends/saml/#identity-provider-metadata",
    "relUrl": "/security/authentication-backends/saml/#identity-provider-metadata"
  },"650": {
    "doc": "SAML",
    "title": "IdP and service provider entity ID",
    "content": "An entity ID is a globally unique name for a SAML entity, either an IdP or a service provider (SP). The IdP entity ID is usually provided by your IdP. The SP entity ID is the name of the configured application or client in your IdP. We recommend adding a new application for OpenSearch Dashboards and using the URL of your OpenSearch Dashboards installation as the SP entity ID. | Name | Description | . | idp.entity_id | The entity ID of your IdP. Required. | . | sp.entity_id | The entity ID of the service provider. Required. | . ",
    "url": "https://vagimeli.github.io/security/authentication-backends/saml/#idp-and-service-provider-entity-id",
    "relUrl": "/security/authentication-backends/saml/#idp-and-service-provider-entity-id"
  },"651": {
    "doc": "SAML",
    "title": "Time disparity compensation for JWT validation",
    "content": "Occasionally you may find that the clock times between the authentication server and the OpenSearch node are not perfectly synchronized. When this is the case, even by a few seconds, the system that either issues or receives a JSON Web Token (JWT) may try to validate nbf (not before) and exp (expiration) claims and fail to authenticate the user due to the time disparity. By default, OpenSearch Security allows for a window of 30 seconds to compensate for possible misalignment between server clock times. To set a custom value for this feature and override the default, you can add the jwt_clock_skew_tolerance_seconds setting to the config.yml. http_authenticator: type: saml challenge: true config: idp: metadata_file: okta.xml jwt_clock_skew_tolerance_seconds: 20 . ",
    "url": "https://vagimeli.github.io/security/authentication-backends/saml/#time-disparity-compensation-for-jwt-validation",
    "relUrl": "/security/authentication-backends/saml/#time-disparity-compensation-for-jwt-validation"
  },"652": {
    "doc": "SAML",
    "title": "OpenSearch Dashboards settings",
    "content": "The web browser SSO profile exchanges information through HTTP GET or POST. For example, after you log in to your IdP, it sends an HTTP POST back to OpenSearch Dashboards containing the SAML response. You must configure the base URL of your OpenSearch Dashboards installation where the HTTP requests are being sent to. | Name | Description | . | kibana_url | The OpenSearch Dashboards base URL. Required. | . ",
    "url": "https://vagimeli.github.io/security/authentication-backends/saml/#opensearch-dashboards-settings",
    "relUrl": "/security/authentication-backends/saml/#opensearch-dashboards-settings"
  },"653": {
    "doc": "SAML",
    "title": "Username and Role attributes",
    "content": "Subjects (for example, user names) are usually stored in the NameID element of a SAML response: . &lt;saml2:Subject&gt; &lt;saml2:NameID&gt;admin&lt;/saml2:NameID&gt; ... &lt;/saml2:Subject&gt; . If your IdP is compliant with the SAML 2.0 specification, you do not need to set anything special. If your IdP uses a different element name, you can also specify its name explicitly. Role attributes are optional. However, most IdPs can be configured to add roles in the SAML assertions as well. If present, you can use these roles in your role mappings: . &lt;saml2:Attribute Name='Role'&gt; &lt;saml2:AttributeValue &gt;Everyone&lt;/saml2:AttributeValue&gt; &lt;saml2:AttributeValue &gt;Admins&lt;/saml2:AttributeValue&gt; &lt;/saml2:Attribute&gt; . If you want to extract roles from the SAML response, you need to specify the element name that contains the roles. | Name | Description | . | subject_key | The attribute in the SAML response where the subject is stored. Optional. If not configured, the NameID attribute is used. | . | roles_key | The attribute in the SAML response where the roles are stored. Optional. If not configured, no roles are used. | . ",
    "url": "https://vagimeli.github.io/security/authentication-backends/saml/#username-and-role-attributes",
    "relUrl": "/security/authentication-backends/saml/#username-and-role-attributes"
  },"654": {
    "doc": "SAML",
    "title": "Request signing",
    "content": "Requests from the Security plugin to the IdP can optionally be signed. Use the following settings to configure request signing. | Name | Description | . | sp.signature_private_key | The private key used to sign the requests or to decode encrypted assertions. Optional. Cannot be used when private_key_filepath is set. | . | sp.signature_private_key_password | The password of the private key, if any. | . | sp.signature_private_key_filepath | Path to the private key. The file must be placed under the OpenSearch config directory, and the path must be specified relative to that same directory. | . | sp.signature_algorithm | The algorithm used to sign the requests. See the next table for possible values. | . The Security plugin supports the following signature algorithms. | Algorithm | Value | . | DSA_SHA1 | http://www.w3.org/2000/09/xmldsig#dsa-sha1; | . | RSA_SHA1 | http://www.w3.org/2000/09/xmldsig#rsa-sha1; | . | RSA_SHA256 | http://www.w3.org/2001/04/xmldsig-more#rsa-sha256; | . | RSA_SHA384 | http://www.w3.org/2001/04/xmldsig-more#rsa-sha384; | . | RSA_SHA512 | http://www.w3.org/2001/04/xmldsig-more#rsa-sha512; | . ",
    "url": "https://vagimeli.github.io/security/authentication-backends/saml/#request-signing",
    "relUrl": "/security/authentication-backends/saml/#request-signing"
  },"655": {
    "doc": "SAML",
    "title": "Logout",
    "content": "Usually, IdPs provide information about their individual logout URL in their SAML 2.0 metadata. If this is the case, the Security plugin uses them to render the correct logout link in OpenSearch Dashboards. If your IdP does not support an explicit logout, you can force a re-login when the user visits OpenSearch Dashboards again. | Name | Description | . | sp.forceAuthn | Force a re-login even if the user has an active session with the IdP. | . Currently, the Security plugin supports only the HTTP-Redirect logout binding. Make sure this is configured correctly in your IdP. ",
    "url": "https://vagimeli.github.io/security/authentication-backends/saml/#logout",
    "relUrl": "/security/authentication-backends/saml/#logout"
  },"656": {
    "doc": "SAML",
    "title": "Exchange key settings",
    "content": "SAML, unlike other protocols, is not meant to be used for exchanging user credentials with each request. The Security plugin trades the SAML response for a lightweight JWT that stores the validated user attributes. This token is signed by an exchange key of your choice. Note that when you change this key, all tokens signed with it become invalid immediately. | Name | Description | . | exchange_key | The key to sign the token. The algorithm is HMAC256, so it should have at least 32 characters. | . ",
    "url": "https://vagimeli.github.io/security/authentication-backends/saml/#exchange-key-settings",
    "relUrl": "/security/authentication-backends/saml/#exchange-key-settings"
  },"657": {
    "doc": "SAML",
    "title": "TLS settings",
    "content": "If you are loading the IdP metadata from a URL, we recommend that you use SSL/TLS. If you use an external IdP like Okta or Auth0 that uses a trusted certificate, you usually do not need to configure anything. If you host the IdP yourself and use your own root CA, you can customize the TLS settings as follows. These settings are used only for loading SAML metadata over HTTPS. | Name | Description | . | idp.enable_ssl | Whether to enable the custom TLS configuration. Default is false (JDK settings are used). | . | idp.verify_hostnames | Whether to verify the hostnames of the server’s TLS certificate. | . Example: . authc: saml_auth_domain: http_enabled: true transport_enabled: false order: 1 http_authenticator: type: saml challenge: true config: idp: enable_ssl: true verify_hostnames: true ... authentication_backend: type: noop . Certificate validation . Configure the root CA used for validating the IdP TLS certificate by setting one of the following configuration options: . config: idp: pemtrustedcas_filepath: path/to/trusted_cas.pem . config: idp: pemtrustedcas_content: |- MIID/jCCAuagAwIBAgIBATANBgkqhkiG9w0BAQUFADCBjzETMBEGCgmSJomT8ixk ARkWA2NvbTEXMBUGCgmSJomT8ixkARkWB2V4YW1wbGUxGTAXBgNVBAoMEEV4YW1w bGUgQ29tIEluYy4xITAfBgNVBAsMGEV4YW1wbGUgQ29tIEluYy4gUm9vdCBDQTEh ... | Name | Description | . | idp.pemtrustedcas_filepath | Path to the PEM file containing the root CAs of your IdP. The files must be placed under the OpenSearch config directory, and you must specify the path relative to that same directory. | . | idp.pemtrustedcas_content | The root CA content of your IdP server. Cannot be used when pemtrustedcas_filepath is set. | . Client authentication . The Security plugin can use TLS client authentication when fetching the IdP metadata. If enabled, the Security plugin sends a TLS client certificate to the IdP for each metadata request. Use the following keys to configure client authentication. | Name | Description | . | idp.enable_ssl_client_auth | Whether to send a client certificate to the IdP server. Default is false. | . | idp.pemcert_filepath | Path to the PEM file containing the client certificate. The file must be placed under the OpenSearch config directory, and the path must be specified relative to the config directory. | . | idp.pemcert_content | The content of the client certificate. Cannot be used when pemcert_filepath is set. | . | idp.pemkey_filepath | Path to the private key of the client certificate. The file must be placed under the OpenSearch config directory, and the path must be specified relative to the config directory. | . | idp.pemkey_content | The content of the private key of your certificate. Cannot be used when pemkey_filepath is set. | . | idp.pemkey_password | The password of your private key, if any. | . Enabled ciphers and protocols . You can limit the allowed ciphers and TLS protocols for the IdP connection. For example, you can only enable strong ciphers and limit the TLS versions to the most recent ones. | Name | Description | . | idp.enabled_ssl_ciphers | Array of enabled TLS ciphers. Only the Java format is supported. | . | idp.enabled_ssl_protocols | Array of enabled TLS protocols. Only the Java format is supported. | . ",
    "url": "https://vagimeli.github.io/security/authentication-backends/saml/#tls-settings",
    "relUrl": "/security/authentication-backends/saml/#tls-settings"
  },"658": {
    "doc": "SAML",
    "title": "Minimal configuration example",
    "content": "The following example shows the minimal configuration: . authc: saml_auth_domain: http_enabled: true transport_enabled: false order: 1 http_authenticator: type: saml challenge: true config: idp: metadata_file: metadata.xml entity_id: http://idp.example.com/ sp: entity_id: https://opensearch-dashboards.example.com kibana_url: https://opensearch-dashboards.example.com:5601/ roles_key: Role exchange_key: 'peuvgOLrjzuhXf ...' authentication_backend: type: noop . ",
    "url": "https://vagimeli.github.io/security/authentication-backends/saml/#minimal-configuration-example",
    "relUrl": "/security/authentication-backends/saml/#minimal-configuration-example"
  },"659": {
    "doc": "SAML",
    "title": "OpenSearch Dashboards configuration",
    "content": "Because most of the SAML-specific configuration is done in the Security plugin, just activate SAML in your opensearch_dashboards.yml by adding the following: . opensearch_security.auth.type: \"saml\" . In addition, you must add the OpenSearch Dashboards endpoint for validating the SAML assertions to your allow list: . server.xsrf.allowlist: [\"/_opendistro/_security/saml/acs\"] . If you use the logout POST binding, you also need to ad the logout endpoint to your allow list: . server.xsrf.allowlist: [\"/_opendistro/_security/saml/acs\", \"/_opendistro/_security/saml/logout\"] . To include SAML with other authentication types in the Dashboards sign-in window, see Configuring sign-in options. Session management with additional cookies . To improve session management—especially for users who have multiple roles assigned to them—Dashboards provides an option to split cookie payloads into multiple cookies and then recombine the payloads when receiving them. This can help prevent larger SAML assertions from exceeding size limits for each cookie. The two settings in the following example allow you to set a prefix name for additional cookies and specify the number of them. They are added to the opensearch_dashboards.yml file. The default number of additional cookies is three: . opensearch_security.saml.extra_storage.cookie_prefix: security_authentication_saml opensearch_security.saml.extra_storage.additional_cookies: 3 . Note that reducing the number of additional cookies can cause some of the cookies that were in use before the change to stop working. We recommend establishing a fixed number of additional cookies and not changing the configuration after that. If the ID token from the IdP is especially large, OpenSearch may throw a server log authentication error indicating that the HTTP header is too large. In this case, you can increase the value for the http.max_header_size setting in the opensearch.yml file. IdP-initiated SSO . To use IdP-initiated SSO, set the Assertion Consumer Service endpoint of your IdP to this: . /_opendistro/_security/saml/acs/idpinitiated . Then add this endpoint to server.xsrf.allowlist in opensearch_dashboards.yml: . server.xsrf.allowlist: [\"/_opendistro/_security/saml/acs/idpinitiated\", \"/_opendistro/_security/saml/acs\", \"/_opendistro/_security/saml/logout\"] . ",
    "url": "https://vagimeli.github.io/security/authentication-backends/saml/#opensearch-dashboards-configuration",
    "relUrl": "/security/authentication-backends/saml/#opensearch-dashboards-configuration"
  },"660": {
    "doc": "Configuring the Security backend",
    "title": "Configuring the Security backend",
    "content": "One of the first steps to using the Security plugin is to decide on an authentication backend, which handles steps 2-3 of the authentication flow. The plugin has an internal user database, but many people prefer to use an existing authentication backend, such as an LDAP server, or some combination of the two. The main configuration file for authentication and authorization backends is config/opensearch-security/config.yml. It defines how the Security plugin retrieves the user credentials, how it verifies these credentials, and how to fetch additional roles from backend systems (optional). config.yml has three main parts: . opensearch_security: dynamic: http: ... authc: ... authz: ... For a more complete example, see the sample file on GitHub. ",
    "url": "https://vagimeli.github.io/security/configuration/configuration/",
    "relUrl": "/security/configuration/configuration/"
  },"661": {
    "doc": "Configuring the Security backend",
    "title": "HTTP",
    "content": "The http section has the following format: . anonymous_auth_enabled: &lt;true|false&gt; xff: # optional section enabled: &lt;true|false&gt; internalProxies: &lt;string&gt; # Regex pattern remoteIpHeader: &lt;string&gt; # Name of the header in which to look. Typically: x-forwarded-for proxiesHeader: &lt;string&gt; trustedProxies: &lt;string&gt; # Regex pattern . If you disable anonymous authentication, the Security plugin won’t initialize if you have not provided at least one authc. ",
    "url": "https://vagimeli.github.io/security/configuration/configuration/#http",
    "relUrl": "/security/configuration/configuration/#http"
  },"662": {
    "doc": "Configuring the Security backend",
    "title": "Authentication",
    "content": "The authc section has the following format: . &lt;name&gt;: http_enabled: &lt;true|false&gt; transport_enabled: &lt;true|false&gt; order: &lt;integer&gt; http_authenticator: ... authentication_backend: ... An entry in the authc section is called an authentication domain. It specifies where to get the user credentials and against which backend they should be authenticated. You can use more than one authentication domain. Each authentication domain has a name (for example, basic_auth_internal), enabled flags, and an order. The order makes it possible to chain authentication domains together. The Security plugin uses them in the order that you provide. If the user successfully authenticates with one domain, the Security plugin skips the remaining domains. http_authenticator specifies which authentication method that you want to use on the HTTP layer. This is the syntax for defining an authenticator on the HTTP layer: . http_authenticator: type: &lt;type&gt; challenge: &lt;true|false&gt; config: ... These are the allowed values for type: . | basic: HTTP basic authentication. No additional configuration is needed. | kerberos: Kerberos authentication. Additional Kerberos-specific configuration is needed. | jwt: JSON Web Token (JWT) authentication. Additional JWT-specific configuration is needed. | clientcert: Authentication through a client TLS certificate. This certificate must be trusted by one of the root CAs in the truststore of your nodes. | . After setting an HTTP authenticator, you must specify against which backend system you want to authenticate the user: . authentication_backend: type: &lt;type&gt; config: ... These are the possible values for type: . | noop: No further authentication against any backend system is performed. Use noop if the HTTP authenticator has already authenticated the user completely, as in the case of JWT, Kerberos, or client certificate authentication. | internal: Use the users and roles defined in internal_users.yml for authentication. | ldap: Authenticate users against an LDAP server. This setting requires additional, LDAP-specific configuration settings. | . ",
    "url": "https://vagimeli.github.io/security/configuration/configuration/#authentication",
    "relUrl": "/security/configuration/configuration/#authentication"
  },"663": {
    "doc": "Configuring the Security backend",
    "title": "Authorization",
    "content": "After the user has been authenticated, the Security plugin can optionally collect additional roles from backend systems. The authorization configuration has the following format: . authz: &lt;name&gt;: http_enabled: &lt;true|false&gt; transport_enabled: &lt;true|false&gt; authorization_backend: type: &lt;type&gt; config: ... You can define multiple entries in this section the same way as you can for authentication entries. In this case, execution order is not relevant, so there is no order field. These are the possible values for type: . | noop: Skip this step altogether. | ldap: Fetch additional roles from an LDAP server. This setting requires additional, LDAP-specific configuration settings. | . ",
    "url": "https://vagimeli.github.io/security/configuration/configuration/#authorization",
    "relUrl": "/security/configuration/configuration/#authorization"
  },"664": {
    "doc": "Configuring the Security backend",
    "title": "Configuration examples",
    "content": "The default config/opensearch-security/config.yml that ships with OpenSearch contains many configuration examples. Use these examples as a starting point, and customize them to your needs. HTTP basic . To set up HTTP basic authentication, you must enable it in the http_authenticator section of the configuration: . http_authenticator: type: basic challenge: true . In most cases, you set the challenge flag to true. The flag defines the behavior of the Security plugin if the Authorization field in the HTTP header is not set. If challenge is set to true, the Security plugin sends a response with status UNAUTHORIZED (401) back to the client. If the client is accessing the cluster with a browser, this triggers the authentication dialog box, and the user is prompted to enter a user name and password. If challenge is set to false and no Authorization header field is set, the Security plugin does not send a WWW-Authenticate response back to the client, and authentication fails. You might want to use this setting if you have another challenge http_authenticator in your configured authentication domains. One such scenario is when you plan to use basic authentication and Kerberos together. Kerberos . Kerberos authentication does not work with OpenSearch Dashboards. To track OpenSearch’s progress in adding support for Kerberos in OpenSearch Dashboards, see issue #907 in the Dashboard’s Security plugin repository. Due to the nature of Kerberos, you must define some settings in opensearch.yml and some in config.yml. In opensearch.yml, define the following: . plugins.security.kerberos.krb5_filepath: '/etc/krb5.conf' plugins.security.kerberos.acceptor_keytab_filepath: 'eskeytab.tab' . | plugins.security.kerberos.krb5_filepath defines the path to your Kerberos configuration file. This file contains various settings regarding your Kerberos installation, for example, the realm names, hostnames, and ports of the Kerberos key distribution center (KDC). | plugins.security.kerberos.acceptor_keytab_filepath defines the path to the keytab file, which contains the principal that the Security plugin uses to issue requests against Kerberos. | plugins.security.kerberos.acceptor_principal: 'HTTP/localhost' defines the principal that the Security plugin uses to issue requests against Kerberos. This value must be present in the keytab file. | . Due to security restrictions, the keytab file must be placed in config or a subdirectory, and the path in opensearch.yml must be relative, not absolute. Dynamic configuration . A typical Kerberos authentication domain in config.yml looks like this: . authc: kerberos_auth_domain: enabled: true order: 1 http_authenticator: type: kerberos challenge: true config: krb_debug: false strip_realm_from_principal: true authentication_backend: type: noop . Authentication against Kerberos through a browser on an HTTP level is achieved using SPNEGO. Kerberos/SPNEGO implementations vary, depending on your browser and operating system. This is important when deciding if you need to set the challenge flag to true or false. As with HTTP Basic Authentication, this flag determines how the Security plugin should react when no Authorization header is found in the HTTP request or if this header does not equal negotiate. If set to true, the Security plugin sends a response with status code 401 and a WWW-Authenticate header set to negotiate. This tells the client (browser) to resend the request with the Authorization header set. If set to false, the Security plugin cannot extract the credentials from the request, and authentication fails. Setting challenge to false thus makes sense only if the Kerberos credentials are sent in the initial request. As the name implies, setting krb_debug to true will output Kerberos-specific debugging messages to stdout. Use this setting if you encounter problems with your Kerberos integration. If you set strip_realm_from_principal to true, the Security plugin strips the realm from the user name. Authentication backend . Because Kerberos/SPNEGO authenticates users on an HTTP level, no additional authentication_backend is needed. Set this value to noop. JSON Web Token . JWTs are JSON-based access tokens that assert one or more claims. They are commonly used to implement single sign-on (SSO) solutions and fall in the category of token-based authentication systems: . | A user logs in to an authentication server by providing credentials (for example, a user name and password). | The authentication server validates the credentials. | The authentication server creates an access token and signs it. | The authentication server returns the token to the user. | The user stores the access token. | The user sends the access token alongside every request to the service that it wants to use. | The service verifies the token and grants or denies access. | . A JWT is self-contained in the sense that it carries within itself all of the information necessary to verify a user. The tokens are base64-encoded, signed JSON objects. JWTs consist of three parts: . | Header | Payload | Signature | . Header . The header contains information about the used signing mechanism, as shown in the following example: . { \"alg\": \"HS256\", \"typ\": \"JWT\" } . In this case, the header states that the message was signed using HMAC-SHA256. Payload . The payload of a JWT contains the JWT claims. A claim can be any piece of information about the user that the application that created the token has verified. The specification defines a set of standard claims with reserved names, referred to as registered claims. Some examples of these claims include token issuer (iss), expiration time (exp), and subject (sub). Public claims, on the other hand, can be created freely by the token issuer. They can contain arbitrary information, such as the user name and the roles of the user. { \"iss\": \"example.com\", \"exp\": 1300819380, \"name\": \"John Doe\", \"roles\": \"admin, devops\" } . Signature . The issuer of the token calculates the signature of the token by applying a cryptographic hash function on the base64-encoded header and payload. These three parts are then concatenated using periods to form a complete JWT: . encoded = base64UrlEncode(header) + \".\" + base64UrlEncode(payload) signature = HMACSHA256(encoded, 'secretkey'); jwt = encoded + \".\" + base64UrlEncode(signature) . Example: . eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJsb2dnZWRJbkFzIjoiYWRtaW4iLCJpYXQiOjE0MjI3Nzk2Mzh9.gzSraSYS8EXBxLN_oWnFSRgCzcmJmMjLiuyu5CSpyHI . ",
    "url": "https://vagimeli.github.io/security/configuration/configuration/#configuration-examples",
    "relUrl": "/security/configuration/configuration/#configuration-examples"
  },"665": {
    "doc": "Configuring the Security backend",
    "title": "Configure JWTs",
    "content": "If you use a JWT as your only authentication method, disable the user cache by setting plugins.security.cache.ttl_minutes: 0. Set up an authentication domain and choose jwt as the HTTP authentication type. Because the tokens already contain all required information to verify the request, challenge must be set to false and authentication_backend to noop. jwt_auth_domain: http_enabled: true transport_enabled: true order: 0 http_authenticator: type: jwt challenge: false config: signing_key: \"base64 encoded key\" jwt_header: \"Authorization\" jwt_url_parameter: null subject_key: null roles_key: null jwt_clock_skew_tolerance_seconds: 20 authentication_backend: I type: noop . The following table shows the configuration parameters. | Name | Description | . | signing_key | The signing key to use when verifying the token. If you use a symmetric key algorithm, it is the base64-encoded shared secret. If you use an asymmetric algorithm, it contains the public key. | . | jwt_header | The HTTP header in which the token is transmitted. This typically is the Authorization header with the Bearer schema: Authorization: Bearer &lt;token&gt;. Default is Authorization. | . | jwt_url_parameter | If the token is not transmitted in the HTTP header, but as an URL parameter, define the name of this parameter here. | . | subject_key | The key in the JSON payload that stores the user name. If not set, the subject registered claim is used. | . | roles_key | The key in the JSON payload that stores the user’s roles. The value of this key must be a comma-separated list of roles. | . | jwt_clock_skew_tolerance_seconds | Sets a window of time, in seconds, to prevent authentication failures due to a misalignment between the JWT authentication server and OpenSearch node clock times. Security sets 30 seconds as the default. Use this setting to apply a custom value. | . Because JWTs are self-contained and the user is authenticated at the HTTP level, no additional authentication_backend is needed. Set this value to noop. Symmetric key algorithms: HMAC . Hash-based message authentication codes (HMACs) are a group of algorithms that provide a way of signing messages by means of a shared key. The key is shared between the authentication server and the Security plugin. It must be configured as a base64-encoded value in the signing_key setting: . jwt_auth_domain: ... config: signing_key: \"a3M5MjEwamRqOTAxOTJqZDE=\" ... Asymmetric key algorithms: RSA and ECDSA . RSA and ECDSA are asymmetric encryption and digital signature algorithms and use a public/private key pair to sign and verify tokens. This means that they use a private key for signing the token, while the Security plugin needs to know only the public key to verify it. Because you cannot issue new tokens with the public key—and because you can make valid assumptions about the creator of the token—RSA and ECDSA are considered more secure than using HMAC. To use RS256, you need to configure only the (non-base64-encoded) public RSA key as signing_key in the JWT configuration: . jwt_auth_domain: ... config: signing_key: |- -----BEGIN PUBLIC KEY----- MIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQK... -----END PUBLIC KEY----- ... The Security plugin automatically detects the algorithm (RSA/ECDSA), and if necessary you can break the key into multiple lines. Bearer authentication for HTTP requests . The most common way of transmitting a JWT in an HTTP request is to add it as an HTTP header with the bearer authentication schema: . Authorization: Bearer &lt;JWT&gt; . The default name of the header is Authorization. If required by your authentication server or proxy, you can also use a different HTTP header name using the jwt_header configuration key. As with HTTP basic authentication, you should use HTTPS instead of HTTP when transmitting JWTs in HTTP requests. URL parameters for HTTP requests . Although the most common way to transmit JWTs in HTTP requests is to use a header field, the Security plugin also supports parameters. Configure the name of the GET parameter using the following key: . config: signing_key: ... jwt_url_parameter: \"parameter_name\" subject_key: ... roles_key: ... As with HTTP basic authentication, you should use HTTPS instead of HTTP. Validated registered claims . The following registered claims are validated automatically: . | “iat” (Issued At) Claim | “nbf” (Not Before) Claim | “exp” (Expiration Time) Claim | . Supported formats and algorithms . The Security plugin supports digitally signed, compact JWTs with all standard algorithms: . HS256: HMAC using SHA-256 HS384: HMAC using SHA-384 HS512: HMAC using SHA-512 RS256: RSASSA-PKCS-v1_5 using SHA-256 RS384: RSASSA-PKCS-v1_5 using SHA-384 RS512: RSASSA-PKCS-v1_5 using SHA-512 PS256: RSASSA-PSS using SHA-256 and MGF1 with SHA-256 PS384: RSASSA-PSS using SHA-384 and MGF1 with SHA-384 PS512: RSASSA-PSS using SHA-512 and MGF1 with SHA-512 ES256: ECDSA using P-256 and SHA-256 ES384: ECDSA using P-384 and SHA-384 ES512: ECDSA using P-521 and SHA-512 . ",
    "url": "https://vagimeli.github.io/security/configuration/configuration/#configure-jwts",
    "relUrl": "/security/configuration/configuration/#configure-jwts"
  },"666": {
    "doc": "Configuring the Security backend",
    "title": "Troubleshooting common issues",
    "content": "This section details how to troubleshoot common issues with your Security plugin configuration. Correct iat . Ensure that the JWT token contains the correct iat (issued at), nbf (not before), and exp (expiry) claims, all of which are validated automatically by OpenSearch. JWT URL parameter . When using the JWT URL parameter containing the default admin role all_access against OpenSearch (for example, curl http://localhost:9200?jwtToken=&lt;jwt-token&gt;) the request fails with: . { \"error\":{ \"root_cause\":[ { \"type\":\"security_exception\", \"reason\":\"no permissions for [cluster:monitor/main] and User [name=admin, backend_roles=[all_access], requestedTenant=null]\" } ], \"type\":\"security_exception\", \"reason\":\"no permissions for [cluster:monitor/main] and User [name=admin, backend_roles=[all_access], requestedTenant=null]\" }, \"status\":403 } . To solve this, ensure that the role all_access is mapped directly to the internal user and not a backend role. To do this, navigate to Security &gt; Roles &gt; all_access and switch to the tab to Mapped Users. Select Manage mapping and add “admin” to the Users section. The user should appear in the Mapped Users tab. OpenSearch Dashboards configuration . Even though JWT URL parameter authentication works when querying OpenSearch directly, it fails when used to access OpenSearch Dashboards. Solution: Ensure the following lines are present in the OpenSearch Dashboards config file opensearch_dashboards.yml . opensearch_security.auth.type: \"jwt\" opensearch_security.jwt.url_param: &lt;your-param-name-here&gt; . ",
    "url": "https://vagimeli.github.io/security/configuration/configuration/#troubleshooting-common-issues",
    "relUrl": "/security/configuration/configuration/#troubleshooting-common-issues"
  },"667": {
    "doc": "Disabling security",
    "title": "Disabling security",
    "content": "You might want to temporarily disable the Security plugin to make testing or internal usage more straightforward. To disable the plugin, add the following line in opensearch.yml: . plugins.security.disabled: true . A more permanent option is to remove the Security plugin entirely: . | Delete the plugins/opensearch-security folder on all nodes. | Delete all plugins.security.* configuration entries from opensearch.yml. | . To perform these steps on the Docker image, see Working with plugins. Disabling or removing the plugin exposes the configuration index for the Security plugin. If the index contains sensitive information, be sure to protect it through some other means. If you no longer need the index, delete it. ",
    "url": "https://vagimeli.github.io/security/configuration/disable/",
    "relUrl": "/security/configuration/disable/"
  },"668": {
    "doc": "Disabling security",
    "title": "Remove OpenSearch Dashboards plugin",
    "content": "The Security plugin is actually two plugins: one for OpenSearch and one for OpenSearch Dashboards. You can use the OpenSearch plugin independently, but the OpenSearch Dashboards plugin depends on a secured OpenSearch cluster. If you disable the Security plugin in opensearch.yml (or delete the plugin entirely) and still want to use OpenSearch Dashboards, you must remove the corresponding OpenSearch Dashboards plugin. For more information, see OpenSearch Dashboards remove plugins. Docker . | Create a new Dockerfile: . FROM opensearchproject/opensearch-dashboards:2.7.0 RUN /usr/share/opensearch-dashboards/bin/opensearch-dashboards-plugin remove securityDashboards COPY --chown=opensearch-dashboards:opensearch-dashboards opensearch_dashboards.yml /usr/share/opensearch-dashboards/config/ . In this case, opensearch_dashboards.yml is a “vanilla” version of the file with no entries for the Security plugin. It might look like this: . --- server.name: opensearch-dashboards server.host: \"0.0.0.0\" opensearch.hosts: http://localhost:9200 . | To build the new Docker image, run the following command: . docker build --tag=opensearch-dashboards-no-security . | In docker-compose.yml, change opensearchproject/opensearch-dashboards:2.7.0 to opensearch-dashboards-no-security. | Change OPENSEARCH_HOSTS or opensearch.hosts to http:// rather than https://. | Enter docker-compose up. | . ",
    "url": "https://vagimeli.github.io/security/configuration/disable/#remove-opensearch-dashboards-plugin",
    "relUrl": "/security/configuration/disable/#remove-opensearch-dashboards-plugin"
  },"669": {
    "doc": "Generating self-signed certificates",
    "title": "Generating self-signed certificates",
    "content": "If you don’t have access to a certificate authority (CA) for your organization and want to use OpenSearch for non-demo purposes, you can generate your own self-signed certificates using OpenSSL. You can probably find OpenSSL in the package manager for your operating system. On CentOS, use Yum: . sudo yum install openssl . On macOS, use Homebrew: . brew install openssl . ",
    "url": "https://vagimeli.github.io/security/configuration/generate-certificates/",
    "relUrl": "/security/configuration/generate-certificates/"
  },"670": {
    "doc": "Generating self-signed certificates",
    "title": "Generate a private key",
    "content": "The first step in this process is to generate a private key using the openssl genrsa command. As the name suggests, you should keep this file private. Private keys must be of sufficient length to be secure, so specify 2048: . openssl genrsa -out root-ca-key.pem 2048 . You can optionally add the -aes256 option to encrypt the key using the AES-256 standard. This option requires a password. ",
    "url": "https://vagimeli.github.io/security/configuration/generate-certificates/#generate-a-private-key",
    "relUrl": "/security/configuration/generate-certificates/#generate-a-private-key"
  },"671": {
    "doc": "Generating self-signed certificates",
    "title": "Generate a root certificate",
    "content": "Next, use the private key to generate a self-signed certificate for the root CA: . openssl req -new -x509 -sha256 -key root-ca-key.pem -out root-ca.pem -days 730 . The default -days value of 30 is only useful for testing purposes. This sample command specifies 730 (two years) for the certificate expiration date, but use whatever value makes sense for your organization. | The -x509 option specifies that you want a self-signed certificate rather than a certificate request. | The -sha256 option sets the hash algorithm to SHA-256. SHA-256 is the default in later versions of OpenSSL, but earlier versions might use SHA-1. | . Follow the prompts to specify details for your organization. Together, these details form the distinguished name (DN) of your CA. ",
    "url": "https://vagimeli.github.io/security/configuration/generate-certificates/#generate-a-root-certificate",
    "relUrl": "/security/configuration/generate-certificates/#generate-a-root-certificate"
  },"672": {
    "doc": "Generating self-signed certificates",
    "title": "Generate an admin certificate",
    "content": "To generate an admin certificate, first create a new key: . openssl genrsa -out admin-key-temp.pem 2048 . Then convert that key to PKCS#8 format for use in Java using a PKCS#12-compatible algorithm (3DES): . openssl pkcs8 -inform PEM -outform PEM -in admin-key-temp.pem -topk8 -nocrypt -v1 PBE-SHA1-3DES -out admin-key.pem . Next, create a certificate signing request (CSR). This file acts as an application to a CA for a signed certificate: . openssl req -new -key admin-key.pem -out admin.csr . Follow the prompts to fill in the details. You don’t need to specify a challenge password. As noted in the OpenSSL Cookbook, “Having a challenge password does not increase the security of the CSR in any way.” . If you generate TLS certificates and have enabled hostname verification by setting plugins.security.ssl.transport.enforce_hostname_verification to true (default), be sure to specify a common name (CN) for each certificate signing request (CSR) that matches the corresponding DNS A record of the intended node. If you want to use the same node certificate on all nodes (not recommended), set hostname verification to false. For more information, see Configure TLS certificates. Now that the private key and signing request have been created, generate the certificate: . openssl x509 -req -in admin.csr -CA root-ca.pem -CAkey root-ca-key.pem -CAcreateserial -sha256 -out admin.pem -days 730 . Just like the root certificate, use the -days option to specify an expiration date of longer than 30 days. ",
    "url": "https://vagimeli.github.io/security/configuration/generate-certificates/#generate-an-admin-certificate",
    "relUrl": "/security/configuration/generate-certificates/#generate-an-admin-certificate"
  },"673": {
    "doc": "Generating self-signed certificates",
    "title": "(Optional) Generate node and client certificates",
    "content": "Similar to the steps in Generate an admin certificate, you will generate keys and CSRs with new file names for each node and as many client certificates as you need. For example, you might generate one client certificate for OpenSearch Dashboards and another for a Python client. Each certificate should use its own private key and should be generated from a unique CSR with matching SAN extension specific to the intended host. A SAN extension is not needed for the admin cert because that cert is not tied to a specific host. To generate a node or client certificate, first create a new key: . openssl genrsa -out node1-key-temp.pem 2048 . Then convert that key to PKCS#8 format for use in Java using a PKCS#12-compatible algorithm (3DES): . openssl pkcs8 -inform PEM -outform PEM -in node1-key-temp.pem -topk8 -nocrypt -v1 PBE-SHA1-3DES -out node1-key.pem . Next, create the CSR: . openssl req -new -key node1-key.pem -out node1.csr . For all host and client certificates, you should specify a subject alternative name (SAN) to ensure compliance with RFC 2818 (HTTP Over TLS). The SAN should match the corresponding CN so that both refer to the same DNS A record. Before generating a signed certificate, create a SAN extension file which describes the DNS A record for the host: . echo 'subjectAltName=DNS:node1.dns.a-record' &gt; node1.ext . Generate the certificate: . openssl x509 -req -in node1.csr -CA root-ca.pem -CAkey root-ca-key.pem -CAcreateserial -sha256 -out node1.pem -days 730 -extfile node1.ext . ",
    "url": "https://vagimeli.github.io/security/configuration/generate-certificates/#optional-generate-node-and-client-certificates",
    "relUrl": "/security/configuration/generate-certificates/#optional-generate-node-and-client-certificates"
  },"674": {
    "doc": "Generating self-signed certificates",
    "title": "Sample script",
    "content": "If you already know the certificate details and don’t want to specify them interactively, use the -subj option in your root-ca.pem and CSR commands. This script creates a root certificate, admin certificate, two node certificates, and a client certificate, all with an expiration dates of two years (730 days): . #!/bin/sh # Root CA openssl genrsa -out root-ca-key.pem 2048 openssl req -new -x509 -sha256 -key root-ca-key.pem -subj \"/C=CA/ST=ONTARIO/L=TORONTO/O=ORG/OU=UNIT/CN=root.dns.a-record\" -out root-ca.pem -days 730 # Admin cert openssl genrsa -out admin-key-temp.pem 2048 openssl pkcs8 -inform PEM -outform PEM -in admin-key-temp.pem -topk8 -nocrypt -v1 PBE-SHA1-3DES -out admin-key.pem openssl req -new -key admin-key.pem -subj \"/C=CA/ST=ONTARIO/L=TORONTO/O=ORG/OU=UNIT/CN=A\" -out admin.csr openssl x509 -req -in admin.csr -CA root-ca.pem -CAkey root-ca-key.pem -CAcreateserial -sha256 -out admin.pem -days 730 # Node cert 1 openssl genrsa -out node1-key-temp.pem 2048 openssl pkcs8 -inform PEM -outform PEM -in node1-key-temp.pem -topk8 -nocrypt -v1 PBE-SHA1-3DES -out node1-key.pem openssl req -new -key node1-key.pem -subj \"/C=CA/ST=ONTARIO/L=TORONTO/O=ORG/OU=UNIT/CN=node1.dns.a-record\" -out node1.csr echo 'subjectAltName=DNS:node1.dns.a-record' &gt; node1.ext openssl x509 -req -in node1.csr -CA root-ca.pem -CAkey root-ca-key.pem -CAcreateserial -sha256 -out node1.pem -days 730 -extfile node1.ext # Node cert 2 openssl genrsa -out node2-key-temp.pem 2048 openssl pkcs8 -inform PEM -outform PEM -in node2-key-temp.pem -topk8 -nocrypt -v1 PBE-SHA1-3DES -out node2-key.pem openssl req -new -key node2-key.pem -subj \"/C=CA/ST=ONTARIO/L=TORONTO/O=ORG/OU=UNIT/CN=node2.dns.a-record\" -out node2.csr echo 'subjectAltName=DNS:node2.dns.a-record' &gt; node2.ext openssl x509 -req -in node2.csr -CA root-ca.pem -CAkey root-ca-key.pem -CAcreateserial -sha256 -out node2.pem -days 730 -extfile node2.ext # Client cert openssl genrsa -out client-key-temp.pem 2048 openssl pkcs8 -inform PEM -outform PEM -in client-key-temp.pem -topk8 -nocrypt -v1 PBE-SHA1-3DES -out client-key.pem openssl req -new -key client-key.pem -subj \"/C=CA/ST=ONTARIO/L=TORONTO/O=ORG/OU=UNIT/CN=client.dns.a-record\" -out client.csr echo 'subjectAltName=DNS:client.dns.a-record' &gt; client.ext openssl x509 -req -in client.csr -CA root-ca.pem -CAkey root-ca-key.pem -CAcreateserial -sha256 -out client.pem -days 730 -extfile client.ext # Cleanup rm admin-key-temp.pem rm admin.csr rm node1-key-temp.pem rm node1.csr rm node1.ext rm node2-key-temp.pem rm node2.csr rm node2.ext rm client-key-temp.pem rm client.csr rm client.ext . ",
    "url": "https://vagimeli.github.io/security/configuration/generate-certificates/#sample-script",
    "relUrl": "/security/configuration/generate-certificates/#sample-script"
  },"675": {
    "doc": "Generating self-signed certificates",
    "title": "Add distinguished names to opensearch.yml",
    "content": "You must specify the distinguished names (DNs) for all admin and node certificates in opensearch.yml on all nodes. Using the certificates from the sample script above, part of opensearch.yml might look like this: . plugins.security.authcz.admin_dn: - 'CN=A,OU=UNIT,O=ORG,L=TORONTO,ST=ONTARIO,C=CA' plugins.security.nodes_dn: - 'CN=node1.dns.a-record,OU=UNIT,O=ORG,L=TORONTO,ST=ONTARIO,C=CA' - 'CN=node2.dns.a-record,OU=UNIT,O=ORG,L=TORONTO,ST=ONTARIO,C=CA' . But if you look at the subject of the certificate after creating it, you might see different formatting: . subject=/C=CA/ST=ONTARIO/L=TORONTO/O=ORG/OU=UNIT/CN=node1.dns.a-record . If you compare this string to the ones above, you can see that you need to invert the order of elements and use commas rather than slashes. Enter this command to get the correct string: . openssl x509 -subject -nameopt RFC2253 -noout -in node.pem . Then copy and paste the output into opensearch.yml. ",
    "url": "https://vagimeli.github.io/security/configuration/generate-certificates/#add-distinguished-names-to-opensearchyml",
    "relUrl": "/security/configuration/generate-certificates/#add-distinguished-names-to-opensearchyml"
  },"676": {
    "doc": "Generating self-signed certificates",
    "title": "Add certificate files to opensearch.yml",
    "content": "This process generates many files, but these are the ones you need to add to each node: . | root-ca.pem | admin.pem | admin-key.pem | (Optional) node1.pem | (Optional) node1-key.pem | . On one node, the security configuration portion of opensearch.yml might look like this: . plugins.security.ssl.transport.pemcert_filepath: node1.pem plugins.security.ssl.transport.pemkey_filepath: node1-key.pem plugins.security.ssl.transport.pemtrustedcas_filepath: root-ca.pem plugins.security.ssl.transport.enforce_hostname_verification: false plugins.security.ssl.http.enabled: true plugins.security.ssl.http.pemcert_filepath: node1.pem plugins.security.ssl.http.pemkey_filepath: node1-key.pem plugins.security.ssl.http.pemtrustedcas_filepath: root-ca.pem plugins.security.authcz.admin_dn: - 'CN=A,OU=UNIT,O=ORG,L=TORONTO,ST=ONTARIO,C=CA' plugins.security.nodes_dn: - 'CN=node1.dns.a-record,OU=UNIT,O=ORG,L=TORONTO,ST=ONTARIO,C=CA' - 'CN=node2.dns.a-record,OU=UNIT,O=ORG,L=TORONTO,ST=ONTARIO,C=CA' . For more information about adding and using these certificates in your own setup, see Configuring basic security settings for Docker, Configure TLS certificates, and Client certificate authentication. ",
    "url": "https://vagimeli.github.io/security/configuration/generate-certificates/#add-certificate-files-to-opensearchyml",
    "relUrl": "/security/configuration/generate-certificates/#add-certificate-files-to-opensearchyml"
  },"677": {
    "doc": "Generating self-signed certificates",
    "title": "Run securityadmin.sh",
    "content": "After configuring your certificates and starting OpenSearch, run securityadmin.sh to initialize the Security plugin. For information about how to use this script, see Applying changes to configuration files. ",
    "url": "https://vagimeli.github.io/security/configuration/generate-certificates/#run-securityadminsh",
    "relUrl": "/security/configuration/generate-certificates/#run-securityadminsh"
  },"678": {
    "doc": "Generating self-signed certificates",
    "title": "OpenSearch Dashboards",
    "content": "For information on using your root CA and a client certificate to enable TLS for OpenSearch Dashboards, see Configure TLS for OpenSearch Dashboards. ",
    "url": "https://vagimeli.github.io/security/configuration/generate-certificates/#opensearch-dashboards",
    "relUrl": "/security/configuration/generate-certificates/#opensearch-dashboards"
  },"679": {
    "doc": "Configuration",
    "title": "Security configuration",
    "content": "The plugin includes demo certificates so that you can get up and running quickly. To use OpenSearch in a production environment, you must configure it manually: . | Replace the demo certificates. | Reconfigure opensearch.yml to use your certificates. | Reconfigure config.yml to use your authentication backend (if you don’t plan to use the internal user database). | Modify the configuration YAML files. | If you plan to use the internal user database, set a password policy in opensearch.yml. | Apply changes using the securityadmin script. | Start OpenSearch. | Add users, roles, role mappings, and tenants. | . If you don’t want to use the plugin, see Disable security. The Security plugin has several default users, roles, action groups, permissions, and settings for OpenSearch Dashboards that use kibana in their names. We will change these names in a future release. ",
    "url": "https://vagimeli.github.io/security/configuration/index/#security-configuration",
    "relUrl": "/security/configuration/index/#security-configuration"
  },"680": {
    "doc": "Configuration",
    "title": "Configuration",
    "content": " ",
    "url": "https://vagimeli.github.io/security/configuration/index/",
    "relUrl": "/security/configuration/index/"
  },"681": {
    "doc": "Configuring sign-in options",
    "title": "Configuring Dashboards sign-in for multiple authentication options",
    "content": "You can configure the sign-in window for OpenSearch Dashboards to provide either a single option for authenticating users at sign-in or multiple options. Currently, Dashboards supports basic authentication, OpenID Connect, and SAML as the multiple options. ",
    "url": "https://vagimeli.github.io/security/configuration/multi-auth/#configuring-dashboards-sign-in-for-multiple-authentication-options",
    "relUrl": "/security/configuration/multi-auth/#configuring-dashboards-sign-in-for-multiple-authentication-options"
  },"682": {
    "doc": "Configuring sign-in options",
    "title": "General steps for configuring multiple authentication options",
    "content": "Consider the following sequence of steps before configuring the sign-in window for multiple authentication options. | Decide which types of authentication to make available at sign-in. | Configure each authentication type, including an authentication domain for the identity provider (IdP) and the essential settings that give each type sign-in access to OpenSearch Dashboards. For OpenId Connect backend configuration, see OpenID Connect. For SAML backend configuration, see SAML. | Add, enable, and configure multiple option authentication settings in the opensearch_dashboards.yml file. | . ",
    "url": "https://vagimeli.github.io/security/configuration/multi-auth/#general-steps-for-configuring-multiple-authentication-options",
    "relUrl": "/security/configuration/multi-auth/#general-steps-for-configuring-multiple-authentication-options"
  },"683": {
    "doc": "Configuring sign-in options",
    "title": "Enabling multiple authentication options",
    "content": "By default, Dashboards provides basic authentication for sign-in. To enable multiple options for authentication, begin by adding opensearch_security.auth.multiple_auth_enabled to the opensearch_dashboards.yml file and setting it to true. To specify the multiple authentication types as options during sign-in, add the opensearch_security.auth.type setting to the opensearch_dashboards.yml file and enter multiple types as values. When more than one authentication type is added to the setting, the Dashboards sign-in window recognizes multiple types and adjusts to accommodate the sign-in options. When setting up Dashboards to provide multiple authentication options, basic authentication is always required as one of the values for the setting. Add a single value to the setting when only one authentication type is needed. opensearch_security.auth.type: \"openid\" . For multiple authentication options, add values to the setting as an array separated by commas. As a reminder, Dashboards currently supports a combination of basic authentication, OpenID Connect, and SAML as a valid set of values. In the setting, these values are expressed as \"basicauth\", \"openid\", and \"saml\". opensearch_security.auth.type: [\"basicauth\",\"openid\"] opensearch_security.auth.multiple_auth_enabled: true . opensearch_security.auth.type: [\"basicauth\",\"saml\"] opensearch_security.auth.multiple_auth_enabled: true . opensearch_security.auth.type: [\"basicauth\",\"saml\",\"openid\"] opensearch_security.auth.multiple_auth_enabled: true . When the opensearch_security.auth.type setting contains basicauth and one other authentication type, the sign-in window appears as in the following example. With all three valid authentication types specified, the sign-in window appears as in the following example. ",
    "url": "https://vagimeli.github.io/security/configuration/multi-auth/#enabling-multiple-authentication-options",
    "relUrl": "/security/configuration/multi-auth/#enabling-multiple-authentication-options"
  },"684": {
    "doc": "Configuring sign-in options",
    "title": "Customizing the sign-in environment",
    "content": "In addition to the essential sign-in settings for each authentication type, you can configure additional settings in the opensearch_dashboards.yml file to customize the sign-in window so that it clearly represents the options that are available. For example, you can replace the label on the sign-in button with the name and icon of the IdP. Refer to the settings and descriptions that follow. Basic authentication settings . These settings allow you to customize the basic username and password sign-in button. | Setting | Description | . | opensearch_security.ui.basicauth.login.brandimage | Login button logo. Supported file types are SVG, PNG, and GIF. | . | opensearch_security.ui.basicauth.login.showbrandimage | Determines whether a logo for the login button is displayed or not. Default is true. | . OpenID Connect authentication settings . These settings allow you to customize the sign-in button associated with OpenID Connect authentication. For the essential settings required to use OpenID Connect as a single sign-in option, see OpenSearch Dashboards single sign-on. | Setting | Description | . | opensearch_security.ui.openid.login.buttonname | Display name for the login button. “Log in with single sign-on” by default. | . | opensearch_security.ui.openid.login.brandimage | Login button logo. Supported file types are SVG, PNG, and GIF. | . | opensearch_security.ui.openid.login.showbrandimage | Determines whether a logo for the login button is displayed or not. Default is false. | . SAML authentication settings . These settings allow you to customize the sign-in button associated with SAML authentication. For the essential settings required to use SAML as a sign-in option, see OpenSearch Dashboards configuration. | Setting | Description | . | opensearch_security.ui.saml.login.buttonname | Display name for the login button. “Log in with single sign-on” by default. | . | opensearch_security.ui.saml.login.brandimage | Login button logo. Supported file types are SVG, PNG, and GIF. | . | opensearch_security.ui.saml.login.showbrandimage | Determines whether a logo for the login button is displayed or not. Default is false. | . ",
    "url": "https://vagimeli.github.io/security/configuration/multi-auth/#customizing-the-sign-in-environment",
    "relUrl": "/security/configuration/multi-auth/#customizing-the-sign-in-environment"
  },"685": {
    "doc": "Configuring sign-in options",
    "title": "Sample setup",
    "content": "The following example shows basic settings in the opensearch_dashboards.yml file when it is configured for two types of authentication at sign-in. # The several settings directly below are typical of all `opensearch_dashboards.yml` configurations. # server.host: 0.0.0.0 server.port: 5601 opensearch.hosts: [\"https://localhost:9200\"] opensearch.ssl.verificationMode: none opensearch.username: &lt;preferred username&gt; opensearch.password: &lt;preferred password&gt; opensearch.requestHeadersAllowlist: [\"securitytenant\",\"Authorization\"] opensearch_security.multitenancy.enabled: true opensearch_security.multitenancy.tenants.preferred: [\"Private\", \"Global\"] opensearch_security.readonly_mode.roles: [\"&lt;role_for_read_only&gt;\"] # Settings that enable multiple option authentication in the sign-in window # opensearch_security.auth.multiple_auth_enabled: true opensearch_security.auth.type: [\"basicauth\",\"openid\"] # Basic authentication customization # opensearch_security.ui.basicauth.login.brandimage: &lt;path/to/OSlogo.png&gt; opensearch_security.ui.basicauth.login.showbrandimage: true # OIDC auth customization and start settings # opensearch_security.ui.openid.login.buttonname: Log in with &lt;IdP name or other&gt; opensearch_security.ui.openid.login.brandimage: &lt;path/to/brand-logo.png&gt; opensearch_security.ui.openid.login.showbrandimage: true opensearch_security.openid.base_redirect_url: &lt;\"OIDC redirect URL\"&gt; opensearch_security.openid.verify_hostnames: false opensearch_security.openid.refresh_tokens: false opensearch_security.openid.logout_url: &lt;\"OIDC logout URL\"&gt; opensearch_security.openid.connect_url: &lt;\"OIDC connect URL\"&gt; opensearch_security.openid.client_id: &lt;Client ID&gt; opensearch_security.openid.client_secret: &lt;Client secret&gt; . ",
    "url": "https://vagimeli.github.io/security/configuration/multi-auth/#sample-setup",
    "relUrl": "/security/configuration/multi-auth/#sample-setup"
  },"686": {
    "doc": "Configuring sign-in options",
    "title": "Configuring sign-in options",
    "content": " ",
    "url": "https://vagimeli.github.io/security/configuration/multi-auth/",
    "relUrl": "/security/configuration/multi-auth/"
  },"687": {
    "doc": "Applying changes to configuration files",
    "title": "Applying changes to configuration files",
    "content": "On Windows, use securityadmin.bat in place of securityadmin.sh. For more information, see Windows usage. The Security plugin stores its configuration—including users, roles, permissions, and backend settings—in a system index on the OpenSearch cluster. Storing these settings in an index lets you change settings without restarting the cluster and eliminates the need to edit configuration files on every individual node. This is accomplished by running the securityadmin.sh script. The first job of the script is to initialize the .opendistro_security index. This loads your initial configuration into the index using the configuration files in /config/opensearch-security. After the .opendistro_security index is initialized, you can use OpenSearch Dashboards or the REST API to manage your users, roles, and permissions. The script can be found at /plugins/opensearch-security/tools/securityadmin.sh. This is a relative path showing where the securityadmin.sh script is located. The absolute path depends on the directory where you’ve installed OpenSearch. For example, if you use Docker to install OpenSearch, the path will resemble the following: /usr/share/opensearch/plugins/opensearch-security/tools/securityadmin.sh. ",
    "url": "https://vagimeli.github.io/security/configuration/security-admin/",
    "relUrl": "/security/configuration/security-admin/"
  },"688": {
    "doc": "Applying changes to configuration files",
    "title": "A word of caution",
    "content": "If you make changes to the configuration files in config/opensearch-security, OpenSearch does not automatically apply these changes. Instead, you must run securityadmin.sh to load the updated files into the index. Running securityadmin.sh overwrites one or more portions of the .opendistro_security index. Run it with extreme care to avoid losing your existing resources. Consider the following example: . | You initialize the .opendistro_security index. | You create ten users using the REST API. | You decide to create a new reserved user using internal_users.yml. | You run securityadmin.sh again to load the new reserved user into the index. | You lose all ten users that you created using the REST API. | . To avoid this situation, back up your current configuration before making changes and re-running the script: ./securityadmin.sh -backup my-backup-directory \\ -icl \\ -nhnv \\ -cacert ../../../config/root-ca.pem \\ -cert ../../../config/kirk.pem \\ -key ../../../config/kirk-key.pem . If you use the -f argument rather than -cd, you can load a single YAML file into the index rather than the entire directory of YAML files. For example, if you create ten new roles, you can safely load internal_users.yml into the index without losing your roles; only the internal users get overwritten./securityadmin.sh -f ../../../config/opensearch-security/internal_users.yml \\ -t internalusers \\ -icl \\ -nhnv \\ -cacert ../../../config/root-ca.pem \\ -cert ../../../config/kirk.pem \\ -key ../../../config/kirk-key.pem . To resolve all environment variables before applying the security configurations, use the -rev parameter./securityadmin.sh -cd ../../../config/opensearch-security/ \\ -rev \\ -cacert ../../../root-ca.pem \\ -cert ../../../kirk.pem \\ -key ../../../kirk.key.pem . The following example shows an environment variable in the config.yml file: . password: ${env.LDAP_PASSWORD} . ",
    "url": "https://vagimeli.github.io/security/configuration/security-admin/#a-word-of-caution",
    "relUrl": "/security/configuration/security-admin/#a-word-of-caution"
  },"689": {
    "doc": "Applying changes to configuration files",
    "title": "Configure the admin certificate",
    "content": "In order to use securityadmin.sh, you must add the distinguished names (DNs) of all admin certificates to opensearch.yml. If you use the demo certificates, for example, opensearch.yml might contain the following lines for the kirk certificate: . plugins.security.authcz.admin_dn: - CN=kirk,OU=client,O=client,L=test,C=DE . You can’t use node certificates as admin certificates. The two must be separate. Also, do not add whitespace between the parts of the DN. ",
    "url": "https://vagimeli.github.io/security/configuration/security-admin/#configure-the-admin-certificate",
    "relUrl": "/security/configuration/security-admin/#configure-the-admin-certificate"
  },"690": {
    "doc": "Applying changes to configuration files",
    "title": "Basic usage",
    "content": "The securityadmin.sh tool can be run from any machine that has access to the HTTP port of your OpenSearch cluster (the default port is 9200). You can change the Security plugin configuration without having to access your nodes through SSH. securityadmin.sh requires that SSL/TLS transport is enabled on your opensearch cluster. In other words, make sure that the plugins.security.ssl.http.enabled: true is set in opensearch.yml before proceeding. Each node also includes the tool at plugins/opensearch-security/tools/securityadmin.sh. You might need to make the script executable before running it: . chmod +x plugins/opensearch-security/tools/securityadmin.sh . To print all available command line options, run the script with no arguments: ./plugins/opensearch-security/tools/securityadmin.sh . To load your initial configuration (all YAML files), you might use the following command: ./securityadmin.sh -cd ../../../config/opensearch-security/ -icl -nhnv \\ -cacert ../../../config/root-ca.pem \\ -cert ../../../config/kirk.pem \\ -key ../../../config/kirk-key.pem . | The -cd option specifies where the Security plugin configuration files can be found. | The -icl (--ignore-clustername) option tells the Security plugin to upload the configuration regardless of the cluster name. As an alternative, you can also specify the cluster name with the -cn (--clustername) option. | Because the demo certificates are self-signed, this command disables hostname verification with the -nhnv (--disable-host-name-verification) option. | The -cacert, -cert and -key options define the location of your root CA certificate, the admin certificate, and the private key for the admin certificate. If the private key has a password, specify it with the -keypass option. | . The following table shows the PEM options. | Name | Description | . | -cert | The location of the PEM file containing the admin certificate and all intermediate certificates, if any. You can use an absolute or relative path. Relative paths are resolved relative to the execution directory of securityadmin.sh. | . | -key | The location of the PEM file containing the private key of the admin certificate. You can use an absolute or relative path. Relative paths are resolved relative to the execution directory of securityadmin.sh. The key must be in PKCS#8 format. | . | -keypass | The password of the private key of the admin certificate, if any. | . | -cacert | The location of the PEM file containing the root certificate. You can use an absolute or relative path. Relative paths are resolved relative to the execution directory of securityadmin.sh. | . ",
    "url": "https://vagimeli.github.io/security/configuration/security-admin/#basic-usage",
    "relUrl": "/security/configuration/security-admin/#basic-usage"
  },"691": {
    "doc": "Applying changes to configuration files",
    "title": "Sample commands",
    "content": "Apply all YAML files in config/opensearch-security/ using PEM certificates: . /usr/share/opensearch/plugins/opensearch-security/tools/securityadmin.sh \\ -cacert /etc/opensearch/root-ca.pem \\ -cert /etc/opensearch/kirk.pem \\ -key /etc/opensearch/kirk-key.pem \\ -cd /usr/share/opensearch/config/opensearch-security/ . Apply a single YAML file (config.yml) using PEM certificates: ./securityadmin.sh \\ -f ../../../config/opensearch-security/config.yml \\ -icl -nhnv -cert /etc/opensearch/kirk.pem \\ -cacert /etc/opensearch/root-ca.pem \\ -key /etc/opensearch/kirk-key.pem \\ -t config . Apply all YAML files in config/opensearch-security/ with keystore and truststore files: ./securityadmin.sh \\ -cd /usr/share/opensearch/config/opensearch-security/ \\ -ks /path/to/keystore.jks \\ -kspass changeit \\ -ts /path/to/truststore.jks \\ -tspass changeit -nhnv -icl . ",
    "url": "https://vagimeli.github.io/security/configuration/security-admin/#sample-commands",
    "relUrl": "/security/configuration/security-admin/#sample-commands"
  },"692": {
    "doc": "Applying changes to configuration files",
    "title": "Using securityadmin with keystore and truststore files",
    "content": "You can also use keystore files in JKS format in conjunction with securityadmin.sh: ./securityadmin.sh -cd ../../../config/opensearch-security -icl -nhnv -ts &lt;path/to/truststore&gt; -tspass &lt;truststore password&gt; -ks &lt;path/to/keystore&gt; -kspass &lt;keystore password&gt; . Use the following options to control the key and truststore settings. | Name | Description | . | -ks | The location of the keystore containing the admin certificate and all intermediate certificates, if any. You can use an absolute or relative path. Relative paths are resolved relative to the execution directory of securityadmin.sh. | . | -kspass | The password for the keystore. | . | -kst | The key store type, either JKS or PKCS#12/PFX. If not specified, the Security plugin tries to determine the type from the file extension. | . | -ksalias | The alias of the admin certificate, if any. | . | -ts | The location of the truststore containing the root certificate. You can use an absolute or relative path. Relative paths are resolved relative to the execution directory of securityadmin.sh. | . | -tspass | The password for the truststore. | . | -tst | The truststore type, either JKS or PKCS#12/PFX. If not specified, the Security plugin tries to determine the type from the file extension. | . | -tsalias | The alias for the root certificate, if any. | . OpenSearch settings . If you run a default OpenSearch installation, which listens on port 9200 and uses opensearch as a cluster name, you can omit the following settings altogether. Otherwise, specify your OpenSearch settings by using the following switches. | Name | Description | . | -h | OpenSearch hostname. Default is localhost. | . | -p | OpenSearch port. Default is 9200 - not the HTTP port. | . | -cn | Cluster name. Default is opensearch. | . | -icl | Ignore cluster name. | . | -sniff | Sniff cluster nodes. Sniffing detects available nodes using the OpenSearch _cluster/state API. | . | -arc,--accept-red-cluster | Execute securityadmin.sh even if the cluster state is red. Default is false, which means the script will not execute on a red cluster. | . Certificate validation settings . Use the following options to control certificate validation. | Name | Description | . | -nhnv | Do not validate hostname. Default is false. | . | -nrhn | Do not resolve hostname. Only relevant if -nhnv is not set. | . | -noopenssl | Do not use OpenSSL, even if available. Default is to use OpenSSL if it is available. | . Configuration files settings . The following switches define which configuration files you want to push to the Security plugin. You can either push a single file or specify a directory containing one or more configuration files. | Name | Description | . | -cd | Directory containing multiple Security plugin configuration files. | . | -f | Single configuration file. Can’t be used with -cd. | . | -t | File type. | . | -rl | Reload the current configuration and flush the internal cache. | . To upload all configuration files in a directory, use this: ./securityadmin.sh -cd ../../../config/opensearch-security -ts ... -tspass ... -ks ... -kspass ... If you want to push a single configuration file, use this: ./securityadmin.sh -f ../../../config/opensearch-security/internal_users.yml -t internalusers \\ -ts ... -tspass ... -ks ... -kspass ... The file type must be one of the following: . | config | roles | rolesmapping | internalusers | actiongroups | . Cipher settings . You probably won’t need to change cipher settings. If you need to, use the following options. | Name | Description | . | -ec | Comma-separated list of enabled TLS ciphers. | . | -ep | Comma-separated list of enabled TLS protocols. | . Backup, restore, and migrate . You can download all current configuration files from your cluster with the following command: ./securityadmin.sh -backup my-backup-directory -ts ... -tspass ... -ks ... -kspass ... This command dumps the current Security plugin configuration from your cluster to individual files in the directory you specify. You can then use these files as backups or to load the configuration into a different cluster. This command is useful when moving a proof-of-concept to production or if you need to add additional reserved or hidden resources: ./securityadmin.sh \\ -backup my-backup-directory \\ -icl \\ -nhnv \\ -cacert ../../../config/root-ca.pem \\ -cert ../../../config/kirk.pem \\ -key ../../../config/kirk-key.pem . To upload the dumped files to another cluster: ./securityadmin.sh -h production.example.com -p 9301 -cd /etc/backup/ -ts ... -tspass ... -ks ... -kspass ... To migrate configuration YAML files from the Open Distro for Elasticsearch 0.x.x format to the OpenSearch 1.x.x format: ./securityadmin.sh -migrate ../../../config/opensearch-security -ts ... -tspass ... -ks ... -kspass ... | Name | Description | . | -backup | Retrieve the current Security plugin configuration from a running cluster and dump it to the working directory. | . | -migrate | Migrate configuration YAML files from Open Distro for Elasticsearch 0.x.x to OpenSearch 1.x.x. | . Other options . | Name | Description | . | -dci | Delete the Security plugin configuration index and exit. This option is useful if the cluster state is red due to a corrupted Security plugin index. | . | -esa | Enable shard allocation and exit. This option is useful if you disabled shard allocation while performing a full cluster restart and need to recreate the Security plugin index. | . | -w | Displays information about the used admin certificate. | . | -rl | By default, the Security plugin caches authenticated users, along with their roles and permissions, for one hour. This option reloads the current Security plugin configuration stored in your cluster, invalidating any cached users, roles, and permissions. | . | -i | The Security plugin index name. Default is .opendistro_security. | . | -er | Set explicit number of replicas or auto-expand expression for the opensearch_security index. | . | -era | Enable replica auto-expand. | . | -dra | Disable replica auto-expand. | . | -us | Update the replica settings. | . ",
    "url": "https://vagimeli.github.io/security/configuration/security-admin/#using-securityadmin-with-keystore-and-truststore-files",
    "relUrl": "/security/configuration/security-admin/#using-securityadmin-with-keystore-and-truststore-files"
  },"693": {
    "doc": "Applying changes to configuration files",
    "title": "Windows usage",
    "content": "On Windows, the equivalent of securityadmin.sh is the securityadmin.bat script located in the \\path\\to\\opensearch-2.7.0\\plugins\\opensearch-security\\tools\\ directory. When running the example commands in the preceding sections, use the command prompt or Powershell. Open the command prompt by entering cmd or Powershell by entering powershell in the search box next to Start on the taskbar. For example, to print all available command line options, run the script with no arguments: .\\plugins\\opensearch-security\\tools\\securityadmin.bat . When entering a multiline command, use the caret (^) character to escape the next character in the command line. For example, to load your initial configuration (all YAML files), use the following command: .\\securityadmin.bat -cd ..\\..\\..\\config\\opensearch-security\\ -icl -nhnv ^ -cacert ..\\..\\..\\config\\root-ca.pem ^ -cert ..\\..\\..\\config\\kirk.pem ^ -key ..\\..\\..\\config\\kirk-key.pem . ",
    "url": "https://vagimeli.github.io/security/configuration/security-admin/#windows-usage",
    "relUrl": "/security/configuration/security-admin/#windows-usage"
  },"694": {
    "doc": "System indexes",
    "title": "System indexes",
    "content": "By default, OpenSearch has a protected system index, .opendistro_security, which is used to store the Security configuration YAML files. You create this index using securityadmin.sh. Even with a user account that has read permissions for all indexes, you can’t directly access the data in this system index. Instead, you first need to authenticate with an admin certificate to gain access: . curl -k --cert ./kirk.pem --key ./kirk-key.pem -XGET 'https://localhost:9200/.opendistro_security/_search' . When Security is installed, the demo configuration automatically creates the .opendistro_security system index. It also adds several other indexes for the various OpenSearch plugins that integrate with the Security plugin: . plugins.security.system_indices.enabled: true plugins.security.system_indices.indices: [\".opendistro-alerting-config\", \".opendistro-alerting-alert*\", \".opendistro-anomaly-results*\", \".opendistro-anomaly-detector*\", \".opendistro-anomaly-checkpoints\", \".opendistro-anomaly-detection-state\", \".opendistro-reports-*\", \".opendistro-notifications-*\", \".opendistro-notebooks\", \".opendistro-asynchronous-search-response*\"] . You can add additional system indexes in opensearch.yml. An alternative way to remove a system index is to delete it from the plugins.security.system_indices.indices list on each node and restart OpenSearch. ",
    "url": "https://vagimeli.github.io/security/configuration/system-indices/",
    "relUrl": "/security/configuration/system-indices/"
  },"695": {
    "doc": "Configuring TLS certificates",
    "title": "Configuring TLS certificates",
    "content": "TLS is configured in opensearch.yml. Certificates are used to secure transport-layer traffic (node-to-node communication within your cluster) and REST-layer traffic (communication between a client and a node within your cluster). TLS is optional for the REST layer and mandatory for the transport layer. You can find an example configuration template with all options on GitHub. ",
    "url": "https://vagimeli.github.io/security/configuration/tls/",
    "relUrl": "/security/configuration/tls/"
  },"696": {
    "doc": "Configuring TLS certificates",
    "title": "X.509 PEM certificates and PKCS #8 keys",
    "content": "The following tables contain the settings you can use to configure the location of your PEM certificates and private keys. Transport layer TLS . | Name | Description | . | plugins.security.ssl.transport.pemkey_filepath | Path to the certificate’s key file (PKCS #8), which must be under the config directory, specified using a relative path. Required. | . | plugins.security.ssl.transport.pemkey_password | Key password. Omit this setting if the key has no password. Optional. | . | plugins.security.ssl.transport.pemcert_filepath | Path to the X.509 node certificate chain (PEM format), which must be under the config directory, specified using a relative path. Required. | . | plugins.security.ssl.transport.pemtrustedcas_filepath | Path to the root CAs (PEM format), which must be under the config directory, specified using a relative path. Required. | . REST layer TLS . | Name | Description | . | plugins.security.ssl.http.pemkey_filepath | Path to the certificate’s key file (PKCS #8), which must be under the config directory, specified using a relative path. Required. | . | plugins.security.ssl.http.pemkey_password | Key password. Omit this setting if the key has no password. Optional. | . | plugins.security.ssl.http.pemcert_filepath | Path to the X.509 node certificate chain (PEM format), which must be under the config directory, specified using a relative path. Required. | . | plugins.security.ssl.http.pemtrustedcas_filepath | Path to the root CAs (PEM format), which must be under the config directory, specified using a relative path. Required. | . ",
    "url": "https://vagimeli.github.io/security/configuration/tls/#x509-pem-certificates-and-pkcs-8-keys",
    "relUrl": "/security/configuration/tls/#x509-pem-certificates-and-pkcs-8-keys"
  },"697": {
    "doc": "Configuring TLS certificates",
    "title": "Keystore and truststore files",
    "content": "As an alternative to certificates and private keys in PEM format, you can instead use keystore and truststore files in JKS or PKCS12/PFX format. For the Security plugin to operate, you need certificates and private keys. The following settings configure the location and password of your keystore and truststore files. If you want, you can use different keystore and truststore files for the REST and the transport layer. Transport layer TLS . | Name | Description | . | plugins.security.ssl.transport.keystore_type | The type of the keystore file, JKS or PKCS12/PFX. Optional. Default is JKS. | . | plugins.security.ssl.transport.keystore_filepath | Path to the keystore file, which must be under the config directory, specified using a relative path. Required. | . | plugins.security.ssl.transport.keystore_alias: my_alias | Alias name. Optional. Default is the first alias. | . | plugins.security.ssl.transport.keystore_password | Keystore password. Default is changeit. | . | plugins.security.ssl.transport.truststore_type | The type of the truststore file, JKS or PKCS12/PFX. Default is JKS. | . | plugins.security.ssl.transport.truststore_filepath | Path to the truststore file, which must be under the config directory, specified using a relative path. Required. | . | plugins.security.ssl.transport.truststore_alias | Alias name. Optional. Default is all certificates. | . | plugins.security.ssl.transport.truststore_password | Truststore password. Default is changeit. | . REST layer TLS . | Name | Description | . | plugins.security.ssl.http.enabled | Whether to enable TLS on the REST layer. If enabled, only HTTPS is allowed. Optional. Default is false. | . | plugins.security.ssl.http.keystore_type | The type of the keystore file, JKS or PKCS12/PFX. Optional. Default is JKS. | . | plugins.security.ssl.http.keystore_filepath | Path to the keystore file, which must be under the config directory, specified using a relative path. Required. | . | plugins.security.ssl.http.keystore_alias | Alias name. Optional. Default is the first alias. | . | plugins.security.ssl.http.keystore_password | Keystore password. Default is changeit. | . | plugins.security.ssl.http.truststore_type | The type of the truststore file, JKS or PKCS12/PFX. Default is JKS. | . | plugins.security.ssl.http.truststore_filepath | Path to the truststore file, which must be under the config directory, specified using a relative path. Required. | . | plugins.security.ssl.http.truststore_alias | Alias name. Optional. Default is all certificates. | . | plugins.security.ssl.http.truststore_password | Truststore password. Default is changeit. | . ",
    "url": "https://vagimeli.github.io/security/configuration/tls/#keystore-and-truststore-files",
    "relUrl": "/security/configuration/tls/#keystore-and-truststore-files"
  },"698": {
    "doc": "Configuring TLS certificates",
    "title": "Configuring node certificates",
    "content": "OpenSearch Security needs to identify requests between the nodes in the cluster. It uses node certificates to secure these requests. The simplest way to configure node certificates is to list the Distinguished Names (DNs) of these certificates in opensearch.yml. All DNs must be included in opensearch.yml on all nodes. Keep in mind that the Security plugin supports wildcards and regular expressions: . plugins.security.nodes_dn: - 'CN=node.other.com,OU=SSL,O=Test,L=Test,C=DE' - 'CN=*.example.com,OU=SSL,O=Test,L=Test,C=DE' - 'CN=elk-devcluster*' - '/CN=.*regex/' . If your node certificates have an Object ID (OID) identifier in the SAN section, you can omit this configuration. ",
    "url": "https://vagimeli.github.io/security/configuration/tls/#configuring-node-certificates",
    "relUrl": "/security/configuration/tls/#configuring-node-certificates"
  },"699": {
    "doc": "Configuring TLS certificates",
    "title": "Configuring admin certificates",
    "content": "Admin certificates are regular client certificates that have elevated rights to perform administrative tasks. You need an admin certificate to change the Security plugin configuration using plugins/opensearch-security/tools/securityadmin.sh or the REST API. Admin certificates are configured in opensearch.yml by stating their DN(s): . plugins.security.authcz.admin_dn: - CN=admin,OU=SSL,O=Test,L=Test,C=DE . For security reasons, you can’t use wildcards or regular expressions here. ",
    "url": "https://vagimeli.github.io/security/configuration/tls/#configuring-admin-certificates",
    "relUrl": "/security/configuration/tls/#configuring-admin-certificates"
  },"700": {
    "doc": "Configuring TLS certificates",
    "title": "(Advanced) OpenSSL",
    "content": "The Security plugin supports OpenSSL, but we only recommend it if you use Java 8. If you use Java 11, we recommend the default configuration. To use OpenSSL, you must install OpenSSL, the Apache Portable Runtime, and a Netty version with OpenSSL support matching your platform on all nodes. If OpenSSL is enabled, but for one reason or another the installation does not work, the Security plugin falls back to the Java JCE as the security engine. | Name | Description | . | plugins.security.ssl.transport.enable_openssl_if_available | Enable OpenSSL on the transport layer if available. Optional. Default is true. | . | plugins.security.ssl.http.enable_openssl_if_available | Enable OpenSSL on the REST layer if available. Optional. Default is true. | . ",
    "url": "https://vagimeli.github.io/security/configuration/tls/#advanced-openssl",
    "relUrl": "/security/configuration/tls/#advanced-openssl"
  },"701": {
    "doc": "Configuring TLS certificates",
    "title": "(Advanced) Hostname verification and DNS lookup",
    "content": "In addition to verifying the TLS certificates against the root CA and/or intermediate CA(s), the Security plugin can apply additional checks on the transport layer. With enforce_hostname_verification enabled, the Security plugin verifies that the hostname of the communication partner matches the hostname in the certificate. The hostname is taken from the subject or SAN entries of your certificate. For example, if the hostname of your node is node-0.example.com, then the hostname in the TLS certificate has to be set to node-0.example.com, as well. Otherwise, errors are thrown: . [ERROR][c.a.o.s.s.t.opensearchSecuritySSLNettyTransport] [WX6omJY] SSL Problem No name matching &lt;hostname&gt; found [ERROR][c.a.o.s.s.t.opensearchSecuritySSLNettyTransport] [WX6omJY] SSL Problem Received fatal alert: certificate_unknown . In addition, when resolve_hostname is enabled, the Security plugin resolves the (verified) hostname against your DNS. If the hostname does not resolve, errors are thrown: . | Name | Description | . | plugins.security.ssl.transport.enforce_hostname_verification | Whether to verify hostnames on the transport layer. Optional. Default is true. | . | plugins.security.ssl.transport.resolve_hostname | Whether to resolve hostnames against DNS on the transport layer. Optional. Default is true. Only works if hostname verification is also enabled. | . ",
    "url": "https://vagimeli.github.io/security/configuration/tls/#advanced-hostname-verification-and-dns-lookup",
    "relUrl": "/security/configuration/tls/#advanced-hostname-verification-and-dns-lookup"
  },"702": {
    "doc": "Configuring TLS certificates",
    "title": "(Advanced) Client authentication",
    "content": "With TLS client authentication enabled, REST clients can send a TLS certificate with the HTTP request to provide identity information to the Security plugin. There are three main usage scenarios for TLS client authentication: . | Providing an admin certificate when using the REST management API. | Configuring roles and permissions based on a client certificate. | Providing identity information for tools like OpenSearch Dashboards, Logstash, or Beats. | . TLS client authentication has three modes: . | NONE: The Security plugin does not accept TLS client certificates. If one is sent, it is discarded. | OPTIONAL: The Security plugin accepts TLS client certificates if they are sent, but does not require them. | REQUIRE: The Security plugin only accepts REST requests when a valid client TLS certificate is sent. | . For the REST management API, the client authentication modes has to be OPTIONAL at a minimum. You can configure the client authentication mode by using the following setting: . | Name | Description | . | plugins.security.ssl.http.clientauth_mode | The TLS client authentication mode to use. Can be one of NONE, OPTIONAL (default) or REQUIRE. Optional. | . ",
    "url": "https://vagimeli.github.io/security/configuration/tls/#advanced-client-authentication",
    "relUrl": "/security/configuration/tls/#advanced-client-authentication"
  },"703": {
    "doc": "Configuring TLS certificates",
    "title": "(Advanced) Enabled ciphers and protocols",
    "content": "You can limit the allowed ciphers and TLS protocols for the REST layer. For example, you can only allow strong ciphers and limit the TLS versions to the most recent ones. If this setting is not enabled, the ciphers and TLS versions are negotiated between the browser and the Security plugin automatically, which in some cases can lead to a weaker cipher suite being used. You can configure the ciphers and protocols using the following settings. | Name | Data type | Description | . | plugins.security.ssl.http.enabled_ciphers | Array | Enabled TLS cipher suites for the REST layer. Only Java format is supported. | . | plugins.security.ssl.http.enabled_protocols | Array | Enabled TLS protocols for the REST layer. Only Java format is supported. | . | plugins.security.ssl.transport.enabled_ciphers | Array | Enabled TLS cipher suites for the transport layer. Only Java format is supported. | . | plugins.security.ssl.transport.enabled_protocols | Array | Enabled TLS protocols for the transport layer. Only Java format is supported. | . Example settings . plugins.security.ssl.http.enabled_ciphers: - \"TLS_DHE_RSA_WITH_AES_256_CBC_SHA\" - \"TLS_DHE_DSS_WITH_AES_128_CBC_SHA256\" plugins.security.ssl.http.enabled_protocols: - \"TLSv1.1\" - \"TLSv1.2\" . Because it is insecure, the Security plugin disables TLSv1 by default. If you need to use TLSv1 and accept the risks, you can still enable it: . plugins.security.ssl.http.enabled_protocols: - \"TLSv1\" - \"TLSv1.1\" - \"TLSv1.2\" . ",
    "url": "https://vagimeli.github.io/security/configuration/tls/#advanced-enabled-ciphers-and-protocols",
    "relUrl": "/security/configuration/tls/#advanced-enabled-ciphers-and-protocols"
  },"704": {
    "doc": "Configuring TLS certificates",
    "title": "(Advanced) Disabling client initiated renegotiation for Java 8",
    "content": "Set -Djdk.tls.rejectClientInitiatedRenegotiation=true to disable secure client initiated renegotiation, which is enabled by default. This can be set via OPENSEARCH_JAVA_OPTS in config/jvm.options. ",
    "url": "https://vagimeli.github.io/security/configuration/tls/#advanced-disabling-client-initiated-renegotiation-for-java-8",
    "relUrl": "/security/configuration/tls/#advanced-disabling-client-initiated-renegotiation-for-java-8"
  },"705": {
    "doc": "Modifying the YAML files",
    "title": "Modifying the YAML files",
    "content": "The Security installation provides a number of YAML confguration files that are used to store the necessary settings that define the way Security manages users, roles, and activity within the cluster. These settings range from configurations for authentication backends to lists of allowed endpoints and HTTP requests. Before running securityadmin.sh to load the settings into the .opendistro_security index, perform an initial configuration of the YAML files. The files can be found in the config/opensearch-security directory. It’s also good practice to back up these files so that you can reuse them for other clusters. The approach we recommend for using the YAML files is to first configure reserved and hidden resources, such as the admin and kibanaserver users. Thereafter you can create other users, roles, mappings, action groups, and tenants using OpenSearch Dashboards or the REST API. ",
    "url": "https://vagimeli.github.io/security/configuration/yaml/",
    "relUrl": "/security/configuration/yaml/"
  },"706": {
    "doc": "Modifying the YAML files",
    "title": "internal_users.yml",
    "content": "This file contains any initial users that you want to add to the Security plugin’s internal user database. The file format requires a hashed password. To generate one, run plugins/opensearch-security/tools/hash.sh -p &lt;new-password&gt;. If you decide to keep any of the demo users, change their passwords and re-run securityadmin.sh to apply the new passwords. --- # This is the internal user database # The hash value is a bcrypt hash and can be generated with plugin/tools/hash.sh _meta: type: \"internalusers\" config_version: 2 # Define your internal users here new-user: hash: \"$2y$12$88IFVl6IfIwCFh5aQYfOmuXVL9j2hz/GusQb35o.4sdTDAEMTOD.K\" reserved: false hidden: false opendistro_security_roles: - \"specify-some-security-role-here\" backend_roles: - \"specify-some-backend-role-here\" attributes: attribute1: \"value1\" static: false ## Demo users admin: hash: \"$2a$12$VcCDgh2NDk07JGN0rjGbM.Ad41qVR/YFJcgHp0UGns5JDymv..TOG\" reserved: true backend_roles: - \"admin\" description: \"Demo admin user\" kibanaserver: hash: \"$2a$12$4AcgAt3xwOWadA5s5blL6ev39OXDNhmOesEoo33eZtrq2N0YrU3H.\" reserved: true description: \"Demo user for the OpenSearch Dashboards server\" kibanaro: hash: \"$2a$12$JJSXNfTowz7Uu5ttXfeYpeYE0arACvcwlPBStB1F.MI7f0U9Z4DGC\" reserved: false backend_roles: - \"kibanauser\" - \"readall\" attributes: attribute1: \"value1\" attribute2: \"value2\" attribute3: \"value3\" description: \"Demo read-only user for OpenSearch dashboards\" logstash: hash: \"$2a$12$u1ShR4l4uBS3Uv59Pa2y5.1uQuZBrZtmNfqB3iM/.jL0XoV9sghS2\" reserved: false backend_roles: - \"logstash\" description: \"Demo logstash user\" readall: hash: \"$2a$12$ae4ycwzwvLtZxwZ82RmiEunBbIPiAmGZduBAjKN0TXdwQFtCwARz2\" reserved: false backend_roles: - \"readall\" description: \"Demo readall user\" snapshotrestore: hash: \"$2y$12$DpwmetHKwgYnorbgdvORCenv4NAK8cPUg8AI6pxLCuWf/ALc0.v7W\" reserved: false backend_roles: - \"snapshotrestore\" description: \"Demo snapshotrestore user\" . ",
    "url": "https://vagimeli.github.io/security/configuration/yaml/#internal_usersyml",
    "relUrl": "/security/configuration/yaml/#internal_usersyml"
  },"707": {
    "doc": "Modifying the YAML files",
    "title": "opensearch.yml",
    "content": "In addition to many OpenSearch settings, this file contains paths to TLS certificates and their attributes, such as distinguished names and trusted certificate authorities. plugins.security.ssl.transport.pemcert_filepath: esnode.pem plugins.security.ssl.transport.pemkey_filepath: esnode-key.pem plugins.security.ssl.transport.pemtrustedcas_filepath: root-ca.pem plugins.security.ssl.transport.enforce_hostname_verification: false plugins.security.ssl.http.enabled: true plugins.security.ssl.http.pemcert_filepath: esnode.pem plugins.security.ssl.http.pemkey_filepath: esnode-key.pem plugins.security.ssl.http.pemtrustedcas_filepath: root-ca.pem plugins.security.allow_unsafe_democertificates: true plugins.security.allow_default_init_securityindex: true plugins.security.authcz.admin_dn: - CN=kirk,OU=client,O=client,L=test, C=de plugins.security.audit.type: internal_opensearch plugins.security.enable_snapshot_restore_privilege: true plugins.security.check_snapshot_restore_write_privileges: true plugins.security.cache.ttl_minutes: 60 plugins.security.restapi.roles_enabled: [\"all_access\", \"security_rest_api_access\"] plugins.security.system_indices.enabled: true plugins.security.system_indices.indices: [\".opendistro-alerting-config\", \".opendistro-alerting-alert*\", \".opendistro-anomaly-results*\", \".opendistro-anomaly-detector*\", \".opendistro-anomaly-checkpoints\", \".opendistro-anomaly-detection-state\", \".opendistro-reports-*\", \".opendistro-notifications-*\", \".opendistro-notebooks\", \".opendistro-asynchronous-search-response*\"] node.max_local_storage_nodes: 3 . If you want to run your users’ passwords against some validation, specify a regular expression (regex) in this file. You can also include an error message that loads when passwords don’t pass validation. The following example demonstrates how to include a regex so OpenSearch requires new passwords to be a minimum of eight characters with at least one uppercase, one lowercase, one digit, and one special character. Note that OpenSearch validates only users and passwords created through OpenSearch Dashboards or the REST API. plugins.security.restapi.password_validation_regex: '(?=.*[A-Z])(?=.*[^a-zA-Z\\d])(?=.*[0-9])(?=.*[a-z]).{8,}' plugins.security.restapi.password_validation_error_message: \"Password must be minimum 8 characters long and must contain at least one uppercase letter, one lowercase letter, one digit, and one special character.\" . The opensearch.yml file also contains the plugins.security.allow_default_init_securityindex property. When set to true, the Security plugin uses default security settings if an attempt to create the security index fails when OpenSearch launches. Default security settings are stored in YAML files contained in the opensearch-project/security/config directory. By default, this setting is false. plugins.security.allow_default_init_securityindex: true . Authentication cache for the Security plugin exists to help speed up authentication by temporarily storing user objects returned from the backend so that the Security plugin is not required to make repeated requests for them. To determine how long it takes for caching to time out, you can use the plugins.security.cache.ttl_minutes property to set a value in minutes. The default is 60. You can disable caching by setting the value to 0. plugins.security.cache.ttl_minutes: 60 . ",
    "url": "https://vagimeli.github.io/security/configuration/yaml/#opensearchyml",
    "relUrl": "/security/configuration/yaml/#opensearchyml"
  },"708": {
    "doc": "Modifying the YAML files",
    "title": "allowlist.yml",
    "content": "You can use allowlist.yml to add any endpoints and HTTP requests to a list of allowed endpoints and requests. If enabled, all users except the super admin are allowed access to only the specified endpoints and HTTP requests, and all other HTTP requests associated with the endpoint are denied. For example, if GET _cluster/settings is added to the allow list, users cannot submit PUT requests to _cluster/settings to update cluster settings. Note that while you can configure access to endpoints this way, for most cases, it is still best to configure permissions using the Security plugin’s users and roles, which have more granular settings. --- _meta: type: \"allowlist\" config_version: 2 # Description: # enabled - feature flag. # if enabled is false, all endpoints are accessible. # if enabled is true, all users except the SuperAdmin can only submit the allowed requests to the specified endpoints. # SuperAdmin can access all APIs. # SuperAdmin is defined by the SuperAdmin certificate, which is configured with the opensearch.yml setting plugins.security.authcz.admin_dn: # Refer to the example setting in opensearch.yml to learn more about configuring SuperAdmin. # # requests - map of allow listed endpoints and HTTP requests #this name must be config config: enabled: true requests: /_cluster/settings: - GET /_cat/nodes: - GET . To enable PUT requests to cluster settings, add PUT to the list of allowed operations under /_cluster/settings. requests: /_cluster/settings: - GET - PUT . You can also add custom indices to the allow list. allowlist.yml doesn’t support wildcards, so you must manually specify all of the indexes you want to add. requests: # Only allow GET requests to /sample-index1/_doc/1 and /sample-index2/_doc/1 /sample-index1/_doc/1: - GET /sample-index2/_doc/1: - GET . ",
    "url": "https://vagimeli.github.io/security/configuration/yaml/#allowlistyml",
    "relUrl": "/security/configuration/yaml/#allowlistyml"
  },"709": {
    "doc": "Modifying the YAML files",
    "title": "roles.yml",
    "content": "This file contains any initial roles that you want to add to the Security plugin. Aside from some metadata, the default file is empty, because the Security plugin has a number of static roles that it adds automatically. --- complex-role: reserved: false hidden: false cluster_permissions: - \"read\" - \"cluster:monitor/nodes/stats\" - \"cluster:monitor/task/get\" index_permissions: - index_patterns: - \"opensearch_dashboards_sample_data_*\" dls: \"{\\\"match\\\": {\\\"FlightDelay\\\": true}}\" fls: - \"~FlightNum\" masked_fields: - \"Carrier\" allowed_actions: - \"read\" tenant_permissions: - tenant_patterns: - \"analyst_*\" allowed_actions: - \"kibana_all_write\" static: false _meta: type: \"roles\" config_version: 2 . ",
    "url": "https://vagimeli.github.io/security/configuration/yaml/#rolesyml",
    "relUrl": "/security/configuration/yaml/#rolesyml"
  },"710": {
    "doc": "Modifying the YAML files",
    "title": "roles_mapping.yml",
    "content": "--- manage_snapshots: reserved: true hidden: false backend_roles: - \"snapshotrestore\" hosts: [] users: [] and_backend_roles: [] logstash: reserved: false hidden: false backend_roles: - \"logstash\" hosts: [] users: [] and_backend_roles: [] own_index: reserved: false hidden: false backend_roles: [] hosts: [] users: - \"*\" and_backend_roles: [] description: \"Allow full access to an index named like the username\" kibana_user: reserved: false hidden: false backend_roles: - \"kibanauser\" hosts: [] users: [] and_backend_roles: [] description: \"Maps kibanauser to kibana_user\" complex-role: reserved: false hidden: false backend_roles: - \"ldap-analyst\" hosts: [] users: - \"new-user\" and_backend_roles: [] _meta: type: \"rolesmapping\" config_version: 2 all_access: reserved: true hidden: false backend_roles: - \"admin\" hosts: [] users: [] and_backend_roles: [] description: \"Maps admin to all_access\" readall: reserved: true hidden: false backend_roles: - \"readall\" hosts: [] users: [] and_backend_roles: [] kibana_server: reserved: true hidden: false backend_roles: [] hosts: [] users: - \"kibanaserver\" and_backend_roles: [] . ",
    "url": "https://vagimeli.github.io/security/configuration/yaml/#roles_mappingyml",
    "relUrl": "/security/configuration/yaml/#roles_mappingyml"
  },"711": {
    "doc": "Modifying the YAML files",
    "title": "action_groups.yml",
    "content": "This file contains any initial action groups that you want to add to the Security plugin. Aside from some metadata, the default file is empty, because the Security plugin has a number of static action groups that it adds automatically. These static action groups cover a wide variety of use cases and are a great way to get started with the plugin. --- my-action-group: reserved: false hidden: false allowed_actions: - \"indices:data/write/index*\" - \"indices:data/write/update*\" - \"indices:admin/mapping/put\" - \"indices:data/write/bulk*\" - \"read\" - \"write\" static: false _meta: type: \"actiongroups\" config_version: 2 . ",
    "url": "https://vagimeli.github.io/security/configuration/yaml/#action_groupsyml",
    "relUrl": "/security/configuration/yaml/#action_groupsyml"
  },"712": {
    "doc": "Modifying the YAML files",
    "title": "tenants.yml",
    "content": "You can use this file to specify and add any number of OpenSearch Dashboards tenants to your OpenSearch cluster. For more information about tenants, see OpenSearch Dashboards multi-tenancy. Like all of the other YAML files, we recommend you use tenants.yml to add any tenants you must have in your cluster, and then use OpenSearch Dashboards or the REST API if you need to further configure or create any other tenants. --- _meta: type: \"tenants\" config_version: 2 admin_tenant: reserved: false description: \"Demo tenant for admin user\" . ",
    "url": "https://vagimeli.github.io/security/configuration/yaml/#tenantsyml",
    "relUrl": "/security/configuration/yaml/#tenantsyml"
  },"713": {
    "doc": "Modifying the YAML files",
    "title": "nodes_dn.yml",
    "content": "nodes_dn.yml lets you add certificates’ distinguished names (DNs) an allow list to enable communication between any number of nodes and/or clusters. For example, a node that has the DN CN=node1.example.com in its allow list accepts communication from any other node or certificate that uses that DN. The DNs get indexed into a system index that only a super admin or an admin with a Transport Layer Security (TLS) certificate can access. If you want to programmatically add DNs to your allow lists, use the REST API. --- _meta: type: \"nodesdn\" config_version: 2 # Define nodesdn mapping name and corresponding values # cluster1: # nodes_dn: # - CN=*.example.com . ",
    "url": "https://vagimeli.github.io/security/configuration/yaml/#nodes_dnyml",
    "relUrl": "/security/configuration/yaml/#nodes_dnyml"
  },"714": {
    "doc": "About Security",
    "title": "About Security in OpenSearch",
    "content": "Security in OpenSearch is built around four main features that work together to safeguard data and track activity within a cluster. Separately, these features are: . | Encryption. | Authentication. | Access control. | Audit logging and compliance. | . Used together they provide effective protection of sensitive data by placing it behind multiple layers of defense and granting or restricting access to the data at different levels in the OpenSearch data structure. Most implementations use a combination of options for these features to meet specific security needs. ",
    "url": "https://vagimeli.github.io/security/index/#about-security-in-opensearch",
    "relUrl": "/security/index/#about-security-in-opensearch"
  },"715": {
    "doc": "About Security",
    "title": "Features at a glance",
    "content": "The following topics provide a general description of the features that define security in OpenSearch. Encryption . Encryption typically addresses the protection of data both at rest and in transit. OpenSearch Security is responsible for managing encryption in transit. In transit, Security encrypts data moving to, from, and within the cluster. OpenSearch uses the TLS protocol, which covers both client-to-node encryption (the REST layer) and node-to-node encryption (the transport layer). This combination of in-transit encryption helps ensure that both requests to OpenSearch and the movement of data among different nodes are safe from tampering. You can find out more about configuring TLS in the Configuring TLS certificates section. Encryption at rest, on the other hand, protects data stored in the cluster, including indexes, logs, swap files, automated snapshots, and all data in the application directory. This type of encryption is managed by the operating system on each OpenSearch node. For information about enabling encryption at rest, see Encryption at rest. Authentication . Authentication is used to validate the identity of users and works by verifying an end user’s credentials against a backend configuration. These credentials can be a simple name and password, a JSON web token, or a TLS certificate. Once the authentication domain extracts those credentials from a user’s request, it can check their validity against the authentication backend. The backend used for validation can be OpenSearch’s built-in internal user database—used for storing user and role configurations and hashed passwords—or one of a wide range of industry-standard identification protocols such as LDAP, Active Directory, SAML, or OpenID Connect. A common practice is to chain together more than one authentication method to create a more robust defense against unauthorized access. This might involve, for example, HTTP basic authentication followed by a backend configuration that specifies the LDAP protocol. See the Configuring the Security backend section to learn more about setting up the backend. Access control . Access control (or authorization) generally involves selectively assigning permissions to users that allow them to perform specific tasks, such as clearing the cache for a particular index or taking a snapshot of a cluster. However, rather than assign individual permissions directly to users, OpenSearch assigns these permissions to roles and then maps the roles to users. For more on setting up these relationships, see Users and roles. Roles, therefore, define the actions that users can perform, including the data they can read, the cluster settings they can modify, the indexes to which they can write, and so on. Roles are reusable across multiple users, and users can have multiple roles. Another notable characteristic of access control in OpenSearch is the ability to assign user access through levels of increasing granularity. Fine-grained access control (FGAC) means that a role can control permissions for users at not only the cluster level but also the index level, the document level, and even the field level. For example, a role may provide a user access to certain cluster-level permissions but at the same time prevent the user from accessing a given group of indexes. Likewise, that role may grant access to certain types of documents but not others, or it may even include access to specific fields within a document but exclude access to other sensitive fields. Field masking further extends FGAC by providing options to mask certain types of data, such as a list of emails, which can still be aggregated but not made viewable to a role. To learn more about this feature, see the Access control section of the security documentation. Audit logging and compliance . Finally, audit logging and compliance refer to mechanisms that allow for tracking and analysis of activity within a cluster. This is important after data breaches (unauthorized access) or when data suffers unintended exposure, as could happen when the data is left vulnerable in an unsecured location. However, audit logging can be just as valuable a tool for assessing excessive loads on a cluster or surveying trends for a given task. This feature allows you to review changes made anywhere in a cluster and track access patterns and API requests of all types, whether valid or invalid. How OpenSearch archives logging is configurable at many levels of detail, and there are a number of options for where those logs are stored. Compliance features also ensure that all data is available if and when compliance auditing is required. In this case, the logging can be automated to focus on data especially pertinent to those compliance requirements. See the Audit logs section of the security documentation to read more about this feature. ",
    "url": "https://vagimeli.github.io/security/index/#features-at-a-glance",
    "relUrl": "/security/index/#features-at-a-glance"
  },"716": {
    "doc": "About Security",
    "title": "Other features and functionality",
    "content": "OpenSearch includes other features that complement the security infrastructure. Dashboards multi-tenancy . One such feature is OpenSearch Dashboards multi-tenancy. Tenants are work spaces that include visualizations, index patterns, and other Dashboards objects. Multi-tenancy allows for the sharing of tenants among users of Dashboards and leverages OpenSearch roles to manage access to tenants and safely make them available to others. For more information on creating tenants, see OpenSearch Dashboards multi-tenancy. Cross-cluster search . Another notable feature is cross-cluster search. This feature provides users with the ability to perform searches from one node in a cluster across other clusters that have been set up to coordinate this type of search. As with other features, cross-cluster search is supported by the OpenSearch access control infrastructure, which defines the permissions users have for working with this feature. To learn more, see Cross-cluster search. ",
    "url": "https://vagimeli.github.io/security/index/#other-features-and-functionality",
    "relUrl": "/security/index/#other-features-and-functionality"
  },"717": {
    "doc": "About Security",
    "title": "Next steps",
    "content": "To get started, see the configuration overview in the Security configuration section, which provides the basic steps for setting up security in your OpenSearch implementation and includes links to information about customizing security for your business needs. ",
    "url": "https://vagimeli.github.io/security/index/#next-steps",
    "relUrl": "/security/index/#next-steps"
  },"718": {
    "doc": "About Security",
    "title": "About Security",
    "content": " ",
    "url": "https://vagimeli.github.io/security/index/",
    "relUrl": "/security/index/"
  },"719": {
    "doc": "Dynamic configuration in OpenSearch Dashboards",
    "title": "Dynamic configuration in OpenSearch Dashboards",
    "content": "Multi-tenancy includes dynamic configuration options in OpenSearch Dashboards so you can manage common settings for tenancy without having to make changes to the configuration YAML files on each node and then restart the cluster. You can take advantage of this functionality by using the Dashboards interface or the REST API. The following list includes descriptions of the options currently covered by dynamic configuration: . | Disable or enable multi-tenancy - Administrators can disable and enable multi-tenancy dynamically. Disabling multi-tenancy does not pose a risk of data loss. If and when an administrator chooses to reenable tenancy, all previously saved objects are preserved and made available. The default is true. This setting does not have an impact on the global tenant, which always remains enabled. | Disable or enable private tenant - This option allows administrators to disable and enable private tenants. As with the enable multi-tenancy setting, when private tenants are reenabled all previously saved objects are preserved and made available. | Default tenant - This option allows an administrator to choose either a global, private, or custom tenant as the default when users log in. In cases where a user doesn’t have access to the default tenant (for example, if a custom tenant unavailable to the user was specified as the default), the default transitions to the preferred tenant, which is specified by the opensearch_security.multitenancy.tenants.preferred setting in the opensearch-dashboards.yml file. See Multi-tenancy configuration for more information about this setting. | . Depending on the specific changes made to multi-tenancy using dynamic configuration, some users may be logged out of their Dashboards session once the changes are saved. For example, if an admin user disables multi-tenancy, users with either a private or custom tenant as their selected tenant will be logged out and will need to log back in. Similarly, if an admin user disables private tenants, users with the private tenant selected will be logged out and will need to log back in. The global tenant, however, is a special case. Because this tenant is never disabled, users with the global tenant selected as their active tenant will experience no interruption to their session. Furthermore, changing the default tenant has no impact on a user’s session. ",
    "url": "https://vagimeli.github.io/security/multi-tenancy/dynamic-config/",
    "relUrl": "/security/multi-tenancy/dynamic-config/"
  },"720": {
    "doc": "Dynamic configuration in OpenSearch Dashboards",
    "title": "Configuring multi-tenancy in OpenSearch Dashboards",
    "content": "To configure multi-tenancy in Dashboards, follow these steps: . | Begin by selecting Security in the Dashboards home page menu. Then select Tenancy from the Security menu on the left side of the screen. The Multi-tenancy page is displayed. | By default, the Manage tab is displayed. Select the Configure tab to display the dynamic settings for multi-tenancy. | In the Multi-tenancy field, select the Enable tenancy check box to enable multi-tenancy. Clear the check box to disable the feature. The default is true. | In the Tenants field, you can enable or disable private tenants for users. By default the check box is selected and the feature is enabled. | In the Default tenant field, use the dropdown menu to select a default tenant. The menu includes Global, Private, and any other custom tenants that are available to users. | . | After making your preferred changes, select Save changes in the lower right corner of the window. A pop-up window appears listing the configuration items you’ve changed and asks you to review your changes. | Select the check boxes beside the items you want to confirm and then select Apply changes. The changes are implemented dynamically. | . ",
    "url": "https://vagimeli.github.io/security/multi-tenancy/dynamic-config/#configuring-multi-tenancy-in-opensearch-dashboards",
    "relUrl": "/security/multi-tenancy/dynamic-config/#configuring-multi-tenancy-in-opensearch-dashboards"
  },"721": {
    "doc": "Dynamic configuration in OpenSearch Dashboards",
    "title": "Configuring multi-tenancy with the REST API",
    "content": "In addition to using the Dashboards interface, you can manage dynamic configurations using the REST API. Get tenancy configuration . The GET call retrieves settings for the dynamic configuration: . GET /_plugins/_security/api/tenancy/config . copy . Example response . { \"mulitenancy_enabled\": true, \"private_tenant_enabled\": true, \"default_tenant\": \"global tenant\" } . Update tenant configuration . The PUT call updates settings for dynamic configuration: . PUT /_plugins/_security/api/tenancy/config { \"default_tenant\": \"custom tenant 1\", \"private_tenant_enabled\": false, \"mulitenancy_enabled\": true } . copy . Example response . { \"mulitenancy_enabled\": true, \"private_tenant_enabled\": false, \"default_tenant\": \"custom tenant 1\" } . Dashboardsinfo API . You can also use the Dashboardsinfo API to retrieve the status of multi-tenancy settings for the user logged in to Dashboards: . GET /_plugins/_security/dashboardsinfo . copy . Example response . { \"user_name\" : \"admin\", \"not_fail_on_forbidden_enabled\" : false, \"opensearch_dashboards_mt_enabled\" : true, \"opensearch_dashboards_index\" : \".kibana\", \"opensearch_dashboards_server_user\" : \"kibanaserver\", \"multitenancy_enabled\" : true, \"private_tenant_enabled\" : true, \"default_tenant\" : \"Private\" } . ",
    "url": "https://vagimeli.github.io/security/multi-tenancy/dynamic-config/#configuring-multi-tenancy-with-the-rest-api",
    "relUrl": "/security/multi-tenancy/dynamic-config/#configuring-multi-tenancy-with-the-rest-api"
  },"722": {
    "doc": "Multi-tenancy aggregate view for saved objects",
    "title": "OpenSearch Dashboards multi-tenancy aggregate view for saved objects",
    "content": "Aggregate view for saved objects is an experimental feature released in OpenSearch 2.4. Therefore, we do not recommend enabling the feature in a production environment at this time. For updates on the progress of aggregate view for saved objects, or if you’d like to leave feedback that could help improve the feature, see the Dashboards object sharing GitHub issue. For a more comprehensive view of the proposed future development of multi-tenancy, see the Dashboards object sharing issue. Aggregate view for saved objects allows a user who has access to multiple tenants to see all saved objects associated with those tenants in a single view without having to switch between tenants to do so. This includes both tenants created by the user and tenants shared with the user. Aggregate view introduces a Tenant dropdown menu and column in the Saved Objects table that gives the user the option to filter by tenants and make visible their associated saved objects. Once you identify a saved object of interest, you can then switch to that tenant to work with the object. To access saved objects, expand the top menu and select Management &gt; Stack Management &gt; Saved Objects. The Saved Objects window opens. By default, all tenants the user has permissions for are displayed along with all saved objects associated with the tenants. As an experimental feature, aggregate view for saved objects is kept behind a feature flag and must be enabled in the opensearch_dashboards.yml file before the feature is made available. See Enabling aggregate view for more information. Feature benefits . | Implementing an aggregate view for all saved objects on one screen allows you to quickly locate an object of interest and determine which tenant is associated with it. Once you locate an object, you can select the appropriate tenant and work with the object. | This feature also adds a Tenant dropdown menu to the Saved Objects table, which allows you to filter the view by tenants and their associated saved objects. | . Plans for future development . In subsequent releases, we plan to expand the functionality of this feature to include the ability to perform actions directly from aggregate view and share items without having to first select a specific tenant. In the longer term, OpenSearch plans to evolve multi-tenancy so that it becomes a much more flexible tool for sharing objects among users and employs a more sophisticated way of assigning the roles and permissions that facilitate sharing. To learn more about the features being proposed for future releases, see the GitHub issue Dashboards object sharing. Known limitations . In this first experimental phase of development, there are some limitations that should be observed before enabling the feature and using it in a test environment: . | The feature can only be used in a new cluster. At this time, the feature is not supported by clusters already in use. | Also, the feature should be used only in a test environment, not in production. | Finally, once the feature has been enabled and used in a test cluster, the feature cannot be disabled for the cluster. Disabling the feature once it has been used to work with tenants and saved objects can result in the loss of saved objects and can have an impact on tenant-to-tenant functionality. This can occur when disabling the feature in any one of three ways: disabling the aggregate view feature with the feature flag; disabling multi-tenancy with the traditional multi-tenancy configuration setting; or disabling multi-tenancy with dynamic configuration settings. | . These limitations will be addressed in upcoming releases. ",
    "url": "https://vagimeli.github.io/security/multi-tenancy/mt-agg-view/#opensearch-dashboards-multi-tenancy-aggregate-view-for-saved-objects",
    "relUrl": "/security/multi-tenancy/mt-agg-view/#opensearch-dashboards-multi-tenancy-aggregate-view-for-saved-objects"
  },"723": {
    "doc": "Multi-tenancy aggregate view for saved objects",
    "title": "Enabling aggregate view for saved objects",
    "content": "By default, the aggregate view in the Saved Objects table is disabled. To enable the feature, add the opensearch_security.multitenancy.enable_aggregation_view flag to the opensearch_dashboards.yml file and set it to true: . opensearch_security.multitenancy.enable_aggregation_view: true . After enabling the feature you can start the new cluster and then launch Dashboards. ",
    "url": "https://vagimeli.github.io/security/multi-tenancy/mt-agg-view/#enabling-aggregate-view-for-saved-objects",
    "relUrl": "/security/multi-tenancy/mt-agg-view/#enabling-aggregate-view-for-saved-objects"
  },"724": {
    "doc": "Multi-tenancy aggregate view for saved objects",
    "title": "Working in aggregate view",
    "content": "Select the Tenant dropdown arrow to display the list of tenants available to the user. You can select multiple tenants while the menu is open. Each time you select a tenant in the menu, the list of saved objects is filtered by that tenant and any others with a check mark beside their name. After you finish specifying tenants, select anywhere outside the menu to collapse it. | The Title column displays the names of the available saved objects. | The Tenant column displays the tenants associated with the saved objects. | Also, the number of tenants selected for filtering is shown in a red box beside the Tenant dropdown menu label. | . Use the Type dropdown menu to filter saved objects by type. The behavior of the Type dropdown menu is the same as the behavior of the Tenant dropdown menu. Selecting and working with a saved object . After identifying a saved object that you would like to work with, follow these steps to access the object: . | Note the tenant associated with the object in the Tenant column. | In the upper-right corner of the window, open the user menu and select Switch tenants. | In the Select your tenant window, choose either the Global or Private option, or one of the custom tenant options, to specify the correct tenant. Select the Confirm button. The tenant becomes active and is displayed in the user menu. | After the tenant is active, you can use the controls in the Actions column to work with saved objects associated with the tenant. | . When a tenant is not active, you cannot use the Actions column controls to work with its associated objects. To work with those objects, follow the preceding steps to make the tenant active. ",
    "url": "https://vagimeli.github.io/security/multi-tenancy/mt-agg-view/#working-in-aggregate-view",
    "relUrl": "/security/multi-tenancy/mt-agg-view/#working-in-aggregate-view"
  },"725": {
    "doc": "Multi-tenancy aggregate view for saved objects",
    "title": "Multi-tenancy aggregate view for saved objects",
    "content": " ",
    "url": "https://vagimeli.github.io/security/multi-tenancy/mt-agg-view/",
    "relUrl": "/security/multi-tenancy/mt-agg-view/"
  },"726": {
    "doc": "Multi-tenancy configuration",
    "title": "Multi-tenancy configuration",
    "content": "Multi-tenancy is enabled by default, but you can disable it or change its settings using config/opensearch-security/config.yml: . config: dynamic: kibana: multitenancy_enabled: true private_tenant_enabled: true default_tenant: global tenant server_username: kibanaserver index: '.kibana' do_not_fail_on_forbidden: false . | Setting | Description | . | multitenancy_enabled | Enable or disable multi-tenancy. Default is true. | . | private_tenant_enabled | Enable or disable the private tenant. Default is true. | . | default_tenant | Use to set the tenant that is available when users log in. | . | server_username | Must match the name of the OpenSearch Dashboards server user from opensearch_dashboards.yml. Default is kibanaserver. | . | index | Must match the name of the OpenSearch Dashboards index from opensearch_dashboards.yml. Default is .kibana. | . | do_not_fail_on_forbidden | When true, the Security plugin removes any content that a user is not allowed to see from the search results. When false, the plugin returns a security exception. Default is false. | . The opensearch_dashboards.yml file includes additional settings: . opensearch.username: kibanaserver opensearch.password: kibanaserver opensearch.requestHeadersAllowlist: [\"securitytenant\",\"Authorization\"] opensearch_security.multitenancy.enabled: true opensearch_security.multitenancy.tenants.enable_global: true opensearch_security.multitenancy.tenants.enable_private: true opensearch_security.multitenancy.tenants.preferred: [\"Private\", \"Global\"] opensearch_security.multitenancy.enable_filter: false . | Setting | Description | . | opensearch.requestHeadersAllowlist | OpenSearch Dashboards requires that you add all HTTP headers to the allow list so that the headers pass to OpenSearch. Multi-tenancy uses a specific header, securitytenant, that must be present with the standard Authorization header. If the securitytenant header is not on the allow list, OpenSearch Dashboards starts with a red status. | . | opensearch_security.multitenancy.enabled | Enables or disables multi-tenancy in OpenSearch Dashboards. Default is true. | . | opensearch_security.multitenancy.tenants.enable_global | Enables or disables the global tenant. Default is true. | . | opensearch_security.multitenancy.tenants.enable_private | Enables or disables private tenants. Default is true. | . | opensearch_security.multitenancy.tenants.preferred | Lets you change ordering in the Tenants tab of OpenSearch Dashboards. By default, the list starts with Global and Private (if enabled) and then proceeds alphabetically. You can add tenants here to move them to the top of the list. | . | opensearch_security.multitenancy.enable_filter | If you have many tenants, you can add a search bar to the top of the list. Default is false. | . ",
    "url": "https://vagimeli.github.io/security/multi-tenancy/multi-tenancy-config/",
    "relUrl": "/security/multi-tenancy/multi-tenancy-config/"
  },"727": {
    "doc": "Multi-tenancy configuration",
    "title": "Add tenants",
    "content": "To create tenants, use OpenSearch Dashboards, the REST API, or tenants.yml. OpenSearch Dashboards . | Open OpenSearch Dashboards. | Choose Security, Tenants, and Create tenant. | Give the tenant a name and description. | Choose Create. | . REST API . See Create tenant. tenants.yml . --- _meta: type: \"tenants\" config_version: 2 ## Demo tenants admin_tenant: reserved: false description: \"Demo tenant for admin user\" . ",
    "url": "https://vagimeli.github.io/security/multi-tenancy/multi-tenancy-config/#add-tenants",
    "relUrl": "/security/multi-tenancy/multi-tenancy-config/#add-tenants"
  },"728": {
    "doc": "Multi-tenancy configuration",
    "title": "Give roles access to tenants",
    "content": "After creating a tenant, give a role access to it using OpenSearch Dashboards, the REST API, or roles.yml. | Read-write (kibana_all_write) permissions let the role view and modify objects in the tenant. | Read-only (kibana_all_read) permissions let the role view objects, but not modify them. | . OpenSearch Dashboards . | Open OpenSearch Dashboards. | Choose Security, Roles, and a role. | For Tenant permissions, add tenants, press Enter, and give the role read and/or write permissions to it. | . REST API . See Create role. roles.yml . --- test-role: reserved: false hidden: false cluster_permissions: - \"cluster_composite_ops\" - \"indices_monitor\" index_permissions: - index_patterns: - \"movies*\" dls: \"\" fls: [] masked_fields: [] allowed_actions: - \"read\" tenant_permissions: - tenant_patterns: - \"human_resources\" allowed_actions: - \"kibana_all_read\" static: false _meta: type: \"roles\" config_version: 2 . ",
    "url": "https://vagimeli.github.io/security/multi-tenancy/multi-tenancy-config/#give-roles-access-to-tenants",
    "relUrl": "/security/multi-tenancy/multi-tenancy-config/#give-roles-access-to-tenants"
  },"729": {
    "doc": "Multi-tenancy configuration",
    "title": "Manage OpenSearch Dashboards indices",
    "content": "The open source version of OpenSearch Dashboards saves all objects to a single index: .kibana. The Security plugin uses this index for the global tenant, but separate indices for every other tenant. Each user also has a private tenant, so you might see a large number of indices that follow two patterns: .kibana_&lt;hash&gt;_&lt;tenant_name&gt; .kibana_&lt;hash&gt;_&lt;username&gt; . The Security plugin scrubs these index names of special characters, so they might not be a perfect match of tenant names and usernames. To back up your OpenSearch Dashboards data, take a snapshot of all tenant indexes using an index pattern such as .kibana*. ",
    "url": "https://vagimeli.github.io/security/multi-tenancy/multi-tenancy-config/#manage-opensearch-dashboards-indices",
    "relUrl": "/security/multi-tenancy/multi-tenancy-config/#manage-opensearch-dashboards-indices"
  },"730": {
    "doc": "OpenSearch Dashboards multi-tenancy",
    "title": "OpenSearch Dashboards multi-tenancy",
    "content": "Tenants in OpenSearch Dashboards are spaces for saving index patterns, visualizations, dashboards, and other OpenSearch Dashboards objects. OpenSearch allows users to create multiple tenants for multiple uses. Tenants are useful for safely sharing your work with other OpenSearch Dashboards users. You can control which roles have access to a tenant and whether those roles have read or write access. By default, all OpenSearch Dashboards users have access to two independent tenants: the global tenant and a private tenant. Multi-tenancy also provides the option to create custom tenants. | Global – This tenant is shared between every OpenSearch Dashboards user. It does allow for sharing objects among users who have access to it. | Private – This tenant is exclusive to each user and can’t be shared. It does not allow you to access routes or index patterns created by the user’s global tenant. | Custom – Administrators can create custom tenants and assign them to specific roles. Once created, these tenants can then provide spaces for specific groups of users. | . The global tenant is not a primary tenant in the sense that it replicates its content in a private tenant. To the contrary, if you make a change to your global tenant, you won’t see that change reflected in your private tenant. Some example changes include the following: . | Change advanced settings | Create visualizations | Create index patterns | . To provide a practical example, you might use the private tenant for exploratory work, create detailed visualizations with your team in an analysts tenant, and maintain a summary dashboard for corporate leadership in an executive tenant. If you share a visualization or dashboard with someone, you can see that the URL includes the tenant: . http://&lt;opensearch_dashboards_host&gt;:5601/app/opensearch-dashboards?security_tenant=analysts#/visualize/edit/c501fa50-7e52-11e9-ae4e-b5d69947d32e?_g=() . ",
    "url": "https://vagimeli.github.io/security/multi-tenancy/tenant-index/",
    "relUrl": "/security/multi-tenancy/tenant-index/"
  },"731": {
    "doc": "OpenSearch Dashboards multi-tenancy",
    "title": "Next steps",
    "content": "To get started with tenants, see Multi-tenancy configuration for information about enabling multi-tenancy, adding tenants, and assigning roles to tenants. For information about making dynamic changes to the multi-tenancy configuration, see Dynamic configuration in OpenSearch Dashboards. ",
    "url": "https://vagimeli.github.io/security/multi-tenancy/tenant-index/#next-steps",
    "relUrl": "/security/multi-tenancy/tenant-index/#next-steps"
  },"732": {
    "doc": "Alerts and findings APIs",
    "title": "Alerts and findings APIs",
    "content": "The following APIs can be used for tasks related to alerts and findings. ",
    "url": "https://vagimeli.github.io/security-analytics/api-tools/alert-finding-api/",
    "relUrl": "/security-analytics/api-tools/alert-finding-api/"
  },"733": {
    "doc": "Alerts and findings APIs",
    "title": "Get Alerts",
    "content": "Provides an option for retrieving alerts related to a specific detector type or detector ID. Parameters . You can specify the following parameters when requesting an alert. | Parameter | Description | . | detectorId | The ID of the detector used to fetch alerts. Optional when the detectorType is specified. Otherwise required. | . | detectorType | The type of detector used to fetch alerts. Optional when the detectorId is specified. Otherwise required. | . | severityLevel | Used to filter by alert severity level. Optional. | . | alertState | Used to filter by alert state. Possible values: ACTIVE, ACKNOWLEDGED, COMPLETED, ERROR, DELETED. Optional. | . | sortString | This field specifies which string Security Analytics uses to sort the alerts. Optional. | . | sortOrder | The order used to sort the list of findings, either ascending or descending. Optional. | . | missing | A list of fields for which there are no found alias mappings. Optional. | . | size | An optional limit for the maximum number of results returned in the response. Optional. | . | startIndex | The pagination indicator. Optional. | . | searchString | The alert attribute you want returned in the search. Optional. | . Example request . GET /_plugins/_security_analytics/alerts?detectorType=windows . Example response . { \"alerts\": [{ \"detector_id\": \"detector_12345\", \"id\": \"alert_id_1\", \"version\": -3, \"schema_version\": 0, \"trigger_id\": \"trigger_id_1\", \"trigger_name\": \"my_trigger\", \"finding_ids\": [\"finding_id_1\"], \"related_doc_ids\": [\"docId1\"], \"state\": \"ACTIVE\", \"error_message\": null, \"alert_history\": [], \"severity\": null, \"action_execution_results\": [{ \"action_id\": \"action_id_1\", \"last_execution_time\": 1665693544996, \"throttled_count\": 0 }], \"start_time\": \"2022-10-13T20:39:04.995023Z\", \"last_notification_time\": \"2022-10-13T20:39:04.995028Z\", \"end_time\": \"2022-10-13T20:39:04.995027Z\", \"acknowledged_time\": \"2022-10-13T20:39:04.995028Z\" }], \"total_alerts\": 1, \"detectorType\": \"windows\" } . Response fields . Alerts persist until you resolve the root cause and have the following states: . | State | Description | . | ACTIVE | The alert is ongoing and unacknowledged. Alerts remain in this state until you acknowledge them, delete the trigger associated with the alert, or delete the monitor entirely. | . | ACKNOWLEDGED | Someone has acknowledged the alert but not fixed the root cause. | . | COMPLETED | The alert is no longer ongoing. Alerts enter this state after the corresponding trigger evaluates to false. | . | ERROR | An error occurred while executing the trigger. This error is usually the result of a bad trigger or destination. | . | DELETED | Someone deleted the detector or trigger associated with this alert while the alert was ongoing. | . ",
    "url": "https://vagimeli.github.io/security-analytics/api-tools/alert-finding-api/#get-alerts",
    "relUrl": "/security-analytics/api-tools/alert-finding-api/#get-alerts"
  },"734": {
    "doc": "Alerts and findings APIs",
    "title": "Acknowledge Alerts",
    "content": "Example request . POST /_plugins/_security_analytics/&lt;detector_id&gt;/_acknowledge/alerts {\"alerts\":[\"4dc7f5a9-2c82-4786-81ca-433a209d5205\"]} . Example response . { \"acknowledged\": [ { \"detector_id\": \"8YT5fYQBZ8IUM4axics6\", \"id\": \"4dc7f5a9-2c82-4786-81ca-433a209d5205\", \"version\": 1, \"schema_version\": 4, \"trigger_id\": \"1TP5fYQBMkkIGY6Pg-q8\", \"trigger_name\": \"test-trigger\", \"finding_ids\": [ \"2e167f4b-8063-40ef-80f8-2afd9bf095b8\" ], \"related_doc_ids\": [ \"1|windows\" ], \"state\": \"ACTIVE\", \"error_message\": null, \"alert_history\": [], \"severity\": \"1\", \"action_execution_results\": [ { \"action_id\": \"BopdoIJKXd\", \"last_execution_time\": 1668560817925, \"throttled_count\": 0 } ], \"start_time\": \"2022-11-16T01:06:57.748Z\", \"last_notification_time\": \"2022-11-16T01:06:57.748Z\", \"end_time\": null, \"acknowledged_time\": null } ], \"failed\": [], \"missing\": [] } . ",
    "url": "https://vagimeli.github.io/security-analytics/api-tools/alert-finding-api/#acknowledge-alerts",
    "relUrl": "/security-analytics/api-tools/alert-finding-api/#acknowledge-alerts"
  },"735": {
    "doc": "Alerts and findings APIs",
    "title": "Get Findings",
    "content": "The Get findings API based on detector attributes. Example request . GET /_plugins/_security_analytics/findings/_search?*detectorType*= { \"total_findings\":2, \"findings\":[ { \"detectorId\":\"12345\", \"id\":\"2b9663f4-ae77-4df8-b84f-688a0195723b\", \"related_doc_ids\":[ \"5\" ], \"index\":\"sbwhrzgdlg\", \"queries\":[ { \"id\":\"f1bff160-587b-4500-b60c-ab22c7abc652\", \"name\":\"3\", \"query\":\"test_field:\\\"us-west-2\\\"\", \"tags\":[ ] } ], \"timestamp\":1664401088804, \"document_list\":[ { \"index\":\"sbwhrzgdlg\", \"id\":\"5\", \"found\":true, \"document\":\"{\\n \\\"message\\\" : \\\"This is an error from IAD region\\\",\\n \\\"test_strict_date_time\\\" : \\\"2022-09-28T21:38:02.888Z\\\",\\n \\\"test_field\\\" : \\\"us-west-2\\\"\\n }\" } ] }, { \"detectorId\":\"12345\", \"id\":\"f43a2701-0ef5-4931-8254-bdf510f73952\", \"related_doc_ids\":[ \"1\" ], \"index\":\"sbwhrzgdlg\", \"queries\":[ { \"id\":\"f1bff160-587b-4500-b60c-ab22c7abc652\", \"name\":\"3\", \"query\":\"test_field:\\\"us-west-2\\\"\", \"tags\":[ ] } ], \"timestamp\":1664401088746, \"document_list\":[ { \"index\":\"sbwhrzgdlg\", \"id\":\"1\", \"found\":true, \"document\":\"{\\n \\\"message\\\" : \\\"This is an error from IAD region\\\",\\n \\\"test_strict_date_time\\\" : \\\"2022-09-28T21:38:02.888Z\\\",\\n \\\"test_field\\\" : \\\"us-west-2\\\"\\n }\" } ] } ] } . ",
    "url": "https://vagimeli.github.io/security-analytics/api-tools/alert-finding-api/#get-findings",
    "relUrl": "/security-analytics/api-tools/alert-finding-api/#get-findings"
  },"736": {
    "doc": "Detector APIs",
    "title": "Detector APIs",
    "content": "The following APIs can be used for a number of tasks related to detectors, from creating detectors to updating and searching for detectors. Many API calls use the detector ID in the request, which can be retrieved using the Search Detector API. ",
    "url": "https://vagimeli.github.io/security-analytics/api-tools/detector-api/",
    "relUrl": "/security-analytics/api-tools/detector-api/"
  },"737": {
    "doc": "Detector APIs",
    "title": "Create Detector API",
    "content": "Creates a new detector. POST _plugins/_security_analytics/detectors . Request fields . You can specify the following fields when creating a detector. | Field | Type | Description | . | enabled | Boolean | Sets the detector as either active (true) or inactive (false). Default is true when a new detector is created. Required. | . | name | String | Name of the detector. Name should only consist of upper and lowercase letters, numbers 0-9, hyphens, spaces, and underscores. Use between 5 and 50 characters. Required. | . | detector_type | String | The log type that defines the detector. Options are linux, network ,windows, ad_ldap, apache_access, cloudtrail, dns, and s3. Required. | . | schedule | Object | The schedule that determines how often the detector runs. For information on specifying fixed intervals in the API, see Cron expression reference. | . | schedule.period | Object | Details for the frequency of the schedule. | . | schedule.period.interval | Integer | The interval at which the detector runs. | . | schedule.period.unit | String | The interval’s unit of time. | . | inputs | Object | Detector inputs. | . | inputs.detector_input | Array | An array that contains the index and definition used to create the detector. Currently, only one log data source is allowed for the detector. | . | inputs.detector_input.description | String | Description of the detector. Optional. | . | inputs.detector_input.custom_rules | Array | Detector inputs for custom rules. At least one rule must be specified for a detector. Optional if pre-packaged rules are specified. | . | inputs.detector_input.custom_rules.id | String | A valid rule ID for the custom rule generated by the user. Valid rules are formatted as a globally, or, universally unique identifier (UUID) See Universally unique identifier for more information. | . | inputs.detector_input.indices | Array | The log data source used for the detector, which can be either an index name or index pattern. Currently, only one entry is supported with plans to support multiple indexes in a future release. Required. | . | inputs.detector_input.pre_packaged_rules | Array | Detector inputs for pre-packaged rules (as opposed to custom rules). At least one rule must be specified for a detector. Optional if custom rules are specified. | . | inputs.detector_input.pre_packaged_rules.id | String | The rule ID for pre-packaged rules. See Search Pre-Packaged Rules for information on how to use the API to search for rules and obtain rule IDs in results. | . | triggers | Array | Trigger settings for alerts. | . | triggers.ids | Array | A list of rule IDs that become part of the trigger condition. | . | triggers.tags | Array | Tags are specified in a security rule. Tags can then be selected and applied to the alert trigger to focus the trigger conditions for alerts. See an example of how tags are used in a Sigma rule in Sigma’s Rule Creation Guide. | . | triggers.id | String | The unique ID for the trigger. | . | triggers.sev_levels | Array | Sigma rule severity levels: informational; low; medium; high; criticial. See Level in the Sigma Rule Creation Guide. | . | triggers.name | String | The name of the trigger. Name should only consist of upper and lowercase letters, numbers 0-9, hyphens, spaces, and underscores. Use between 5 and 50 characters. Required. | . | triggers.severity | Integer | Severity level for the trigger expressed as an integer: 1 = highest; 2 = high; 3 = medium; 4 = low; 5 = lowest. Trigger severity is part of the alert definition. | . | triggers.actions | Object | Actions send notifications when trigger conditions are met. Optional, as a notification message is not required as part of an alert. | . | triggers.actions.id | String | Unique ID for the action. User generated. | . | triggers.actions.destination_id | String | Unique ID for the notification destination. User generated. | . | triggers.actions.subject_template | Object | Contains the information for the subject field of the notification message. Optional. | . | triggers.actions.subject_template.source | String | The subject for the notification message. | . | triggers.actions.subject_template.lang | String | The scripting language used to define the subject. Must be Mustache. See the Mustache Manual for more information about templates. | . | triggers.actions.name | String | Name for the trigger alert. Name should only consist of upper and lowercase letters, numbers 0-9, hyphens, spaces, and underscores. Use between 5 and 50 characters. | . | triggers.actions.message_template | String | Contains the information for the body of the notification message. Optional. | . | triggers.actions.message_template.source | String | The body of the notification message. | . | triggers.actions.message_template.lang | String | The scripting language used to define the message. Must be Mustache. | . | triggers.actions.throttle_enabled | Boolean | Enables throttling for alert notifications. Optional. Default is false. | . | triggers.actions.throttle | Object | Throttling limits the number of notifications you receive within a given span of time. | . | triggers.actions.throttle.unit | String | Unit of time for throttling. | . | triggers.actions.throttle.value | Integer | The value for the unit of time. | . Example request . POST _plugins/_security_analytics/detectors { \"enabled\": true, \"schedule\": { \"period\": { \"interval\": 1, \"unit\": \"MINUTES\" } }, \"detector_type\": \"WINDOWS\", \"type\": \"detector\", \"inputs\": [ { \"detector_input\": { \"description\": \"windows detector for security analytics\", \"custom_rules\": [ { \"id\": \"bc2RB4QBrbtylUb_1Pbm\" } ], \"indices\": [ \"windows\" ], \"pre_packaged_rules\": [ { \"id\": \"06724a9a-52fc-11ed-bdc3-0242ac120002\" } ] } } ], \"triggers\": [ { \"ids\": [ \"06724a9a-52fc-11ed-bdc3-0242ac120002\" ], \"types\": [], \"tags\": [ \"attack.defense_evasion\" ], \"severity\": \"1\", \"actions\": [{ \"id\": \"hVTLkZYzlA\", \"destination_id\": \"6r8ZBoQBKW_6dKriacQb\", \"subject_template\": { \"source\": \"Trigger: \", \"lang\": \"mustache\" }, \"name\": \"hello_world\", \"throttle_enabled\": false, \"message_template\": { \"source\": \"Detector just entered alert status. Please investigate the issue.\" + \"- Trigger: \" + \"- Severity: \", \"lang\": \"mustache\" }, \"throttle\": { \"unit\": \"MINUTES\", \"value\": 108 } } ], \"id\": \"8qhrBoQBYK1JzUUDzH-N\", \"sev_levels\": [], \"name\": \"test-trigger\" } ], \"name\": \"nbReFCjlfn\" } . copy . Example response . { \"_id\": \"dc2VB4QBrbtylUb_Hfa3\", \"_version\": 1, \"detector\": { \"name\": \"nbReFCjlfn\", \"detector_type\": \"windows\", \"enabled\": true, \"schedule\": { \"period\": { \"interval\": 1, \"unit\": \"MINUTES\" } }, \"inputs\": [ { \"detector_input\": { \"description\": \"windows detector for security analytics\", \"indices\": [ \"windows\" ], \"custom_rules\": [ { \"id\": \"bc2RB4QBrbtylUb_1Pbm\" } ], \"pre_packaged_rules\": [ { \"id\": \"06724a9a-52fc-11ed-bdc3-0242ac120002\" } ] } } ], \"triggers\": [ { \"id\": \"8qhrBoQBYK1JzUUDzH-N\", \"name\": \"test-trigger\", \"severity\": \"1\", \"types\": [], \"ids\": [ \"06724a9a-52fc-11ed-bdc3-0242ac120002\" ], \"sev_levels\": [], \"tags\": [ \"attack.defense_evasion\" ], \"actions\": [ { \"id\": \"hVTLkZYzlA\", \"name\": \"hello_world\", \"destination_id\": \"6r8ZBoQBKW_6dKriacQb\", \"message_template\": { \"source\": \"Trigger: \", \"lang\": \"mustache\" }, \"throttle_enabled\": false, \"subject_template\": { \"source\": \"Detector just entered alert status. Please investigate the issue.\" + \"- Trigger: \" + \"- Severity: \", \"lang\": \"mustache\" }, \"throttle\": { \"value\": 108, \"unit\": \"MINUTES\" } } ] } ], \"last_update_time\": \"2022-10-24T01:22:03.738379671Z\", \"enabled_time\": \"2022-10-24T01:22:03.738376103Z\" } } . ",
    "url": "https://vagimeli.github.io/security-analytics/api-tools/detector-api/#create-detector-api",
    "relUrl": "/security-analytics/api-tools/detector-api/#create-detector-api"
  },"738": {
    "doc": "Detector APIs",
    "title": "Update Detector API",
    "content": "The Update Detector API can be used to update a detector definition. It requires the detector ID to specify the detector. PUT /_plugins/_security_analytics/detectors/&lt;detector_Id&gt; . Request fields . You can specify the following fields when updating a detector. | Field | Type | Description | . | detector_type | String | The log type that defines the detector. Options are linux, network ,windows, ad_ldap, apache_access, cloudtrail, dns, and s3. | . | name | String | The name of the detector. Name should only consist of upper and lowercase letters, numbers 0-9, hyphens, spaces, and underscores. Use between 5 and 50 characters. Required. | . | enabled | Boolean | Sets the detector as either Active (true) or Inactive (false). | . | schedule.period.interval | Integer | The interval at which the detector runs. | . | schedule.period.unit | String | The interval’s unit of time. | . | inputs.input.description | String | Description of the detector. | . | inputs.input.indices | Array | The log data source used for the detector. Only one source is allowed at this time. | . | inputs.input.rules.id | Array | A list of security rules for the detector definition. | . | triggers.sev_levels | Array | Sigma rule severity levels: informational; low; medium; high; criticial. See Level in the Sigma Rule Creation Guide. | . | triggers.tags | Array | Tags are specified in a security rule. Tags can then be selected and applied to the alert trigger to focus the trigger conditions for alerts. See an example of how tags are used in a Sigma rule in Sigma’s Rule Creation Guide. | . | triggers.actions | Object | Actions send notifications when trigger conditions are met. See trigger actions for Create Detector API. | . Example request . PUT /_plugins/_security_analytics/detectors/J1RX1IMByX0LvTiGTddR { \"type\": \"detector\", \"detector_type\": \"windows\", \"name\": \"windows_detector\", \"enabled\": true, \"createdBy\": \"chip\", \"schedule\": { \"period\": { \"interval\": 1, \"unit\": \"MINUTES\" } }, \"inputs\": [ { \"input\": { \"description\": \"windows detector for security analytics\", \"indices\": [ \"windows\" ], \"custom_rules\": [], \"pre_packaged_rules\": [ { \"id\": \"73a883d0-0348-4be4-a8d8-51031c2564f8\" }, { \"id\": \"1a4bd6e3-4c6e-405d-a9a3-53a116e341d4\" } ] } } ], \"triggers\": [ { \"sev_levels\": [], \"tags\": [], \"actions\": [], \"types\": [ \"windows\" ], \"name\": \"test-trigger\", \"id\": \"fyAy1IMBK2A1DZyOuW_b\" } ] } . copy . Example response . { \"_id\": \"J1RX1IMByX0LvTiGTddR\", \"_version\": 1, \"detector\": { \"name\": \"windows_detector\", \"detector_type\": \"windows\", \"enabled\": true, \"schedule\": { \"period\": { \"interval\": 1, \"unit\": \"MINUTES\" } }, \"inputs\": [ { \"detector_input\": { \"description\": \"windows detector for security analytics\", \"indices\": [ \"windows\" ], \"rules\": [ { \"id\": \"LFRY1IMByX0LvTiGZtfh\" } ] } } ], \"triggers\": [], \"last_update_time\": \"2022-10-14T02:36:32.909581688Z\", \"enabled_time\": \"2022-10-14T02:33:34.197Z\" } } . Response fields . | Field | Type | Description | . | _version | String | Version number for the update. | . | detector.last_update_time | String | Date and time of the last update. | . | detector.enabled_time | String | Date and time when the detector was last enabled. | . ",
    "url": "https://vagimeli.github.io/security-analytics/api-tools/detector-api/#update-detector-api",
    "relUrl": "/security-analytics/api-tools/detector-api/#update-detector-api"
  },"739": {
    "doc": "Detector APIs",
    "title": "Delete Detector API",
    "content": "This API uses the detector ID to specify and delete a detector. Path and HTTP methods . DELETE /_plugins/_security_analytics/detectors/IJAXz4QBrmVplM4JYxx_ . Example request . DELETE /_plugins/_security_analytics/detectors/&lt;detector Id&gt; . copy . Example response . { \"_id\" : \"IJAXz4QBrmVplM4JYxx_\", \"_version\" : 1 } . ",
    "url": "https://vagimeli.github.io/security-analytics/api-tools/detector-api/#delete-detector-api",
    "relUrl": "/security-analytics/api-tools/detector-api/#delete-detector-api"
  },"740": {
    "doc": "Detector APIs",
    "title": "Get Detector API",
    "content": "The Get Detector API retrieves the detector details. Use the detector ID in the call to fetch detector details. Path and HTTP methods . GET /_plugins/_security_analytics/detectors/x-dwFIYBT6_n8WeuQjo4 . Example request . GET /_plugins/_security_analytics/detectors/&lt;detector Id&gt; . copy . Example response . { \"_id\" : \"x-dwFIYBT6_n8WeuQjo4\", \"_version\" : 1, \"detector\" : { \"name\" : \"DetectorTest1\", \"detector_type\" : \"windows\", \"enabled\" : true, \"schedule\" : { \"period\" : { \"interval\" : 1, \"unit\" : \"MINUTES\" } }, \"inputs\" : [ { \"detector_input\" : { \"description\" : \"Test and delete\", \"indices\" : [ \"windows1\" ], \"custom_rules\" : [ ], \"pre_packaged_rules\" : [ { \"id\" : \"847def9e-924d-4e90-b7c4-5f581395a2b4\" } ] } } ], \"last_update_time\" : \"2023-02-02T23:22:26.454Z\", \"enabled_time\" : \"2023-02-02T23:22:26.454Z\" } } . ",
    "url": "https://vagimeli.github.io/security-analytics/api-tools/detector-api/#get-detector-api",
    "relUrl": "/security-analytics/api-tools/detector-api/#get-detector-api"
  },"741": {
    "doc": "Detector APIs",
    "title": "Search Detector API",
    "content": "The Search Detector API searches for detector matches by detector ID, detector name, or detector type. Request fields . | Field | Type | Description | . | _id | String | Version number for the update. | . | detector.name | String | Name of the detector. | . | detector_type | String | The log type for the detector. Options are linux, network ,windows, ad_ldap, apache_access, cloudtrail, dns, and s3. | . Example request . Detector ID . POST /_plugins/_security_analytics/detectors/_search { \"query\": { \"match\": { \"_id\": \"MFRg1IMByX0LvTiGHtcN\" } } } . copy . Detector name . POST /_plugins/_security_analytics/detectors/_search { \"size\": 30, \"query\": { \"nested\": { \"path\": \"detector\", \"query\": { \"bool\": { \"must\": [ { \"match\": {\"detector.name\": \"DetectorTest1\"} } ] } } } } } . copy . Example response . { \"took\" : 0, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 1, \"relation\" : \"eq\" }, \"max_score\" : 3.671739, \"hits\" : [ { \"_index\" : \".opensearch-sap-detectors-config\", \"_id\" : \"x-dwFIYBT6_n8WeuQjo4\", \"_version\" : 1, \"_seq_no\" : 76, \"_primary_term\" : 17, \"_score\" : 3.671739, \"_source\" : { \"type\" : \"detector\", \"name\" : \"DetectorTest1\", \"detector_type\" : \"windows\", \"enabled\" : true, \"enabled_time\" : 1675380146454, \"schedule\" : { \"period\" : { \"interval\" : 1, \"unit\" : \"MINUTES\" } }, \"inputs\" : [ { \"detector_input\" : { \"description\" : \"Test and delete\", \"indices\" : [ \"windows1\" ], \"custom_rules\" : [ ], \"pre_packaged_rules\" : [ { \"id\" : \"847def9e-924d-4e90-b7c4-5f581395a2b4\" } ] } } ], \"triggers\" : [ { \"id\" : \"w-dwFIYBT6_n8WeuQToW\", \"name\" : \"trigger 1\", \"severity\" : \"1\", \"types\" : [ \"windows\" ], \"ids\" : [ \"847def9e-924d-4e90-b7c4-5f581395a2b4\" ], \"sev_levels\" : [ \"critical\" ], \"tags\" : [ \"attack.t1003.002\" ], \"actions\" : [ { \"id\" : \"\", \"name\" : \"Triggered alert condition: - Severity: 1 (Highest) - Threat detector: DetectorTest1\", \"destination_id\" : \"\", \"message_template\" : { \"source\" : \"\"\"Triggered alert condition: Severity: 1 (Highest) Threat detector: DetectorTest1 Description: Test and delete Detector data sources: windows1\"\"\", \"lang\" : \"mustache\" }, \"throttle_enabled\" : false, \"subject_template\" : { \"source\" : \"Triggered alert condition: - Severity: 1 (Highest) - Threat detector: DetectorTest1\", \"lang\" : \"mustache\" }, \"throttle\" : { \"value\" : 10, \"unit\" : \"MINUTES\" } } ] } ], \"last_update_time\" : 1675380146454, \"monitor_id\" : [ \"xOdwFIYBT6_n8WeuQToa\" ], \"bucket_monitor_id_rule_id\" : { \"-1\" : \"xOdwFIYBT6_n8WeuQToa\" }, \"rule_topic_index\" : \".opensearch-sap-windows-detectors-queries\", \"alert_index\" : \".opensearch-sap-windows-alerts\", \"alert_history_index\" : \".opensearch-sap-windows-alerts-history\", \"alert_history_index_pattern\" : \"&lt;.opensearch-sap-windows-alerts-history-{now/d}-1&gt;\", \"findings_index\" : \".opensearch-sap-windows-findings\", \"findings_index_pattern\" : \"&lt;.opensearch-sap-windows-findings-{now/d}-1&gt;\" } } ] } } . ",
    "url": "https://vagimeli.github.io/security-analytics/api-tools/detector-api/#search-detector-api",
    "relUrl": "/security-analytics/api-tools/detector-api/#search-detector-api"
  },"742": {
    "doc": "API tools",
    "title": "API tools",
    "content": "Security Analytics includes a number of APIs to help administrators maintain and update an implementation. The APIs often mimic the same controls available for setting up Security Analytics in OpenSearch Dashboards, and they provide another option for administering the plugin. The APIs for Security Analytics are separated into the following categories: . | Detector APIs | Rules APIs | Mappings APIs | Alerts and findings APIs | . ",
    "url": "https://vagimeli.github.io/security-analytics/api-tools/index/",
    "relUrl": "/security-analytics/api-tools/index/"
  },"743": {
    "doc": "Mappings APIs",
    "title": "Mappings APIs",
    "content": "The following APIs can be used for a number of tasks related to mappings, from creating to getting and updating mappings. ",
    "url": "https://vagimeli.github.io/security-analytics/api-tools/mappings-api/",
    "relUrl": "/security-analytics/api-tools/mappings-api/"
  },"744": {
    "doc": "Mappings APIs",
    "title": "Get Mappings View",
    "content": "Example request . GET /_plugins/_security_analytics/mappings/view { \"index_name\": \"windows\", \"rule_topic\": \"windows\" } . Example response . { \"properties\": { \"windows-event_data-CommandLine\": { \"path\": \"CommandLine\", \"type\": \"alias\" }, \"event_uid\": { \"path\": \"EventID\", \"type\": \"alias\" } }, \"unmapped_index_fields\": [ \"windows-event_data-CommandLine\", \"unmapped_HiveName\", \"src_ip\", \"sha1\", \"processPath\", \"CallerProcessName\", \"CallTrace\", \"AuthenticationPackageName\", \"AuditSourceName\", \"AuditPolicyChanges\", \"AttributeValue\", \"AttributeLDAPDisplayName\", \"ApplicationPath\", \"Application\", \"AllowedToDelegateTo\", \"Address\", \"Action\", \"AccountType\", \"AccountName\", \"Accesses\", \"AccessMask\", \"AccessList\" ] } . ",
    "url": "https://vagimeli.github.io/security-analytics/api-tools/mappings-api/#get-mappings-view",
    "relUrl": "/security-analytics/api-tools/mappings-api/#get-mappings-view"
  },"745": {
    "doc": "Mappings APIs",
    "title": "Create Mappings",
    "content": "Example request . POST /_plugins/_security_analytics/mappings { \"index_name\": \"windows\", \"rule_topic\": \"windows\", \"partial\": true, \"alias_mappings\": { \"properties\": { \"event_uid\": { \"type\": \"alias\", \"path\": \"EventID\" } } } } . Example response . { \"acknowledged\": true } . ",
    "url": "https://vagimeli.github.io/security-analytics/api-tools/mappings-api/#create-mappings",
    "relUrl": "/security-analytics/api-tools/mappings-api/#create-mappings"
  },"746": {
    "doc": "Mappings APIs",
    "title": "Get Mappings",
    "content": "Example request . GET /_plugins/_security_analytics/mappings . Example response . { \"windows\": { \"mappings\": { \"properties\": { \"windows-event_data-CommandLine\": { \"type\": \"alias\", \"path\": \"CommandLine\" }, \"event_uid\": { \"type\": \"alias\", \"path\": \"EventID\" } } } } } . ",
    "url": "https://vagimeli.github.io/security-analytics/api-tools/mappings-api/#get-mappings",
    "relUrl": "/security-analytics/api-tools/mappings-api/#get-mappings"
  },"747": {
    "doc": "Mappings APIs",
    "title": "Update Mappings",
    "content": "Example request . PUT /_plugins/_security_analytics/mappings { \"index_name\": \"windows\", \"field\": \"CommandLine\", \"alias\": \"windows-event_data-CommandLine\" } . Example response . { \"acknowledged\": true } . ",
    "url": "https://vagimeli.github.io/security-analytics/api-tools/mappings-api/#update-mappings",
    "relUrl": "/security-analytics/api-tools/mappings-api/#update-mappings"
  },"748": {
    "doc": "Rule APIs",
    "title": "Rule APIs",
    "content": "The following APIs can be used for a number of tasks related to rules, from searching for pre-packaged rules to creating and updating custom rules. ",
    "url": "https://vagimeli.github.io/security-analytics/api-tools/rule-api/",
    "relUrl": "/security-analytics/api-tools/rule-api/"
  },"749": {
    "doc": "Rule APIs",
    "title": "Create Custom Rule",
    "content": "The Create custom rule API uses Sigma security rule formatting to create a custom rule. For information on how to write a rule in Sigma format, see information provided at Sigma’s GitHub repository. POST /_plugins/_security_analytics/rules?category=windows . Example request . Header: Content-Type: application/json Body: title: Moriya Rootkit id: 25b9c01c-350d-4b95-bed1-836d04a4f324 description: Detects the use of Moriya rootkit as described in the securelist's Operation TunnelSnake report status: experimental author: Bhabesh Raj date: 2021/05/06 modified: 2021/11/30 references: - https://securelist.com/operation-tunnelsnake-and-moriya-rootkit/101831 tags: - attack.persistence - attack.privilege_escalation - attack.t1543.003 logsource: product: windows service: system detection: selection: Provider_Name: 'Service Control Manager' EventID: 7045 ServiceName: ZzNetSvc condition: selection level: critical falsepositives: - Unknown . Example response . Sample 1: . { \"_id\": \"M1Rm1IMByX0LvTiGvde2\", \"_version\": 1, \"rule\": { \"category\": \"windows\", \"title\": \"Moriya Rootkit\", \"log_source\": \"\", \"description\": \"Detects the use of Moriya rootkit as described in the securelist's Operation TunnelSnake report\", \"tags\": [ { \"value\": \"attack.persistence\" }, { \"value\": \"attack.privilege_escalation\" }, { \"value\": \"attack.t1543.003\" } ], \"references\": [ { \"value\": \"https://securelist.com/operation-tunnelsnake-and-moriya-rootkit/101831\" } ], \"level\": \"critical\", \"false_positives\": [ { \"value\": \"Unknown\" } ], \"author\": \"Bhabesh Raj\", \"status\": \"experimental\", \"last_update_time\": \"2021-05-06T00:00:00.000Z\", \"rule\": \"title: Moriya Rootkit\\nid: 25b9c01c-350d-4b95-bed1-836d04a4f324\\ndescription: Detects the use of Moriya rootkit as described in the securelist's Operation TunnelSnake report\\nstatus: experimental\\nauthor: Bhabesh Raj\\ndate: 2021/05/06\\nmodified: 2021/11/30\\nreferences:\\n - https://securelist.com/operation-tunnelsnake-and-moriya-rootkit/101831\\ntags:\\n - attack.persistence\\n - attack.privilege_escalation\\n - attack.t1543.003\\nlogsource:\\n product: windows\\n service: system\\ndetection:\\n selection:\\n Provider_Name: 'Service Control Manager'\\n EventID: 7045\\n ServiceName: ZzNetSvc\\n condition: selection\\nlevel: critical\\nfalsepositives:\\n - Unknown\" } } . Sample 2: . { \"error\": { \"root_cause\": [ { \"type\": \"security_analytics_exception\", \"reason\": \"{\\\"error\\\":\\\"Sigma rule must have a log source\\\",\\\"error\\\":\\\"Sigma rule must have a detection definitions\\\"}\" } ], \"type\": \"security_analytics_exception\", \"reason\": \"{\\\"error\\\":\\\"Sigma rule must have a log source\\\",\\\"error\\\":\\\"Sigma rule must have a detection definitions\\\"}\", \"caused_by\": { \"type\": \"exception\", \"reason\": \"java.util.Arrays$ArrayList: {\\\"error\\\":\\\"Sigma rule must have a log source\\\",\\\"error\\\":\\\"Sigma rule must have a detection definitions\\\"}\" } }, \"status\": 400 } . ",
    "url": "https://vagimeli.github.io/security-analytics/api-tools/rule-api/#create-custom-rule",
    "relUrl": "/security-analytics/api-tools/rule-api/#create-custom-rule"
  },"750": {
    "doc": "Rule APIs",
    "title": "Update Custom Rule (not forced)",
    "content": "Example request . PUT /_plugins/_security_analytics/rules/ZaFv1IMBdLpXWBiBa1XI?category=windows Content-Type: application/json Body: title: Moriya Rooskit id: 25b9c01c-350d-4b95-bed1-836d04a4f324 description: Detects the use of Moriya rootkit as described in the securelist's Operation TunnelSnake report status: experimental author: Bhabesh Raj date: 2021/05/06 modified: 2021/11/30 references: - https://securelist.com/operation-tunnelsnake-and-moriya-rootkit/101831 tags: - attack.persistence - attack.privilege_escalation - attack.t1543.003 logsource: product: windows service: system detection: selection: Provider_Name: 'Service Control Manager' EventID: 7045 ServiceName: ZzNetSvc condition: selection level: critical falsepositives: - Unknown . Example response . { \"error\": { \"root_cause\": [ { \"type\": \"security_analytics_exception\", \"reason\": \"Rule with id ZaFv1IMBdLpXWBiBa1XI is actively used by detectors. Update can be forced by setting forced flag to true\" } ], \"type\": \"security_analytics_exception\", \"reason\": \"Rule with id ZaFv1IMBdLpXWBiBa1XI is actively used by detectors. Update can be forced by setting forced flag to true\", \"caused_by\": { \"type\": \"exception\", \"reason\": \"org.opensearch.OpenSearchStatusException: Rule with id ZaFv1IMBdLpXWBiBa1XI is actively used by detectors. Update can be forced by setting forced flag to true\" } }, \"status\": 500 } . ",
    "url": "https://vagimeli.github.io/security-analytics/api-tools/rule-api/#update-custom-rule-not-forced",
    "relUrl": "/security-analytics/api-tools/rule-api/#update-custom-rule-not-forced"
  },"751": {
    "doc": "Rule APIs",
    "title": "Update Custom Rule (forced)",
    "content": "Example request . PUT /_plugins/_security_analytics/rules/ZaFv1IMBdLpXWBiBa1XI?category=windows&amp;forced=true Content-Type: application/json Body: title: Moriya Rooskit id: 25b9c01c-350d-4b95-bed1-836d04a4f324 description: Detects the use of Moriya rootkit as described in the securelist's Operation TunnelSnake report status: experimental author: Bhabesh Raj date: 2021/05/06 modified: 2021/11/30 references: - https://securelist.com/operation-tunnelsnake-and-moriya-rootkit/101831 tags: - attack.persistence - attack.privilege_escalation - attack.t1543.003 logsource: product: windows service: system detection: selection: Provider_Name: 'Service Control Manager' EventID: 7045 ServiceName: ZzNetSvc condition: selection level: critical falsepositives: - Unknown . Example response . { \"_id\": \"ZaFv1IMBdLpXWBiBa1XI\", \"_version\": 1, \"rule\": { \"category\": \"windows\", \"title\": \"Moriya Rooskit\", \"log_source\": \"\", \"description\": \"Detects the use of Moriya rootkit as described in the securelist's Operation TunnelSnake report\", \"tags\": [ { \"value\": \"attack.persistence\" }, { \"value\": \"attack.privilege_escalation\" }, { \"value\": \"attack.t1543.003\" } ], \"references\": [ { \"value\": \"https://securelist.com/operation-tunnelsnake-and-moriya-rootkit/101831\" } ], \"level\": \"critical\", \"false_positives\": [ { \"value\": \"Unknown\" } ], \"author\": \"Bhabesh Raj\", \"status\": \"experimental\", \"last_update_time\": \"2021-05-06T00:00:00.000Z\", \"rule\": \"title: Moriya Rooskit\\nid: 25b9c01c-350d-4b95-bed1-836d04a4f324\\ndescription: Detects the use of Moriya rootkit as described in the securelist's Operation TunnelSnake report\\nstatus: experimental\\nauthor: Bhabesh Raj\\ndate: 2021/05/06\\nmodified: 2021/11/30\\nreferences:\\n - https://securelist.com/operation-tunnelsnake-and-moriya-rootkit/101831\\ntags:\\n - attack.persistence\\n - attack.privilege_escalation\\n - attack.t1543.003\\nlogsource:\\n product: windows\\n service: system\\ndetection:\\n selection:\\n Provider_Name: 'Service Control Manager'\\n EventID: 7045\\n ServiceName: ZzNetSvc\\n condition: selection\\nlevel: critical\\nfalsepositives:\\n - Unknown\" } } . ",
    "url": "https://vagimeli.github.io/security-analytics/api-tools/rule-api/#update-custom-rule-forced",
    "relUrl": "/security-analytics/api-tools/rule-api/#update-custom-rule-forced"
  },"752": {
    "doc": "Rule APIs",
    "title": "Search Pre-Packaged Rules",
    "content": "Example request . POST /_plugins/_security_analytics/rules/_search?pre_packaged=true { \"from\": 0, \"size\": 20, \"query\": { \"nested\": { \"path\": \"rule\", \"query\": { \"bool\": { \"must\": [ { \"match\": { \"rule.category\": \"windows\" } } ] } } } } } . Example response . { \"took\": 3, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 1580, \"relation\": \"eq\" }, \"max_score\": 0.25863406, \"hits\": [ { \"_index\": \".opensearch-pre-packaged-rules-config\", \"_id\": \"6KFv1IMBdLpXWBiBelZg\", \"_version\": 1, \"_seq_no\": 386, \"_primary_term\": 1, \"_score\": 0.25863406, \"_source\": { \"category\": \"windows\", \"title\": \"Change Outlook Security Setting in Registry\", \"log_source\": \"registry_set\", \"description\": \"Change outlook email security settings\", \"references\": [ { \"value\": \"https://github.com/redcanaryco/atomic-red-team/blob/master/atomics/T1137/T1137.md\" }, { \"value\": \"https://docs.microsoft.com/en-us/outlook/troubleshoot/security/information-about-email-security-settings\" } ], \"tags\": [ { \"value\": \"attack.persistence\" }, { \"value\": \"attack.t1137\" } ], \"level\": \"medium\", \"false_positives\": [ { \"value\": \"Administrative scripts\" } ], \"author\": \"frack113\", \"status\": \"experimental\", \"last_update_time\": \"2021-12-28T00:00:00.000Z\", \"queries\": [ { \"value\": \"((TargetObject: *\\\\\\\\SOFTWARE\\\\\\\\Microsoft\\\\\\\\Office\\\\\\\\*) AND (TargetObject: *\\\\\\\\Outlook\\\\\\\\Security\\\\\\\\*)) AND (EventType: \\\"SetValue\\\")\" } ], \"rule\": \"title: Change Outlook Security Setting in Registry\\nid: c3cefdf4-6703-4e1c-bad8-bf422fc5015a\\ndescription: Change outlook email security settings\\nauthor: frack113\\ndate: 2021/12/28\\nmodified: 2022/03/26\\nstatus: experimental\\nreferences:\\n - https://github.com/redcanaryco/atomic-red-team/blob/master/atomics/T1137/T1137.md\\n - https://docs.microsoft.com/en-us/outlook/troubleshoot/security/information-about-email-security-settings\\nlogsource:\\n category: registry_set\\n product: windows\\ndetection:\\n selection:\\n TargetObject|contains|all:\\n - '\\\\SOFTWARE\\\\Microsoft\\\\Office\\\\'\\n - '\\\\Outlook\\\\Security\\\\'\\n EventType: SetValue\\n condition: selection\\nfalsepositives:\\n - Administrative scripts\\nlevel: medium\\ntags:\\n - attack.persistence\\n - attack.t1137\\n\" } } ] } } . ",
    "url": "https://vagimeli.github.io/security-analytics/api-tools/rule-api/#search-pre-packaged-rules",
    "relUrl": "/security-analytics/api-tools/rule-api/#search-pre-packaged-rules"
  },"753": {
    "doc": "Rule APIs",
    "title": "Search Custom Rules",
    "content": "Example request . POST /_plugins/_security_analytics/rules/_search?pre_packaged=false Body: { \"from\": 0, \"size\": 20, \"query\": { \"nested\": { \"path\": \"rule\", \"query\": { \"bool\": { \"must\": [ { \"match\": { \"rule.category\": \"windows\" } } ] } } } } } . Example response . { \"took\": 1, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 1, \"relation\": \"eq\" }, \"max_score\": 0.2876821, \"hits\": [ { \"_index\": \".opensearch-custom-rules-config\", \"_id\": \"ZaFv1IMBdLpXWBiBa1XI\", \"_version\": 2, \"_seq_no\": 1, \"_primary_term\": 1, \"_score\": 0.2876821, \"_source\": { \"category\": \"windows\", \"title\": \"Moriya Rooskit\", \"log_source\": \"\", \"description\": \"Detects the use of Moriya rootkit as described in the securelist's Operation TunnelSnake report\", \"references\": [ { \"value\": \"https://securelist.com/operation-tunnelsnake-and-moriya-rootkit/101831\" } ], \"tags\": [ { \"value\": \"attack.persistence\" }, { \"value\": \"attack.privilege_escalation\" }, { \"value\": \"attack.t1543.003\" } ], \"level\": \"critical\", \"false_positives\": [ { \"value\": \"Unknown\" } ], \"author\": \"Bhabesh Raj\", \"status\": \"experimental\", \"last_update_time\": \"2021-05-06T00:00:00.000Z\", \"queries\": [ { \"value\": \"(Provider_Name: \\\"Service_ws_Control_ws_Manager\\\") AND (event_uid: 7045) AND (ServiceName: \\\"ZzNetSvc\\\")\" } ], \"rule\": \"title: Moriya Rooskit\\nid: 25b9c01c-350d-4b95-bed1-836d04a4f324\\ndescription: Detects the use of Moriya rootkit as described in the securelist's Operation TunnelSnake report\\nstatus: experimental\\nauthor: Bhabesh Raj\\ndate: 2021/05/06\\nmodified: 2021/11/30\\nreferences:\\n - https://securelist.com/operation-tunnelsnake-and-moriya-rootkit/101831\\ntags:\\n - attack.persistence\\n - attack.privilege_escalation\\n - attack.t1543.003\\nlogsource:\\n product: windows\\n service: system\\ndetection:\\n selection:\\n Provider_Name: 'Service Control Manager'\\n EventID: 7045\\n ServiceName: ZzNetSvc\\n condition: selection\\nlevel: critical\\nfalsepositives:\\n - Unknown\" } } ] } } . ",
    "url": "https://vagimeli.github.io/security-analytics/api-tools/rule-api/#search-custom-rules",
    "relUrl": "/security-analytics/api-tools/rule-api/#search-custom-rules"
  },"754": {
    "doc": "Rule APIs",
    "title": "Delete Custom Rule (not forced)",
    "content": "Example request . DELETE /_plugins/_security_analytics/rules/ZaFv1IMBdLpXWBiBa1XI . Example response . { \"error\": { \"root_cause\": [ { \"type\": \"security_analytics_exception\", \"reason\": \"Rule with id ZaFv1IMBdLpXWBiBa1XI is actively used by detectors. Deletion can be forced by setting forced flag to true\" } ], \"type\": \"security_analytics_exception\", \"reason\": \"Rule with id ZaFv1IMBdLpXWBiBa1XI is actively used by detectors. Deletion can be forced by setting forced flag to true\", \"caused_by\": { \"type\": \"exception\", \"reason\": \"org.opensearch.OpenSearchStatusException: Rule with id ZaFv1IMBdLpXWBiBa1XI is actively used by detectors. Deletion can be forced by setting forced flag to true\" } }, \"status\": 500 } . ",
    "url": "https://vagimeli.github.io/security-analytics/api-tools/rule-api/#delete-custom-rule-not-forced",
    "relUrl": "/security-analytics/api-tools/rule-api/#delete-custom-rule-not-forced"
  },"755": {
    "doc": "Rule APIs",
    "title": "Delete Custom Rule (forced)",
    "content": "Example request . DELETE /_plugins/_security_analytics/rules/ZaFv1IMBdLpXWBiBa1XI?forced=true . Example response . { \"_id\": \"ZaFv1IMBdLpXWBiBa1XI\", \"_version\": 1 } . ",
    "url": "https://vagimeli.github.io/security-analytics/api-tools/rule-api/#delete-custom-rule-forced",
    "relUrl": "/security-analytics/api-tools/rule-api/#delete-custom-rule-forced"
  },"756": {
    "doc": "About Security Analytics",
    "title": "About Security Analytics",
    "content": "Security Analytics is a security information and event management (SIEM) solution for OpenSearch, designed to investigate, detect, analyze, and respond to security threats that can jeopardize the success of businesses and organizations and their online operations. These threats include the potential exposure of confidential data, cyber attacks, and other adverse security events. Security Analytics provides an out-of-the-box solution that installs automatically with any OpenSearch distribution. It includes the tools and features necessary for defining detection parameters, generating alerts, and responding effectively to potential threats. Resources and information . As part of the OpenSearch Project, Security Analytics exists in the open source community and benefits from the feedback and contributions of that community. To learn more about proposals for its development, options for making contributions, and general information on the platform, see the Security Analytics repository at GitHub. If you would like to leave feedback that could help improve Security Analytics, join the discussion on the OpenSearch forum. ",
    "url": "https://vagimeli.github.io/security-analytics/index/",
    "relUrl": "/security-analytics/index/"
  },"757": {
    "doc": "About Security Analytics",
    "title": "Components and concepts",
    "content": "Security Analytics includes a number of tools and features elemental to its operation. The major components that compose the plugin are summarized in the following sections. Detectors . Detectors are core components that are configured to identify a range of cybersecurity threats corresponding to an ever-growing knowldege base of adversary tactics and techniques maintained by the MITRE ATT&amp;CK organization. Detectors use log data to evaluate events occuring in the system. They then apply a set of security rules specified for the detector and determine findings from these events. For information on configuring detectors, see Creating detectors. Log types . Log types provide the data used to evaluate events occuring in a system. OpenSearch supports several types of logs and provides out-of-the-box mappings for the most common log sources. Currently supported log sources include: . | Network events | DNS logs | Apache access logs | Windows logs | AD/LDAP logs | System logs | AWS CloudTrail logs | Amazon S3 access logs | Google Workspace logs | GitHub actions | Microsoft 365 logs | Okta events | Microsoft Azure logs | . Log types are specified during the creation of detectors, including steps for mapping log fields to the detector. Security Analytics also automatically selects an appropriate set of rules based on a specific log type and populates them for the detector. Rules . Rules, or threat detection rules, define the conditional logic applied to ingested log data that allows the system to identify an event of interest. Security Analytics uses prepackaged, open source Sigma rules as a starting point for describing relevant log events. But with their inherently flexible format and easy portability, Sigma rules provide users of Security Analytics with options for importing and customizing the rules. You can take advantage of these options using either Dashboards or the API. For information on configuring rules, see Working with rules. Findings . Findings are generated every time a detector matches a rule with a log event. Findings do not necessarily point to imminent threats within the system, but they always isolate an event of interest. Because they represent the result of a specific definition for a detector, findings include a unique combination of select rules, a log type, and a rule severity. As such, you can search for specific findings in the Findings window, and you can filter findings in the list based on severity and log type. To learn more about findings, see Working with findings. Alerts . When defining a detector, you can specify certain conditions that will trigger an alert. When an event triggers an alert, the system sends a notification to a preferred channel, such as Amazon Chime, Slack, or email. The alert can be triggered when the detector matches one or multiple rules. Further conditions can be set by rule severity and tags. You can also create a notification message with a customized subject line and message body. For information on setting up alerts, see Step 3. Set up alerts in detector creation documentation. For information on managing alerts on the Alerts window, see Working with alerts. Correlation engine . The correlation engine is an experimental feature released in OpenSearch 2.7. Therefore, we do not recommend using the feature in a production environment at this time. For updates on the progress of the correlation engine, see Security Analytics Correlation Engine on GitHub. To share ideas and provide feedback, join the Security Analytics forum. The correlation engine gives Security Analytics the ability to compare findings from different log types and draw correlations between them. This facilitates understanding of the relationships between findings from different systems in an infrastructure and increases confidence that an event is meaningful and requires attention. The correlation engine uses correlation rules to define threat scenarios involving different log types. It can then perform queries on logs to match relevant findings from those different log sources. To depict relationships between events occurring in different logs, a correlation graph provides a visual representation of findings, their connections, and the proximity of those connections. While the correlation rules define what threat scenarios to look for, the graph provides a visualization that helps you identify the relationships between different findings in a chain of security events. To learn more about defining threat scenarios for correlation rules, see Creating correlation rules. To learn more about using the correlation graph, see Working with the correlation graph. ",
    "url": "https://vagimeli.github.io/security-analytics/index/#components-and-concepts",
    "relUrl": "/security-analytics/index/#components-and-concepts"
  },"758": {
    "doc": "About Security Analytics",
    "title": "First steps",
    "content": "To get started with Security Analytics, you need to define detectors, ingest log data, generate findings, define correlation rules, and configure alerts. See Setting up Security Analytics to begin configuring the platform to meet your objectives. ",
    "url": "https://vagimeli.github.io/security-analytics/index/#first-steps",
    "relUrl": "/security-analytics/index/#first-steps"
  },"759": {
    "doc": "Creating correlation rules",
    "title": "Creating correlation rules",
    "content": "The correlation engine is an experimental feature released in OpenSearch 2.7. Therefore, we do not recommend using the feature in a production environment at this time. For updates on the progress of the correlation engine, see Security Analytics Correlation Engine on GitHub. To share ideas and provide feedback, join the Security Analytics forum. Correlation rules allow you to define threat scenarios involving multiple systems in an infrastructure by matching the signatures of threat events occurring in different log types. Once a rule contains at least two different log sources and the preferred fields and field values that define an intended threat scenario, the correlation engine can query the indexes specified in the correlation rule and identify any correlations between the findings. ",
    "url": "https://vagimeli.github.io/security-analytics/sec-analytics-config/correlation-config/",
    "relUrl": "/security-analytics/sec-analytics-config/correlation-config/"
  },"760": {
    "doc": "Creating correlation rules",
    "title": "Configuring rules",
    "content": "Having at least two data sources in the rule configuration is the basis for making connections between different systems in an infrastructure and identifying correlations. Therefore, a minimum of two queries is required for each correlation rule. However, you can include more than two queries to better define a threat scenario and look for correlations between multiple systems. Follow these steps to create a correlation rule: . | Begin by selecting Security Analytics in the OpenSearch Dashboards main menu. Then select Correlation rules from the Security Analytics menu on the left side of the screen. The Correlation rules page is displayed, as shown in the following image. | Select Create correlation rule. The Create correlation rule window opens. | In the Correlation rule details field, enter a name for the rule, as shown in the following image. | The Correlation queries field contains two dropdown lists. In the Select index dropdown list, specify an index or index pattern for the data source. In the Log type dropdown list, specify the log type associated with the index, as shown in the following image. | In the Field dropdown list, specify a log field. In the Field value text box, enter a value for the field, as shown in the following image. | To add more fields to the query, select Add field. | After configuring the first query, repeat the previous step to configure a second query. You can select Add query at the bottom of the window to add more queries for the rule, as shown in the following image. | Once the rule is complete, select Create correlation rule in the lower-right corner of the window. OpenSearch creates a new rule, the screen returns to the Correlation rules window, and the new rule appears in the table of correlation rules. | . ",
    "url": "https://vagimeli.github.io/security-analytics/sec-analytics-config/correlation-config/#configuring-rules",
    "relUrl": "/security-analytics/sec-analytics-config/correlation-config/#configuring-rules"
  },"761": {
    "doc": "Creating correlation rules",
    "title": "Setting a time window",
    "content": "The Cluster Settings API allows you to correlate findings within a set time window. For example, if your time window is three minutes, the system attempts to correlate findings defined in the threat scenario only when they occur within three minutes of one another. By default, the time window is five minutes. For more information about the Cluster Settings API, see Cluster settings. Example request . The following PUT call sets the time window to two minutes: . PUT /_cluster/settings { \"transient\": { \"plugins.security_analytics.correlation_time_window\": \"2m\" } } . copy . ",
    "url": "https://vagimeli.github.io/security-analytics/sec-analytics-config/correlation-config/#setting-a-time-window",
    "relUrl": "/security-analytics/sec-analytics-config/correlation-config/#setting-a-time-window"
  },"762": {
    "doc": "Creating correlation rules",
    "title": "Next steps",
    "content": "After creating detectors and correlation rules, you can use the correlation graph to observe the correlations between findings from different log sources. For information about working with the correlation graph, see Working with the correlation graph. ",
    "url": "https://vagimeli.github.io/security-analytics/sec-analytics-config/correlation-config/#next-steps",
    "relUrl": "/security-analytics/sec-analytics-config/correlation-config/#next-steps"
  },"763": {
    "doc": "Creating detectors",
    "title": "Creating detectors",
    "content": "Security Analytics provides the options and functionality to monitor and respond to a wide range of security threats. Detectors are the essential components that determine what to look for and how to respond to those threats. This section covers their creation and configuration. ",
    "url": "https://vagimeli.github.io/security-analytics/sec-analytics-config/detectors-config/",
    "relUrl": "/security-analytics/sec-analytics-config/detectors-config/"
  },"764": {
    "doc": "Creating detectors",
    "title": "Step 1. Define a detector",
    "content": "You can define a new detector by naming the detector, selecting a data source and detector type, and specifying a detector schedule. After defining a detector, you can also configure field mappings and set up alerts. Follow the steps in this section to accomplish all three of these setup tasks. | On the Threat detectors page, choose Create detector. The Define detector page opens. | In Detector details, give the detector a name. Adding a description for the detector is optional. | In the Data source section, select the dropdown arrow and select one or multiple sources for the log data. When multiple data sources are selected, the logs must be of the same type. We recommend creating separate detectors for different log types. | In the Log types and rules section, select the log type for the data source. The Sigma security rules associated with the log data are automatically populated in the Detection rules section, as shown in the following image. When selecting Network events, CloudTrail logs, or S3 access logs as the log type, a detector dashboard is automatically created. The dashboard offers visualizations for the detector and can provide security-related insight into log source data. For more information about visualizations, see Building data visualizations. You can skip the next step for applying select rules if you are satisfied with those automatically populated by the system. Otherwise, go to the next step to select rules individually. | In the Detection rules section, specify only those rules you want applied to the detector, as shown in the following image. | Use the toggle to the left of Rule name to select or deselect rules. | Use the Rule severity and Source dropdown lists to filter the rules you want to select from. | Use the Search bar to search for specific rules. | . To quickly select one or more known rules and dismiss others, first deselect all rules by moving the Rule name toggle to the left, then search for your target rule names and select each individually by moving its toggle to the right. | In the Detector schedule section, set how often the detector will run. Specify a unit of time and a corresponding number to set the interval. | Choose Next in the lower-right corner of the screen to continue. The Configure field mapping page appears. | . ",
    "url": "https://vagimeli.github.io/security-analytics/sec-analytics-config/detectors-config/#step-1-define-a-detector",
    "relUrl": "/security-analytics/sec-analytics-config/detectors-config/#step-1-define-a-detector"
  },"765": {
    "doc": "Creating detectors",
    "title": "Step 2. Create field mappings",
    "content": "The field mapping step matches field names from the rule with field names from the log index being used to provide data. Creating field mappings allows the system to accurately pass event data from the log to the detector and then use the data to trigger alerts. The data source (log index), log type, and detection rules specified in the first step determine which fields are available for mapping. For example, when “Windows logs” is selected as the log type, this parameter, along with the specific detection rules, determines the list of rule field names available for the mapping. Similarly, the selected data source (log index) determines the list of log source field names that are available for the mapping. Because the system uses prepackaged Sigma rules for detector creation, it can automatically map important fields for a specific log type with the corresponding fields in the Sigma rules. The field mapping step presents a view of automatically mapped fields while also providing the option to customize, change, or add new field mappings. When a detector includes custom rules, you can follow this step to manually map rule field names to log source field names. A note on field names . The field mapping process requires that you are familiar with the field names in the log index and have an understanding of the data contained in those fields. If you have an understanding of the log fields in the index, the mapping is typically a straightforward process. Security Analytics takes advantage of prepackaged Sigma rules for security event detection. Therefore, the field names are derived from a Sigma rule field standard. To make them easier to identify, however, we have created aliases for the Sigma rule fields based on the open-source Elastic Common Schema (ECS) specification. These alias rule field names are the field names used in these steps. They appear in the Detector field name column of the mapping tables. Although the ECS rule field names are largely self-explanatory, you can find predefined mappings of the Sigma rule field names to ECS rule field names, for all supported log types, in the GitHub Security Analytics repository. Navigate to the OSMappings folder, choose the folder named for the log type, and open the fieldmappings.yml file. For example, to see the Sigma rule fields that correspond to ECS rule fields for the Windows log type, open the fieldmappings.yml file in the windows folder. Automatically mapped fields . Once you navigate to the Configure field mapping page, the system attempts to automatically map fields between the two sources. The Automatically mapped fields table contains mappings that the system created automatically after defining the detector. When the field names are similar to one another, the system can successfully match the two, as shown in the following image. Although these automatic matches are normally dependable, it’s still a good idea to review the mappings in the Automatically mapped fields table and verify that they are correct and matched as expected. If you find a mapping that doesn’t appear to be accurate, you can use the dropdown list to search for and select the correct field name. For more on matching field names, see the Pending field mappings section that follows. Pending field mappings . The field names that are not automatically mapped appear in the Pending field mappings table. In this table you can manually map rule fields to log source fields, as shown in the following image. While mapping fields, consider the following: . | The Detector field name column lists field names based on all of the prepackaged rules associated with the selected log type. | The Log source field name column includes a dropdown list for each of the detector fields. Each dropdown list contains field names extracted from the log index. | To map a detector field name to a log source field name, use the dropdown arrow to open the list of log source fields and select the log field name from the list. To search for names in the log field list, enter text in the Select a mapping field box, as shown in the following image. | . | Once the log source field name is selected and mapped to the detector field name, the icon in the Status column to the right changes to a green check mark. | Make as many matches between field names as possible to complete an accurate mapping for the detector and log source fields. | . After completing the mappings, choose Next in the lower-right corner of the screen. The Set up alerts page appears and displays settings for an alert trigger. ",
    "url": "https://vagimeli.github.io/security-analytics/sec-analytics-config/detectors-config/#step-2-create-field-mappings",
    "relUrl": "/security-analytics/sec-analytics-config/detectors-config/#step-2-create-field-mappings"
  },"766": {
    "doc": "Creating detectors",
    "title": "Step 3. Set up alerts",
    "content": "The third step in creating a detector involves setting up alerts. Alerts are configured to create triggers that, when matched with a set of detection rule criteria, send a notification of a possible security event. You can select rule names, rule severity, and tags in any combination to define a trigger. Once a trigger is defined, the alert setup lets you choose the channel on which to be notified and provides options for customizing a message for the notification. At least one alert condition is required before a detector can begin generating findings. You can also configure alerts from the Findings window. To see how to set up alerts from the Findings window, see The findings list. A final option for adding additional alerts is to edit a detector and navigate to the Alert triggers tab, where you can edit existing alerts as well as add new ones. For details, see Editing a detector. To set up an alert for a detector, continue with the following steps: . | In the Trigger name box, enter a name for the trigger. | To define rule matches for the alert, select security rules, severity levels, and tags. | Select one rule or multiple rules that will trigger the alert. Put the cursor in the Rule names box and type a name to search for it. To remove a rule name, select the X beside the name. To remove all rule names, select the X beside the dropdown list’s down arrow. | . | Select one or more rule severities as conditions for the alert. | Select from a list of tags to include as conditions for the alert. | . | To define a notification for the alert, assign an alert severity, select a channel for the notification, and customize a message generated for the alert. | Assign a level of severity for the alert to give the recipient an indication of its urgency. | Select a channel for the notification. Examples include Slack, Chime, or email. Select the Manage channels link to the right of the field to link the notification to a preferred channel. | Select the Show notify message label to expand message preferences. You can add a subject for the message and a note to inform recipients of the nature of the message. | . | After configuring the conditions in the preceding fields, select Next in the lower-right corner of the screen. The Review and create page opens. | . After reviewing the specifications for the detector, choose Create in the lower-right corner of the screen to create the detector. The screen returns to the list of all detectors, and the new detector appears in the list. ",
    "url": "https://vagimeli.github.io/security-analytics/sec-analytics-config/detectors-config/#step-3-set-up-alerts",
    "relUrl": "/security-analytics/sec-analytics-config/detectors-config/#step-3-set-up-alerts"
  },"767": {
    "doc": "Creating detectors",
    "title": "What’s next",
    "content": "If you are ready to view findings for the new detector, see the Working with findings section. If you would like to import rules or set up custom rules before working with findings, see the Working with rules section. ",
    "url": "https://vagimeli.github.io/security-analytics/sec-analytics-config/detectors-config/#whats-next",
    "relUrl": "/security-analytics/sec-analytics-config/detectors-config/#whats-next"
  },"768": {
    "doc": "Setting up Security Analytics",
    "title": "Setting up Security Analytics",
    "content": "Before Security Analytics can begin generating findings and sending alerts, administrators must create detectors and make log data available to the system. Once detectors are able to generate findings, you can fine-tune your alerts to focus on specific areas of interest. The following steps outline the basic workflow for setting up components in Security Analytics. | Create security detectors and alerts, and ingest log data. See Creating detectors for details. | Inspect findings generated from detector output and create any additional alerts. | If desired, create custom rules by duplicating and then modifying pre-packaged rules. See Customizing rules for details. | . ",
    "url": "https://vagimeli.github.io/security-analytics/sec-analytics-config/index/",
    "relUrl": "/security-analytics/sec-analytics-config/index/"
  },"769": {
    "doc": "Setting up Security Analytics",
    "title": "Navigate to Security Analytics",
    "content": ". | To get started, select the top menu on the Dashboards home page and then select Security Analytics. The Overview page for Security Analytics is displayed. | From the options on the left side of the page, select Detectors to begin creating a detector. | . ",
    "url": "https://vagimeli.github.io/security-analytics/sec-analytics-config/index/#navigate-to-security-analytics",
    "relUrl": "/security-analytics/sec-analytics-config/index/#navigate-to-security-analytics"
  },"770": {
    "doc": "OpenSearch Security for Security Analytics",
    "title": "OpenSearch Security for Security Analytics",
    "content": "You can use OpenSearch Security with Security Analytics to assign user permissions and manage the actions that users can and cannot perform. For example, you might want one group of users to be able to create, update, or delete detectors and another group of users to only view detectors. You may want still another group to be able to receive and acknowledge alerts but to be prevented from performing other tasks. The OpenSearch Security framework allows you to control the level of access users have to Security Analytics functionality. ",
    "url": "https://vagimeli.github.io/security-analytics/security/",
    "relUrl": "/security-analytics/security/"
  },"771": {
    "doc": "OpenSearch Security for Security Analytics",
    "title": "Security Analytics system indexes",
    "content": "Security Analytics indexes are protected as system indexes and treated differently than other indexes in a cluster. System indexes store configurations and other system settings and, for that reason, cannot be modified using the REST API or the OpenSearch Dashboards interface. Only a user with a TLS admin certificate can access system indexes. For more information about working with this type of index, see System indexes. ",
    "url": "https://vagimeli.github.io/security-analytics/security/#security-analytics-system-indexes",
    "relUrl": "/security-analytics/security/#security-analytics-system-indexes"
  },"772": {
    "doc": "OpenSearch Security for Security Analytics",
    "title": "Basic permissions",
    "content": "As an administrator, you can use OpenSearch Dashboards or the Security REST API to assign specific permissions to users based on the specific APIs they need to access. For a list of supported APIs, see API tools. OpenSearch Security has three built-in roles that cover most Security Analytics use cases: security_analytics_full_access, security_analytics_read_access, and security_analytics_ack_alerts. For descriptions of these and other roles, see Predefined roles. If these roles don’t meet your needs, mix and match individual Security Analytics permissions to suit your use case. Each action corresponds to an operation in the REST API. For example, the cluster:admin/opensearch/securityanalytics/detector/delete permission allows you to delete detectors. ",
    "url": "https://vagimeli.github.io/security-analytics/security/#basic-permissions",
    "relUrl": "/security-analytics/security/#basic-permissions"
  },"773": {
    "doc": "OpenSearch Security for Security Analytics",
    "title": "(Advanced) Limit access by backend role",
    "content": "You can use backend roles to configure fine-grained access to individual detectors based on roles. For example, backend roles can be assigned to users working in different departments of an organization so that they can view only those detectors owned by the departments in which they work. First, make sure your users have the appropriate backend roles. Backend roles usually come from an LDAP server or SAML provider. However, if you use the internal user database, you can use the REST API to add them manually. Next, enable the following setting: . PUT /_cluster/settings { \"transient\": { \"plugins.security_analytics.filter_by_backend_roles\": \"true\" } } . copy . Now when users view Security Analytics resources in OpenSearch Dashboards (or make REST API calls), they only see detectors created by users who share at least one backend role. For example, consider two users: alice and bob. The following example assigns the user alice the analyst backend role: . PUT /_plugins/_security/api/internalusers/alice { \"password\": \"alice\", \"backend_roles\": [ \"analyst\" ], \"attributes\": {} } . copy . The next example assigns the user bob the human-resources backend role: . PUT /_plugins/_security/api/internalusers/bob { \"password\": \"bob\", \"backend_roles\": [ \"human-resources\" ], \"attributes\": {} } . copy . Finally, this last example assigns both alice and bob the role that gives them full access to Security Analytics: . PUT /_plugins/_security/api/rolesmapping/security_analytics_full_access { \"backend_roles\": [], \"hosts\": [], \"users\": [ \"alice\", \"bob\" ] } . copy . However, because they have different backend roles, alice and bob cannot view each other’s detectors or their results. ",
    "url": "https://vagimeli.github.io/security-analytics/security/#advanced-limit-access-by-backend-role",
    "relUrl": "/security-analytics/security/#advanced-limit-access-by-backend-role"
  },"774": {
    "doc": "OpenSearch Security for Security Analytics",
    "title": "A note on using fine-grained access control with the plugin",
    "content": "When a trigger generates an alert, the detector configurations, the alert itself, and any notifications that are sent to a channel may include metadata describing the index being queried. By design, the plugin must extract the data and store it as metadata outside of the index. Document-level security (DLS) and field-level security (FLS) access controls are designed to protect the data in the index. But once the data is stored outside the index as metadata, users with access to the detector and monitor configurations, alerts, and their notifications will be able to view this metadata and possibly infer the contents and quality of data in the index, which would otherwise be concealed by DLS and FLS access control. To reduce the chances of unintended users viewing metadata that could describe an index, we recommend that administrators enable role-based access control and keep these kinds of design elements in mind when assigning permissions to the intended group of users. See Limit access by backend role for more information. ",
    "url": "https://vagimeli.github.io/security-analytics/security/#a-note-on-using-fine-grained-access-control-with-the-plugin",
    "relUrl": "/security-analytics/security/#a-note-on-using-fine-grained-access-control-with-the-plugin"
  },"775": {
    "doc": "Working with alerts",
    "title": "Working with alerts",
    "content": "The Alerts window includes features for viewing and working with alerts. The two main features are: . | The bar graph with alert information arranged by count, date, and alert status or alert severity. | The Alerts list arranged by time of the alert, the alert’s trigger name, which detector triggered it, and other details. | . You can select the Refresh button at any time to refresh information on the Alerts page. ",
    "url": "https://vagimeli.github.io/security-analytics/usage/alerts/",
    "relUrl": "/security-analytics/usage/alerts/"
  },"776": {
    "doc": "Working with alerts",
    "title": "The Alerts graph",
    "content": "The Alerts graph can display alerts by their status or severity. Use the Group by dropdown list to specify either Alert status or Alert severity. To specify the date range you would like the graph to display, first select the calendar dropdown arrow. The date selector window opens. You can use the Quick select settings to specify an exact window of time. | Select either Last or Next in the first dropdown list to set the window of time behind the current setting or ahead of the current setting. | Select a number in the second dropdown list to define a value for the range. | Select a unit of time in the third dropdown list. Available options are seconds, minutes, hours, days, weeks, months, and years. Select the Apply button to apply the range of dates to the graph. Information on the graph changes accordingly. | . You can use the left and right arrows to move the window of time behind the current range of dates or ahead of the current range of dates. When you use these arrows, the start date and end date appear in the date range field. You can then select each one to set an absolute, relative, or current date and time. For absolute and relative changes, select the Update button to apply the changes. As an alternative, you can select an option in the Commonly used section (see the preceding image of the calendar dropdown list) to conveniently set a window of time. Options include date ranges such as Today, Yesterday, this week, and week to date. When one of the commonly used windows of time is selected, you can select the Show dates label in the date range field to populate the range of dates. Following that, you can select either the start date or end date to specify by an absolute, relative, or current date and time setting. For absolute and relative changes, select the Update button to apply the changes. As one more alternative, you can select an option from the Recently used date ranges section to go back to a previous setting. ",
    "url": "https://vagimeli.github.io/security-analytics/usage/alerts/#the-alerts-graph",
    "relUrl": "/security-analytics/usage/alerts/#the-alerts-graph"
  },"777": {
    "doc": "Working with alerts",
    "title": "The Alerts list",
    "content": "The Alerts list displays all findings according to the time when the alert was triggered, the alert’s trigger name, the detector that triggered the alert, the alert status, and alert severity. Use the Alert severity dropdown list to filter the list of alerts by severity. Use the Status dropdown list to filter the list by alert status. ",
    "url": "https://vagimeli.github.io/security-analytics/usage/alerts/#the-alerts-list",
    "relUrl": "/security-analytics/usage/alerts/#the-alerts-list"
  },"778": {
    "doc": "Working with the correlation graph",
    "title": "Working with the correlation graph",
    "content": "The correlation engine is an experimental feature released in OpenSearch 2.7. Therefore, we do not recommend using the feature in a production environment at this time. For updates on the progress of the correlation engine, see Security Analytics Correlation Engine on GitHub. To share ideas and provide feedback, join the Security Analytics forum. The correlation graph is a security findings knowledge graph. It provides a visualization of information generated by the correlation engine and allows you to focus on specific correlations and inspect them in greater detail. Information on the graph includes findings by log type, the severity levels for the findings, the correlations drawn between findings, and the relevance of the correlations, among other details. You can also manipulate the graph to gain further insight into specific events of interest. This includes filtering findings by date and time, zooming in on the relationship between specific findings and their correlations, and filtering by log type and severity level. Use this section to learn more about using the graph. ",
    "url": "https://vagimeli.github.io/security-analytics/usage/correlation-graph/",
    "relUrl": "/security-analytics/usage/correlation-graph/"
  },"779": {
    "doc": "Working with the correlation graph",
    "title": "Acccessing the graph",
    "content": "Begin by selecting Security Analytics in the OpenSearch Dashboards main menu. Then select Correlations from the Security Analytics menu on the left side of the screen. The Correlations page is displayed, as shown in the following image. ",
    "url": "https://vagimeli.github.io/security-analytics/usage/correlation-graph/#acccessing-the-graph",
    "relUrl": "/security-analytics/usage/correlation-graph/#acccessing-the-graph"
  },"780": {
    "doc": "Working with the correlation graph",
    "title": "Interpreting the graph",
    "content": "The graph displays findings as nodes with colored borders expressing their severity level. A three-letter abbreviation inside the node indicates the log type. The lines that connect the findings represent the correlations between them. A heavy line indicates a strong correlation, while a light line shows a weaker connection. ",
    "url": "https://vagimeli.github.io/security-analytics/usage/correlation-graph/#interpreting-the-graph",
    "relUrl": "/security-analytics/usage/correlation-graph/#interpreting-the-graph"
  },"781": {
    "doc": "Working with the correlation graph",
    "title": "Using the graph",
    "content": "You can control which findings are displayed on the graph by filtering by severity level, log type, and time filter. The time filter controls the findings that appear on the graph by setting a date range in which they were generated. | Use the Severity dropdown list to select which findings appear on the graph according to their severity level. The number beside the list name indicates how many severity levels are being shown on the graph. | Use the Log types dropdown list to select which log types to show on the graph. The number beside the list name indicates how many log types are being shown on the graph. | Select Reset filters to return the dropdown lists to their default settings, showing all items. | Use the time filter to set the date range and show only those findings that were generated within that time span. Select Refresh to bring the current number of findings up to date. | . You can focus on a particular area of the graph to look at correlations associated with a specific finding by selecting the finding on the graph. The graph then changes to show only the selected finding along with the constellation of findings correlated to it, as shown in the following image. After narrowing the focus of the graph, informational cards for each of the findings appear on the right-hand side of the screen. The selected finding appears at the top of the cards, and the correlated findings are listed below it in order of their correlation relevance, represented by a correlation score, as shown in the following image. You can select one of the correlated findings on the graph to shift the perspective of the correlation relationships. This sends the newly selected finding to the top of the informational cards and displays the other findings as relative correlations. The cards display the following details about each finding: . | The severity level of the finding: 1, critical; 2, high; 3, medium; 4, low; 5, informational. | A correlation score for correlated findings. The score is based on the proximity of relevant findings in the threat scenario defined by the correlation rule. | The detection rule that generated the finding. | For correlated findings, the correlation rule used to associate it with the selected finding. | . ",
    "url": "https://vagimeli.github.io/security-analytics/usage/correlation-graph/#using-the-graph",
    "relUrl": "/security-analytics/usage/correlation-graph/#using-the-graph"
  },"782": {
    "doc": "Working with detectors",
    "title": "Working with detectors",
    "content": "After creating a detector, it appears on the Threat detectors page along with others saved to the system. You can then perform a number of actions for each detector, from editing its details to changing its status. See the following sections for description of the available actions. ",
    "url": "https://vagimeli.github.io/security-analytics/usage/detectors/",
    "relUrl": "/security-analytics/usage/detectors/"
  },"783": {
    "doc": "Working with detectors",
    "title": "Threat detector list",
    "content": "The list of threat detectors includes the search bar, the Status dropdown list, and the Log type dropdown list. | Use the search bar to filter by detector name. | Select the Status dropdown list to filter detectors in the list by Active and Inactive status. | Select the Log type dropdown list to filter detectors by any log type that appears in the list (the options depend on the detectors present in the list and their log types). | . Editing a detector . To edit a detector, begin by selecting the link to the detector in the Detector name column of the list. The detector’s details window opens and shows details about the detector’s configuration. | In the upper-left portion of the window, the details window shows the name of the detector and its status, either Active or Inactive. | In the upper-right corner of the window, you can select View alerts to go to the Alerts window or View findings to go to the Findings window. You can also select Actions to perform actions for the detector. See Detector actions. | In the lower portion of the window, select the Edit button for either Detector details or Detection rules to make changes accordingly. | Finally, you can select the Field mappings tab to edit field mappings for the detector, or select the Alert triggers tab to make edits to alerts associated with the detector. | . After you select the Alert triggers tab, you also have the option to add additional alerts for the detector by selecting Add another alert condition at the bottom of the page. ",
    "url": "https://vagimeli.github.io/security-analytics/usage/detectors/#threat-detector-list",
    "relUrl": "/security-analytics/usage/detectors/#threat-detector-list"
  },"784": {
    "doc": "Working with detectors",
    "title": "Detector actions",
    "content": "Threat detector actions allow you to stop and start detectors or delete a detector. To enable actions, first select the checkbox beside one or more detectors in the list. Changing detector status . | Select the detector or detectors in the list whose status you would like to change. The Actions dropdown list becomes enabled. | Depending on whether the detector is currently active or inactive, select either Stop detector or Start detector. After a moment, the change in status of the detector appears in the detector list as either Inactive or Active. | . Deleting a detector . | Select the detector or detectors in the list that you would like to delete. The Actions dropdown list becomes enabled. | Select Delete in the dropdown list. The Delete detector popup window opens and asks you to verify that you want to delete the detector or detectors. | Select Cancel to decline the action. Select Delete detector to delete the detector or detectors permanently from the list. | . ",
    "url": "https://vagimeli.github.io/security-analytics/usage/detectors/#detector-actions",
    "relUrl": "/security-analytics/usage/detectors/#detector-actions"
  },"785": {
    "doc": "Working with findings",
    "title": "Working with findings",
    "content": "The Findings window includes features for viewing and working with findings. The two main features are: . | The bar graph with findings information arranged by count, date, and log type or rule severity. | The Findings list arranged by time, finding ID, rule name, and other details. | . You can choose Refresh at any time to refresh information on the Findings page. ",
    "url": "https://vagimeli.github.io/security-analytics/usage/findings/",
    "relUrl": "/security-analytics/usage/findings/"
  },"786": {
    "doc": "Working with findings",
    "title": "The Findings graph",
    "content": "The findings graph can display findings by log type or rule severity. Use the Group by dropdown list to specify either log type or rule severity. To specify the date range you would like the graph to display, first select the calendar dropdown list. The date selector window opens. You can use the Quick select settings to specify an exact window of time. | Select either Last or Next in the first dropdown list to set the window of time behind the current setting or ahead of the current setting. | Select a number in the second dropdown list to define a value for the range. | Select a unit of time in the third dropdown list. Available options are seconds, minutes, hours, days, weeks, months, and years. Choose Apply to apply the range of dates to the graph. Information on the graph changes accordingly, as shown in the following image. | . You can use the left and right arrows to move the window of time behind the current range of dates or ahead of the current range of dates. When you use these arrows, the start and end dates appear in the date range field. You can then select each one to set an absolute, relative, or current date and time. For absolute and relative changes, choose Update to apply the changes. As an alternative, you can select an option in the Commonly used section (see the preceding image of the calendar dropdown list) to conveniently set a window of time. Options include date ranges such as Today, Yesterday, this week, and week to date. When one of the commonly used windows of time is selected, you can choose Show dates in the date range field to populate the range of dates. Following that, you can select either the start date or end date to specify an absolute, relative, or current date and time setting. For absolute and relative changes, choose Update to apply the changes. As one more alternative, you can select an option from the Recently used date ranges section to go back to a previous setting. ",
    "url": "https://vagimeli.github.io/security-analytics/usage/findings/#the-findings-graph",
    "relUrl": "/security-analytics/usage/findings/#the-findings-graph"
  },"787": {
    "doc": "Working with findings",
    "title": "The Findings list",
    "content": "The Findings list displays all findings according to the time of the finding, the finding ID, the rule name that generated the finding, the detector that captured the finding, and other details, as shown in the following image. Use the Rule severity dropdown list to filter the list of findings by severity. Use the log type dropdown list to filter the list by log type. The Actions column includes two options for each finding: . | The diagonal arrow provides a way to open the Finding details pane, which describes the finding according to parameters defined when creating the detector and includes the document that generated the finding. | The bell icon allows you to open the Create detector alert trigger pane, where you can quickly set up an alert for the specific finding and modify rules and their conditions as required. For details on setting up an alert, see Step 3. Set up alerts in detector creation documentation. | . Finding details . Each finding in the list also includes a Finding ID. In addition to using the diagonal arrow in Actions, you can select the ID to open the Finding details pane. An example of Finding details is shown in the following image. Viewing surrounding documents . The Finding details pane contains specific information about the finding, including the document that generated the finding. To investigate the series of events that led to the finding or followed the finding, you can select View surrounding documents to open the document in the Discover panel and view other documents preceding or following it. | Open Finding details by selecting the Finding ID in the Findings list. | In the Documents section, select View surrounding documents. If an index pattern already exists for the document, the Discover panel opens and displays the document. If an index pattern does not exist, the Create index pattern to view documents window opens and prompts you to create an index pattern, as shown in the following image. | In the Create index pattern to view documents window, the index pattern name is automatically populated. Enter the appropriate time field from the log index used to determine the timing for log events. For information on mapping log fields to detector fields, see Step 2. Create field mappings. Choose Create index pattern. The Create index pattern to view documents confirmation window opens. | Select View surrounding documents in the confirmation window. The Discover panel opens, as shown in the following image. | . The Discover panel displays the document that generated the finding with a highlighted background. Other documents that came either before or after the event are also displayed. For details about working with Discover in OpenSearch Dashboards, see Exploring data. Viewing correlated findings . Correlations between findings are generated by the correlation engine, which is an experimental feature released in OpenSearch 2.7. Therefore, we do not recommend using the feature in a production environment at this time. For updates on the progress of the correlation engine, see Security Analytics Correlation Engine on GitHub. To share ideas and provide feedback, join the Security Analytics forum. To see how the finding is correlated with other findings, select the Correlations tab. Correlations are relationships between findings that express a particular threat scenario involving multiple log types. Information in the Correlated findings table shows the time at which a correlated finding was generated, a finding’s ID, the log type used to generate the finding, its threat severity, and the correlation score—a measure of its proximity to the reference finding—as shown in the following image. You can select View correlations graph to visualize correlations between the findings. For more information about using the correlation graph, see Working with the correlation graph. ",
    "url": "https://vagimeli.github.io/security-analytics/usage/findings/#the-findings-list",
    "relUrl": "/security-analytics/usage/findings/#the-findings-list"
  },"788": {
    "doc": "Using Security Analytics",
    "title": "Using Security Analytics",
    "content": "After creating detectors and generating findings, functionality within the several Security Analytics windows offers visualizations and tools to help you investigate and manage findings, create focused alerts and notifications, import or customize rules, and edit detectors, among other tasks. This section discusses available features, their uses, and general navigation while working in the various windows. You can use the links below to go directly to information on a specific window. | The Overview page | Working with detectors | Working with findings | Working with rules | Working with the correlation graph | Working with alerts | . ",
    "url": "https://vagimeli.github.io/security-analytics/usage/index/",
    "relUrl": "/security-analytics/usage/index/"
  },"789": {
    "doc": "The Overview page",
    "title": "The Overview page",
    "content": "When you select Security Analytics from the top menu, the Overview page is displayed. The Overview page consists of five sections: . | Findings and alert count | Top recent alerts | Top recent findings | Most frequent detection rules | Detectors | . Each section provides a summary description for each element of Security Analytics, along with controls that let you take action for each item. ",
    "url": "https://vagimeli.github.io/security-analytics/usage/overview/",
    "relUrl": "/security-analytics/usage/overview/"
  },"790": {
    "doc": "The Overview page",
    "title": "Overview and getting started",
    "content": "The upper portion of the Overview page contains two control buttons for refreshing information and getting started with Security Analytics. You can select the Refresh button to refresh all of the information on the page. You can also select the Getting started link to expand the Get started with Security Analytics window, which includes a summary of the setup steps as well as control buttons that allow you to jump to any of the steps. | In step 1 of setup, select Create detector to define a detector. | In step 2, select View findings to go to the Findings page. For details about this page, see Working with findings. | In step 3, select View alerts to go to the Security alerts page. For details about this page, see Working with alerts. | In step 4, select Manage rules to go to the Rules page. For more on rules, see Working with rules. | . ",
    "url": "https://vagimeli.github.io/security-analytics/usage/overview/#overview-and-getting-started",
    "relUrl": "/security-analytics/usage/overview/#overview-and-getting-started"
  },"791": {
    "doc": "The Overview page",
    "title": "Findings and alert count",
    "content": "The Findings and alert count section provides a graph showing data on the latest findings. Use the Group by dropdown list to select either All findings or Log type. ",
    "url": "https://vagimeli.github.io/security-analytics/usage/overview/#findings-and-alert-count",
    "relUrl": "/security-analytics/usage/overview/#findings-and-alert-count"
  },"792": {
    "doc": "The Overview page",
    "title": "Recent alerts",
    "content": "The Recent alerts table displays recent alerts by time, trigger name, and alert severity. Select View alerts to go to the Alerts page. ",
    "url": "https://vagimeli.github.io/security-analytics/usage/overview/#recent-alerts",
    "relUrl": "/security-analytics/usage/overview/#recent-alerts"
  },"793": {
    "doc": "The Overview page",
    "title": "Recent findings",
    "content": "The Recent findings table displays recent findings by time, rule name, rule severity, and detector. Select View all findings to go to the Findings page. ",
    "url": "https://vagimeli.github.io/security-analytics/usage/overview/#recent-findings",
    "relUrl": "/security-analytics/usage/overview/#recent-findings"
  },"794": {
    "doc": "The Overview page",
    "title": "Most frequent detection rules",
    "content": "This section provides a graphical representation of detection rules that trigger findings most often and how they compare to others as a percentage of the whole. The rule names represented by the graph are listed to the right. ",
    "url": "https://vagimeli.github.io/security-analytics/usage/overview/#most-frequent-detection-rules",
    "relUrl": "/security-analytics/usage/overview/#most-frequent-detection-rules"
  },"795": {
    "doc": "The Overview page",
    "title": "Detectors",
    "content": "The Detectors section displays a list of available detectors by detector name, status (active/inactive), and log type. Select View all detectors to go to the Detectors page. Select Create detector to go directly to the Define detector page. ",
    "url": "https://vagimeli.github.io/security-analytics/usage/overview/#detectors",
    "relUrl": "/security-analytics/usage/overview/#detectors"
  },"796": {
    "doc": "Working with rules",
    "title": "Working with rules",
    "content": "The Rules window lists all security rules and provides options for filtering the list and viewing details for each rule. Further options let you import rules and create new rules by first duplicating a Sigma rule then modifying it. This section covers navigation of the Rules page and description of the actions you can perform. ",
    "url": "https://vagimeli.github.io/security-analytics/usage/rules/",
    "relUrl": "/security-analytics/usage/rules/"
  },"797": {
    "doc": "Working with rules",
    "title": "Viewing and filtering rules",
    "content": "When you open the Rules page, all rules are listed in the table. Use the search bar to search for specific rules by entering a full or partial name and pressing Return/Enter on your keyboard. The list is filtered and displays matching results. Alternatively, you can use the Rule type, Rule severity, and Source dropdown lists to drill down in the alerts and filter for preferred results. You can select multiple options from each list and use all three in combination to narrow results. Rule details . To see rule details, select the rule in the Rule name column of the list. The rule details pane opens. In Visual view, rule details are arranged in fields, and the links are active. Select YAML to display the rule in YAML file format. | Rule details are formatted as a YAML file according to the Sigma rule specification. | To copy the rule, select the copy icon in the top right corner of the rule. To quickly create a new and customized rule, you can paste the rule into the YAML editor and make any modifications before saving it. See Customizing rules for details. | . ",
    "url": "https://vagimeli.github.io/security-analytics/usage/rules/#viewing-and-filtering-rules",
    "relUrl": "/security-analytics/usage/rules/#viewing-and-filtering-rules"
  },"798": {
    "doc": "Working with rules",
    "title": "Creating rules",
    "content": "There are several ways to create rules on the Rules page. The first is to manually fill in the necessary fields that complete the rule, using either the Visual Editor or YAML Editor. To do this, select the Create new rule button in the uppper-right corner of the Rules window. The Create a rule window opens. If you choose to create the rule manually, you can refer to Sigma’s Rule Creation Guide to help understand details for each field. | By default, the Visual Editor is displayed. Enter the appropriate content in each field and select Create in the lower-right corner of the window to save the rule. | The Create a rule window also provides the YAML Editor so that you can create the rule directly in a YAML file format. Select YAML Editor and then enter information for the pre-populated field types. | . The alternatives to manually creating a rule, however, simplify and speed up the process. They involve either importing a rule in a YAML file or duplicating an existing rule and customizing it. See the next two sections for detailed steps. ",
    "url": "https://vagimeli.github.io/security-analytics/usage/rules/#creating-rules",
    "relUrl": "/security-analytics/usage/rules/#creating-rules"
  },"799": {
    "doc": "Working with rules",
    "title": "Importing rules",
    "content": "At this time, Security Analytics supports the import of Sigma rules in YAML format. The following sample file shows the basic formatting of a rule in YAML. title: RDP Sensitive Settings Changed logsource: product: windows service: system description: 'Detects changes to RDP terminal service sensitive settings' detection: selection_reg: EventType: SetValue TargetObject|contains: - \\services\\TermService\\Parameters\\ServiceDll - \\Control\\Terminal Server\\fSingleSessionPerUser - \\Control\\Terminal Server\\fDenyTSConnections - \\Policies\\Microsoft\\Windows NT\\Terminal Services\\Shadow - \\Control\\Terminal Server\\WinStations\\RDP-Tcp\\InitialProgram condition: selection_reg level: high tags: - attack.defense_evasion - attack.t1112 references: - https://blog.menasec.net/2019/02/threat-hunting-rdp-hijacking-via.html - https://knowledge.insourcess.com/Supporting_Technologies/Wonderware/Tech_Notes/TN_WW213_How_to_shadow_an_established_RDP_Session_on_Windows_10_Pro - https://twitter.com/SagieSec/status/1469001618863624194?t=HRf0eA0W1YYzkTSHb-Ky1A&amp;s=03 - http://etutorials.org/Microsoft+Products/microsoft+windows+server+2003+terminal+services/Chapter+6+Registry/Registry+Keys+for+Terminal+Services/ falsepositives: - Unknown author: - Samir Bousseaden - David ANDRE status: experimental . copy . | To begin, select the Import rule button in the upper-right corner of the page. The Import rule page opens. | Either drag a YAML-formatted Sigma rule into the window or browse for the file by selecting the link and opening it. The Import a rule window opens and the rule definition fields are automatically populated in both the Visual Editor and YAML Editor. | Verify or modify the information in the fields. | After you confirm the information for the rule is accurate, select the Create button in the lower-right corner of the window. A new rule is created, and it appears in the list of rules on the main page of the Rules window. | . ",
    "url": "https://vagimeli.github.io/security-analytics/usage/rules/#importing-rules",
    "relUrl": "/security-analytics/usage/rules/#importing-rules"
  },"800": {
    "doc": "Working with rules",
    "title": "Customizing rules",
    "content": "An alternative to importing a rule is duplicating a Sigma rule and then modifying it to create a custom rule. First search for or filter rules in the Rules list to locate the rule you want to duplicate. | To begin, select the rule in the Rule name column. The rule details pane opens. | Select the Duplicate button in the upper-right corner of the pane. The Duplicate rule window opens in Visual Editor view and all of the fields are automatically populated with the rule’s details. Details are also populated in YAML Editor view. | In either Visual Editor view or YAML Editor view, modify any of the fields to customize the rule. | After performing any modifications to the rule, select the Create button in the lower-right corner of the window. A new and customized rule is created, and it appears in the list of rules on the main page of the Rules window. | . You cannot modify the Sigma rule itself. The original Sigma rule always remains in the system. Its duplicate, after modification, becomes the custom rule that is added to the list of rules. ",
    "url": "https://vagimeli.github.io/security-analytics/usage/rules/#customizing-rules",
    "relUrl": "/security-analytics/usage/rules/#customizing-rules"
  },"801": {
    "doc": "Asynchronous search",
    "title": "Asynchronous search",
    "content": "Searching large volumes of data can take a long time, especially if you’re searching across warm nodes or multiple remote clusters. Asynchronous search in OpenSearch lets you send search requests that run in the background. You can monitor the progress of these searches and get back partial results as they become available. After the search finishes, you can save the results to examine at a later time. ",
    "url": "https://vagimeli.github.io/search-plugins/async/index/",
    "relUrl": "/search-plugins/async/index/"
  },"802": {
    "doc": "Asynchronous search",
    "title": "REST API",
    "content": "Introduced 1.0 . To perform an asynchronous search, send requests to _plugins/_asynchronous_search, with your query in the request body: . POST _plugins/_asynchronous_search . You can specify the following options. | Options | Description | Default value | Required | . | wait_for_completion_timeout | The amount of time that you plan to wait for the results. You can see whatever results you get within this time just like in a normal search. You can poll the remaining results based on an ID. The maximum value is 300 seconds. | 1 second | No | . | keep_on_completion | Whether you want to save the results in the cluster after the search is complete. You can examine the stored results at a later time. | false | No | . | keep_alive | The amount of time that the result is saved in the cluster. For example, 2d means that the results are stored in the cluster for 48 hours. The saved search results are deleted after this period or if the search is canceled. Note that this includes the query execution time. If the query overruns this time, the process cancels this query automatically. | 12 hours | No | . Example request . POST _plugins/_asynchronous_search/?pretty&amp;size=10&amp;wait_for_completion_timeout=1ms&amp;keep_on_completion=true&amp;request_cache=false { \"aggs\": { \"city\": { \"terms\": { \"field\": \"city\", \"size\": 10 } } } } . Example response . { \"*id*\": \"FklfVlU4eFdIUTh1Q1hyM3ZnT19fUVEUd29KLWZYUUI3TzRpdU5wMjRYOHgAAAAAAAAABg==\", \"state\": \"RUNNING\", \"start_time_in_millis\": 1599833301297, \"expiration_time_in_millis\": 1600265301297, \"response\": { \"took\": 15, \"timed_out\": false, \"terminated_early\": false, \"num_reduce_phases\": 4, \"_shards\": { \"total\": 21, \"successful\": 4, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 807, \"relation\": \"eq\" }, \"max_score\": null, \"hits\": [] }, \"aggregations\": { \"city\": { \"doc_count_error_upper_bound\": 16, \"sum_other_doc_count\": 403, \"buckets\": [ { \"key\": \"downsville\", \"doc_count\": 1 }, .......... { \"key\": \"blairstown\", \"doc_count\": 1 } ] } } } } . Response parameters . | Options | Description | . | id | The ID of an asynchronous search. Use this ID to monitor the progress of the search, get its partial results, and/or delete the results. If the asynchronous search finishes within the timeout period, the response doesn’t include the ID because the results aren’t stored in the cluster. | . | state | Specifies whether the search is still running or if it has finished, and if the results persist in the cluster. The possible states are RUNNING, COMPLETED, and PERSISTED. | . | start_time_in_millis | The start time in milliseconds. | . | expiration_time_in_millis | The expiration time in milliseconds. | . | took | The total time that the search is running. | . | response | The actual search response. | . | num_reduce_phases | The number of times that the coordinating node aggregates results from batches of shard responses (5 by default). If this number increases compared to the last retrieved results, you can expect additional results to be included in the search response. | . | total | The total number of shards that run the search. | . | successful | The number of shard responses that the coordinating node received successfully. | . | aggregations | The partial aggregation results that have been completed by the shards so far. | . ",
    "url": "https://vagimeli.github.io/search-plugins/async/index/#rest-api",
    "relUrl": "/search-plugins/async/index/#rest-api"
  },"803": {
    "doc": "Asynchronous search",
    "title": "Get partial results",
    "content": "Introduced 1.0 . After you submit an asynchronous search request, you can request partial responses with the ID that you see in the asynchronous search response. GET _plugins/_asynchronous_search/&lt;ID&gt;?pretty . Example response . { \"id\": \"Fk9lQk5aWHJIUUltR2xGWnpVcWtFdVEURUN1SWZYUUJBVkFVMEJCTUlZUUoAAAAAAAAAAg==\", \"state\": \"STORE_RESIDENT\", \"start_time_in_millis\": 1599833907465, \"expiration_time_in_millis\": 1600265907465, \"response\": { \"took\": 83, \"timed_out\": false, \"_shards\": { \"total\": 20, \"successful\": 20, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 1000, \"relation\": \"eq\" }, \"max_score\": 1, \"hits\": [ { \"_index\": \"bank\", \"_id\": \"1\", \"_score\": 1, \"_source\": { \"email\": \"amberduke@abc.com\", \"city\": \"Brogan\", \"state\": \"IL\" } }, {....} ] }, \"aggregations\": { \"city\": { \"doc_count_error_upper_bound\": 0, \"sum_other_doc_count\": 997, \"buckets\": [ { \"key\": \"belvoir\", \"doc_count\": 2 }, { \"key\": \"aberdeen\", \"doc_count\": 1 }, { \"key\": \"abiquiu\", \"doc_count\": 1 } ] } } } } . After the response is successfully persisted, you get back the STORE_RESIDENT state in the response. You can poll the ID with the wait_for_completion_timeout parameter to wait for the results received for the time that you specify. For asynchronous searches with keep_on_completion as true and a sufficiently long keep_alive time, you can keep polling the IDs until the search finishes. If you don’t want to periodically poll each ID, you can retain the results in your cluster with the keep_alive parameter and come back to it at a later time. ",
    "url": "https://vagimeli.github.io/search-plugins/async/index/#get-partial-results",
    "relUrl": "/search-plugins/async/index/#get-partial-results"
  },"804": {
    "doc": "Asynchronous search",
    "title": "Delete searches and results",
    "content": "Introduced 1.0 . To delete an asynchronous search: . DELETE _plugins/_asynchronous_search/&lt;ID&gt;?pretty . | If the search is still running, OpenSearch cancels it. | If the search is complete, OpenSearch deletes the saved results. | . Example response . { \"acknowledged\": \"true\" } . ",
    "url": "https://vagimeli.github.io/search-plugins/async/index/#delete-searches-and-results",
    "relUrl": "/search-plugins/async/index/#delete-searches-and-results"
  },"805": {
    "doc": "Asynchronous search",
    "title": "Monitor stats",
    "content": "Introduced 1.0 . You can use the stats API operation to monitor asynchronous searches that are running, completed, and/or persisted. GET _plugins/_asynchronous_search/stats . Example response . { \"_nodes\": { \"total\": 8, \"successful\": 8, \"failed\": 0 }, \"cluster_name\": \"264071961897:asynchronous-search\", \"nodes\": { \"JKEFl6pdRC-xNkKQauy7Yg\": { \"asynchronous_search_stats\": { \"submitted\": 18236, \"initialized\": 112, \"search_failed\": 56, \"search_completed\": 56, \"rejected\": 18124, \"persist_failed\": 0, \"cancelled\": 1, \"running_current\": 399, \"persisted\": 100 } } } } . Response parameters . | Options | Description | . | submitted | The number of asynchronous search requests that were submitted. | . | initialized | The number of asynchronous search requests that were initialized. | . | rejected | The number of asynchronous search requests that were rejected. | . | search_completed | The number of asynchronous search requests that completed with a successful response. | . | search_failed | The number of asynchronous search requests that completed with a failed response. | . | persisted | The number of asynchronous search requests whose final result successfully persisted in the cluster. | . | persist_failed | The number of asynchronous search requests whose final result failed to persist in the cluster. | . | running_current | The number of asynchronous search requests that are running on a given coordinator node. | . | cancelled | The number of asynchronous search requests that were canceled while the search was running. | . ",
    "url": "https://vagimeli.github.io/search-plugins/async/index/#monitor-stats",
    "relUrl": "/search-plugins/async/index/#monitor-stats"
  },"806": {
    "doc": "Asynchronous search security",
    "title": "Asynchronous search security",
    "content": "You can use the Security plugin with asynchronous searches to limit non-admin users to specific actions. For example, you might want some users to only be able to submit or delete asynchronous searches, while you might want others to only view the results. All asynchronous search indexes are protected as system indexes. Only a super admin user or an admin user with a Transport Layer Security (TLS) certificate can access system indexes. For more information, see System indexes. ",
    "url": "https://vagimeli.github.io/search-plugins/async/security/",
    "relUrl": "/search-plugins/async/security/"
  },"807": {
    "doc": "Asynchronous search security",
    "title": "Basic permissions",
    "content": "As an admin user, you can use the Security plugin to assign specific permissions to users based on which API operations they need access to. For a list of supported APIs operations, see Asynchronous search. The Security plugin has two built-in roles that cover most asynchronous search use cases: asynchronous_search_full_access and asynchronous_search_read_access. For descriptions of each, see Predefined roles. If these roles don’t meet your needs, mix and match individual asynchronous search permissions to suit your use case. Each action corresponds to an operation in the REST API. For example, the cluster:admin/opensearch/asynchronous_search/delete permission lets you delete a previously submitted asynchronous search. A note on Asynchronous Search and fine-grained access control . By design, the Asynchronous Search plugin extracts data from a target index and stores the data in a separate index to make search results available to users with the proper permissions. Although a user with either the asynchronous_search_read_access or cluster:admin/opensearch/asynchronous_search/get permission cannot submit the asynchronous search request itself, that user can get and view the search results using the associated search ID. Document-level security (DLS) and field-level security (FLS) access controls are designed to protect the data in the target index. But once the data is stored outside this index, users with these access permissions are able to use search IDs to get and view asynchronous search results, which may include data that is otherwise concealed by DLS and FLS access control in the target index. To reduce the chances of unintended users viewing search results that could describe an index, we recommend that administrators enable role-based access control and keep these kinds of design elements in mind when assigning permissions to the intended group of users. See Limit access by backend role for details. ",
    "url": "https://vagimeli.github.io/search-plugins/async/security/#basic-permissions",
    "relUrl": "/search-plugins/async/security/#basic-permissions"
  },"808": {
    "doc": "Asynchronous search security",
    "title": "(Advanced) Limit access by backend role",
    "content": "Use backend roles to configure fine-grained access to asynchronous searches based on roles. For example, users of different departments in an organization can view asynchronous searches owned by their own department. First, make sure your users have the appropriate backend roles. Backend roles usually come from an LDAP server or SAML provider. However, if you use the internal user database, you can use the REST API to add them manually. Now when users view asynchronous search resources in OpenSearch Dashboards (or make REST API calls), they only see asynchronous searches submitted by users who have a subset of the backend role. For example, consider two users: judy and elon. judy has an IT backend role: . PUT _plugins/_security/api/internalusers/judy { \"password\": \"judy\", \"backend_roles\": [ \"IT\" ], \"attributes\": {} } . elon has an admin backend role: . PUT _plugins/_security/api/internalusers/elon { \"password\": \"elon\", \"backend_roles\": [ \"admin\" ], \"attributes\": {} } . Both judy and elon have full access to asynchronous search: . PUT _plugins/_security/api/rolesmapping/async_full_access { \"backend_roles\": [], \"hosts\": [], \"users\": [ \"judy\", \"elon\" ] } . Because they have different backend roles, an asynchronous search submitted by judy will not be visible to elon and vice versa. judy needs to have at least the superset of all roles that elon has to see elon’s asynchronous searches. For example, if judy has five backend roles and elon has one of these roles, then judy can see asynchronous searches submitted by elon, but elon can’t see the asynchronous searches submitted by judy. This means that judy can perform GET and DELETE operations on asynchronous searches submitted by elon, but not the reverse. If none of the users have any backend roles, all three will be able to see the others’ searches. For example, consider three users: judy, elon, and jack. judy, elon, and jack have no backend roles set up: . PUT _plugins/_security/api/internalusers/judy { \"password\": \"judy\", \"backend_roles\": [], \"attributes\": {} } . PUT _plugins/_security/api/internalusers/elon { \"password\": \"elon\", \"backend_roles\": [], \"attributes\": {} } . PUT _plugins/_security/api/internalusers/jack { \"password\": \"jack\", \"backend_roles\": [], \"attributes\": {} } . Both judy and elon have full access to asynchronous search: . PUT _plugins/_security/api/rolesmapping/async_full_access { \"backend_roles\": [], \"hosts\": [], \"users\": [\"judy\",\"elon\"] } . jack has read access to asynchronous search results: . PUT _plugins/_security/api/rolesmapping/async_read_access { \"backend_roles\": [], \"hosts\": [], \"users\": [\"jack\"] } . Because none of the users have backend roles, they will be able to see each other’s asynchronous searches. So, if judy submits an asynchronous search, elon, who has full access, will be able to see that search. jack, who has read access, will also be able to see judy’s asynchronous search. ",
    "url": "https://vagimeli.github.io/search-plugins/async/security/#advanced-limit-access-by-backend-role",
    "relUrl": "/search-plugins/async/security/#advanced-limit-access-by-backend-role"
  },"809": {
    "doc": "Settings",
    "title": "Settings",
    "content": "The Asynchronous Search plugin adds several settings to the standard OpenSearch cluster settings. They are dynamic, so you can change the default behavior of the plugin without restarting your cluster. You can mark the settings as persistent or transient. For example, to update the retention period of the result index: . PUT _cluster/settings { \"transient\": { \"plugins.asynchronous_search.max_wait_for_completion_timeout\": \"5m\" } } . | Setting | Default | Description | . | plugins.asynchronous_search.max_search_running_time | 12 hours | The maximum running time for the search beyond which the search is terminated. | . | plugins.asynchronous_search.node_concurrent_running_searches | 20 | The concurrent searches running per coordinator node. | . | plugins.asynchronous_search.max_keep_alive | 5 days | The maximum amount of time that search results can be stored in the cluster. | . | plugins.asynchronous_search.max_wait_for_completion_timeout | 1 minute | The maximum value for the wait_for_completion_timeout parameter. | . | plugins.asynchronous_search.persist_search_failures | false | Persist asynchronous search results that end with a search failure in the system index. | . ",
    "url": "https://vagimeli.github.io/search-plugins/async/settings/",
    "relUrl": "/search-plugins/async/settings/"
  },"810": {
    "doc": "Search",
    "title": "Search",
    "content": "OpenSearch provides several features for customizing your search use cases and improving search relevance. In OpenSearch, you can: . | Use SQL and Piped Processing Language (PPL) as alternatives to query domain-specific language (DSL) to search data. | Run resource-intensive queries asynchronously with asynchronous search. | Search for k-nearest neighbors with k-NN search. | Abstract OpenSearch queries into search templates. | Integrate machine learning (ML) language models into your search workloads with neural search. | Compare search results to tune search relevance. | Use a dataset that is fixed in time to paginate results with Point in Time. | Paginate and sort search results, highlight search terms, and use the autocomplete and did-you-mean functionality. | Rewrite queries with Querqy. | . ",
    "url": "https://vagimeli.github.io/search-plugins/index/",
    "relUrl": "/search-plugins/index/"
  },"811": {
    "doc": "API",
    "title": "k-NN plugin API",
    "content": "The k-NN plugin adds several APIs for managing, monitoring and optimizing your k-NN workload. ",
    "url": "https://vagimeli.github.io/search-plugins/knn/api/#k-nn-plugin-api",
    "relUrl": "/search-plugins/knn/api/#k-nn-plugin-api"
  },"812": {
    "doc": "API",
    "title": "Stats",
    "content": "Introduced 1.0 . The k-NN stats API provides information about the current status of the k-NN plugin. The plugin keeps track of both cluster-level and node-level statistics. Cluster-level statistics have a single value for the entire cluster. Node-level statistics have a single value for each node in the cluster. You can filter the query by nodeId and statName: . GET /_plugins/_knn/nodeId1,nodeId2/stats/statName1,statName2 . | Statistic | Description | . | circuit_breaker_triggered | Indicates whether the circuit breaker is triggered. This statistic is only relevant to approximate k-NN search. | . | total_load_time | The time in nanoseconds that k-NN has taken to load native library indices into the cache. This statistic is only relevant to approximate k-NN search. | . | eviction_count | The number of native library indices that have been evicted from the cache due to memory constraints or idle time. This statistic is only relevant to approximate k-NN search. Note: Explicit evictions that occur because of index deletion aren’t counted. | . | hit_count | The number of cache hits. A cache hit occurs when a user queries a native library index that’s already loaded into memory. This statistic is only relevant to approximate k-NN search. | . | miss_count | The number of cache misses. A cache miss occurs when a user queries a native library index that isn’t loaded into memory yet. This statistic is only relevant to approximate k-NN search. | . | graph_memory_usage | The amount of native memory native library indices are using on the node in kilobytes. | . | graph_memory_usage_percentage | The amount of native memory native library indices are using on the node as a percentage of the maximum cache capacity. | . | graph_index_requests | The number of requests to add the knn_vector field of a document into a native library index. | . | graph_index_errors | The number of requests to add the knn_vector field of a document into a native library index that have produced an error. | . | graph_query_requests | The number of native library index queries that have been made. | . | graph_query_errors | The number of native library index queries that have produced an error. | . | knn_query_requests | The number of k-NN query requests received. | . | cache_capacity_reached | Whether knn.memory.circuit_breaker.limit has been reached. This statistic is only relevant to approximate k-NN search. | . | load_success_count | The number of times k-NN successfully loaded a native library index into the cache. This statistic is only relevant to approximate k-NN search. | . | load_exception_count | The number of times an exception occurred when trying to load a native library index into the cache. This statistic is only relevant to approximate k-NN search. | . | indices_in_cache | For each OpenSearch index with a knn_vector field and approximate k-NN turned on, this statistic provides the number of native library indices that OpenSearch index has and the total graph_memory_usage that the OpenSearch index is using, in kilobytes. | . | script_compilations | The number of times the k-NN script has been compiled. This value should usually be 1 or 0, but if the cache containing the compiled scripts is filled, the k-NN script might be recompiled. This statistic is only relevant to k-NN score script search. | . | script_compilation_errors | The number of errors during script compilation. This statistic is only relevant to k-NN score script search. | . | script_query_requests | The total number of script queries. This statistic is only relevant to k-NN score script search. | . | script_query_errors | The number of errors during script queries. This statistic is only relevant to k-NN score script search. | . | nmslib_initialized | Boolean value indicating whether the nmslib JNI library has been loaded and initialized on the node. | . | faiss_initialized | Boolean value indicating whether the faiss JNI library has been loaded and initialized on the node. | . | model_index_status | Status of model system index. Valid values are “red”, “yellow”, “green”. If the index does not exist, this will be null. | . | indexing_from_model_degraded | Boolean value indicating if indexing from a model is degraded. This will happen if there is not enough JVM memory to cache the models. | . | training_requests | The number of training requests made to the node. | . | training_errors | The number of training errors that have occurred on the node. | . | training_memory_usage | The amount of native memory training is using on the node in kilobytes. | . | training_memory_usage_percentage | The amount of native memory training is using on the node as a percentage of the maximum cache capacity. | . Note: Some stats contain graph in the name. In these cases, graph is synonymous with native library index. The term graph is a legacy detail, coming from when the plugin only supported the HNSW algorithm, which consists of hierarchical graphs. Usage . GET /_plugins/_knn/stats?pretty { \"_nodes\" : { \"total\" : 1, \"successful\" : 1, \"failed\" : 0 }, \"cluster_name\" : \"my-cluster\", \"circuit_breaker_triggered\" : false, \"model_index_status\" : \"YELLOW\", \"nodes\" : { \"JdfxIkOS1-43UxqNz98nw\" : { \"graph_memory_usage_percentage\" : 3.68, \"graph_query_requests\" : 1420920, \"graph_memory_usage\" : 2, \"cache_capacity_reached\" : false, \"load_success_count\" : 179, \"training_memory_usage\" : 0, \"indices_in_cache\" : { \"myindex\" : { \"graph_memory_usage\" : 2, \"graph_memory_usage_percentage\" : 3.68, \"graph_count\" : 2 } }, \"script_query_errors\" : 0, \"hit_count\" : 1420775, \"knn_query_requests\" : 147092, \"total_load_time\" : 2436679306, \"miss_count\" : 179, \"training_memory_usage_percentage\" : 0.0, \"graph_index_requests\" : 656, \"faiss_initialized\" : true, \"load_exception_count\" : 0, \"training_errors\" : 0, \"eviction_count\" : 0, \"nmslib_initialized\" : false, \"script_compilations\" : 0, \"script_query_requests\" : 0, \"graph_query_errors\" : 0, \"indexing_from_model_degraded\" : false, \"graph_index_errors\" : 0, \"training_requests\" : 17, \"script_compilation_errors\" : 0 } } } . GET /_plugins/_knn/HYMrXXsBSamUkcAjhjeN0w/stats/circuit_breaker_triggered,graph_memory_usage?pretty { \"_nodes\" : { \"total\" : 1, \"successful\" : 1, \"failed\" : 0 }, \"cluster_name\" : \"my-cluster\", \"circuit_breaker_triggered\" : false, \"nodes\" : { \"HYMrXXsBSamUkcAjhjeN0w\" : { \"graph_memory_usage\" : 1 } } } . ",
    "url": "https://vagimeli.github.io/search-plugins/knn/api/#stats",
    "relUrl": "/search-plugins/knn/api/#stats"
  },"813": {
    "doc": "API",
    "title": "Warmup operation",
    "content": "Introduced 1.0 . The native library indices used to perform approximate k-Nearest Neighbor (k-NN) search are stored as special files with other Apache Lucene segment files. In order for you to perform a search on these indices using the k-NN plugin, the plugin needs to load these files into native memory. If the plugin hasn’t loaded the files into native memory, it loads them when it receives a search request. The loading time can cause high latency during initial queries. To avoid this situation, users often run random queries during a warmup period. After this warmup period, the files are loaded into native memory and their production workloads can begin. This loading process is indirect and requires extra effort. As an alternative, you can avoid this latency issue by running the k-NN plugin warmup API operation on whatever indices you’re interested in searching. This operation loads all the native library files for all of the shards (primaries and replicas) of all the indices specified in the request into native memory. After the process finishes, you can start searching against the indices with no initial latency penalties. The warmup API operation is idempotent, so if a segment’s native library files are already loaded into memory, this operation has no impact. It only loads files that aren’t currently in memory. Usage . This request performs a warmup on three indices: . GET /_plugins/_knn/warmup/index1,index2,index3?pretty { \"_shards\" : { \"total\" : 6, \"successful\" : 6, \"failed\" : 0 } } . total indicates how many shards the k-NN plugin attempted to warm up. The response also includes the number of shards the plugin succeeded and failed to warm up. The call doesn’t return results until the warmup operation finishes or the request times out. If the request times out, the operation still continues on the cluster. To monitor the warmup operation, use the OpenSearch _tasks API: . GET /_tasks . After the operation has finished, use the k-NN _stats API operation to see what the k-NN plugin loaded into the graph. Best practices . For the warmup operation to function properly, follow these best practices: . | Don’t run merge operations on indices that you want to warm up. During merge, the k-NN plugin creates new segments, and old segments are sometimes deleted. For example, you could encounter a situation in which the warmup API operation loads native library indices A and B into native memory, but segment C is created from segments A and B being merged. The native library indices for A and B would no longer be in memory, and native library index C would also not be in memory. In this case, the initial penalty for loading native library index C is still present. | Confirm that all native library indices you want to warm up can fit into native memory. For more information about the native memory limit, see the knn.memory.circuit_breaker.limit statistic. High graph memory usage causes cache thrashing, which can lead to operations constantly failing and attempting to run again. | Don’t index any documents that you want to load into the cache. Writing new information to segments prevents the warmup API operation from loading the native library indices until they’re searchable. This means that you would have to run the warmup operation again after indexing finishes. | . ",
    "url": "https://vagimeli.github.io/search-plugins/knn/api/#warmup-operation",
    "relUrl": "/search-plugins/knn/api/#warmup-operation"
  },"814": {
    "doc": "API",
    "title": "Get Model",
    "content": "Introduced 1.2 . Used to retrieve information about models present in the cluster. Some native library index configurations require a training step before indexing and querying can begin. The output of training is a model that can then be used to initialize native library index files during indexing. The model is serialized in the k-NN model system index. GET /_plugins/_knn/models/{model_id} . | Response Field | Description | . | model_id | The id of the fetched model. | . | model_blob | The base64 encoded string of the serialized model. | . | state | Current state of the model. Either “created”, “failed”, “training”. | . | timestamp | Time when the model was created. | . | description | User provided description of the model. | . | error | Error message explaining why the model is in the failed state. | . | space_type | Space type this model is trained for. | . | dimension | Dimension this model is for. | . | engine | Native library used to create model. Either “faiss” or “nmslib”. | . Usage . GET /_plugins/_knn/models/test-model?pretty { \"model_id\" : \"test-model\", \"model_blob\" : \"SXdGbIAAAAAAAAAAAA...\", \"state\" : \"created\", \"timestamp\" : \"2021-11-15T18:45:07.505369036Z\", \"description\" : \"Default\", \"error\" : \"\", \"space_type\" : \"l2\", \"dimension\" : 128, \"engine\" : \"faiss\" } . GET /_plugins/_knn/models/test-model?pretty&amp;filter_path=model_id,state { \"model_id\" : \"test-model\", \"state\" : \"created\" } . ",
    "url": "https://vagimeli.github.io/search-plugins/knn/api/#get-model",
    "relUrl": "/search-plugins/knn/api/#get-model"
  },"815": {
    "doc": "API",
    "title": "Search Model",
    "content": "Introduced 1.2 . Use an OpenSearch query to search for models in the index. Usage . GET/POST /_plugins/_knn/models/_search?pretty&amp;_source_excludes=model_blob { \"query\": { ... } } { \"took\" : 0, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 1, \"relation\" : \"eq\" }, \"max_score\" : 1.0, \"hits\" : [ { \"_index\" : \".opensearch-knn-models\", \"_id\" : \"test-model\", \"_score\" : 1.0, \"_source\" : { \"engine\" : \"faiss\", \"space_type\" : \"l2\", \"description\" : \"Default\", \"model_id\" : \"test-model\", \"state\" : \"created\", \"error\" : \"\", \"dimension\" : 128, \"timestamp\" : \"2021-11-15T18:45:07.505369036Z\" } } ] } } . ",
    "url": "https://vagimeli.github.io/search-plugins/knn/api/#search-model",
    "relUrl": "/search-plugins/knn/api/#search-model"
  },"816": {
    "doc": "API",
    "title": "Delete Model",
    "content": "Introduced 1.2 . Used to delete a particular model in the cluster. Usage . DELETE /_plugins/_knn/models/{model_id} { \"model_id\": {model_id}, \"acknowledged\": true } . ",
    "url": "https://vagimeli.github.io/search-plugins/knn/api/#delete-model",
    "relUrl": "/search-plugins/knn/api/#delete-model"
  },"817": {
    "doc": "API",
    "title": "Train Model",
    "content": "Introduced 1.2 . Create and train a model that can be used for initializing k-NN native library indices during indexing. This API will pull training data from a knn_vector field in a training index and then create and train a model and then serialize it to the model system index. Training data must match the dimension passed into the body of the request. This request will return when training begins. To monitor the state of the model, use the Get model API. | Query Parameter | Description | . | model_id | (Optional) The id of the fetched model. If not specified, a random id will be generated. | . | node_id | (Optional) Preferred node to execute training. If set, this node will be used to perform training if it is deemed to be capable. | . | Request Parameter | Description | . | training_index | Index from where training data from. | . | training_field | knn_vector field from training_index to grab training data from. Dimension of this field must match dimension passed in to this request. | . | dimension | Dimension this model is for. | . | max_training_vector_count | (Optional) Maximum number of vectors from the training index to use for training. Defaults to all of the vectors in the index. | . | search_size | (Optional) Training data is pulled from the training index with scroll queries. Defines the number of results to return per scroll query. Defaults to 10,000. | . | description | (Optional) User provided description of the model. | . | method | Configuration of ANN method used for search. For more information on possible methods, refer to the method documentation. Method must require training to be valid. | . Usage . POST /_plugins/_knn/models/{model_id}/_train?preference={node_id} { \"training_index\": \"train-index-name\", \"training_field\": \"train-field-name\", \"dimension\": 16, \"max_training_vector_count\": 1200, \"search_size\": 100, \"description\": \"My model\", \"method\": { \"name\":\"ivf\", \"engine\":\"faiss\", \"space_type\": \"l2\", \"parameters\":{ \"nlist\":128, \"encoder\":{ \"name\":\"pq\", \"parameters\":{ \"code_size\":8 } } } } } { \"model_id\": \"model_x\" } . POST /_plugins/_knn/models/_train?preference={node_id} { \"training_index\": \"train-index-name\", \"training_field\": \"train-field-name\", \"dimension\": 16, \"max_training_vector_count\": 1200, \"search_size\": 100, \"description\": \"My model\", \"method\": { \"name\":\"ivf\", \"engine\":\"faiss\", \"space_type\": \"l2\", \"parameters\":{ \"nlist\":128, \"encoder\":{ \"name\":\"pq\", \"parameters\":{ \"code_size\":8 } } } } } { \"model_id\": \"dcdwscddscsad\" } . ",
    "url": "https://vagimeli.github.io/search-plugins/knn/api/#train-model",
    "relUrl": "/search-plugins/knn/api/#train-model"
  },"818": {
    "doc": "API",
    "title": "API",
    "content": " ",
    "url": "https://vagimeli.github.io/search-plugins/knn/api/",
    "relUrl": "/search-plugins/knn/api/"
  },"819": {
    "doc": "Approximate search",
    "title": "Approximate k-NN search",
    "content": "Standard k-NN search methods compute similarity using a brute-force approach that measures the nearest distance between a query and a number of points, which produces exact results. This works well in many applications. However, in the case of extremely large datasets with high dimensionality, this creates a scaling problem that reduces the efficiency of the search. Approximate k-NN search methods can overcome this by employing tools that restructure indexes more efficiently and reduce the dimensionality of searchable vectors. Using this approach requires a sacrifice in accuracy but increases search processing speeds appreciably. The Approximate k-NN search methods leveraged by OpenSearch use approximate nearest neighbor (ANN) algorithms from the nmslib, faiss, and Lucene libraries to power k-NN search. These search methods employ ANN to improve search latency for large datasets. Of the three search methods the k-NN plugin provides, this method offers the best search scalability for large datasets. This approach is the preferred method when a dataset reaches hundreds of thousands of vectors. For details on the algorithms the plugin currently supports, see k-NN Index documentation. The k-NN plugin builds a native library index of the vectors for each knn-vector field/Lucene segment pair during indexing, which can be used to efficiently find the k-nearest neighbors to a query vector during search. To learn more about Lucene segments, see the Apache Lucene documentation. These native library indexes are loaded into native memory during search and managed by a cache. To learn more about preloading native library indexes into memory, refer to the warmup API. Additionally, you can see which native library indexes are already loaded in memory. To learn more about this, see the stats API section. Because the native library indexes are constructed during indexing, it is not possible to apply a filter on an index and then use this search method. All filters are applied on the results produced by the approximate nearest neighbor search. Recommendations for engines and cluster node sizing . Each of the three engines used for approximate k-NN search has its own attributes that make one more sensible to use than the others in a given situation. You can follow the general information below to help determine which engine will best meet your requirements. In general, nmslib outperforms both faiss and Lucene on search. However, to optimize for indexing throughput, faiss is a good option. For relatively smaller datasets (up to a few million vectors), the Lucene engine demonstrates better latencies and recall. At the same time, the size of the index is smallest compared to the other engines, which allows it to use smaller AWS instances for data nodes. Also, the Lucene engine uses a pure Java implementation and does not share any of the limitations that engines using platform-native code experience. However, one exception to this is that the maximum dimension count for the Lucene engine is 1,024, compared with 16,000 for the other engines. Refer to the sample mapping parameters in the following section to see where this is configured. When considering cluster node sizing, a general approach is to first establish an even distribution of the index across the cluster. However, there are other considerations. To help make these choices, you can refer to the OpenSearch managed service guidance in the section Sizing domains. ",
    "url": "https://vagimeli.github.io/search-plugins/knn/approximate-knn/#approximate-k-nn-search",
    "relUrl": "/search-plugins/knn/approximate-knn/#approximate-k-nn-search"
  },"820": {
    "doc": "Approximate search",
    "title": "Get started with approximate k-NN",
    "content": "To use the k-NN plugin’s approximate search functionality, you must first create a k-NN index with index.knn set to true. This setting tells the plugin to create native library indexes for the index. Next, you must add one or more fields of the knn_vector data type. This example creates an index with two knn_vector fields, one using faiss and the other using nmslib fields: . PUT my-knn-index-1 { \"settings\": { \"index\": { \"knn\": true, \"knn.algo_param.ef_search\": 100 } }, \"mappings\": { \"properties\": { \"my_vector1\": { \"type\": \"knn_vector\", \"dimension\": 2, \"method\": { \"name\": \"hnsw\", \"space_type\": \"l2\", \"engine\": \"nmslib\", \"parameters\": { \"ef_construction\": 128, \"m\": 24 } } }, \"my_vector2\": { \"type\": \"knn_vector\", \"dimension\": 4, \"method\": { \"name\": \"hnsw\", \"space_type\": \"innerproduct\", \"engine\": \"faiss\", \"parameters\": { \"ef_construction\": 256, \"m\": 48 } } } } } } . In the example above, both knn_vector fields are configured from method definitions. Additionally, knn_vector fields can also be configured from models. You can learn more about this in the knn_vector data type section. The knn_vector data type supports a vector of floats that can have a dimension count of up to 16,000 for the nmslib and faiss engines, as set by the dimension mapping parameter. The maximum dimension count for the Lucene library is 1,024. In OpenSearch, codecs handle the storage and retrieval of indexes. The k-NN plugin uses a custom codec to write vector data to native library indexes so that the underlying k-NN search library can read it. After you create the index, you can add some data to it: . POST _bulk { \"index\": { \"_index\": \"my-knn-index-1\", \"_id\": \"1\" } } { \"my_vector1\": [1.5, 2.5], \"price\": 12.2 } { \"index\": { \"_index\": \"my-knn-index-1\", \"_id\": \"2\" } } { \"my_vector1\": [2.5, 3.5], \"price\": 7.1 } { \"index\": { \"_index\": \"my-knn-index-1\", \"_id\": \"3\" } } { \"my_vector1\": [3.5, 4.5], \"price\": 12.9 } { \"index\": { \"_index\": \"my-knn-index-1\", \"_id\": \"4\" } } { \"my_vector1\": [5.5, 6.5], \"price\": 1.2 } { \"index\": { \"_index\": \"my-knn-index-1\", \"_id\": \"5\" } } { \"my_vector1\": [4.5, 5.5], \"price\": 3.7 } { \"index\": { \"_index\": \"my-knn-index-1\", \"_id\": \"6\" } } { \"my_vector2\": [1.5, 5.5, 4.5, 6.4], \"price\": 10.3 } { \"index\": { \"_index\": \"my-knn-index-1\", \"_id\": \"7\" } } { \"my_vector2\": [2.5, 3.5, 5.6, 6.7], \"price\": 5.5 } { \"index\": { \"_index\": \"my-knn-index-1\", \"_id\": \"8\" } } { \"my_vector2\": [4.5, 5.5, 6.7, 3.7], \"price\": 4.4 } { \"index\": { \"_index\": \"my-knn-index-1\", \"_id\": \"9\" } } { \"my_vector2\": [1.5, 5.5, 4.5, 6.4], \"price\": 8.9 } . Then you can execute an approximate nearest neighbor search on the data using the knn query type: . GET my-knn-index-1/_search { \"size\": 2, \"query\": { \"knn\": { \"my_vector2\": { \"vector\": [2, 3, 5, 6], \"k\": 2 } } } } . k is the number of neighbors the search of each graph will return. You must also include the size option, which indicates how many results the query actually returns. The plugin returns k amount of results for each shard (and each segment) and size amount of results for the entire query. The plugin supports a maximum k value of 10,000. Building a k-NN index from a model . For some of the algorithms that we support, the native library index needs to be trained before it can be used. It would be expensive to training every newly created segment, so, instead, we introduce the concept of a model that is used to initialize the native library index during segment creation. A model is created by calling the Train API, passing in the source of training data as well as the method definition of the model. Once training is complete, the model will be serialized to a k-NN model system index. Then, during indexing, the model is pulled from this index to initialize the segments. To train a model, we first need an OpenSearch index with training data in it. Training data can come from any knn_vector field that has a dimension matching the dimension of the model you want to create. Training data can be the same data that you are going to index or have in a separate set. Let’s create a training index: . PUT /train-index { \"settings\": { \"number_of_shards\": 3, \"number_of_replicas\": 0 }, \"mappings\": { \"properties\": { \"train-field\": { \"type\": \"knn_vector\", \"dimension\": 4 } } } } . Notice that index.knn is not set in the index settings. This ensures that you do not create native library indexes for this index. You can now add some data to the index: . POST _bulk { \"index\": { \"_index\": \"train-index\", \"_id\": \"1\" } } { \"train-field\": [1.5, 5.5, 4.5, 6.4]} { \"index\": { \"_index\": \"train-index\", \"_id\": \"2\" } } { \"train-field\": [2.5, 3.5, 5.6, 6.7]} { \"index\": { \"_index\": \"train-index\", \"_id\": \"3\" } } { \"train-field\": [4.5, 5.5, 6.7, 3.7]} { \"index\": { \"_index\": \"train-index\", \"_id\": \"4\" } } { \"train-field\": [1.5, 5.5, 4.5, 6.4]} . After indexing into the training index completes, we can call the Train API: . POST /_plugins/_knn/models/my-model/_train { \"training_index\": \"train-index\", \"training_field\": \"train-field\", \"dimension\": 4, \"description\": \"My model description\", \"method\": { \"name\": \"ivf\", \"engine\": \"faiss\", \"space_type\": \"l2\", \"parameters\": { \"nlist\": 4, \"nprobes\": 2 } } } . The Train API will return as soon as the training job is started. To check its status, we can use the Get Model API: . GET /_plugins/_knn/models/my-model?filter_path=state&amp;pretty { \"state\": \"training\" } . Once the model enters the “created” state, you can create an index that will use this model to initialize its native library indexes: . PUT /target-index { \"settings\": { \"number_of_shards\": 3, \"number_of_replicas\": 1, \"index.knn\": true }, \"mappings\": { \"properties\": { \"target-field\": { \"type\": \"knn_vector\", \"model_id\": \"my-model\" } } } } . Lastly, we can add the documents we want to be searched to the index: . POST _bulk { \"index\": { \"_index\": \"target-index\", \"_id\": \"1\" } } { \"target-field\": [1.5, 5.5, 4.5, 6.4]} { \"index\": { \"_index\": \"target-index\", \"_id\": \"2\" } } { \"target-field\": [2.5, 3.5, 5.6, 6.7]} { \"index\": { \"_index\": \"target-index\", \"_id\": \"3\" } } { \"target-field\": [4.5, 5.5, 6.7, 3.7]} { \"index\": { \"_index\": \"target-index\", \"_id\": \"4\" } } { \"target-field\": [1.5, 5.5, 4.5, 6.4]} ... After data is ingested, it can be search just like any other knn_vector field! . Using approximate k-NN with filters . If you use the knn query alongside filters or other clauses (e.g. bool, must, match), you might receive fewer than k results. In this example, post_filter reduces the number of results from 2 to 1: . GET my-knn-index-1/_search { \"size\": 2, \"query\": { \"knn\": { \"my_vector2\": { \"vector\": [2, 3, 5, 6], \"k\": 2 } } }, \"post_filter\": { \"range\": { \"price\": { \"gte\": 5, \"lte\": 10 } } } } . ",
    "url": "https://vagimeli.github.io/search-plugins/knn/approximate-knn/#get-started-with-approximate-k-nn",
    "relUrl": "/search-plugins/knn/approximate-knn/#get-started-with-approximate-k-nn"
  },"821": {
    "doc": "Approximate search",
    "title": "Spaces",
    "content": "A space corresponds to the function used to measure the distance between two points in order to determine the k-nearest neighbors. From the k-NN perspective, a lower score equates to a closer and better result. This is the opposite of how OpenSearch scores results, where a greater score equates to a better result. To convert distances to OpenSearch scores, we take 1 / (1 + distance). The k-NN plugin the spaces the plugin supports are below. Not every method supports each of these spaces. Be sure to check out the method documentation to make sure the space you are interested in is supported. | spaceType | Distance Function (d) | OpenSearch Score | . | l1 | \\[ d(\\mathbf{x}, \\mathbf{y}) = \\sum_{i=1}^n |x_i - y_i| \\] | \\[ score = {1 \\over 1 + d } \\] | . | l2 | \\[ d(\\mathbf{x}, \\mathbf{y}) = \\sum_{i=1}^n (x_i - y_i)^2 \\] | \\[ score = {1 \\over 1 + d } \\] | . | linf | \\[ d(\\mathbf{x}, \\mathbf{y}) = max(|x_i - y_i|) \\] | \\[ score = {1 \\over 1 + d } \\] | . | cosinesimil | \\[ d(\\mathbf{x}, \\mathbf{y}) = 1 - cos { \\theta } = 1 - {\\mathbf{x} &middot; \\mathbf{y} \\over \\|\\mathbf{x}\\| &middot; \\|\\mathbf{y}\\|}\\]\\[ = 1 - {\\sum_{i=1}^n x_i y_i \\over \\sqrt{\\sum_{i=1}^n x_i^2} &middot; \\sqrt{\\sum_{i=1}^n y_i^2}}\\] where \\(\\|\\mathbf{x}\\|\\) and \\(\\|\\mathbf{y}\\|\\) represent the norms of vectors x and y respectively. | nmslib and faiss:\\[ score = {1 \\over 1 + d } \\]Lucene:\\[ score = {1 + d \\over 2}\\] | . | innerproduct (not supported for Lucene) | \\[ d(\\mathbf{x}, \\mathbf{y}) = - {\\mathbf{x} &middot; \\mathbf{y}} = - \\sum_{i=1}^n x_i y_i \\] | \\[ \\text{If} d \\ge 0, \\] \\[score = {1 \\over 1 + d }\\] \\[\\text{If} d &lt; 0, score = &minus;d + 1\\] | . The cosine similarity formula does not include the 1 - prefix. However, because similarity search libraries equates smaller scores with closer results, they return 1 - cosineSimilarity for cosine similarity space—that’s why 1 - is included in the distance function. ",
    "url": "https://vagimeli.github.io/search-plugins/knn/approximate-knn/#spaces",
    "relUrl": "/search-plugins/knn/approximate-knn/#spaces"
  },"822": {
    "doc": "Approximate search",
    "title": "Approximate search",
    "content": " ",
    "url": "https://vagimeli.github.io/search-plugins/knn/approximate-knn/",
    "relUrl": "/search-plugins/knn/approximate-knn/"
  },"823": {
    "doc": "k-NN search with filters",
    "title": "k-NN search with filters",
    "content": "To refine k-NN results, you can filter a k-NN search using one of the following methods: . | Scoring script filter: This approach involves pre-filtering a document set and then running an exact k-NN search on the filtered subset. It does not scale for large filtered subsets. | Boolean filter: This approach runs an approximate nearest neighbor (ANN) search and then applies a filter to the results. Because of post-filtering, it may return significantly fewer than k results for a restrictive filter. | Lucene k-NN filter: This approach applies filtering during the k-NN search, as opposed to before or after the k-NN search, which ensures that k results are returned. You can only use this method with the Hierarchical Navigable Small World (HNSW) algorithm implemented by the Lucene search engine in k-NN plugin versions 2.4 and later. | . ",
    "url": "https://vagimeli.github.io/search-plugins/knn/filter-search-knn/",
    "relUrl": "/search-plugins/knn/filter-search-knn/"
  },"824": {
    "doc": "k-NN search with filters",
    "title": "Filtered search optimization",
    "content": "Depending on your dataset and use case, you might be more interested in maximizing recall or minimizing latency. The following table provides guidance on various k-NN search configurations and the filtering methods used to optimize for higher recall or lower latency. The first three columns of the table provide several example k-NN search configurations. A search configuration consists of: . | The number of documents in an index, where one OpenSearch document corresponds to one k-NN vector. | The percentage of documents left in the results after filtering. This value depends on the restrictiveness of the filter that you provide in the query. The most restrictive filter in the table returns 2.5% of documents in the index, while the least restrictive filter returns 80% of documents. | The desired number of returned results (k). | . Once you’ve estimated the number of documents in your index, the restrictiveness of your filter, and the desired number of nearest neighbors, use the following table to choose a filtering method that optimizes for recall or latency. | Number of documents in an index | Percentage of documents the filter returns | k | Filtering method to use for higher recall | Filtering method to use for lower latency | . | 10M | 2.5 | 100 | Scoring script | Scoring script | . | 10M | 38 | 100 | Lucene filter | Boolean filter | . | 10M | 80 | 100 | Scoring script | Lucene filter | . | 1M | 2.5 | 100 | Lucene filter | Scoring script | . | 1M | 38 | 100 | Lucene filter | Lucene filter/scoring script | . | 1M | 80 | 100 | Boolean filter | Lucene filter | . ",
    "url": "https://vagimeli.github.io/search-plugins/knn/filter-search-knn/#filtered-search-optimization",
    "relUrl": "/search-plugins/knn/filter-search-knn/#filtered-search-optimization"
  },"825": {
    "doc": "k-NN search with filters",
    "title": "Scoring script filter",
    "content": "A scoring script filter first filters the documents and then uses a brute-force exact k-NN search on the results. For example, the following query searches for hotels with a rating between 8 and 10, inclusive, that provide parking and then performs a k-NN search to return the 3 hotels that are closest to the specified location: . POST /hotels-index/_search { \"size\": 3, \"query\": { \"script_score\": { \"query\": { \"bool\": { \"filter\": { \"bool\": { \"must\": [ { \"range\": { \"rating\": { \"gte\": 8, \"lte\": 10 } } }, { \"term\": { \"parking\": \"true\" } } ] } } } }, \"script\": { \"source\": \"knn_score\", \"lang\": \"knn\", \"params\": { \"field\": \"location\", \"query_value\": [ 5.0, 4.0 ], \"space_type\": \"l2\" } } } } } . copy . ",
    "url": "https://vagimeli.github.io/search-plugins/knn/filter-search-knn/#scoring-script-filter",
    "relUrl": "/search-plugins/knn/filter-search-knn/#scoring-script-filter"
  },"826": {
    "doc": "k-NN search with filters",
    "title": "Boolean filter with ANN search",
    "content": "A Boolean filter consists of a Boolean query that contains a k-NN query and a filter. For example, the following query searches for hotels that are closest to the specified location and then filters the results to return hotels with a rating between 8 and 10, inclusive, that provide parking: . POST /hotels-index/_search { \"size\": 3, \"query\": { \"bool\": { \"filter\": { \"bool\": { \"must\": [ { \"range\": { \"rating\": { \"gte\": 8, \"lte\": 10 } } }, { \"term\": { \"parking\": \"true\" } } ] } }, \"must\": [ { \"knn\": { \"location\": { \"vector\": [ 5, 4 ], \"k\": 20 } } } ] } } } . The response includes documents containing the matching hotels: . { \"took\" : 95, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 5, \"relation\" : \"eq\" }, \"max_score\" : 0.72992706, \"hits\" : [ { \"_index\" : \"hotels-index\", \"_id\" : \"3\", \"_score\" : 0.72992706, \"_source\" : { \"location\" : [ 4.9, 3.4 ], \"parking\" : \"true\", \"rating\" : 9 } }, { \"_index\" : \"hotels-index\", \"_id\" : \"6\", \"_score\" : 0.3012048, \"_source\" : { \"location\" : [ 6.4, 3.4 ], \"parking\" : \"true\", \"rating\" : 9 } }, { \"_index\" : \"hotels-index\", \"_id\" : \"5\", \"_score\" : 0.24154587, \"_source\" : { \"location\" : [ 3.3, 4.5 ], \"parking\" : \"true\", \"rating\" : 8 } } ] } } . ",
    "url": "https://vagimeli.github.io/search-plugins/knn/filter-search-knn/#boolean-filter-with-ann-search",
    "relUrl": "/search-plugins/knn/filter-search-knn/#boolean-filter-with-ann-search"
  },"827": {
    "doc": "k-NN search with filters",
    "title": "Lucene k-NN filter implementation",
    "content": "k-NN plugin version 2.2 introduced support for running k-NN searches with the Lucene engine using HNSW graphs. Starting with version 2.4, which is based on Lucene version 9.4, you can use Lucene filters for k-NN searches. When you specify a Lucene filter for a k-NN search, the Lucene algorithm decides whether to perform an exact k-NN search with pre-filtering or an approximate search with modified post-filtering. The algorithm uses the following variables: . | N: The number of documents in the index. | P: The number of documents in the document subset after the filter is applied (P &lt;= N). | k: The maximum number of vectors to return in the response. | . The following flow chart outlines the Lucene algorithm. For more information about the Lucene filtering implementation and the underlying KnnVectorQuery, see the Apache Lucene documentation. ",
    "url": "https://vagimeli.github.io/search-plugins/knn/filter-search-knn/#lucene-k-nn-filter-implementation",
    "relUrl": "/search-plugins/knn/filter-search-knn/#lucene-k-nn-filter-implementation"
  },"828": {
    "doc": "k-NN search with filters",
    "title": "Using a Lucene k-NN filter",
    "content": "Consider a dataset that includes 12 documents containing hotel information. The following image shows all hotels on an xy coordinate plane by location. Additionally, the points for hotels that have a rating between 8 and 10, inclusive, are depicted with orange dots, and hotels that provide parking are depicted with green circles. The search point is colored in red: . In this example, you will create an index and search for the three hotels with high ratings and parking that are the closest to the search location. Step 1: Create a new index . Before you can run a k-NN search with a filter, you need to create an index with a knn_vector field. For this field, you need to specify lucene as the engine and hnsw as the method in the mapping. The following request creates a new index called hotels-index with a knn-filter field called location: . PUT /hotels-index { \"settings\": { \"index\": { \"knn\": true, \"knn.algo_param.ef_search\": 100, \"number_of_shards\": 1, \"number_of_replicas\": 0 } }, \"mappings\": { \"properties\": { \"location\": { \"type\": \"knn_vector\", \"dimension\": 2, \"method\": { \"name\": \"hnsw\", \"space_type\": \"l2\", \"engine\": \"lucene\", \"parameters\": { \"ef_construction\": 100, \"m\": 16 } } } } } } . copy . Step 2: Add data to your index . Next, add data to your index. The following request adds 12 documents that contain hotel location, rating, and parking information: . POST /_bulk { \"index\": { \"_index\": \"hotels-index\", \"_id\": \"1\" } } { \"location\": [5.2, 4.4], \"parking\" : \"true\", \"rating\" : 5 } { \"index\": { \"_index\": \"hotels-index\", \"_id\": \"2\" } } { \"location\": [5.2, 3.9], \"parking\" : \"false\", \"rating\" : 4 } { \"index\": { \"_index\": \"hotels-index\", \"_id\": \"3\" } } { \"location\": [4.9, 3.4], \"parking\" : \"true\", \"rating\" : 9 } { \"index\": { \"_index\": \"hotels-index\", \"_id\": \"4\" } } { \"location\": [4.2, 4.6], \"parking\" : \"false\", \"rating\" : 6} { \"index\": { \"_index\": \"hotels-index\", \"_id\": \"5\" } } { \"location\": [3.3, 4.5], \"parking\" : \"true\", \"rating\" : 8 } { \"index\": { \"_index\": \"hotels-index\", \"_id\": \"6\" } } { \"location\": [6.4, 3.4], \"parking\" : \"true\", \"rating\" : 9 } { \"index\": { \"_index\": \"hotels-index\", \"_id\": \"7\" } } { \"location\": [4.2, 6.2], \"parking\" : \"true\", \"rating\" : 5 } { \"index\": { \"_index\": \"hotels-index\", \"_id\": \"8\" } } { \"location\": [2.4, 4.0], \"parking\" : \"true\", \"rating\" : 8 } { \"index\": { \"_index\": \"hotels-index\", \"_id\": \"9\" } } { \"location\": [1.4, 3.2], \"parking\" : \"false\", \"rating\" : 5 } { \"index\": { \"_index\": \"hotels-index\", \"_id\": \"10\" } } { \"location\": [7.0, 9.9], \"parking\" : \"true\", \"rating\" : 9 } { \"index\": { \"_index\": \"hotels-index\", \"_id\": \"11\" } } { \"location\": [3.0, 2.3], \"parking\" : \"false\", \"rating\" : 6 } { \"index\": { \"_index\": \"hotels-index\", \"_id\": \"12\" } } { \"location\": [5.0, 1.0], \"parking\" : \"true\", \"rating\" : 3 } . copy . Step 3: Search your data with a filter . Now you can create a k-NN search with filters. In the k-NN query clause, include the point of interest that is used to search for nearest neighbors, the number of nearest neighbors to return (k), and a filter with the restriction criteria. Depending on how restrictive you want your filter to be, you can add multiple query clauses to a single request. The following request creates a k-NN query that searches for the top three hotels near the location with the coordinates [5, 4] that are rated between 8 and 10, inclusive, and provide parking: . POST /hotels-index/_search { \"size\": 3, \"query\": { \"knn\": { \"location\": { \"vector\": [ 5, 4 ], \"k\": 3, \"filter\": { \"bool\": { \"must\": [ { \"range\": { \"rating\": { \"gte\": 8, \"lte\": 10 } } }, { \"term\": { \"parking\": \"true\" } } ] } } } } } } . copy . The response returns the three hotels that are nearest to the search point and have met the filter criteria: . { \"took\" : 47, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 3, \"relation\" : \"eq\" }, \"max_score\" : 0.72992706, \"hits\" : [ { \"_index\" : \"hotels-index\", \"_id\" : \"3\", \"_score\" : 0.72992706, \"_source\" : { \"location\" : [ 4.9, 3.4 ], \"parking\" : \"true\", \"rating\" : 9 } }, { \"_index\" : \"hotels-index\", \"_id\" : \"6\", \"_score\" : 0.3012048, \"_source\" : { \"location\" : [ 6.4, 3.4 ], \"parking\" : \"true\", \"rating\" : 9 } }, { \"_index\" : \"hotels-index\", \"_id\" : \"5\", \"_score\" : 0.24154587, \"_source\" : { \"location\" : [ 3.3, 4.5 ], \"parking\" : \"true\", \"rating\" : 8 } } ] } } . Note that there are multiple ways to construct a filter that returns hotels that provide parking, for example: . | A term query clause in the should clause | A wildcard query clause in the should clause | A regexp query clause in the should clause | A must_not clause to eliminate hotels with parking set to false. | . The following request illustrates these four different ways of searching for hotels with parking: . POST /hotels-index/_search { \"size\": 3, \"query\": { \"knn\": { \"location\": { \"vector\": [ 5.0, 4.0 ], \"k\": 3, \"filter\": { \"bool\": { \"must\": { \"range\": { \"rating\": { \"gte\": 1, \"lte\": 6 } } }, \"should\": [ { \"term\": { \"parking\": \"true\" } }, { \"wildcard\": { \"parking\": { \"value\": \"t*e\" } } }, { \"regexp\": { \"parking\": \"[a-zA-Z]rue\" } } ], \"must_not\": [ { \"term\": { \"parking\": \"false\" } } ], \"minimum_should_match\": 1 } } } } } } . copy . ",
    "url": "https://vagimeli.github.io/search-plugins/knn/filter-search-knn/#using-a-lucene-k-nn-filter",
    "relUrl": "/search-plugins/knn/filter-search-knn/#using-a-lucene-k-nn-filter"
  },"829": {
    "doc": "k-NN",
    "title": "k-NN",
    "content": "Short for k-nearest neighbors, the k-NN plugin enables users to search for the k-nearest neighbors to a query point across an index of vectors. To determine the neighbors, you can specify the space (the distance function) you want to use to measure the distance between points. Use cases include recommendations (for example, an “other songs you might like” feature in a music application), image recognition, and fraud detection. For more background information on k-NN search, see Wikipedia. This plugin supports three different methods for obtaining the k-nearest neighbors from an index of vectors: . | Approximate k-NN . The first method takes an approximate nearest neighbor approach—it uses one of several algorithms to return the approximate k-nearest neighbors to a query vector. Usually, these algorithms sacrifice indexing speed and search accuracy in return for performance benefits such as lower latency, smaller memory footprints and more scalable search. To learn more about the algorithms, refer to nmslib’s and faiss’s documentation. Approximate k-NN is the best choice for searches over large indices (i.e. hundreds of thousands of vectors or more) that require low latency. You should not use approximate k-NN if you want to apply a filter on the index before the k-NN search, which greatly reduces the number of vectors to be searched. In this case, you should use either the script scoring method or painless extensions. For more details about this method, including recommendations for which engine to use, see Approximate k-NN search. | Script Score k-NN . The second method extends OpenSearch’s script scoring functionality to execute a brute force, exact k-NN search over “knn_vector” fields or fields that can represent binary objects. With this approach, you can run k-NN search on a subset of vectors in your index (sometimes referred to as a pre-filter search). Use this approach for searches over smaller bodies of documents or when a pre-filter is needed. Using this approach on large indices may lead to high latencies. For more details about this method, see Exact k-NN with scoring script. | Painless extensions . The third method adds the distance functions as painless extensions that you can use in more complex combinations. Similar to the k-NN Script Score, you can use this method to perform a brute force, exact k-NN search across an index, which also supports pre-filtering. This approach has slightly slower query performance compared to the k-NN Script Score. If your use case requires more customization over the final score, you should use this approach over Script Score k-NN. For more details about this method, see Painless scripting functions. | . Overall, for larger data sets, you should generally choose the approximate nearest neighbor method because it scales significantly better. For smaller data sets, where you may want to apply a filter, you should choose the custom scoring approach. If you have a more complex use case where you need to use a distance function as part of their scoring method, you should use the painless scripting approach. ",
    "url": "https://vagimeli.github.io/search-plugins/knn/index/",
    "relUrl": "/search-plugins/knn/index/"
  },"830": {
    "doc": "JNI libraries",
    "title": "JNI libraries",
    "content": "To integrate nmslib and faiss approximate k-NN functionality (implemented in C++) into the k-NN plugin (implemented in Java), we created a Java Native Interface, which lets the k-NN plugin make calls to the native libraries. The interface includes three libraries: libopensearchknn_nmslib, the JNI library that interfaces with nmslib, libopensearchknn_faiss, the JNI library that interfaces with faiss, and libopensearchknn_common, a library containing common shared functionality between native libraries. The Lucene library is not implemented using a native library. The libraries libopensearchknn_faiss and libopensearchknn_nmslib are lazily loaded when they are first called in the plugin. This means that if you are only planning on using one of the libraries, the plugin never loads the other library. To build the libraries from source, refer to the DEVELOPER_GUIDE. For more information about JNI, see Java Native Interface on Wikipedia. ",
    "url": "https://vagimeli.github.io/search-plugins/knn/jni-libraries/",
    "relUrl": "/search-plugins/knn/jni-libraries/"
  },"831": {
    "doc": "k-NN Index",
    "title": "k-NN Index",
    "content": " ",
    "url": "https://vagimeli.github.io/search-plugins/knn/knn-index/",
    "relUrl": "/search-plugins/knn/knn-index/"
  },"832": {
    "doc": "k-NN Index",
    "title": "knn_vector data type",
    "content": "The k-NN plugin introduces a custom data type, the knn_vector, that allows users to ingest their k-NN vectors into an OpenSearch index and perform different kinds of k-NN search. The knn_vector field is highly configurable and can serve many different k-NN workloads. In general, a knn_vector field can be built either by providing a method definition or specifying a model id. Method definitions are used when the underlying Approximate k-NN algorithm does not require training. For example, the following knn_vector field specifies that nmslib’s implementation of hnsw should be used for Approximate k-NN search. During indexing, nmslib will build the corresponding hnsw segment files. \"my_vector\": { \"type\": \"knn_vector\", \"dimension\": 4, \"method\": { \"name\": \"hnsw\", \"space_type\": \"l2\", \"engine\": \"nmslib\", \"parameters\": { \"ef_construction\": 128, \"m\": 24 } } } . Model IDs are used when the underlying Approximate k-NN algorithm requires a training step. As a prerequisite, the model has to be created with the Train API. The model contains the information needed to initialize the native library segment files. \"type\": \"knn_vector\", \"model_id\": \"my-model\" } . However, if you intend to just use painless scripting or a k-NN score script, you only need to pass the dimension. \"type\": \"knn_vector\", \"dimension\": 128 } . ",
    "url": "https://vagimeli.github.io/search-plugins/knn/knn-index/#knn_vector-data-type",
    "relUrl": "/search-plugins/knn/knn-index/#knn_vector-data-type"
  },"833": {
    "doc": "k-NN Index",
    "title": "Method Definitions",
    "content": "A method definition refers to the underlying configuration of the Approximate k-NN algorithm you want to use. Method definitions are used to either create a knn_vector field (when the method does not require training) or create a model during training that can then be used to create a knn_vector field. A method definition will always contain the name of the method, the space_type the method is built for, the engine (the library) to use, and a map of parameters. | Mapping Parameter | Required | Default | Updatable | Description | . | name | true | n/a | false | The identifier for the nearest neighbor method. | . | space_type | false | l2 | false | The vector space used to calculate the distance between vectors. | . | engine | false | nmslib | false | The approximate k-NN library to use for indexing and search. The available libraries are faiss, nmslib, and Lucene. | . | parameters | false | null | false | The parameters used for the nearest neighbor method. | . Supported nmslib methods . | Method Name | Requires Training? | Supported Spaces | Description | . | hnsw | false | l2, innerproduct, cosinesimil, l1, linf | Hierarchical proximity graph approach to Approximate k-NN search. For more details on the algorithm, see this abstract. | . HNSW parameters . | Parameter Name | Required | Default | Updatable | Description | . | ef_construction | false | 512 | false | The size of the dynamic list used during k-NN graph creation. Higher values lead to a more accurate graph but slower indexing speed. | . | m | false | 16 | false | The number of bidirectional links that the plugin creates for each new element. Increasing and decreasing this value can have a large impact on memory consumption. Keep this value between 2 and 100. | . For nmslib, ef_search is set in the index settings. Supported faiss methods . | Method Name | Requires Training? | Supported Spaces | Description | . | hnsw | false | l2, innerproduct | Hierarchical proximity graph approach to Approximate k-NN search. | . | ivf | true | l2, innerproduct | Bucketing approach where vectors are assigned different buckets based on clustering and, during search, only a subset of the buckets is searched. | . For hnsw, “innerproduct” is not available when PQ is used. HNSW parameters . | Parameter Name | Required | Default | Updatable | Description | . | ef_search | false | 512 | false | The size of the dynamic list used during k-NN searches. Higher values lead to more accurate but slower searches. | . | ef_construction | false | 512 | false | The size of the dynamic list used during k-NN graph creation. Higher values lead to a more accurate graph but slower indexing speed. | . | m | false | 16 | false | The number of bidirectional links that the plugin creates for each new element. Increasing and decreasing this value can have a large impact on memory consumption. Keep this value between 2 and 100. | . | encoder | false | flat | false | Encoder definition for encoding vectors. Encoders can reduce the memory footprint of your index, at the expense of search accuracy. | . IVF parameters . | Parameter Name | Required | Default | Updatable | Description | . | nlist | false | 4 | false | Number of buckets to partition vectors into. Higher values may lead to more accurate searches at the expense of memory and training latency. For more information about choosing the right value, refer to Guidelines to choose an index. | . | nprobes | false | 1 | false | Number of buckets to search during query. Higher values lead to more accurate but slower searches. | . | encoder | false | flat | false | Encoder definition for encoding vectors. Encoders can reduce the memory footprint of your index, at the expense of search accuracy. | . For more information about setting these parameters, please refer to faiss’s documentation. IVF training requirements . The IVF algorithm requires a training step. To create an index that uses IVF, you need to train a model with the Train API, passing the IVF method definition. IVF requires that, at a minimum, there should be nlist training data points, but it is recommended that you use more. Training data can be composed of either the same data that is going to be ingested or a separate dataset. Supported Lucene methods . | Method Name | Requires Training? | Supported Spaces | Description | . | hnsw | false | l2, cosinesimil | Hierarchical proximity graph approach to Approximate k-NN search. | . HNSW parameters . | Parameter Name | Required | Default | Updatable | Description | . | ef_construction | false | 512 | false | The size of the dynamic list used during k-NN graph creation. Higher values lead to a more accurate graph but slower indexing speed.The Lucene engine uses the proprietary term “beam_width” to describe this function, which corresponds directly to “ef_construction”. To be consistent throughout OpenSearch documentation, we retain the term “ef_construction” to label this parameter. | . | m | false | 16 | false | The number of bidirectional links that the plugin creates for each new element. Increasing and decreasing this value can have a large impact on memory consumption. Keep this value between 2 and 100.The Lucene engine uses the proprietary term “max_connections” to describe this function, which corresponds directly to “m”. To be consistent throughout OpenSearch documentation, we retain the term “m” to label this parameter. | . Lucene HNSW implementation ignores ef_search and dynamically sets it to the value of “k” in the search request. Therefore, there is no need to make settings for ef_search when using the Lucene engine. { \"type\": \"knn_vector\", \"dimension\": 100, \"method\": { \"name\":\"hnsw\", \"engine\":\"lucene\", \"space_type\": \"l2\", \"parameters\":{ \"m\":2048, \"ef_construction\": 245 } } } . Supported faiss encoders . You can use encoders to reduce the memory footprint of a k-NN index at the expense of search accuracy. faiss has several encoder types, but the plugin currently only supports flat and pq encoding. An example method definition that specifies an encoder may look something like this: . \"method\": { \"name\":\"hnsw\", \"engine\":\"faiss\", \"parameters\":{ \"encoder\":{ \"name\":\"pq\", \"parameters\":{ \"code_size\": 8, \"m\": 8 } } } } . | Encoder Name | Requires Training? | Description | . | flat | false | Encode vectors as floating point arrays. This encoding does not reduce memory footprint. | . | pq | true | Short for product quantization, it is a lossy compression technique that encodes a vector into a fixed size of bytes using clustering, with the goal of minimizing the drop in k-NN search accuracy. From a high level, vectors are broken up into m subvectors, and then each subvector is represented by a code_size code obtained from a code book produced during training. For more details on product quantization, here is a great blog post! | . PQ parameters . | Paramater Name | Required | Default | Updatable | Description | . | m | false | 1 | false | Determine how many many sub-vectors to break the vector into. sub-vectors are encoded independently of each other. This dimension of the vector must be divisible by m. Max value is 1024. | . | code_size | false | 8 | false | Determines the number of bits to encode a sub-vector into. Max value is 8. Note — for IVF, this value must be less than or equal to 8. For HNSW, this value can only be 8. | . Choosing the right method . There are a lot of options to choose from when building your knn_vector field. To determine the correct methods and parameters to choose, you should first understand what requirements you have for your workload and what trade-offs you are willing to make. Factors to consider are (1) query latency, (2) query quality, (3) memory limits, (4) indexing latency. If memory is not a concern, HNSW offers a very strong query latency/query quality tradeoff. If you want to use less memory and index faster than HNSW, while maintaining similar query quality, you should evaluate IVF. If memory is a concern, consider adding a PQ encoder to your HNSW or IVF index. Because PQ is a lossy encoding, query quality will drop. Memory Estimation . In a typical OpenSearch cluster, a certain portion of RAM is set aside for the JVM heap. The k-NN plugin allocates native library indexes to a portion of the remaining RAM. This portion’s size is determined by the circuit_breaker_limit cluster setting. By default, the limit is set at 50%. Having a replica doubles the total number of vectors. HNSW memory estimation . The memory required for HNSW is estimated to be 1.1 * (4 * dimension + 8 * M) bytes/vector. As an example, assume you have a million vectors with a dimension of 256 and M of 16. The memory requirement can be estimated as follows: . 1.1 * (4 * 256 + 8 * 16) * 1,000,000 ~= 1.267 GB . IVF memory estimation . The memory required for IVF is estimated to be 1.1 * (((4 * dimension) * num_vectors) + (4 * nlist * d)) bytes. As an example, assume you have a million vectors with a dimension of 256 and nlist of 128. The memory requirement can be estimated as follows: . 1.1 * (((4 * 256) * 1,000,000) + (4 * 128 * 256)) ~= 1.126 GB . ",
    "url": "https://vagimeli.github.io/search-plugins/knn/knn-index/#method-definitions",
    "relUrl": "/search-plugins/knn/knn-index/#method-definitions"
  },"834": {
    "doc": "k-NN Index",
    "title": "Index settings",
    "content": "Additionally, the k-NN plugin introduces several index settings that can be used to configure the k-NN structure as well. At the moment, several parameters defined in the settings are in the deprecation process. Those parameters should be set in the mapping instead of the index settings. Parameters set in the mapping will override the parameters set in the index settings. Setting the parameters in the mapping allows an index to have multiple knn_vector fields with different parameters. | Setting | Default | Updateable | Description | . | index.knn | false | false | Whether the index should build native library indices for the knn_vector fields. If set to false, the knn_vector fields will be stored in doc values, but Approximate k-NN search functionality will be disabled. | . | index.knn.algo_param.ef_search | 512 | true | The size of the dynamic list used during k-NN searches. Higher values lead to more accurate but slower searches. Only available for nmslib. | . | index.knn.algo_param.ef_construction | 512 | false | Deprecated in 1.0.0. Use the mapping parameters to set this value instead. | . | index.knn.algo_param.m | 16 | false | Deprecated in 1.0.0. Use the mapping parameters to set this value instead. | . | index.knn.space_type | l2 | false | Deprecated in 1.0.0. Use the mapping parameters to set this value instead. | . ",
    "url": "https://vagimeli.github.io/search-plugins/knn/knn-index/#index-settings",
    "relUrl": "/search-plugins/knn/knn-index/#index-settings"
  },"835": {
    "doc": "Exact k-NN with scoring script",
    "title": "Exact k-NN with scoring script",
    "content": "The k-NN plugin implements the OpenSearch score script plugin that you can use to find the exact k-nearest neighbors to a given query point. Using the k-NN score script, you can apply a filter on an index before executing the nearest neighbor search. This is useful for dynamic search cases where the index body may vary based on other conditions. Because the score script approach executes a brute force search, it doesn’t scale as well as the approximate approach. In some cases, it might be better to think about refactoring your workflow or index structure to use the approximate approach instead of the score script approach. ",
    "url": "https://vagimeli.github.io/search-plugins/knn/knn-score-script/",
    "relUrl": "/search-plugins/knn/knn-score-script/"
  },"836": {
    "doc": "Exact k-NN with scoring script",
    "title": "Getting started with the score script for vectors",
    "content": "Similar to approximate nearest neighbor search, in order to use the score script on a body of vectors, you must first create an index with one or more knn_vector fields. If you intend to just use the score script approach (and not the approximate approach) you can set index.knn to false and not set index.knn.space_type. You can choose the space type during search. See spaces for the spaces the k-NN score script suppports. This example creates an index with two knn_vector fields: . PUT my-knn-index-1 { \"mappings\": { \"properties\": { \"my_vector1\": { \"type\": \"knn_vector\", \"dimension\": 2 }, \"my_vector2\": { \"type\": \"knn_vector\", \"dimension\": 4 } } } } . If you only want to use the score script, you can omit \"index.knn\": true. The benefit of this approach is faster indexing speed and lower memory usage, but you lose the ability to perform standard k-NN queries on the index. After you create the index, you can add some data to it: . POST _bulk { \"index\": { \"_index\": \"my-knn-index-1\", \"_id\": \"1\" } } { \"my_vector1\": [1.5, 2.5], \"price\": 12.2 } { \"index\": { \"_index\": \"my-knn-index-1\", \"_id\": \"2\" } } { \"my_vector1\": [2.5, 3.5], \"price\": 7.1 } { \"index\": { \"_index\": \"my-knn-index-1\", \"_id\": \"3\" } } { \"my_vector1\": [3.5, 4.5], \"price\": 12.9 } { \"index\": { \"_index\": \"my-knn-index-1\", \"_id\": \"4\" } } { \"my_vector1\": [5.5, 6.5], \"price\": 1.2 } { \"index\": { \"_index\": \"my-knn-index-1\", \"_id\": \"5\" } } { \"my_vector1\": [4.5, 5.5], \"price\": 3.7 } { \"index\": { \"_index\": \"my-knn-index-1\", \"_id\": \"6\" } } { \"my_vector2\": [1.5, 5.5, 4.5, 6.4], \"price\": 10.3 } { \"index\": { \"_index\": \"my-knn-index-1\", \"_id\": \"7\" } } { \"my_vector2\": [2.5, 3.5, 5.6, 6.7], \"price\": 5.5 } { \"index\": { \"_index\": \"my-knn-index-1\", \"_id\": \"8\" } } { \"my_vector2\": [4.5, 5.5, 6.7, 3.7], \"price\": 4.4 } { \"index\": { \"_index\": \"my-knn-index-1\", \"_id\": \"9\" } } { \"my_vector2\": [1.5, 5.5, 4.5, 6.4], \"price\": 8.9 } . Finally, you can execute an exact nearest neighbor search on the data using the knn script: . GET my-knn-index-1/_search { \"size\": 4, \"query\": { \"script_score\": { \"query\": { \"match_all\": {} }, \"script\": { \"source\": \"knn_score\", \"lang\": \"knn\", \"params\": { \"field\": \"my_vector2\", \"query_value\": [2.0, 3.0, 5.0, 6.0], \"space_type\": \"cosinesimil\" } } } } } . All parameters are required. | lang is the script type. This value is usually painless, but here you must specify knn. | source is the name of the script, knn_score. This script is part of the k-NN plugin and isn’t available at the standard _scripts path. A GET request to _cluster/state/metadata doesn’t return it, either. | field is the field that contains your vector data. | query_value is the point you want to find the nearest neighbors for. For the Euclidean and cosine similarity spaces, the value must be an array of floats that matches the dimension set in the field’s mapping. For Hamming bit distance, this value can be either of type signed long or a base64-encoded string (for the long and binary field types, respectively). | space_type corresponds to the distance function. See the spaces section. | . The post filter example in the approximate approach shows a search that returns fewer than k results. If you want to avoid this situation, the score script method lets you essentially invert the order of events. In other words, you can filter down the set of documents over which to execute the k-nearest neighbor search. This example shows a pre-filter approach to k-NN search with the score script approach. First, create the index: . PUT my-knn-index-2 { \"mappings\": { \"properties\": { \"my_vector\": { \"type\": \"knn_vector\", \"dimension\": 2 }, \"color\": { \"type\": \"keyword\" } } } } . Then add some documents: . POST _bulk { \"index\": { \"_index\": \"my-knn-index-2\", \"_id\": \"1\" } } { \"my_vector\": [1, 1], \"color\" : \"RED\" } { \"index\": { \"_index\": \"my-knn-index-2\", \"_id\": \"2\" } } { \"my_vector\": [2, 2], \"color\" : \"RED\" } { \"index\": { \"_index\": \"my-knn-index-2\", \"_id\": \"3\" } } { \"my_vector\": [3, 3], \"color\" : \"RED\" } { \"index\": { \"_index\": \"my-knn-index-2\", \"_id\": \"4\" } } { \"my_vector\": [10, 10], \"color\" : \"BLUE\" } { \"index\": { \"_index\": \"my-knn-index-2\", \"_id\": \"5\" } } { \"my_vector\": [20, 20], \"color\" : \"BLUE\" } { \"index\": { \"_index\": \"my-knn-index-2\", \"_id\": \"6\" } } { \"my_vector\": [30, 30], \"color\" : \"BLUE\" } . Finally, use the script_score query to pre-filter your documents before identifying nearest neighbors: . GET my-knn-index-2/_search { \"size\": 2, \"query\": { \"script_score\": { \"query\": { \"bool\": { \"filter\": { \"term\": { \"color\": \"BLUE\" } } } }, \"script\": { \"lang\": \"knn\", \"source\": \"knn_score\", \"params\": { \"field\": \"my_vector\", \"query_value\": [9.9, 9.9], \"space_type\": \"l2\" } } } } } . ",
    "url": "https://vagimeli.github.io/search-plugins/knn/knn-score-script/#getting-started-with-the-score-script-for-vectors",
    "relUrl": "/search-plugins/knn/knn-score-script/#getting-started-with-the-score-script-for-vectors"
  },"837": {
    "doc": "Exact k-NN with scoring script",
    "title": "Getting started with the score script for binary data",
    "content": "The k-NN score script also allows you to run k-NN search on your binary data with the Hamming distance space. In order to use Hamming distance, the field of interest must have either a binary or long field type. If you’re using binary type, the data must be a base64-encoded string. This example shows how to use the Hamming distance space with a binary field type: . PUT my-index { \"mappings\": { \"properties\": { \"my_binary\": { \"type\": \"binary\", \"doc_values\": true }, \"color\": { \"type\": \"keyword\" } } } } . Then add some documents: . POST _bulk { \"index\": { \"_index\": \"my-index\", \"_id\": \"1\" } } { \"my_binary\": \"SGVsbG8gV29ybGQh\", \"color\" : \"RED\" } { \"index\": { \"_index\": \"my-index\", \"_id\": \"2\" } } { \"my_binary\": \"ay1OTiBjdXN0b20gc2NvcmluZyE=\", \"color\" : \"RED\" } { \"index\": { \"_index\": \"my-index\", \"_id\": \"3\" } } { \"my_binary\": \"V2VsY29tZSB0byBrLU5O\", \"color\" : \"RED\" } { \"index\": { \"_index\": \"my-index\", \"_id\": \"4\" } } { \"my_binary\": \"SSBob3BlIHRoaXMgaXMgaGVscGZ1bA==\", \"color\" : \"BLUE\" } { \"index\": { \"_index\": \"my-index\", \"_id\": \"5\" } } { \"my_binary\": \"QSBjb3VwbGUgbW9yZSBkb2NzLi4u\", \"color\" : \"BLUE\" } { \"index\": { \"_index\": \"my-index\", \"_id\": \"6\" } } { \"my_binary\": \"TGFzdCBvbmUh\", \"color\" : \"BLUE\" } . Finally, use the script_score query to pre-filter your documents before identifying nearest neighbors: . GET my-index/_search { \"size\": 2, \"query\": { \"script_score\": { \"query\": { \"bool\": { \"filter\": { \"term\": { \"color\": \"BLUE\" } } } }, \"script\": { \"lang\": \"knn\", \"source\": \"knn_score\", \"params\": { \"field\": \"my_binary\", \"query_value\": \"U29tZXRoaW5nIEltIGxvb2tpbmcgZm9y\", \"space_type\": \"hammingbit\" } } } } } . Similarly, you can encode your data with the long field and run a search: . GET my-long-index/_search { \"size\": 2, \"query\": { \"script_score\": { \"query\": { \"bool\": { \"filter\": { \"term\": { \"color\": \"BLUE\" } } } }, \"script\": { \"lang\": \"knn\", \"source\": \"knn_score\", \"params\": { \"field\": \"my_long\", \"query_value\": 23, \"space_type\": \"hammingbit\" } } } } } . ",
    "url": "https://vagimeli.github.io/search-plugins/knn/knn-score-script/#getting-started-with-the-score-script-for-binary-data",
    "relUrl": "/search-plugins/knn/knn-score-script/#getting-started-with-the-score-script-for-binary-data"
  },"838": {
    "doc": "Exact k-NN with scoring script",
    "title": "Spaces",
    "content": "A space corresponds to the function used to measure the distance between two points in order to determine the k-nearest neighbors. From the k-NN perspective, a lower score equates to a closer and better result. This is the opposite of how OpenSearch scores results, where a greater score equates to a better result. The following table illustrates how OpenSearch converts spaces to scores: . | spaceType | Distance Function (d) | OpenSearch Score | . | l1 | \\[ d(\\mathbf{x}, \\mathbf{y}) = \\sum_{i=1}^n |x_i - y_i| \\] | \\[ score = {1 \\over 1 + d } \\] | . | l2 | \\[ d(\\mathbf{x}, \\mathbf{y}) = \\sum_{i=1}^n (x_i - y_i)^2 \\] | \\[ score = {1 \\over 1 + d } \\] | . | linf | \\[ d(\\mathbf{x}, \\mathbf{y}) = max(|x_i - y_i|) \\] | \\[ score = {1 \\over 1 + d } \\] | . | cosinesimil | \\[ d(\\mathbf{x}, \\mathbf{y}) = cos \\theta = {\\mathbf{x} &middot; \\mathbf{y} \\over \\|\\mathbf{x}\\| &middot; \\|\\mathbf{y}\\|}\\]\\[ = {\\sum_{i=1}^n x_i y_i \\over \\sqrt{\\sum_{i=1}^n x_i^2} &middot; \\sqrt{\\sum_{i=1}^n y_i^2}}\\] where \\(\\|\\mathbf{x}\\|\\) and \\(\\|\\mathbf{y}\\|\\) represent normalized vectors. | \\[ score = 1 + d \\] | . | innerproduct (not supported for Lucene) | \\[ d(\\mathbf{x}, \\mathbf{y}) = - {\\mathbf{x} &middot; \\mathbf{y}} = - \\sum_{i=1}^n x_i y_i \\] | \\[ \\text{If} d \\ge 0, \\] \\[score = {1 \\over 1 + d }\\] \\[\\text{If} d &lt; 0, score = &minus;d + 1\\] | . | hammingbit | \\[ d(\\mathbf{x}, \\mathbf{y}) = \\text{countSetBits}(\\mathbf{x} \\oplus \\mathbf{y})\\] | \\[ score = {1 \\over 1 + d } \\] | . Cosine similarity returns a number between -1 and 1, and because OpenSearch relevance scores can’t be below 0, the k-NN plugin adds 1 to get the final score. ",
    "url": "https://vagimeli.github.io/search-plugins/knn/knn-score-script/#spaces",
    "relUrl": "/search-plugins/knn/knn-score-script/#spaces"
  },"839": {
    "doc": "k-NN Painless extensions",
    "title": "k-NN Painless Scripting extensions",
    "content": "With the k-NN plugin’s Painless Scripting extensions, you can use k-NN distance functions directly in your Painless scripts to perform operations on knn_vector fields. Painless has a strict list of allowed functions and classes per context to ensure its scripts are secure. The k-NN plugin adds Painless Scripting extensions to a few of the distance functions used in k-NN score script, so you can use them to customize your k-NN workload. ",
    "url": "https://vagimeli.github.io/search-plugins/knn/painless-functions/#k-nn-painless-scripting-extensions",
    "relUrl": "/search-plugins/knn/painless-functions/#k-nn-painless-scripting-extensions"
  },"840": {
    "doc": "k-NN Painless extensions",
    "title": "Get started with k-NN’s Painless Scripting functions",
    "content": "To use k-NN’s Painless Scripting functions, first create an index with knn_vector fields like in k-NN score script. Once the index is created and you ingest some data, you can use the painless extensions: . GET my-knn-index-2/_search { \"size\": 2, \"query\": { \"script_score\": { \"query\": { \"bool\": { \"filter\": { \"term\": { \"color\": \"BLUE\" } } } }, \"script\": { \"source\": \"1.0 + cosineSimilarity(params.query_value, doc[params.field])\", \"params\": { \"field\": \"my_vector\", \"query_value\": [9.9, 9.9] } } } } } . field needs to map to a knn_vector field, and query_value needs to be a floating point array with the same dimension as field. ",
    "url": "https://vagimeli.github.io/search-plugins/knn/painless-functions/#get-started-with-k-nns-painless-scripting-functions",
    "relUrl": "/search-plugins/knn/painless-functions/#get-started-with-k-nns-painless-scripting-functions"
  },"841": {
    "doc": "k-NN Painless extensions",
    "title": "Function types",
    "content": "The following table describes the available painless functions the k-NN plugin provides: . | Function name | Function signature | Description | . | l2Squared | float l2Squared (float[] queryVector, doc['vector field']) | This function calculates the square of the L2 distance (Euclidean distance) between a given query vector and document vectors. The shorter the distance, the more relevant the document is, so this example inverts the return value of the l2Squared function. If the document vector matches the query vector, the result is 0, so this example also adds 1 to the distance to avoid divide by zero errors. | . | l1Norm | float l1Norm (float[] queryVector, doc['vector field']) | This function calculates the square of the L2 distance (Euclidean distance) between a given query vector and document vectors. The shorter the distance, the more relevant the document is, so this example inverts the return value of the l2Squared function. If the document vector matches the query vector, the result is 0, so this example also adds 1 to the distance to avoid divide by zero errors. | . | cosineSimilarity | float cosineSimilarity (float[] queryVector, doc['vector field']) | Cosine similarity is an inner product of the query vector and document vector normalized to both have a length of 1. If the magnitude of the query vector doesn’t change throughout the query, you can pass the magnitude of the query vector to improve performance, instead of calculating the magnitude every time for every filtered document: float cosineSimilarity (float[] queryVector, doc['vector field'], float normQueryVector) In general, the range of cosine similarity is [-1, 1]. However, in the case of information retrieval, the cosine similarity of two documents ranges from 0 to 1 because the tf-idf statistic can’t be negative. Therefore, the k-NN plugin adds 1.0 in order to always yield a positive cosine similarity score. | . ",
    "url": "https://vagimeli.github.io/search-plugins/knn/painless-functions/#function-types",
    "relUrl": "/search-plugins/knn/painless-functions/#function-types"
  },"842": {
    "doc": "k-NN Painless extensions",
    "title": "Constraints",
    "content": ". | If a document’s knn_vector field has different dimensions than the query, the function throws an IllegalArgumentException. | If a vector field doesn’t have a value, the function throws an IllegalStateException. You can avoid this situation by first checking if a document has a value in its field: . \"source\": \"doc[params.field].size() == 0 ? 0 : 1 / (1 + l2Squared(params.query_value, doc[params.field]))\", . Because scores can only be positive, this script ranks documents with vector fields higher than those without. | . ",
    "url": "https://vagimeli.github.io/search-plugins/knn/painless-functions/#constraints",
    "relUrl": "/search-plugins/knn/painless-functions/#constraints"
  },"843": {
    "doc": "k-NN Painless extensions",
    "title": "k-NN Painless extensions",
    "content": " ",
    "url": "https://vagimeli.github.io/search-plugins/knn/painless-functions/",
    "relUrl": "/search-plugins/knn/painless-functions/"
  },"844": {
    "doc": "Performance tuning",
    "title": "Performance tuning",
    "content": "This topic provides performance tuning recommendations to improve indexing and search performance for approximate k-NN (ANN). From a high level, k-NN works according to these principles: . | Native library indices are created per knn_vector field / (Lucene) segment pair. | Queries execute on segments sequentially inside the shard (same as any other OpenSearch query). | Each native library index in the segment returns &lt;=k neighbors. | The coordinator node picks up final size number of neighbors from the neighbors returned by each shard. | . This topic also provides recommendations for comparing approximate k-NN to exact k-NN with score script. ",
    "url": "https://vagimeli.github.io/search-plugins/knn/performance-tuning/",
    "relUrl": "/search-plugins/knn/performance-tuning/"
  },"845": {
    "doc": "Performance tuning",
    "title": "Indexing performance tuning",
    "content": "Take the following steps to improve indexing performance, especially when you plan to index a large number of vectors at once: . | Disable the refresh interval . Either disable the refresh interval (default = 1 sec), or set a long duration for the refresh interval to avoid creating multiple small segments: . PUT /&lt;index_name&gt;/_settings { \"index\" : { \"refresh_interval\" : \"-1\" } } . Note: Make sure to reenable refresh_interval after indexing finishes. | Disable replicas (no OpenSearch replica shard) . Set replicas to 0 to prevent duplicate construction of native library indices in both primary and replica shards. When you enable replicas after indexing finishes, the serialized native library indices are directly copied. If you have no replicas, losing nodes might cause data loss, so it’s important that the data lives elsewhere so this initial load can be retried in case of an issue. | Increase the number of indexing threads . If the hardware you choose has multiple cores, you can allow multiple threads in native library index construction by speeding up the indexing process. Determine the number of threads to allot with the knn.algo_param.index_thread_qty setting. Keep an eye on CPU utilization and choose the correct number of threads. Because native library index construction is costly, having multiple threads can cause additional CPU load. | . ",
    "url": "https://vagimeli.github.io/search-plugins/knn/performance-tuning/#indexing-performance-tuning",
    "relUrl": "/search-plugins/knn/performance-tuning/#indexing-performance-tuning"
  },"846": {
    "doc": "Performance tuning",
    "title": "Search performance tuning",
    "content": "Take the following steps to improve search performance: . | Reduce segment count . To improve search performance, you must keep the number of segments under control. Lucene’s IndexSearcher searches over all of the segments in a shard to find the ‘size’ best results. Ideally, having one segment per shard provides the optimal performance with respect to search latency. You can configure an index to have multiple shards to avoid giant shards and achieve more parallelism. You can control the number of segments by choosing a larger refresh interval, or during indexing by asking OpenSearch to slow down segment creation by disabling the refresh interval. | Warm up the index . Native library indices are constructed during indexing, but they’re loaded into memory during the first search. In Lucene, each segment is searched sequentially (so, for k-NN, each segment returns up to k nearest neighbors of the query point), and the top ‘size’ number of results based on the score are returned from all the results returned by segements at a shard level (higher score = better result). Once a native library index is loaded (native library indices are loaded outside OpenSearch JVM), OpenSearch caches them in memory. Initial queries are expensive and take a few seconds, while subsequent queries are faster and take milliseconds (assuming the k-NN circuit breaker isn’t hit). To avoid this latency penalty during your first queries, you can use the warmup API operation on the indices you want to search: . GET /_plugins/_knn/warmup/index1,index2,index3?pretty { \"_shards\" : { \"total\" : 6, \"successful\" : 6, \"failed\" : 0 } } . The warmup API operation loads all native library indices for all shards (primary and replica) for the specified indices into the cache, so there’s no penalty to load native library indices during initial searches. Note: This API operation only loads the segments of the indices it sees into the cache. If a merge or refresh operation finishes after the API runs, or if you add new documents, you need to rerun the API to load those native library indices into memory. | Avoid reading stored fields . If your use case is simply to read the IDs and scores of the nearest neighbors, you can disable reading stored fields, which saves time retrieving the vectors from stored fields. | Use mmap file I/O . For the Lucene-based approximate k-NN search, there is no dedicated cache layer that speeds up read/write operations. Instead, the plugin relies on the existing caching mechanism in OpenSearch core. In versions 2.4 and earlier of the Lucene-based approximate k-NN search, read/write operations were based on Java NIO by default, which can be slow, depending on the Lucene version and number of segments per shard. Starting with version 2.5, k-NN enables mmap file I/O by default when the store type is hybridfs (the default store type in OpenSearch). This leads to fast file I/O operations and improves the overall performance of both data ingestion and search. The two file extensions specific to vector values that use mmap are .vec and .vem. For more information about these file extensions, see the Lucene documentation. The mmap file I/O uses the system file cache rather than memory allocated for the Java heap, so no additional allocation is required. To change the default list of extensions set by the plugin, update the index.store.hybrid.mmap.extensions setting at the cluster level using the Cluster Settings API. Note: This is an expert-level setting that requires closing the index before updating the setting and reopening it after the update. | . ",
    "url": "https://vagimeli.github.io/search-plugins/knn/performance-tuning/#search-performance-tuning",
    "relUrl": "/search-plugins/knn/performance-tuning/#search-performance-tuning"
  },"847": {
    "doc": "Performance tuning",
    "title": "Improving recall",
    "content": "Recall depends on multiple factors like number of vectors, number of dimensions, segments, and so on. Searching over a large number of small segments and aggregating the results leads to better recall than searching over a small number of large segments and aggregating results. The larger the native library index, the more chances of losing recall if you’re using smaller algorithm parameters. Choosing larger values for algorithm parameters should help solve this issue but sacrifices search latency and indexing time. That being said, it’s important to understand your system’s requirements for latency and accuracy, and then choose the number of segments you want your index to have based on experimentation. The default parameters work on a broader set of use cases, but make sure to run your own experiments on your data sets and choose the appropriate values. For index-level settings, see Index settings. ",
    "url": "https://vagimeli.github.io/search-plugins/knn/performance-tuning/#improving-recall",
    "relUrl": "/search-plugins/knn/performance-tuning/#improving-recall"
  },"848": {
    "doc": "Performance tuning",
    "title": "Approximate nearest neighbor versus score script",
    "content": "The standard k-NN query and custom scoring option perform differently. Test with a representative set of documents to see if the search results and latencies match your expectations. Custom scoring works best if the initial filter reduces the number of documents to no more than 20,000. Increasing shard count can improve latency, but be sure to keep shard size within the recommended guidelines. ",
    "url": "https://vagimeli.github.io/search-plugins/knn/performance-tuning/#approximate-nearest-neighbor-versus-score-script",
    "relUrl": "/search-plugins/knn/performance-tuning/#approximate-nearest-neighbor-versus-score-script"
  },"849": {
    "doc": "Settings",
    "title": "k-NN settings",
    "content": "The k-NN plugin adds several new cluster settings. ",
    "url": "https://vagimeli.github.io/search-plugins/knn/settings/#k-nn-settings",
    "relUrl": "/search-plugins/knn/settings/#k-nn-settings"
  },"850": {
    "doc": "Settings",
    "title": "Cluster settings",
    "content": "| Setting | Default | Description | . | knn.algo_param.index_thread_qty | 1 | The number of threads used for native library index creation. Keeping this value low reduces the CPU impact of the k-NN plugin, but also reduces indexing performance. | . | knn.cache.item.expiry.enabled | false | Whether to remove native library indices that have not been accessed for a certain duration from memory. | . | knn.cache.item.expiry.minutes | 3h | If enabled, the idle time before removing a native library index from memory. | . | knn.circuit_breaker.unset.percentage | 75% | The native memory usage threshold for the circuit breaker. Memory usage must be below this percentage of knn.memory.circuit_breaker.limit for knn.circuit_breaker.triggered to remain false. | . | knn.circuit_breaker.triggered | false | True when memory usage exceeds the knn.circuit_breaker.unset.percentage value. | . | knn.memory.circuit_breaker.limit | 50% | The native memory limit for native library indices. At the default value, if a machine has 100 GB of memory and the JVM uses 32 GB, the k-NN plugin uses 50% of the remaining 68 GB (34 GB). If memory usage exceeds this value, k-NN removes the least recently used native library indices. | . | knn.memory.circuit_breaker.enabled | true | Whether to enable the k-NN memory circuit breaker. | . | knn.plugin.enabled | true | Enables or disables the k-NN plugin. | . | knn.model.index.number_of_shards | 1 | Number of shards to use for the model system index, the OpenSearch index that stores the models used for Approximate k-NN Search. | . | knn.model.index.number_of_replicas | 1 | Number of replica shards to use for the model system index. Generally, in a multi-node cluster, this should be at least 1 to increase stability. | . ",
    "url": "https://vagimeli.github.io/search-plugins/knn/settings/#cluster-settings",
    "relUrl": "/search-plugins/knn/settings/#cluster-settings"
  },"851": {
    "doc": "Settings",
    "title": "Settings",
    "content": " ",
    "url": "https://vagimeli.github.io/search-plugins/knn/settings/",
    "relUrl": "/search-plugins/knn/settings/"
  },"852": {
    "doc": "Neural Search plugin",
    "title": "Neural Search plugin",
    "content": "The Neural Search plugin is an experimental feature. For updates on the progress of the Neural Search plugin, or if you want to leave feedback that could help improve the feature, join the discussion in the Neural Search forum. The OpenSearch Neural Search plugin enables the integration of machine learning (ML) language models into your search workloads. During ingestion and search, the Neural Search plugin transforms text into vectors. Then, Neural Search uses the transformed vectors in vector-based search. The Neural Search plugin comes bundled with OpenSearch. For more information, see Managing plugins. ",
    "url": "https://vagimeli.github.io/search-plugins/neural-search/",
    "relUrl": "/search-plugins/neural-search/"
  },"853": {
    "doc": "Neural Search plugin",
    "title": "Ingest data with Neural Search",
    "content": "In order to ingest vectorized documents, you need to create a Neural Search pipeline. A pipeline consists of a series of processors that manipulate documents during ingestion, allowing the documents to be vectorized. The following API operation creates a Neural Search pipeline: . PUT _ingest/pipeline/&lt;pipeline_name&gt; . In the pipeline request body, The text_embedding processor, the only processor supported by Neural Search, converts a document’s text to vector embeddings. text_embedding uses field_maps to determine what fields from which to generate vector embeddings and also which field to store the embedding. Path parameter . Use pipeline_name to create a name for your Neural Search pipeline. Request fields . | Field | Data type | Description | . | description | string | A description of the processor. | . | model_id | string | The ID of the model that will be used in the embedding interface. The model must be indexed in OpenSearch before it can be used in Neural Search. For more information, see Model Serving Framework | . | input_field_name | string | The field name used to cache text for text embeddings. | . | output_field_name | string | The name of the field in which output text is stored. | . Example request . Use the following example request to create a pipeline: . PUT _ingest/pipeline/nlp-pipeline { \"description\": \"An example neural search pipeline\", \"processors\" : [ { \"text_embedding\": { \"model_id\": \"bxoDJ7IHGM14UqatWc_2j\", \"field_map\": { \"passage_text\": \"passage_embedding\" } } } ] } . Example response . OpenSearch responds with an acknowledgment of the pipeline’s creation. PUT _ingest/pipeline/nlp-pipeline { \"acknowledged\" : true } . ",
    "url": "https://vagimeli.github.io/search-plugins/neural-search/#ingest-data-with-neural-search",
    "relUrl": "/search-plugins/neural-search/#ingest-data-with-neural-search"
  },"854": {
    "doc": "Neural Search plugin",
    "title": "Create an index for ingestion",
    "content": "In order to use the text embedding processor defined in your pipelines, create an index with mapping data that aligns with the maps specified in your pipeline. For example, the output_fields defined in the field_map field of your processor request must map to the k-NN vector fields with a dimension that matches the model. Similarly, the text_fields defined in your processor should map to the text_fields in your index. Example request . The following example request creates an index that attaches to a Neural Search pipeline. Because the index maps to k-NN vector fields, the index setting field index-knn is set to true. Furthermore, mapping settings use k-NN method definitions to match the maps defined in the Neural Search pipeline. PUT /my-nlp-index-1 { \"settings\": { \"index.knn\": true, \"default_pipeline\": \"&lt;pipeline_name&gt;\" }, \"mappings\": { \"properties\": { \"passage_embedding\": { \"type\": \"knn_vector\", \"dimension\": int, \"method\": { \"name\": \"string\", \"space_type\": \"string\", \"engine\": \"string\", \"parameters\": json_object } }, \"passage_text\": { \"type\": \"text\" }, } } } . Example response . OpenSearch responds with information about your new index: . { \"acknowledged\" : true, \"shards_acknowledged\" : true, \"index\" : \"my-nlp-index-1\" } . ",
    "url": "https://vagimeli.github.io/search-plugins/neural-search/#create-an-index-for-ingestion",
    "relUrl": "/search-plugins/neural-search/#create-an-index-for-ingestion"
  },"855": {
    "doc": "Neural Search plugin",
    "title": "Ingest documents into Neural Search",
    "content": "Document ingestion is managed by OpenSearch’s Ingest API, similarly to other OpenSearch indexes. For example, you can ingest a document that contains the passage_text: \"Hello world\" with a simple POST method: . POST /my-nlp-index-1/_doc { \"passage_text\": \"Hello world\" } . With the text_embedding processor in place through a Neural Search pipeline, the example indexes “Hello world” as a text_field and converts “Hello world” into an associated k-NN vector field. ",
    "url": "https://vagimeli.github.io/search-plugins/neural-search/#ingest-documents-into-neural-search",
    "relUrl": "/search-plugins/neural-search/#ingest-documents-into-neural-search"
  },"856": {
    "doc": "Neural Search plugin",
    "title": "Search a neural index",
    "content": "If you want to use a language model to convert a text query into a k-NN vector query, use the neural query fields in your query. The neural query request fields can be used in both the k-NN plugin API and Query DSL. Furthermore, you can use a k-NN search filter to refine your neural search query. Neural request fields . Include the following request fields under the neural field in your query: . | Field | Data type | Description | . | vector_field | string | The vector field against which to run a search query. | . | query_text | string | The query text from which to produce queries. | . | model_id | string | The ID of the model that will be used in the embedding interface. The model must be indexed in OpenSearch before it can be used in Neural Search. | . | k | integer | The number of results the k-NN search returns. | . Example request . The following example request uses a search query that returns vectors for the “Hello World” query text: . GET my_index/_search { \"query\": { \"bool\" : { \"filter\": { \"range\": { \"distance\": { \"lte\" : 20 } } }, \"should\" : [ { \"script_score\": { \"query\": { \"neural\": { \"passage_vector\": { \"query_text\": \"Hello world\", \"model_id\": \"xzy76xswsd\", \"k\": 100 } } }, \"script\": { \"source\": \"_score * 1.5\" } } } , { \"script_score\": { \"query\": { \"match\": { \"passage_text\": \"Hello world\" } }, \"script\": { \"source\": \"_score * 1.7\" } } } ] } } } . ",
    "url": "https://vagimeli.github.io/search-plugins/neural-search/#search-a-neural-index",
    "relUrl": "/search-plugins/neural-search/#search-a-neural-index"
  },"857": {
    "doc": "Point in Time API",
    "title": "Point in Time API",
    "content": "Use the Point in Time (PIT) API to manage PITs. . | Create a PIT . | Path and HTTP methods | Path parameters | Query parameters | Response fields | . | Extend a PIT time | List all PITs . | Cross-cluster behavior | Response fields | . | Delete PITs . | Cross-cluster behavior | Request fields | Response fields | . | PIT segments . | Request fields | . | PIT settings | . ",
    "url": "https://vagimeli.github.io/search-plugins/point-in-time-api/",
    "relUrl": "/search-plugins/point-in-time-api/"
  },"858": {
    "doc": "Point in Time API",
    "title": "Create a PIT",
    "content": "Introduced 2.4 . Creates a PIT. The keep_alive query parameter is required; it specifies how long to keep a PIT. Path and HTTP methods . POST /&lt;target_indexes&gt;/_search/point_in_time?keep_alive=1h&amp;routing=&amp;expand_wildcards=&amp;preference= . Path parameters . | Parameter | Data type | Description | . | target_indexes | String | The name(s) of the target index(es) for the PIT. May contain a comma-separated list or a wildcard index pattern. | . Query parameters . | Parameter | Data type | Description | . | keep_alive | Time | The amount of time to keep the PIT. Every time you access a PIT by using the Search API, the PIT lifetime is extended by the amount of time equal to the keep_alive parameter. Required. | . | preference | String | The node or the shard used to perform the search. Optional. Default is random. | . | routing | String | Specifies to route search requests to a specific shard. Optional. Default is the document’s _id. | . | expand_wildcards | String | The type of index that can match the wildcard pattern. Supports comma-separated values. Valid values are the following:- all: Match any index or data stream, including hidden ones. - open: Match open, non-hidden indexes or non-hidden data streams. - closed: Match closed, non-hidden indexes or non-hidden data streams. - hidden: Match hidden indexes or data streams. Must be combined with open, closed or both open and closed.- none: No wildcard patterns are accepted. Optional. Default is open. | . | allow_partial_pit_creation | Boolean | Specifies whether to create a PIT with partial failures. Optional. Default is true. | . Example request . POST /my-index-1/_search/point_in_time?keep_alive=100m . Example response . { \"pit_id\": \"o463QQEPbXktaW5kZXgtMDAwMDAxFnNOWU43ckt3U3IyaFVpbGE1UWEtMncAFjFyeXBsRGJmVFM2RTB6eVg1aVVqQncAAAAAAAAAAAIWcDVrM3ZIX0pRNS1XejE5YXRPRFhzUQEWc05ZTjdyS3dTcjJoVWlsYTVRYS0ydwAA\", \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"creation_time\": 1658146050064 } . Response fields . | Field | Data type | Description | . | pit_id | Base64 encoded binary | The PIT ID. | . | creation_time | long | The time the PIT was created, in milliseconds since the epoch. | . ",
    "url": "https://vagimeli.github.io/search-plugins/point-in-time-api/#create-a-pit",
    "relUrl": "/search-plugins/point-in-time-api/#create-a-pit"
  },"859": {
    "doc": "Point in Time API",
    "title": "Extend a PIT time",
    "content": "You can extend a PIT time by providing a keep_alive parameter in the pit object when you perform a search: . GET /_search { \"size\": 10000, \"query\": { \"match\" : { \"user.id\" : \"elkbee\" } }, \"pit\": { \"id\": \"46ToAwMDaWR5BXV1aWQyKwZub2RlXzMAAAAAAAAAACoBYwADaWR4BXV1aWQxAgZub2RlXzEAAAAAAAAAAAEBYQADaWR5BXV1aWQyKgZub2RlXzIAAAAAAAAAAAwBYgACBXV1aWQyAAAFdXVpZDEAAQltYXRjaF9hbGw_gAAAAA==\", \"keep_alive\": \"100m\" }, \"sort\": [ {\"@timestamp\": {\"order\": \"asc\", \"format\": \"strict_date_optional_time_nanos\"}}, {\"_shard_doc\": \"desc\"} ], \"search_after\": [ \"2021-05-20T05:30:04.832Z\" ] } . The keep_alive parameter in a search request is optional. It specifies the amount by which to extend the time to keep a PIT. ",
    "url": "https://vagimeli.github.io/search-plugins/point-in-time-api/#extend-a-pit-time",
    "relUrl": "/search-plugins/point-in-time-api/#extend-a-pit-time"
  },"860": {
    "doc": "Point in Time API",
    "title": "List all PITs",
    "content": "Introduced 2.4 . Returns all PITs in the OpenSearch cluster. Cross-cluster behavior . The List All PITs API returns only local PITs or mixed PITs (PITs created in both local and remote clusters). It does not return fully remote PITs. Example request . GET /_search/point_in_time/_all . Example response . { \"pits\": [ { \"pit_id\": \"o463QQEPbXktaW5kZXgtMDAwMDAxFnNOWU43ckt3U3IyaFVpbGE1UWEtMncAFjFyeXBsRGJmVFM2RTB6eVg1aVVqQncAAAAAAAAAAAEWcDVrM3ZIX0pRNS1XejE5YXRPRFhzUQEWc05ZTjdyS3dTcjJoVWlsYTVRYS0ydwAA\", \"creation_time\": 1658146048666, \"keep_alive\": 6000000 }, { \"pit_id\": \"o463QQEPbXktaW5kZXgtMDAwMDAxFnNOWU43ckt3U3IyaFVpbGE1UWEtMncAFjFyeXBsRGJmVFM2RTB6eVg1aVVqQncAAAAAAAAAAAIWcDVrM3ZIX0pRNS1XejE5YXRPRFhzUQEWc05ZTjdyS3dTcjJoVWlsYTVRYS0ydwAA\", \"creation_time\": 1658146050064, \"keep_alive\": 6000000 } ] } . Response fields . | Field | Data type | Description | . | pits | Array of JSON objects | The list of all PITs. | . Each PIT object contains the following fields. | Field | Data type | Description | . | pit_id | Base64 encoded binary | The PIT ID. | . | creation_time | long | The time the PIT was created, in milliseconds since the epoch. | . | keep_alive | long | The amount of time to keep the PIT, in milliseconds. | . ",
    "url": "https://vagimeli.github.io/search-plugins/point-in-time-api/#list-all-pits",
    "relUrl": "/search-plugins/point-in-time-api/#list-all-pits"
  },"861": {
    "doc": "Point in Time API",
    "title": "Delete PITs",
    "content": "Introduced 2.4 . Deletes one, several, or all PITs. PITs are automatically deleted when the keep_alive time period elapses. However, to deallocate resources, you can delete a PIT using the Delete PIT API. The Delete PIT API supports deleting a list of PITs by ID or deleting all PITs at once. Cross-cluster behavior . The Delete PITs by ID API fully supports deleting cross-cluster PITs. The Delete All PITs API deletes only local PITs or mixed PITs (PITs created in both local and remote clusters). It does not delete fully remote PITs. Sample Request: Delete all PITs . DELETE /_search/point_in_time/_all . If you want to delete one or several PITs, specify their PIT IDs in the request body. Request fields . | Field | Data type | Description | . | pit_id | Base64 encoded binary or an array of binaries | The PIT IDs of the PITs to be deleted. Required. | . Example request: Delete PITs by ID . DELETE /_search/point_in_time { \"pit_id\": [ \"o463QQEPbXktaW5kZXgtMDAwMDAxFkhGN09fMVlPUkVPLXh6MUExZ1hpaEEAFjBGbmVEZHdGU1EtaFhhUFc4ZkR5cWcAAAAAAAAAAAEWaXBPNVJtZEhTZDZXTWFFR05waXdWZwEWSEY3T18xWU9SRU8teHoxQTFnWGloQQAA\", \"o463QQEPbXktaW5kZXgtMDAwMDAxFkhGN09fMVlPUkVPLXh6MUExZ1hpaEEAFjBGbmVEZHdGU1EtaFhhUFc4ZkR5cWcAAAAAAAAAAAIWaXBPNVJtZEhTZDZXTWFFR05waXdWZwEWSEY3T18xWU9SRU8teHoxQTFnWGloQQAA\" ] } . Example response . For each PIT, the response contains a JSON object with a PIT ID and a successful field that specifies whether the deletion was successful. Partial failures are treated as failures. { \"pits\": [ { \"successful\": true, \"pit_id\": \"o463QQEPbXktaW5kZXgtMDAwMDAxFkhGN09fMVlPUkVPLXh6MUExZ1hpaEEAFjBGbmVEZHdGU1EtaFhhUFc4ZkR5cWcAAAAAAAAAAAEWaXBPNVJtZEhTZDZXTWFFR05waXdWZwEWSEY3T18xWU9SRU8teHoxQTFnWGloQQAA\" }, { \"successful\": false, \"pit_id\": \"o463QQEPbXktaW5kZXgtMDAwMDAxFkhGN09fMVlPUkVPLXh6MUExZ1hpaEEAFjBGbmVEZHdGU1EtaFhhUFc4ZkR5cWcAAAAAAAAAAAIWaXBPNVJtZEhTZDZXTWFFR05waXdWZwEWSEY3T18xWU9SRU8teHoxQTFnWGloQQAA\" } ] } . Response fields . | Field | Data type | Description | . | successful | Boolean | Whether the delete operation was successful. | . | pit_id | Base64 encoded binary | The PIT ID of the PIT to be deleted. | . ",
    "url": "https://vagimeli.github.io/search-plugins/point-in-time-api/#delete-pits",
    "relUrl": "/search-plugins/point-in-time-api/#delete-pits"
  },"862": {
    "doc": "Point in Time API",
    "title": "PIT segments",
    "content": "Introduced 2.4 . Similarly to the CAT Segments API, the PIT Segments API provides low-level information about the disk utilization of a PIT by describing its Lucene segments. The PIT Segments API supports listing segment information of a specific PIT by ID or of all PITs at once. Example request: PIT segments of all PITs . GET /_cat/pit_segments/_all . If you want to list segments for one or several PITs, specify their PIT IDs in the request body. Request fields . | Field | Data type | Description | . | pit_id | Base64 encoded binary or an array of binaries | The PIT IDs of the PITs whose segments are to be listed. Required. | . Example request: PIT segments of PITs by ID . GET /_cat/pit_segments { \"pit_id\": [ \"o463QQEPbXktaW5kZXgtMDAwMDAxFkhGN09fMVlPUkVPLXh6MUExZ1hpaEEAFjBGbmVEZHdGU1EtaFhhUFc4ZkR5cWcAAAAAAAAAAAEWaXBPNVJtZEhTZDZXTWFFR05waXdWZwEWSEY3T18xWU9SRU8teHoxQTFnWGloQQAA\", \"o463QQEPbXktaW5kZXgtMDAwMDAxFkhGN09fMVlPUkVPLXh6MUExZ1hpaEEAFjBGbmVEZHdGU1EtaFhhUFc4ZkR5cWcAAAAAAAAAAAIWaXBPNVJtZEhTZDZXTWFFR05waXdWZwEWSEY3T18xWU9SRU8teHoxQTFnWGloQQAA\" ] } . Example response . index shard prirep ip segment generation docs.count docs.deleted size size.memory committed searchable version compound index1 0 r 10.212.36.190 _0 0 4 0 3.8kb 1364 false true 8.8.2 true index1 1 p 10.212.36.190 _0 0 3 0 3.7kb 1364 false true 8.8.2 true index1 2 r 10.212.74.139 _0 0 2 0 3.6kb 1364 false true 8.8.2 true . ",
    "url": "https://vagimeli.github.io/search-plugins/point-in-time-api/#pit-segments",
    "relUrl": "/search-plugins/point-in-time-api/#pit-segments"
  },"863": {
    "doc": "Point in Time API",
    "title": "PIT settings",
    "content": "You can specify the following settings for a PIT. | Setting | Description | Default | . | point_in_time.max_keep_alive | A cluster-level setting that specifies the maximum value for the keep_alive parameter. | 24h | . | search.max_open_pit_context | A node-level setting that specifies the maximum number of open PIT contexts for the node. | 300 | . ",
    "url": "https://vagimeli.github.io/search-plugins/point-in-time-api/#pit-settings",
    "relUrl": "/search-plugins/point-in-time-api/#pit-settings"
  },"864": {
    "doc": "Point in Time",
    "title": "Point in Time",
    "content": "Point in Time (PIT) lets you run different queries against a dataset that is fixed in time. Normally, if you run a query on an index multiple times, the same query may return different results because documents are continually indexed, updated, and deleted. If you need to run a query against the same data, you can preserve that data’s state by creating a PIT. The main use of the PIT feature is to couple it with the search_after functionality for deep pagination of search results. ",
    "url": "https://vagimeli.github.io/search-plugins/point-in-time/",
    "relUrl": "/search-plugins/point-in-time/"
  },"865": {
    "doc": "Point in Time",
    "title": "Paginating search results",
    "content": "Besides the PIT functionality, there are three ways to paginate search results in OpenSearch: using the Scroll API, specifying from and size parameters for your search, and using the search_after functionality. However, all three have limitations: . | The Scroll API’s search results are frozen at the moment of the request, but they are bound to a particular query. Additionally, scroll can only move forward in the search, so if a request for a page fails, the subsequent request skips that page and returns the following one. | If you specify the from and size parameters for your search, the search results are not frozen in time, so they may be inconsistent because of documents being indexed or deleted. The from and size feature is not recommended for deep pagination because every page request requires processing of all results and filtering them for the requested page. | The search_after search results are not frozen in time, so they may be inconsistent because of concurrent document indexing or deletion. | . The PIT functionality does not have the limitations of other pagination methods, because PIT search is not bound to a query, and it supports consistent pagination going forward and backward. If you have looked at page one of your results and are now on page two, you will see the same page one if you go back. ",
    "url": "https://vagimeli.github.io/search-plugins/point-in-time/#paginating-search-results",
    "relUrl": "/search-plugins/point-in-time/#paginating-search-results"
  },"866": {
    "doc": "Point in Time",
    "title": "PIT search",
    "content": "PIT search has the same capabilities as regular search, except PIT search acts on an older dataset, while a regular search acts on a live dataset. PIT search is not bound to a query, so you can run different queries on the same dataset, which is frozen in time. You can use the Create PIT API to create a PIT. When you create a PIT for a set of indexes, OpenSearch locks a set of segments for those indexes, freezing them in time. On a lower level, none of the resources required for this PIT are modified or deleted. If the segments that are part of a PIT are merged, OpenSearch retains a copy of those segments for the period of time specified at PIT creation by the keep_alive parameter. The create PIT operation returns a PIT ID, which you can use to run multiple queries on the frozen dataset. Even though the indexes continue to ingest data and modify or delete documents, the PIT references the data that has not changed since the PIT creation. When your query contains a PIT ID, you don’t need to pass the indexes to the search because it will use that PIT. A search with a PIT ID will produce exactly the same result when you run it multiple times. In case of a cluster or node failure, all PIT data is lost. ",
    "url": "https://vagimeli.github.io/search-plugins/point-in-time/#pit-search",
    "relUrl": "/search-plugins/point-in-time/#pit-search"
  },"867": {
    "doc": "Point in Time",
    "title": "Pagination with PIT and search_after",
    "content": "When you run a query with a PIT ID, you can use the search_after parameter to retrieve the next page of results. This gives you control over the order of documents in the pages of results. Run a search query with a PIT ID: . GET /_search { \"size\": 10000, \"query\": { \"match\" : { \"user.id\" : \"elkbee\" } }, \"pit\": { \"id\": \"46ToAwMDaWR5BXV1aWQyKwZub2RlXzMAAAAAAAAAACoBYwADaWR4BXV1aWQxAgZub2RlXzEAAAAAAAAAAAEBYQADaWR5BXV1aWQyKgZub2RlXzIAAAAAAAAAAAwBYgACBXV1aWQyAAAFdXVpZDEAAQltYXRjaF9hbGw_gAAAAA==\", \"keep_alive\": \"100m\" }, \"sort\": [ {\"@timestamp\": {\"order\": \"asc\", \"format\": \"strict_date_optional_time_nanos\"}}, {\"_shard_doc\": \"desc\"} ] } . The response contains the first 10,000 documents that match the query. To get the next set of documents, run the same query with the last document’s sort values as the search_after parameter, keeping the same sort and pit.id. You can use the optional keep_alive parameter to extend the PIT time: . GET /_search { \"size\": 10000, \"query\": { \"match\" : { \"user.id\" : \"elkbee\" } }, \"pit\": { \"id\": \"46ToAwMDaWR5BXV1aWQyKwZub2RlXzMAAAAAAAAAACoBYwADaWR4BXV1aWQxAgZub2RlXzEAAAAAAAAAAAEBYQADaWR5BXV1aWQyKgZub2RlXzIAAAAAAAAAAAwBYgACBXV1aWQyAAAFdXVpZDEAAQltYXRjaF9hbGw_gAAAAA==\", \"keep_alive\": \"100m\" }, \"sort\": [ {\"@timestamp\": {\"order\": \"asc\", \"format\": \"strict_date_optional_time_nanos\"}}, {\"_shard_doc\": \"desc\"} ], \"search_after\": [ \"2021-05-20T05:30:04.832Z\" ] } . ",
    "url": "https://vagimeli.github.io/search-plugins/point-in-time/#pagination-with-pit-and-search_after",
    "relUrl": "/search-plugins/point-in-time/#pagination-with-pit-and-search_after"
  },"868": {
    "doc": "Point in Time",
    "title": "Search slicing",
    "content": "Using search_after with PIT for pagination gives you control over ordering of the results. If you don’t need results in any specific order, or if you want the ability to jump from a page to a non-consecutive page, you can use search slicing. Search slicing splits a PIT search into multiple slices that can be consumed independently by a client application. For example, if you have a PIT search query that has 1,000,000 results and you want to return 50,000 results at a time, your client application has to make 20 consecutive calls to receive each batch of results. If you use search slicing, you can parallelize these 20 calls. In your multithreaded client application you can use five slices for each PIT. As a result, you will have 5 10,000-hit slices that can be consumed by five different threads in your client, instead of having a single thread consume 50,000 results. To use search slicing, you have to specify two parameters: . | slice.id is the slice ID you are requesting. | slice.max is the number of slices to break the search response into. | . The following PIT search query illustrates search slicing: . GET /_search { \"slice\": { \"id\": 0, // id is the slice (page) number being requested. In every request we can only query for one slice \"max\": 2 // max is the total number of slices (pages) the search response will be broken down into }, \"query\": { \"match\": { \"message\": \"foo\" } }, \"pit\": { \"id\": \"46ToAwMDaWR5BXV1aWQyKwZub2RlXzMAAAAAAAAAACoBYwADaWR4BXV1aWQxAgZub2RlXzEAAAAAAAAAAAEBYQADaWR5BXV1aWQyKgZub2RlXzIAAAAAAAAAAAwBYgACBXV1aWQyAAAFdXVpZDEAAQltYXRjaF9hbGw_gAAAAA==\" } } . In every request you can only query for one slice, so the next query will be the same as the previous one, except the slice.id will be 1. ",
    "url": "https://vagimeli.github.io/search-plugins/point-in-time/#search-slicing",
    "relUrl": "/search-plugins/point-in-time/#search-slicing"
  },"869": {
    "doc": "Point in Time",
    "title": "Security model",
    "content": "This section describes the permissions needed to use PIT API operations if you are running OpenSearch with the Security plugin enabled. Users can access all PIT API operations using the point_in_time_full_access role. If this role doesn’t meet your needs, mix and match individual PIT permissions to suit your use case. Each action corresponds to an operation in the REST API. For example, the indices:data/read/point_in_time/create permission lets you create a PIT. The following are the possible permissions: . | indices:data/read/point_in_time/create – Create API | indices:data/read/point_in_time/delete – Delete API | indices:data/read/point_in_time/readall – List All PITs API | indices:data/read/search – Search API | indices:monitor/point_in_time/segments – PIT Segments API | . For all API operations, such as list all and delete all, the user needs the all indexes (*) permission. For API operations such as search, create PIT, or delete list, the user only needs individual index permissions. The PIT IDs always contain the underlying (resolved) indexes when saved. The following sections describe the required permissions for aliases and data streams. Alias permissions . For aliases, users must have either index or alias permissions for any PIT operation. Data stream permissions . For data streams, users must have both the data stream and the data stream’s backing index permissions for any PIT operation. For example, the user must have permissions for the data-stream-11 data stream and for its backing index .ds-my-data-stream11-000001. If users have the data stream permissions only, they will be able to create a PIT, but they will not be able to use the PIT ID for other operations, such as search, without the backing index permissions. ",
    "url": "https://vagimeli.github.io/search-plugins/point-in-time/#security-model",
    "relUrl": "/search-plugins/point-in-time/#security-model"
  },"870": {
    "doc": "Point in Time",
    "title": "API",
    "content": "The following table lists all Point in Time API functions. | Function | API | Description | . | Create PIT | POST /&lt;target_indexes&gt;/_search/point_in_time?keep_alive=1h | Creates a PIT. | . | List PIT | GET /_search/point_in_time/_all | Lists all PITs. | . | Delete PIT | DELETE /_search/point_in_time DELETE /_search/point_in_time/_all | Deletes a PIT or all PITs. | . | PIT segments | GET /_cat/pit_segments/_all | Provides information about the disk utilization of a PIT by describing its Lucene segments. | . For information about the relevant cluster and node settings, see PIT Settings. ",
    "url": "https://vagimeli.github.io/search-plugins/point-in-time/#api",
    "relUrl": "/search-plugins/point-in-time/#api"
  },"871": {
    "doc": "Querqy",
    "title": "Querqy",
    "content": "Querqy is a community plugin for query rewriting that helps to solve relevance issues, making search engines more precise regarding matching and scoring. Querqy is currently only supported in OpenSearch 2.3. ",
    "url": "https://vagimeli.github.io/search-plugins/querqy/index/",
    "relUrl": "/search-plugins/querqy/index/"
  },"872": {
    "doc": "Querqy",
    "title": "Querqy plugin installation",
    "content": "The Querqy plugin is now available for OpenSearch 2.3.0. Run the following command to install the Querqy plugin./bin/opensearch-plugin install \\ \"https://repo1.maven.org/maven2/org/querqy/opensearch-querqy/1.0.os2.3.0/opensearch-querqy-1.0.os2.3.0.zip\" . Answer yes to the security prompts during the installation as Querqy requires additional permissions to load query rewriters. After installing the Querqy plugin you can find comprehensive documentation on the Querqy.org site: Querqy . ",
    "url": "https://vagimeli.github.io/search-plugins/querqy/index/#querqy-plugin-installation",
    "relUrl": "/search-plugins/querqy/index/#querqy-plugin-installation"
  },"873": {
    "doc": "Querqy",
    "title": "Path and HTTP methods",
    "content": "POST /myindex/_search . ",
    "url": "https://vagimeli.github.io/search-plugins/querqy/index/#path-and-http-methods",
    "relUrl": "/search-plugins/querqy/index/#path-and-http-methods"
  },"874": {
    "doc": "Querqy",
    "title": "Example query",
    "content": "{ \"query\": { \"querqy\": { \"matching_query\": { \"query\": \"books\" }, \"query_fields\": [ \"title^3.0\", \"words^2.1\", \"shortSummary\"] } } } . ",
    "url": "https://vagimeli.github.io/search-plugins/querqy/index/#example-query",
    "relUrl": "/search-plugins/querqy/index/#example-query"
  },"875": {
    "doc": "Search relevance",
    "title": "Search relevance",
    "content": "Search relevance evaluates the accuracy of the search results returned by a query. The higher the relevance, the better the search engine. Compare Search Results is the first search relevance feature in OpenSearch. ",
    "url": "https://vagimeli.github.io/search-plugins/search-relevance/index/",
    "relUrl": "/search-plugins/search-relevance/index/"
  },"876": {
    "doc": "Search relevance",
    "title": "Compare Search Results",
    "content": "Compare Search Results is an experimental feature. For updates on the progress of Compare Search Results and other search relevance features, or if you want to leave feedback that could help improve the feature, join the discussion on the OpenSearch forum. Compare Search Results in OpenSearch Dashboards lets you compare results from two queries side by side to determine whether one query produces better results than the other. Using this tool, you can evaluate search quality by experimenting with queries. For example, you can see how results change when you apply one of the following query changes: . | Weighting different fields differently | Different stemming or lemmatization strategies | Shingling | . ",
    "url": "https://vagimeli.github.io/search-plugins/search-relevance/index/#compare-search-results",
    "relUrl": "/search-plugins/search-relevance/index/#compare-search-results"
  },"877": {
    "doc": "Search relevance",
    "title": "Prerequisites",
    "content": "Before you get started, you must index data in OpenSearch. To learn how to create a new index, see Index data. Alternatively, you can add sample data in OpenSearch Dashboards using the following steps: . | On the top menu bar, go to OpenSearch Dashboards &gt; Overview. | Select View app directory. | Select Add sample data. | Choose one of the built-in datasets and select Add data. | . ",
    "url": "https://vagimeli.github.io/search-plugins/search-relevance/index/#prerequisites",
    "relUrl": "/search-plugins/search-relevance/index/#prerequisites"
  },"878": {
    "doc": "Search relevance",
    "title": "Using Compare Search Results in OpenSearch Dashboards",
    "content": "To compare search results in OpenSearch Dashboards, perform the following steps. Step 1: On the top menu bar, go to OpenSearch Plugins &gt; Search Relevance. Step 2: Enter the search text in the search bar. Step 3: Select an index for Query 1 and enter a query (request body only) in OpenSearch Query DSL. The GET HTTP method and the _search endpoint are implicit. Use the %SearchText% variable to refer to the text in the search bar. The following is an example query: . { \"query\": { \"multi_match\": { \"query\": \"%SearchText%\", \"fields\": [ \"description\", \"item_name\" ] } } } . Step 4: Select an index for Query 2 and enter a query (request body only). The following example query boosts the title field in search results: . { \"query\": { \"multi_match\": { \"query\": \"%SearchText%\", \"fields\": [ \"description\", \"item_name^3\" ] } } } . Step 5: Select Search and compare the results in Result 1 and Result 2. The following example screen shows a search for the word “cup” in the description and item_name fields with and without boosting the item_name: . If a result in Result 1 appears in Result 2, the Up and Down indicators below the result number signify how many places the result moved up or down compared to the same result in Result 2. In this example, the document with the ID 2 is Up 1 place in Result 2 compared to Result 1 and Down 1 place in Result 1 compared to Result 2. ",
    "url": "https://vagimeli.github.io/search-plugins/search-relevance/index/#using-compare-search-results-in-opensearch-dashboards",
    "relUrl": "/search-plugins/search-relevance/index/#using-compare-search-results-in-opensearch-dashboards"
  },"879": {
    "doc": "Search relevance",
    "title": "Changing the number of results",
    "content": "By default, OpenSearch returns the top 10 results. To change the number of returned results to a different value, specify the size parameter in the query: . { \"size\": 15, \"query\": { \"multi_match\": { \"query\": \"%SearchText%\", \"fields\": [ \"title^3\", \"text\" ] } } } . Setting size to a high value (for example, larger than 250 documents) may degrade performance. You cannot save a given comparison for future use, so Compare Search Results is not suitable for systematic testing. ",
    "url": "https://vagimeli.github.io/search-plugins/search-relevance/index/#changing-the-number-of-results",
    "relUrl": "/search-plugins/search-relevance/index/#changing-the-number-of-results"
  },"880": {
    "doc": "Search relevance",
    "title": "Comparing OpenSearch search results with re-ranked results",
    "content": "One use case for Compare Search Results is to compare raw OpenSearch results with the same results processed by a re-ranking application. An example of such a re-ranker is Kendra Intelligent Ranking for OpenSearch, contributed by the Amazon Kendra team. This plugin takes search results from OpenSearch and applies Amazon Kendra’s semantic relevance rankings calculated using vector embeddings and other semantic search techniques. For many applications, this provides better result rankings. To try Kendra Intelligent Ranking, you must first set up the Amazon Kendra service. To get started, see Amazon Kendra. For detailed information, including plugin setup instructions, see Intelligently ranking OpenSearch (self managed) results using Amazon Kendra. Once you’ve set up Kendra Intelligent Ranking, enter a query in Query 1 and enter the same query using Kendra Intelligent Ranking in Query 2. Then compare the search results from OpenSearch and Amazon Kendra. Example . The following example searches for the text “snacking nuts” in the abo index. The documents in the index contain snack descriptions in the bullet_point array. | Enter snacking nuts in the search bar. | Enter the following query, which searches the bullet_point field for the search text “snacking nuts”, in Query 1: . { \"query\": { \"match\": { \"bullet_point\": \"%SearchText%\" } }, \"size\": 25 } . | Enter the same query with intelligent ranking in Query 2: . { \"query\" : { \"match\" : { \"bullet_point\": \"%SearchText%\" } }, \"size\": 25, \"ext\": { \"search_configuration\":{ \"result_transformer\" : { \"kendra_intelligent_ranking\": { \"order\": 1, \"properties\": { \"title_field\": \"item_name\", \"body_field\": \"bullet_point\" } } } } } } . In the preceding query, body_field refers to the body field of the documents in the index, which Kendra Intelligent Ranking uses to rank the results. The body_field is required, while the title_field is optional. | Select Search and compare the results in Result 1 and Result 2. | . ",
    "url": "https://vagimeli.github.io/search-plugins/search-relevance/index/#comparing-opensearch-search-results-with-re-ranked-results",
    "relUrl": "/search-plugins/search-relevance/index/#comparing-opensearch-search-results-with-re-ranked-results"
  },"881": {
    "doc": "Search Relevance Stats API",
    "title": "Search Relevance Stats API",
    "content": "Introduced 2.7 . The Search Relevance Stats API provides information about Search Relevance plugin operations. The Search Relevance plugin processes operations sent by the Compare Search Results Dashboards tool. The Search Relevance Stats API captures statistics for a one-minute interval during which it receives a request. For example, if a request is received at 23:59:59.004, statistics are collected for the 23:58:00.000–23:58:59.999 time interval. To change the default time interval for which statistics are collected, update the searchRelevanceDashboards.metrics.metricInterval setting in the opensearch_dashboards.yml file with the new time interval in milliseconds. The opensearch_dashboards.yml file is located in the config folder of your OpenSearch Dashboards installation. For example, the following sets the interval to one second: . searchRelevanceDashboards.metrics.metricInterval: 1000 . Example request . You can access the Search Relevance Stats API by providing its URL address in the following format: . &lt;opensearch-dashboards-endpoint-address&gt;/api/relevancy/stats . The OpenSearch Dashboards endpoint address may contain a port number if it is specified in the OpenSearch configuration file. The specific URL format depends on the type of OpenSearch deployment and the network environment in which it is hosted. You can query the endpoint in two ways: . | By accessing the endpoint address (for example, http://localhost:5601/api/relevancy/stats) in a browser . | By using the curl command in the terminal: . curl -X GET http://localhost:5601/api/relevancy/stats . copy . | . Example response . The following is the response for the preceding request: . { \"data\": { \"search_relevance\": { \"fetch_index\": { \"200\": { \"response_time_total\": 28.79286289215088, \"count\": 1 } }, \"single_search\": { \"200\": { \"response_time_total\": 29.817723274230957, \"count\": 1 } }, \"comparison_search\": { \"200\": { \"response_time_total\": 13.265346050262451, \"count\": 2 } } } }, \"overall\": { \"response_time_avg\": 17.968983054161072, \"requests_per_second\": 0.06666666666666667 }, \"counts_by_component\": { \"search_relevance\": 4 }, \"counts_by_status_code\": { \"200\": 4 } } . ",
    "url": "https://vagimeli.github.io/search-plugins/search-relevance/stats-api/",
    "relUrl": "/search-plugins/search-relevance/stats-api/"
  },"882": {
    "doc": "Search Relevance Stats API",
    "title": "Response fields",
    "content": "The following table lists all response fields. | Field | Data type | Description | . | data.search_relevance | Object | Statistics related to Search Relevance operations. | . | overall | Object | The average statistics for all operations. | . | overall.response_time_avg | Double | The average response time for all operations, in milliseconds. | . | overall.requests_per_second | Double | The average number of requests per second for all operations. | . | counts_by_component | Object | The sum of all count values for all child objects of the data object. | . | counts_by_component.search_relevance | The total number of responses for all operations in the search_relevance object. |   | . | counts_by_status_code | Object | Contains a list of all response codes and their counts for all Search Relevance operations. | . The data.search_relevance object . The data.search_relevance object contains the fields described in the following table. | Field | Data type | Description | . | comparison_search | Object | Statistics related to the comparison search operation. A comparison search operation is a request that compares two queries when both Query 1 and Query 2 are entered in the Compare Search Results tool. | . | single_search | Object | Statistics related to a single search operation. A single search operation is a request to run a single query when only Query 1 or Query 2, not both, is entered in the Compare Search Results tool. | . | fetch_index | Object | Statistics related to the operation of fetching the index or indexes for a comparison search or single search. | . Each of the comparison_search, single_search, and fetch_index objects contains a list of HTTP response codes. The following table lists the fields for each response code. | Field | Data type | Description | . | response_time_total | Double | The sum of the response times for the responses with this HTTP code, in milliseconds. | . | count | Integer | The total number of responses with this HTTP code. | . ",
    "url": "https://vagimeli.github.io/search-plugins/search-relevance/stats-api/#response-fields",
    "relUrl": "/search-plugins/search-relevance/stats-api/#response-fields"
  },"883": {
    "doc": "Search templates",
    "title": "Search templates",
    "content": "You can convert your full-text queries into a search template to accept user input and dynamically insert it into your query. For example, if you use OpenSearch as a backend search engine for your application or website, you can take in user queries from a search bar or a form field and pass them as parameters into a search template. That way, the syntax to create OpenSearch queries is abstracted from your end users. When you’re writing code to convert user input into OpenSearch queries, you can simplify your code with search templates. If you need to add fields to your search query, you can just modify the template without making changes to your code. Search templates use the Mustache language. For a list of all syntax options, see the Mustache manual. ",
    "url": "https://vagimeli.github.io/search-plugins/search-template/",
    "relUrl": "/search-plugins/search-template/"
  },"884": {
    "doc": "Search templates",
    "title": "Create search templates",
    "content": "A search template has two components: the query and the parameters. Parameters are user-inputted values that get placed into variables. Variables are represented with double braces in Mustache notation. When encountering a variable like {{var}} in the query, OpenSearch goes to the params section, looks for a parameter called var, and replaces it with the specified value. You can code your application to ask your user what they want to search for and then plug that value into the params object at runtime. This command defines a search template to find a play by its name. The {{play_name}} in the query is replaced by the value Henry IV: . GET _search/template { \"source\": { \"query\": { \"match\": { \"play_name\": \"{{play_name}}\" } } }, \"params\": { \"play_name\": \"Henry IV\" } } . This template runs the search on your entire cluster. To run this search on a specific index, add the index name to the request: . GET shakespeare/_search/template . Specify the from and size parameters: . GET _search/template { \"source\": { \"from\": \"{{from}}\", \"size\": \"{{size}}\", \"query\": { \"match\": { \"play_name\": \"{{play_name}}\" } } }, \"params\": { \"play_name\": \"Henry IV\", \"from\": 10, \"size\": 10 } } . To improve the search experience, you can define defaults so the user doesn’t have to specify every possible parameter. If the parameter is not defined in the params section, OpenSearch uses the default value. The syntax for defining the default value for a variable var is as follows: . {{var}}{{^var}}default value{{/var}} . This command sets the defaults for from as 10 and size as 10: . GET _search/template { \"source\": { \"from\": \"{{from}}{{^from}}10{{/from}}\", \"size\": \"{{size}}{{^size}}10{{/size}}\", \"query\": { \"match\": { \"play_name\": \"{{play_name}}\" } } }, \"params\": { \"play_name\": \"Henry IV\" } } . ",
    "url": "https://vagimeli.github.io/search-plugins/search-template/#create-search-templates",
    "relUrl": "/search-plugins/search-template/#create-search-templates"
  },"885": {
    "doc": "Search templates",
    "title": "Save and execute search templates",
    "content": "After the search template works the way you want it to, you can save the source of that template as a script, making it reusable for different input parameters. When saving the search template as a script, you need to specify the lang parameter as mustache: . POST _scripts/play_search_template { \"script\": { \"lang\": \"mustache\", \"source\": { \"from\": \"{{from}}{{^from}}0{{/from}}\", \"size\": \"{{size}}{{^size}}10{{/size}}\", \"query\": { \"match\": { \"play_name\": \"\" } } }, \"params\": { \"play_name\": \"Henry IV\" } } } . Now you can reuse the template by referring to its id parameter. You can reuse this source template for different input values. GET _search/template { \"id\": \"play_search_template\", \"params\": { \"play_name\": \"Henry IV\", \"from\": 0, \"size\": 1 } } . Sample output . { \"took\": 7, \"timed_out\": false, \"_shards\": { \"total\": 6, \"successful\": 6, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 3205, \"relation\": \"eq\" }, \"max_score\": 3.641852, \"hits\": [ { \"_index\": \"shakespeare\", \"_type\": \"_doc\", \"_id\": \"4\", \"_score\": 3.641852, \"_source\": { \"type\": \"line\", \"line_id\": 5, \"play_name\": \"Henry IV\", \"speech_number\": 1, \"line_number\": \"1.1.2\", \"speaker\": \"KING HENRY IV\", \"text_entry\": \"Find we a time for frighted peace to pant,\" } } ] } } . If you have a stored template and want to validate it, use the render operation: . POST _render/template { \"id\": \"play_search_template\", \"params\": { \"play_name\": \"Henry IV\" } } . Sample output . { \"template_output\": { \"from\": \"0\", \"size\": \"10\", \"query\": { \"match\": { \"play_name\": \"Henry IV\" } } } } . The following render operations are supported: . GET /_render/template POST /_render/template GET /_render/template/&lt;id&gt; POST /_render/template/&lt;id&gt; . ",
    "url": "https://vagimeli.github.io/search-plugins/search-template/#save-and-execute-search-templates",
    "relUrl": "/search-plugins/search-template/#save-and-execute-search-templates"
  },"886": {
    "doc": "Search templates",
    "title": "Advanced parameter conversion with search templates",
    "content": "You have a lot of different syntax options in Mustache to transpose the input parameters into a query. You can specify conditions, run loops, join arrays, convert arrays to JSON, and so on. Conditions . Use the section tag in Mustache to represent conditions: . {{#var}}var{{/var}} . When var is a boolean value, this syntax acts as an if condition. The {{#var}} and {{/var}} tags insert the values placed between them only if var evaluates to true. Using section tags would make your JSON invalid, so you must write your query in a string format instead. This command includes the size parameter in the query only when the limit parameter is set to true. In the following example, the limit parameter is true, so the size parameter is activated. As a result, you would get back only two documents. GET _search/template { \"source\": \"{ {{#limit}} \\\"size\\\": \\\"{{size}}\\\", {{/limit}} \\\"query\\\":{\\\"match\\\":{\\\"play_name\\\": \\\"{{play_name}}\\\"}}}\", \"params\": { \"play_name\": \"Henry IV\", \"limit\": true, \"size\": 2 } } . You can also design an if-else condition. This command sets size to 2 if limit is true. Otherwise, it sets size to 10. GET _search/template { \"source\": \"{ {{#limit}} \\\"size\\\": \\\"2\\\", {{/limit}} {{^limit}} \\\"size\\\": \\\"10\\\", {{/limit}} \\\"query\\\":{\\\"match\\\":{\\\"play_name\\\": \\\"{{play_name}}\\\"}}}\", \"params\": { \"play_name\": \"Henry IV\", \"limit\": true } } . Loops . You can also use the section tag to implement a foreach loop: . {{#var}}{{.}}}{{/var}} . When var is an array, the search template iterates through it and creates a terms query. GET _search/template { \"source\": \"{\\\"query\\\":{\\\"terms\\\":{\\\"play_name\\\":[\\\"{{#play_name}}\\\",\\\"{{.}}\\\",\\\"{{/play_name}}\\\"]}}}\", \"params\": { \"play_name\": [ \"Henry IV\", \"Othello\" ] } } . This template is rendered as: . GET _search/template { \"source\": { \"query\": { \"terms\": { \"play_name\": [ \"Henry IV\", \"Othello\" ] } } } } . Join . You can use the join tag to concatenate values of an array (separated by commas): . GET _search/template { \"source\": { \"query\": { \"match\": { \"text_entry\": \"{{#join}}{{text_entry}}{{/join}}\" } } }, \"params\": { \"text_entry\": [ \"To be\", \"or not to be\" ] } } . Renders as: . GET _search/template { \"source\": { \"query\": { \"match\": { \"text_entry\": \"{0=To be, 1=or not to be}\" } } } } . Convert to JSON . You can use the toJson tag to convert parameters to their JSON representation: . GET _search/template { \"source\": \"{\\\"query\\\":{\\\"bool\\\":{\\\"must\\\":[{\\\"terms\\\": {\\\"text_entries\\\": {{#toJson}}text_entries{{/toJson}} }}] }}}\", \"params\": { \"text_entries\": [ { \"term\": { \"text_entry\" : \"love\" } }, { \"term\": { \"text_entry\" : \"soldier\" } } ] } } . Renders as: . GET _search/template { \"source\": { \"query\": { \"bool\": { \"must\": [ { \"terms\": { \"text_entries\": [ { \"term\": { \"text_entry\": \"love\" } }, { \"term\": { \"text_entry\": \"soldier\" } } ] } } ] } } } } . ",
    "url": "https://vagimeli.github.io/search-plugins/search-template/#advanced-parameter-conversion-with-search-templates",
    "relUrl": "/search-plugins/search-template/#advanced-parameter-conversion-with-search-templates"
  },"887": {
    "doc": "Search templates",
    "title": "Multiple search templates",
    "content": "You can bundle multiple search templates and send them to your OpenSearch cluster in a single request using the msearch operation. This saves network round trip time, so you get back the response more quickly as compared to independent requests. GET _msearch/template {\"index\":\"shakespeare\"} {\"id\":\"if_search_template\",\"params\":{\"play_name\":\"Henry IV\",\"limit\":false,\"size\":2}} {\"index\":\"shakespeare\"} {\"id\":\"play_search_template\",\"params\":{\"play_name\":\"Henry IV\"}} . ",
    "url": "https://vagimeli.github.io/search-plugins/search-template/#multiple-search-templates",
    "relUrl": "/search-plugins/search-template/#multiple-search-templates"
  },"888": {
    "doc": "Search templates",
    "title": "Manage search templates",
    "content": "To list all scripts, run the following command: . GET _cluster/state/metadata?pretty&amp;filter_path=**.stored_scripts . To retrieve a specific search template, run the following command: . GET _scripts/&lt;name_of_search_template&gt; . To delete a search template, run the following command: . DELETE _scripts/&lt;name_of_search_template&gt; . ",
    "url": "https://vagimeli.github.io/search-plugins/search-template/#manage-search-templates",
    "relUrl": "/search-plugins/search-template/#manage-search-templates"
  },"889": {
    "doc": "Autocomplete",
    "title": "Autocomplete functionality",
    "content": "Autocomplete shows suggestions to users while they type. For example, if a user types “pop,” OpenSearch provides suggestions like “popcorn” or “popsicles.” These suggestions preempt your user’s intention and lead them to a possible search term more quickly. OpenSearch lets you design autocomplete that updates with each keystroke, provides a few relevant suggestions, and tolerates typos. Implement autocomplete using one of the following methods: . | Prefix matching | Edge n-gram matching | Search as you type | Completion suggesters | . While prefix matching happens at query time, the other three methods happen at index time. All methods are described in the following sections. ",
    "url": "https://vagimeli.github.io/search-plugins/searching-data/autocomplete/#autocomplete-functionality",
    "relUrl": "/search-plugins/searching-data/autocomplete/#autocomplete-functionality"
  },"890": {
    "doc": "Autocomplete",
    "title": "Prefix matching",
    "content": "Prefix matching finds documents that match the last term in a query string. For example, assume that the user types “qui” into a search UI. To autocomplete this phrase, use the match_phrase_prefix query to search for all text_entry field values that begin with the prefix “qui”: . GET shakespeare/_search { \"query\": { \"match_phrase_prefix\": { \"text_entry\": { \"query\": \"qui\", \"slop\": 3 } } } } . To make the word order and relative positions flexible, specify a slop value. To learn about the slop option, see Other advanced options. Prefix matching doesn’t require any special mappings. It works with your data as is. However, it’s a fairly resource-intensive operation. A prefix of a could match hundreds of thousands of terms and not be useful to your user. To limit the impact of prefix expansion, set max_expansions to a reasonable number: . GET shakespeare/_search { \"query\": { \"match_phrase_prefix\": { \"text_entry\": { \"query\": \"qui\", \"slop\": 3, \"max_expansions\": 10 } } } } . To learn about the max_expansions option, see Other advanced options. The ease of implementing query-time autocomplete comes at the cost of performance. When implementing this feature on a large scale, we recommend an index-time solution. With an index-time solution, you might experience slower indexing, but it’s a price you pay only once and not for every query. The edge n-gram, search-as-you-type, and completion suggester methods are index-time solutions. ",
    "url": "https://vagimeli.github.io/search-plugins/searching-data/autocomplete/#prefix-matching",
    "relUrl": "/search-plugins/searching-data/autocomplete/#prefix-matching"
  },"891": {
    "doc": "Autocomplete",
    "title": "Edge n-gram matching",
    "content": "During indexing, edge n-grams split a word into a sequence of n characters to support a faster lookup of partial search terms. If you n-gram the word “quick,” the results depend on the value of n. | n | Type | n-gram | . | 1 | Unigram | [ q, u, i, c, k ] | . | 2 | Bigram | [ qu, ui, ic, ck ] | . | 3 | Trigram | [ qui, uic, ick ] | . | 4 | Four-gram | [ quic, uick ] | . | 5 | Five-gram | [ quick ] | . Autocomplete needs only the beginning n-grams of a search phrase, so OpenSearch uses a special type of n-gram called edge n-gram. Edge n-gramming the word “quick” results in the following: . | q | qu | qui | quic | quick | . This follows the same sequence the user types. To configure a field to use edge n-grams, create an autocomplete analyzer with an edge_ngram filter: . PUT shakespeare { \"mappings\": { \"properties\": { \"text_entry\": { \"type\": \"text\", \"analyzer\": \"autocomplete\" } } }, \"settings\": { \"analysis\": { \"filter\": { \"edge_ngram_filter\": { \"type\": \"edge_ngram\", \"min_gram\": 1, \"max_gram\": 20 } }, \"analyzer\": { \"autocomplete\": { \"type\": \"custom\", \"tokenizer\": \"standard\", \"filter\": [ \"lowercase\", \"edge_ngram_filter\" ] } } } } } . This example creates the index and instantiates the edge n-gram filter and analyzer. The edge_ngram_filter produces edge n-grams with a minimum n-gram length of 1 (a single letter) and a maximum length of 20. So it offers suggestions for words of up to 20 letters. The autocomplete analyzer tokenizes a string into individual terms, lowercases the terms, and then produces edge n-grams for each term using the edge_ngram_filter. Use the analyze operation to test this analyzer: . POST shakespeare/_analyze { \"analyzer\": \"autocomplete\", \"text\": \"quick\" } . It returns edge n-grams as tokens: . | q | qu | qui | quic | quick | . Use the standard analyzer at search time. Otherwise, the search query splits into edge n-grams and you get results for everything that matches q, u, and i. This is one of the few occasions when you use different analyzers at index time and at query time: . GET shakespeare/_search { \"query\": { \"match\": { \"text_entry\": { \"query\": \"qui\", \"analyzer\": \"standard\" } } } } . The response contains the matching documents: . { \"took\": 5, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 533, \"relation\": \"eq\" }, \"max_score\": 9.712725, \"hits\": [ { \"_index\": \"shakespeare\", \"_id\": \"22006\", \"_score\": 9.712725, \"_source\": { \"type\": \"line\", \"line_id\": 22007, \"play_name\": \"Antony and Cleopatra\", \"speech_number\": 12, \"line_number\": \"5.2.44\", \"speaker\": \"CLEOPATRA\", \"text_entry\": \"Quick, quick, good hands.\" } }, { \"_index\": \"shakespeare\", \"_id\": \"54665\", \"_score\": 9.712725, \"_source\": { \"type\": \"line\", \"line_id\": 54666, \"play_name\": \"Loves Labours Lost\", \"speech_number\": 21, \"line_number\": \"5.1.52\", \"speaker\": \"HOLOFERNES\", \"text_entry\": \"Quis, quis, thou consonant?\" } } ... ] } } . Alternatively, specify the search_analyzer in the mapping itself: . \"mappings\": { \"properties\": { \"text_entry\": { \"type\": \"text\", \"analyzer\": \"autocomplete\", \"search_analyzer\": \"standard\" } } } . ",
    "url": "https://vagimeli.github.io/search-plugins/searching-data/autocomplete/#edge-n-gram-matching",
    "relUrl": "/search-plugins/searching-data/autocomplete/#edge-n-gram-matching"
  },"892": {
    "doc": "Autocomplete",
    "title": "Completion suggester",
    "content": "The completion suggester accepts a list of suggestions and builds them into a finite-state transducer (FST), an optimized data structure that is essentially a graph. This data structure lives in memory and is optimized for fast prefix lookups. To learn more about FSTs, see Wikipedia. As the user types, the completion suggester moves through the FST graph one character at a time along a matching path. After it runs out of user input, it examines the remaining endings to produce a list of suggestions. The completion suggester makes your autocomplete solution as efficient as possible and lets you have explicit control over its suggestions. Use a dedicated field type called completion, which stores the FST-like data structures in the index: . PUT shakespeare { \"mappings\": { \"properties\": { \"text_entry\": { \"type\": \"completion\" } } } } . To get suggestions, use the search endpoint with the suggest parameter: . GET shakespeare/_search { \"suggest\": { \"autocomplete\": { \"prefix\": \"To be\", \"completion\": { \"field\": \"text_entry\" } } } } . The phrase “to be” is prefix matched with the FST of the text_entry field: . { \"took\" : 29, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 0, \"relation\" : \"eq\" }, \"max_score\" : null, \"hits\" : [ ] }, \"suggest\" : { \"autocomplete\" : [ { \"text\" : \"To be\", \"offset\" : 0, \"length\" : 5, \"options\" : [ { \"text\" : \"To be a comrade with the wolf and owl,--\", \"_index\" : \"shakespeare\", \"_id\" : \"50652\", \"_score\" : 1.0, \"_source\" : { \"type\" : \"line\", \"line_id\" : 50653, \"play_name\" : \"King Lear\", \"speech_number\" : 68, \"line_number\" : \"2.4.230\", \"speaker\" : \"KING LEAR\", \"text_entry\" : \"To be a comrade with the wolf and owl,--\" } }, { \"text\" : \"To be a make-peace shall become my age:\", \"_index\" : \"shakespeare\", \"_id\" : \"78566\", \"_score\" : 1.0, \"_source\" : { \"type\" : \"line\", \"line_id\" : 78567, \"play_name\" : \"Richard II\", \"speech_number\" : 20, \"line_number\" : \"1.1.160\", \"speaker\" : \"JOHN OF GAUNT\", \"text_entry\" : \"To be a make-peace shall become my age:\" } }, { \"text\" : \"To be a party in this injury.\", \"_index\" : \"shakespeare\", \"_id\" : \"75259\", \"_score\" : 1.0, \"_source\" : { \"type\" : \"line\", \"line_id\" : 75260, \"play_name\" : \"Othello\", \"speech_number\" : 57, \"line_number\" : \"5.1.93\", \"speaker\" : \"IAGO\", \"text_entry\" : \"To be a party in this injury.\" } }, { \"text\" : \"To be a preparation gainst the Polack;\", \"_index\" : \"shakespeare\", \"_id\" : \"33591\", \"_score\" : 1.0, \"_source\" : { \"type\" : \"line\", \"line_id\" : 33592, \"play_name\" : \"Hamlet\", \"speech_number\" : 17, \"line_number\" : \"2.2.67\", \"speaker\" : \"VOLTIMAND\", \"text_entry\" : \"To be a preparation gainst the Polack;\" } }, { \"text\" : \"To be a public spectacle to all:\", \"_index\" : \"shakespeare\", \"_id\" : \"3709\", \"_score\" : 1.0, \"_source\" : { \"type\" : \"line\", \"line_id\" : 3710, \"play_name\" : \"Henry VI Part 1\", \"speech_number\" : 6, \"line_number\" : \"1.4.41\", \"speaker\" : \"TALBOT\", \"text_entry\" : \"To be a public spectacle to all:\" } } ] } ] } } . To specify the number of suggestions that you want to return, use the size parameter: . GET shakespeare/_search { \"suggest\": { \"autocomplete\": { \"prefix\": \"To n\", \"completion\": { \"field\": \"text_entry\", \"size\": 3 } } } } . The maximum of three documents is returned: . { \"took\" : 4109, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 0, \"relation\" : \"eq\" }, \"max_score\" : null, \"hits\" : [ ] }, \"suggest\" : { \"autocomplete\" : [ { \"text\" : \"To n\", \"offset\" : 0, \"length\" : 4, \"options\" : [ { \"text\" : \"To NESTOR\", \"_index\" : \"shakespeare\", \"_id\" : \"99707\", \"_score\" : 1.0, \"_source\" : { \"type\" : \"line\", \"line_id\" : 99708, \"play_name\" : \"Troilus and Cressida\", \"speech_number\" : 3, \"line_number\" : \"\", \"speaker\" : \"ULYSSES\", \"text_entry\" : \"To NESTOR\" } }, { \"text\" : \"To name the bigger light, and how the less,\", \"_index\" : \"shakespeare\", \"_id\" : \"91884\", \"_score\" : 1.0, \"_source\" : { \"type\" : \"line\", \"line_id\" : 91885, \"play_name\" : \"The Tempest\", \"speech_number\" : 91, \"line_number\" : \"1.2.394\", \"speaker\" : \"CALIBAN\", \"text_entry\" : \"To name the bigger light, and how the less,\" } }, { \"text\" : \"To nature none more bound; his training such,\", \"_index\" : \"shakespeare\", \"_id\" : \"40510\", \"_score\" : 1.0, \"_source\" : { \"type\" : \"line\", \"line_id\" : 40511, \"play_name\" : \"Henry VIII\", \"speech_number\" : 18, \"line_number\" : \"1.2.126\", \"speaker\" : \"KING HENRY VIII\", \"text_entry\" : \"To nature none more bound; his training such,\" } } ] } ] } } . The suggest parameter finds suggestions using only prefix matching. For example, the document “To be, or not to be” is not part of the results. If you want specific documents returned as suggestions, you can manually add curated suggestions and add weights to prioritize your suggestions. Index a document with input suggestions and assign a weight: . PUT shakespeare/_doc/1?refresh=true { \"text_entry\": { \"input\": [ \"To n\", \"To be, or not to be: that is the question:\" ], \"weight\": 10 } } . Perform the same search: . GET shakespeare/_search { \"suggest\": { \"autocomplete\": { \"prefix\": \"To n\", \"completion\": { \"field\": \"text_entry\", \"size\": 3 } } } } . You see the indexed document as the first result: . { \"took\" : 1, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 0, \"relation\" : \"eq\" }, \"max_score\" : null, \"hits\" : [ ] }, \"suggest\" : { \"autocomplete\" : [ { \"text\" : \"To n\", \"offset\" : 0, \"length\" : 4, \"options\" : [ { \"text\" : \"To n\", \"_index\" : \"shakespeare\", \"_id\" : \"1\", \"_score\" : 10.0, \"_source\" : { \"text_entry\" : { \"input\" : [ \"To n\", \"To be, or not to be: that is the question:\" ], \"weight\" : 10 } } }, { \"text\" : \"To NESTOR\", \"_index\" : \"shakespeare\", \"_id\" : \"99707\", \"_score\" : 1.0, \"_source\" : { \"type\" : \"line\", \"line_id\" : 99708, \"play_name\" : \"Troilus and Cressida\", \"speech_number\" : 3, \"line_number\" : \"\", \"speaker\" : \"ULYSSES\", \"text_entry\" : \"To NESTOR\" } }, { \"text\" : \"To name the bigger light, and how the less,\", \"_index\" : \"shakespeare\", \"_id\" : \"91884\", \"_score\" : 1.0, \"_source\" : { \"type\" : \"line\", \"line_id\" : 91885, \"play_name\" : \"The Tempest\", \"speech_number\" : 91, \"line_number\" : \"1.2.394\", \"speaker\" : \"CALIBAN\", \"text_entry\" : \"To name the bigger light, and how the less,\" } } ] } ] } } . You can also allow for misspellings in queries by specifying the fuzzy parameter: . GET shakespeare/_search { \"suggest\": { \"autocomplete\": { \"prefix\": \"rosenkrantz\", \"completion\": { \"field\": \"text_entry\", \"size\": 3, \"fuzzy\" : { \"fuzziness\" : \"AUTO\" } } } } } . The result matches the correct spelling: . { \"took\" : 1, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 0, \"relation\" : \"eq\" }, \"max_score\" : null, \"hits\" : [ ] }, \"suggest\" : { \"autocomplete\" : [ { \"text\" : \"rosenkrantz\", \"offset\" : 0, \"length\" : 11, \"options\" : [ { \"text\" : \"ROSENCRANTZ:\", \"_index\" : \"shakespeare\", \"_id\" : \"35196\", \"_score\" : 5.0, \"_source\" : { \"type\" : \"line\", \"line_id\" : 35197, \"play_name\" : \"Hamlet\", \"speech_number\" : 2, \"line_number\" : \"4.2.1\", \"speaker\" : \"HAMLET\", \"text_entry\" : \"ROSENCRANTZ:\" } } ] } ] } } . You can use a regular expression to define the prefix for the completion suggester query: . GET shakespeare/_search { \"suggest\": { \"autocomplete\": { \"prefix\": \"rosen*\", \"completion\": { \"field\": \"text_entry\", \"size\": 3 } } } } . For more information, see the completion field type documentation. ",
    "url": "https://vagimeli.github.io/search-plugins/searching-data/autocomplete/#completion-suggester",
    "relUrl": "/search-plugins/searching-data/autocomplete/#completion-suggester"
  },"893": {
    "doc": "Autocomplete",
    "title": "Search as you type",
    "content": "OpenSearch has a dedicated search_as_you_type field type that is optimized for search-as-you-type functionality and can match terms using both prefix and infix completion. The search_as_you_type field does not require you to set up a custom analyzer or index suggestions beforehand. First, map the field as search_as_you_type: . PUT shakespeare { \"mappings\": { \"properties\": { \"text_entry\": { \"type\": \"search_as_you_type\" } } } } . After you index a document, OpenSearch automatically creates and stores its n-grams and edge n-grams. For example, consider the string that is the question. First, it is split into terms using the standard analyzer, and the terms are stored in the text_entry field: . [ \"that\", \"is\", \"the\", \"question\" ] . In addition to storing these terms, the following 2-grams for this field are stored in the field text_entry._2gram: . [ \"that is\", \"is the\", \"the question\" ] . The following 3-grams for this field are stored in the field text_entry._3gram: . [ \"that is the\", \"is the question\" ] . Finally, after an edge n-gram token filter is applied, the resulting terms are stored in the text_entry._index_prefix field: . [ \"t\", \"th\", \"tha\", \"that\", ... ] . You can then match terms in any order using the bool_prefix type of a multi-match query: . GET shakespeare/_search { \"query\": { \"multi_match\": { \"query\": \"uncle what\", \"type\": \"bool_prefix\", \"fields\": [ \"text_entry\", \"text_entry._2gram\", \"text_entry._3gram\" ] } }, \"size\": 3 } . The documents in which the words appear in the same order as in the query are ranked higher in the results: . { \"took\" : 1, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 4759, \"relation\" : \"eq\" }, \"max_score\" : 10.437667, \"hits\" : [ { \"_index\" : \"shakespeare\", \"_id\" : \"2817\", \"_score\" : 10.437667, \"_source\" : { \"type\" : \"line\", \"line_id\" : 2818, \"play_name\" : \"Henry IV\", \"speech_number\" : 5, \"line_number\" : \"5.2.31\", \"speaker\" : \"HOTSPUR\", \"text_entry\" : \"Uncle, what news?\" } }, { \"_index\" : \"shakespeare\", \"_id\" : \"37085\", \"_score\" : 9.437667, \"_source\" : { \"type\" : \"line\", \"line_id\" : 37086, \"play_name\" : \"Henry V\", \"speech_number\" : 26, \"line_number\" : \"1.2.262\", \"speaker\" : \"KING HENRY V\", \"text_entry\" : \"What treasure, uncle?\" } }, { \"_index\" : \"shakespeare\", \"_id\" : \"79274\", \"_score\" : 9.358302, \"_source\" : { \"type\" : \"line\", \"line_id\" : 79275, \"play_name\" : \"Richard II\", \"speech_number\" : 29, \"line_number\" : \"2.1.187\", \"speaker\" : \"KING RICHARD II\", \"text_entry\" : \"Why, uncle, whats the matter?\" } } ] } } . To match terms in order, you can use a match_phrase_prefix query: . GET shakespeare/_search { \"query\": { \"match_phrase_prefix\": { \"text_entry\": \"uncle wha\" } }, \"size\": 3 } . The response contains documents that match the prefix: . { \"took\" : 1, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 6, \"relation\" : \"eq\" }, \"max_score\" : 16.37664, \"hits\" : [ { \"_index\" : \"shakespeare\", \"_id\" : \"2817\", \"_score\" : 16.37664, \"_source\" : { \"type\" : \"line\", \"line_id\" : 2818, \"play_name\" : \"Henry IV\", \"speech_number\" : 5, \"line_number\" : \"5.2.31\", \"speaker\" : \"HOTSPUR\", \"text_entry\" : \"Uncle, what news?\" } }, { \"_index\" : \"shakespeare\", \"_id\" : \"6789\", \"_score\" : 16.37664, \"_source\" : { \"type\" : \"line\", \"line_id\" : 6790, \"play_name\" : \"Henry VI Part 2\", \"speech_number\" : 60, \"line_number\" : \"1.3.202\", \"speaker\" : \"KING HENRY VI\", \"text_entry\" : \"Uncle, what shall we say to this in law?\" } }, { \"_index\" : \"shakespeare\", \"_id\" : \"7877\", \"_score\" : 16.37664, \"_source\" : { \"type\" : \"line\", \"line_id\" : 7878, \"play_name\" : \"Henry VI Part 2\", \"speech_number\" : 13, \"line_number\" : \"3.2.28\", \"speaker\" : \"KING HENRY VI\", \"text_entry\" : \"Where is our uncle? whats the matter, Suffolk?\" } } ] } } . Finally, to match the last term exactly and not as a prefix, you can use a match_phrase query: . GET shakespeare/_search { \"query\": { \"match_phrase\": { \"text_entry\": \"uncle what\" } }, \"size\": 5 } . The response contains exact matches: . { \"took\" : 1, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 3, \"relation\" : \"eq\" }, \"max_score\" : 14.437452, \"hits\" : [ { \"_index\" : \"shakespeare\", \"_id\" : \"2817\", \"_score\" : 14.437452, \"_source\" : { \"type\" : \"line\", \"line_id\" : 2818, \"play_name\" : \"Henry IV\", \"speech_number\" : 5, \"line_number\" : \"5.2.31\", \"speaker\" : \"HOTSPUR\", \"text_entry\" : \"Uncle, what news?\" } }, { \"_index\" : \"shakespeare\", \"_id\" : \"6789\", \"_score\" : 9.461917, \"_source\" : { \"type\" : \"line\", \"line_id\" : 6790, \"play_name\" : \"Henry VI Part 2\", \"speech_number\" : 60, \"line_number\" : \"1.3.202\", \"speaker\" : \"KING HENRY VI\", \"text_entry\" : \"Uncle, what shall we say to this in law?\" } }, { \"_index\" : \"shakespeare\", \"_id\" : \"100955\", \"_score\" : 8.947967, \"_source\" : { \"type\" : \"line\", \"line_id\" : 100956, \"play_name\" : \"Troilus and Cressida\", \"speech_number\" : 28, \"line_number\" : \"3.2.98\", \"speaker\" : \"CRESSIDA\", \"text_entry\" : \"Well, uncle, what folly I commit, I dedicate to you.\" } } ] } } . If you modify the text in the previous match_phrase query and omit the last letter, none of the documents in the previous response are returned: . GET shakespeare/_search { \"query\": { \"match_phrase\": { \"text_entry\": \"uncle wha\" } } } . The result is empty: . { \"took\" : 1, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 0, \"relation\" : \"eq\" }, \"max_score\" : null, \"hits\" : [ ] } } . For more information, see the search_as_you_type field type documentation. ",
    "url": "https://vagimeli.github.io/search-plugins/searching-data/autocomplete/#search-as-you-type",
    "relUrl": "/search-plugins/searching-data/autocomplete/#search-as-you-type"
  },"894": {
    "doc": "Autocomplete",
    "title": "Autocomplete",
    "content": " ",
    "url": "https://vagimeli.github.io/search-plugins/searching-data/autocomplete/",
    "relUrl": "/search-plugins/searching-data/autocomplete/"
  },"895": {
    "doc": "Did-you-mean",
    "title": "Did-you-mean",
    "content": "The Did-you-mean suggester shows suggested corrections for misspelled search terms. For example, if a user types “fliud,” OpenSearch suggests a corrected search term like “fluid.” You can then suggest the corrected term to the user or even automatically correct the search term. You can implement the did-you-mean suggester using one of the following methods: . | Use a term suggester to suggest corrections for individual words. | Use a phrase suggester to suggest corrections for phrases. | . ",
    "url": "https://vagimeli.github.io/search-plugins/searching-data/did-you-mean/",
    "relUrl": "/search-plugins/searching-data/did-you-mean/"
  },"896": {
    "doc": "Did-you-mean",
    "title": "Term suggester",
    "content": "Use the term suggester to suggest corrected spellings for individual words. The term suggester uses an edit distance to compute suggestions. The edit distance is the number of single-character insertions, deletions, or substitutions that need to be performed for a term to match another term. For example, to change the word “cat” to “hats”, you need to substitute “h” for “c” and insert an “s”, so the edit distance in this case is 2. To use the term suggester, you don’t need any special field mappings for your index. By default, string field types are mapped as text. A text field is analyzed, so the title in the following example is tokenized into individual words. Indexing the following documents creates a books index where title is a text field: . PUT books/_doc/1 { \"title\": \"Design Patterns (Object-Oriented Software)\" } PUT books/_doc/2 { \"title\": \"Software Architecture Patterns Explained\" } . To check how a string is split into tokens, you can use the _analyze endpoint. To apply the same analyzer that the field uses, you can specify the field’s name in the field parameter: . GET books/_analyze { \"text\": \"Design Patterns (Object-Oriented Software)\", \"field\": \"title\" } . The default analyzer (standard) splits a string at word boundaries, removes punctuation, and lowercases the tokens: . { \"tokens\" : [ { \"token\" : \"design\", \"start_offset\" : 0, \"end_offset\" : 6, \"type\" : \"&lt;ALPHANUM&gt;\", \"position\" : 0 }, { \"token\" : \"patterns\", \"start_offset\" : 7, \"end_offset\" : 15, \"type\" : \"&lt;ALPHANUM&gt;\", \"position\" : 1 }, { \"token\" : \"object\", \"start_offset\" : 17, \"end_offset\" : 23, \"type\" : \"&lt;ALPHANUM&gt;\", \"position\" : 2 }, { \"token\" : \"oriented\", \"start_offset\" : 24, \"end_offset\" : 32, \"type\" : \"&lt;ALPHANUM&gt;\", \"position\" : 3 }, { \"token\" : \"software\", \"start_offset\" : 33, \"end_offset\" : 41, \"type\" : \"&lt;ALPHANUM&gt;\", \"position\" : 4 } ] } . To get suggestions for a misspelled search term, use the term suggester. Specify the input text that needs suggestions in the text field, and specify the field from which to get suggestions in the field field: . GET books/_search { \"suggest\": { \"spell-check\": { \"text\": \"patern\", \"term\": { \"field\": \"title\" } } } } . The term suggester returns a list of corrections for the input text in the options array: . { \"took\" : 2, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 0, \"relation\" : \"eq\" }, \"max_score\" : null, \"hits\" : [ ] }, \"suggest\" : { \"spell-check\" : [ { \"text\" : \"patern\", \"offset\" : 0, \"length\" : 6, \"options\" : [ { \"text\" : \"patterns\", \"score\" : 0.6666666, \"freq\" : 2 } ] } ] } } . The score value is calculated based on the edit distance. The higher the score, the better the suggestion. The freq is the frequency that represents the number of times the term appears in the documents of the specified index. You can include several suggestions in one request. The following example uses the term suggester for two different suggestions: . GET books/_search { \"suggest\": { \"spell-check1\" : { \"text\" : \"patern\", \"term\" : { \"field\" : \"title\" } }, \"spell-check2\" : { \"text\" : \"desing\", \"term\" : { \"field\" : \"title\" } } } } . To receive suggestions for the same input text in multiple fields, you can define the text globally to avoid duplication: . GET books/_search { \"suggest\": { \"text\" : \"patern\", \"spell-check1\" : { \"term\" : { \"field\" : \"title\" } }, \"spell-check2\" : { \"term\" : { \"field\" : \"subject\" } } } } . If text is specified both at the global and individual suggestion levels, the suggestion-level value overrides the global value. Term suggester options . You can specify the following options to the term suggester. | Option | Description | . | field | The field from which to source suggestions. Required. Can be set for each suggestion or globally. | . | analyzer | The analyzer with which to analyze the input text. Defaults to the analyzer configured for the field. | . | size | The maximum number of suggestions to return for each token in the input text. | . | sort | Specifies how suggestions should be sorted in the response. Valid values are:- score: Sort by similarity score, then document frequency, and then the term itself.- frequency: Sort by document frequency, then similarity score, and then the term itself. | . | suggest_mode | The suggest mode specifies the terms for which suggestions should be included in the response. Valid values are:- missing: Return suggestions only for the input text terms that are not in the index. - popular: Return suggestions only if they occur in the documents more frequently than in the original input text. - always: Always return suggestions for each term in the input text.Default is missing. | . | max_edits | The maximum edit distance for suggestions. Valid values are in the [1, 2] range. Default is 2. | . | prefix_length | An integer that specifies the minimum length the matched prefix must be to start returning suggestions. If the prefix of prefix_length is not matched, but the search term is still within the edit distance, no suggestions are returned. Default is 1. Higher values improve spellcheck performance because misspellings don’t tend to occur in the beginning of words. | . | min_word_length | The minimum length a suggestion must be in order to be included in the response. Default is 4. | . | shard_size | The maximum number of candidate suggestions to obtain from each shard. After all candidate suggestions are considered, the top shard_size suggestions are returned. Default is equal to the size value. Shard-level document frequencies may not be exact because terms may reside in different shards. If shard_size is larger than size, the document frequencies for suggestions are more accurate, at the cost of decreased performance. | . | max_inspections | The multiplication factor for shard_size. The maximum number of candidate suggestions OpenSearch inspects to find suggestions is calculated as shard_size multiplied by max_inspection. May improve accuracy at the cost of decreased performance. Default is 5. | . | min_doc_freq | The minimum number or percentage of documents in which a suggestion should appear for it to be returned. May improve accuracy by returning only suggestions with high shard-level document frequencies. Valid values are integers that represent the document frequency or floats in the [0, 1] range that represent the percentage of documents. Default is 0 (feature disabled). | . | max_term_freq | The maximum number of documents in which a suggestion should appear in order for it to be returned. Valid values are integers that represent the document frequency or floats in the [0, 1] range that represent the percentage of documents. Default is 0.01. Excluding high-frequency terms improves spellcheck performance because high-frequency terms are usually spelled correctly. Uses shard-level document frequencies. | . | string_distance | The edit distance algorithm to use to determine similarity. Valid values are:- internal: The default algorithm that is based on the Damerau-Levenshtein algorithm but is highly optimized for comparing edit distances for terms in the index. - damerau_levenshtein: The edit distance algorithm based on the Damerau-Levenshtein algorithm. - levenshtein: The edit distance algorithm based on the Levenshtein edit distance algorithm. - jaro_winkler: The edit distance algorithm based on the Jaro-Winkler algorithm. - ngram: The edit distance algorithm based on character n-grams. | . ",
    "url": "https://vagimeli.github.io/search-plugins/searching-data/did-you-mean/#term-suggester",
    "relUrl": "/search-plugins/searching-data/did-you-mean/#term-suggester"
  },"897": {
    "doc": "Did-you-mean",
    "title": "Phrase suggester",
    "content": "To implement did-you-mean, use a phrase suggester. The phrase suggester is similar to the term suggester, except it uses n-gram language models to suggest whole phrases instead of individual words. To set up a phrase suggester, create a custom analyzer called trigram that uses a shingle filter and lowercases tokens. This filter is similar to the edge_ngram filter, but it applies to words instead of letters. Then configure the field from which you’ll be sourcing suggestions with the custom analyzer you created: . PUT books2 { \"settings\": { \"index\": { \"analysis\": { \"analyzer\": { \"trigram\": { \"type\": \"custom\", \"tokenizer\": \"standard\", \"filter\": [ \"lowercase\", \"shingle\" ] } }, \"filter\": { \"shingle\": { \"type\": \"shingle\", \"min_shingle_size\": 2, \"max_shingle_size\": 3 } } } } }, \"mappings\": { \"properties\": { \"title\": { \"type\": \"text\", \"fields\": { \"trigram\": { \"type\": \"text\", \"analyzer\": \"trigram\" } } } } } } . Index the documents into the new index: . PUT books2/_doc/1 { \"title\": \"Design Patterns\" } PUT books2/_doc/2 { \"title\": \"Software Architecture Patterns Explained\" } . Suppose the user searches for an incorrect phrase: . GET books2/_search { \"suggest\": { \"phrase-check\": { \"text\": \"design paterns\", \"phrase\": { \"field\": \"title.trigram\" } } } } . The phrase suggester returns the corrected phrase: . { \"took\" : 4, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 0, \"relation\" : \"eq\" }, \"max_score\" : null, \"hits\" : [ ] }, \"suggest\" : { \"phrase-check\" : [ { \"text\" : \"design paterns\", \"offset\" : 0, \"length\" : 14, \"options\" : [ { \"text\" : \"design patterns\", \"score\" : 0.31666178 } ] } ] } } . To highlight suggestions, set up the highlight field for the phrase suggester: . GET books2/_search { \"suggest\": { \"phrase-check\": { \"text\": \"design paterns\", \"phrase\": { \"field\": \"title.trigram\", \"gram_size\": 3, \"highlight\": { \"pre_tag\": \"&lt;em&gt;\", \"post_tag\": \"&lt;/em&gt;\" } } } } } . The results contain the highlighted text: . { \"took\" : 2, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 0, \"relation\" : \"eq\" }, \"max_score\" : null, \"hits\" : [ ] }, \"suggest\" : { \"phrase-check\" : [ { \"text\" : \"design paterns\", \"offset\" : 0, \"length\" : 14, \"options\" : [ { \"text\" : \"design patterns\", \"highlighted\" : \"design &lt;em&gt;patterns&lt;/em&gt;\", \"score\" : 0.31666178 } ] } ] } } . Phrase suggester options . You can specify the following options to the phrase suggester. | Option | Description | . | field | The field to use for n-gram lookups. The phrase suggester uses this field to calculate suggestion scores. Required. | . | gram_size | The maximum size n of the n-grams (shingles) in the field. If the field does not contain n-grams (shingles), omit this option or set it to 1. If the field uses a shingle filter, and gram_size is not set, gram_size is set to max_shingle_size. | . | real_word_error_likelihood | The probability that a term is misspelled, even if it exists in the dictionary. Default is 0.95 (5% of the words in the dictionary are misspelled). | . | confidence | The confidence level is a float factor that is multiplied by the input phrase’s score to calculate a threshold score for other suggestions. Only suggestions with higher scores than the threshold are returned. A confidence level of 1.0 will only return suggestions that score higher than the input phrase. If confidence is set to 0, the top size candidates are returned. Default is 1. | . | max_errors | The maximum number or percentage of the terms that can be erroneous (spelled incorrectly) in order to return a suggestion. Valid values are integers that represent the number of terms or floats in the (0, 1) range that represent the percentage of the terms. Default is 1 (return only suggestions with at most one misspelled term). Setting this value to a high number can decrease performance. We recommend setting max_errors to a low number like 1 or 2 to reduce the time spent in suggest calls relative to the time spent in query execution. | . | separator | The separator for the terms in the bigram field. Defaults to the space character. | . | size | The number of candidate suggestions to generate for each query term. Specifying a higher value can result in terms with higher edit distances being returned. Default is 5. | . | analyzer | The analyzer with which to analyze the suggestion text. Defaults to the analyzer configured for the field. | . | shard_size | The maximum number of candidate suggestions to obtain from each shard. After all candidate suggestions are considered, the top shard_size suggestions are returned. Default is 5. | . | collate | Used to prune suggestions for which there are no matching documents in the index. | . | collate.query | Specifies a query against which suggestions are checked to prune the suggestions for which there are no matching documents in the index. | . | collate.prune | Specifies whether to return all suggestions. If prune is set to false, only those suggestions that have matching documents are returned. If prune is set to true, all suggestions are returned; each suggestion has an additional collate_match field that is true if the suggestion has matching documents and is false otherwise. Default is false. | . | highlight | Configures suggestion highlighting. Both pre_tag and post_tag values are required. | . | highlight.pre_tag | The starting tag for highlighting. | . | highlight.post_tag | The ending tag for highlighting. | . | smoothing | Smoothing model to balance the weight of the shingles that exist in the index frequently with the weight of the shingles that exist in the index infrequently. | . Collate field . To filter out spellchecked suggestions that will not return any results, you can use the collate field. This field contains a scripted query that is run for each returned suggestion. See Search templates for information on constructing a templated query. You can specify the current suggestion using the {{suggestion}} variable, or you can pass your own template parameters in the params field (the suggestion value will be added to the variables you specify). The collate query for a suggestion is run only on the shard from which the suggestion was sourced. The query is required. Additionally, if the prune parameter is set to true, a collate_match field is added to each suggestion. If a query returns no results, the collate_match value is false. You can then filter out suggestions based on the collate_match field. The prune parameter’s default value is false. For example, the following query configures the collate field to run a match_phrase query matching the title field to the current suggestion: . GET books2/_search { \"suggest\": { \"phrase-check\": { \"text\": \"design paterns\", \"phrase\": { \"field\": \"title.trigram\", \"collate\" : { \"query\" : { \"source\": { \"match_phrase\" : { \"title\": \"\" } } }, \"prune\": \"true\" } } } } } . The resulting suggestion contains the collate_match field set to true, which means the match_phrase query will return matching documents for the suggestion: . { \"took\" : 7, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 0, \"relation\" : \"eq\" }, \"max_score\" : null, \"hits\" : [ ] }, \"suggest\" : { \"phrase-check\" : [ { \"text\" : \"design paterns\", \"offset\" : 0, \"length\" : 14, \"options\" : [ { \"text\" : \"design patterns\", \"score\" : 0.56759655, \"collate_match\" : true } ] } ] } } . Smoothing models . For most use cases, when calculating a suggestion’s score, you want to take into account not only the frequency of a shingle but also the shingle’s size. Smoothing models are used to calculate scores for shingles of different sizes, balancing the weight of frequent and infrequent shingles. The following smoothing models are supported. | Model | Description | . | stupid_backoff | Backs off to lower-order n-gram models if the higher-order n-gram count is 0 and multiplies the lower-order n-gram model by a constant factor (discount). This is the default smoothing model. | . | stupid.backoff.discount | The factor by which to multiply the lower-order n-gram model. Optional. Default is 0.4. | . | laplace | Uses additive smoothing, adding a constant alpha to all counts to balance weights. | . | laplace.alpha | The constant added to all counts to balance weights, typically 1.0 or smaller. Optional. Default is 0.5. | . By default, OpenSearch uses the Stupid Backoff model—a simple algorithm that starts with the shingles of the highest order and takes lower-order shingles if higher-order shingles are not found. For example, if you set up the phrase suggester to have 3-grams, 2-grams, and 1-grams, the Stupid Backoff model first inspects the 3-grams. If there are no 3-grams, it inspects 2-grams but multiplies the score by the discount factor. If there are no 2-grams, it inspects 1-grams but again multiplies the score by the discount factor. The Stupid Backoff model works well in most cases. If you need to choose the Laplace smoothing model, specify it in the smoothing parameter: . GET books2/_search { \"suggest\": { \"phrase-check\": { \"text\": \"design paterns\", \"phrase\": { \"field\": \"title.trigram\", \"size\" : 1, \"smoothing\" : { \"laplace\" : { \"alpha\" : 0.7 } } } } } } . Candidate generators . Candidate generators provide possible suggestion terms based on the terms in the input text. There is one candidate generator available—direct_generator. A direct generator functions similarly to a term suggester: It is also called for each term in the input text. The phrase suggester supports multiple candidate generators, where each generator is called for each term in the input text. It also lets you specify a pre-filter (an analyzer that analyzes the input text terms before they enter the spellcheck phase) and a post-filter (an analyzer that analyzes the generated suggestions before they are returned). Set up a direct generator for a phrase suggester: . GET books2/_search { \"suggest\": { \"text\": \"design paterns\", \"phrase-check\": { \"phrase\": { \"field\": \"title.trigram\", \"size\": 1, \"direct_generator\": [ { \"field\": \"title.trigram\", \"suggest_mode\": \"always\", \"min_word_length\": 3 } ] } } } } . You can specify the following direct generator options. | Option | Description | . | field | The field from which to source suggestions. Required. Can be set for each suggestion or globally. | . | size | The maximum number of suggestions to return for each token in the input text. | . | suggest_mode | The suggest mode specifies the terms for which suggestions generated on each shard should be included. The suggest mode is applied to suggestions for each shard and is not checked when combining suggestions from different shards. Therefore, if the suggest mode is missing, suggestions will be returned if the term is missing from one shard but exists on another shard. Valid values are:- missing: Return suggestions only for the input text terms that are not in the shard. - popular: Return suggestions only if they occur in the documents more frequently than in the original input text on the shard.- always: Always return suggestions.Default is missing. | . | max_edits | The maximum edit distance for suggestions. Valid values are in the [1, 2] range. Default is 2. | . | prefix_length | An integer that specifies the minimum length the matched prefix must be to start returning suggestions. If the prefix of prefix_length is not matched but the search term is still within the edit distance, no suggestions are returned. Default is 1. Higher values improve spellcheck performance because misspellings don’t tend to occur in the beginning of words. | . | min_word_length | The minimum length a suggestion must be in order to be included. Default is 4. | . | max_inspections | The multiplication factor for shard_size. The maximum number of candidate suggestions OpenSearch inspects to find suggestions is calculated as shard_size multiplied by max_inspection. May improve accuracy at the cost of decreased performance. Default is 5. | . | min_doc_freq | The minimum number or percentage of documents in which a suggestion should appear in order for it to be returned. May improve accuracy by returning only suggestions with high shard-level document frequencies. Valid values are integers that represent the document frequency or floats in the [0, 1] range that represent the percentage of documents. Default is 0 (feature disabled). | . | max_term_freq | The maximum number of documents in which a suggestion should appear in order for it to be returned. Valid values are integers that represent the document frequency or floats in the [0, 1] range that represent the percentage of documents. Default is 0.01. Excluding high-frequency terms improves spellcheck performance because high-frequency terms are usually spelled correctly. Uses shard-level document frequencies. | . | pre_filter | An analyzer that is applied to each input text token passed to the generator before a suggestion is generated. | . | post_filter | An analyzer that is applied to each generated suggestion before it is passed to the phrase scorer. | . ",
    "url": "https://vagimeli.github.io/search-plugins/searching-data/did-you-mean/#phrase-suggester",
    "relUrl": "/search-plugins/searching-data/did-you-mean/#phrase-suggester"
  },"898": {
    "doc": "Highlight query matches",
    "title": "Highlight query matches",
    "content": "Highlighting emphasizes the search term(s) in the results so you can emphasize the query matches. To highlight the search terms, add a highlight parameter outside of the query block: . GET shakespeare/_search { \"query\": { \"match\": { \"text_entry\": \"life\" } }, \"size\": 3, \"highlight\": { \"fields\": { \"text_entry\": {} } } } . Each document in the results contains a highlight object that shows your search term wrapped in an em tag: . { \"took\" : 3, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 805, \"relation\" : \"eq\" }, \"max_score\" : 7.450247, \"hits\" : [ { \"_index\" : \"shakespeare\", \"_id\" : \"33765\", \"_score\" : 7.450247, \"_source\" : { \"type\" : \"line\", \"line_id\" : 33766, \"play_name\" : \"Hamlet\", \"speech_number\" : 60, \"line_number\" : \"2.2.233\", \"speaker\" : \"HAMLET\", \"text_entry\" : \"my life, except my life.\" }, \"highlight\" : { \"text_entry\" : [ \"my &lt;em&gt;life&lt;/em&gt;, except my &lt;em&gt;life&lt;/em&gt;.\" ] } }, { \"_index\" : \"shakespeare\", \"_id\" : \"51877\", \"_score\" : 6.873042, \"_source\" : { \"type\" : \"line\", \"line_id\" : 51878, \"play_name\" : \"King Lear\", \"speech_number\" : 18, \"line_number\" : \"4.6.52\", \"speaker\" : \"EDGAR\", \"text_entry\" : \"The treasury of life, when life itself\" }, \"highlight\" : { \"text_entry\" : [ \"The treasury of &lt;em&gt;life&lt;/em&gt;, when &lt;em&gt;life&lt;/em&gt; itself\" ] } }, { \"_index\" : \"shakespeare\", \"_id\" : \"39245\", \"_score\" : 6.6167283, \"_source\" : { \"type\" : \"line\", \"line_id\" : 39246, \"play_name\" : \"Henry V\", \"speech_number\" : 7, \"line_number\" : \"4.7.31\", \"speaker\" : \"FLUELLEN\", \"text_entry\" : \"mark Alexanders life well, Harry of Monmouths life\" }, \"highlight\" : { \"text_entry\" : [ \"mark Alexanders &lt;em&gt;life&lt;/em&gt; well, Harry of Monmouths &lt;em&gt;life&lt;/em&gt;\" ] } } ] } } . The highlight function works on the actual field contents. OpenSearch retrieves these contents either from the stored field (the field for which the mapping is to be set to true) or from the _source field if the field is not stored. You can force the retrieval of field contents from the _source field by setting the force_source parameter to true. The highlight parameter highlights the original terms even when using synonyms or stemming for the search itself. ",
    "url": "https://vagimeli.github.io/search-plugins/searching-data/highlight/",
    "relUrl": "/search-plugins/searching-data/highlight/"
  },"899": {
    "doc": "Highlight query matches",
    "title": "Methods of obtaining offsets",
    "content": "To highlight the search terms, the highlighter needs the start and end character offsets of each term. The offsets mark the term’s position in the original text. The highlighter can obtain the offsets from the following sources: . | Postings: When documents are indexed, OpenSearch creates an inverted search index—a core data structure used to search for documents. Postings represent the inverted search index and store the mapping of each analyzed term to the list of documents in which it occurs. If you set the index_options parameter to offsets when mapping a text field, OpenSearch adds each term’s start and end character offsets to the inverted index. During highlighting, the highlighter reruns the original query directly on the postings to locate each term. Thus, storing offsets makes highlighting more efficient for large fields because it does not require reanalyzing the text. Storing term offsets requires additional disk space, but uses less disk space than storing term vectors. | | Text reanalysis: In the absence of both postings and term vectors, the highlighter reanalyzes text in order to highlight it. For every document and every field that needs highlighting, the highlighter creates a small in-memory index and reruns the original query through Lucene’s query execution planner to access low-level match information for the current document. Reanalyzing the text works well in most use cases. However, this method is more memory and time intensive for large fields. | . ",
    "url": "https://vagimeli.github.io/search-plugins/searching-data/highlight/#methods-of-obtaining-offsets",
    "relUrl": "/search-plugins/searching-data/highlight/#methods-of-obtaining-offsets"
  },"900": {
    "doc": "Highlight query matches",
    "title": "Highlighter types",
    "content": "OpenSearch supports three highlighter implementations: plain, unified, and fvh (Fast Vector Highlighter). The following table lists the methods of obtaining the offsets for each highlighter. | Highlighter | Method of obtaining offsets | . | unified | Term vectors if term_vector is set to with_positions_offsets, postings if index_options is set to offsets, text reanalysis otherwise. | . | fvh | Term vectors. | . | plain | Text reanalysis. | . Setting the highlighter type . To set the highlighter type, specify it in the type field: . GET shakespeare/_search { \"query\": { \"match\": { \"text_entry\": \"life\" } }, \"highlight\": { \"fields\": { \"text_entry\": { \"type\": \"plain\"} } } } . The unified highlighter . The unified highlighter is based on the Lucene Unified Highlighter and is the default highlighter for OpenSearch. It divides the text into sentences and treats those sentences as individual documents, scoring them in terms of similarity using the BM25 algorithm. The unified highlighter supports both exact phrase and multi-term highlighting, including fuzzy, prefix, and regex. If you’re using complex queries to highlight multiple fields in multiple documents, we recommend using the unified highlighter on postings or term_vector fields. The fvh highlighter . The fvh highlighter is based on the Lucene Fast Vector Highlighter. To use this highlighter, you need to store term vectors with positions offsets, which increases the index size. The fvh highlighter can combine matched terms from multiple fields into one result. It can also assign weights to matches depending on their positions; thus, you can sort phrase matches above term matches when highlighting a query that boosts phrase matches over term matches. Additionally, you can configure the fvh highlighter to select the boundaries of a returned text fragment, and you can highlight multiple words with different tags. The plain highlighter . The plain highlighter is based on the standard Lucene highlighter. It requires the highlighted fields to be stored either individually or in the _source field. The plain highlighter mirrors the query matching logic, in particular word importance and positions in phrase queries. It works for most use cases but may be slow for large fields because it has to reanalyze the text to be highlighted. ",
    "url": "https://vagimeli.github.io/search-plugins/searching-data/highlight/#highlighter-types",
    "relUrl": "/search-plugins/searching-data/highlight/#highlighter-types"
  },"901": {
    "doc": "Highlight query matches",
    "title": "Highlighting options",
    "content": "The following table describes the highlighting options you can specify on a global or field level. Field-level settings override global settings. | Option | Description | . | type | Specifies the highlighter to use. Valid values are unified, fvh, and plain. Default is unified. | . | fields | Specifies the fields to search for text to be highlighted. Supports wildcard expressions. If you use wildcards, only text and keyword fields are highlighted. For example, you can set fields to my_field* to include all text and keyword fields that start with the prefix my_field. | . | force_source | Specifies that field values for highlighting should be obtained from the _source field rather than from stored field values. Default is false. | . | require_field_match | Specifies whether to highlight only fields that contain a search query match. Default is true. To highlight all fields, set this option to false. | . | pre_tags | Specifies the HTML start tags for the highlighted text as an array of strings. | . | post_tags | Specifies the HTML end tags for the highlighted text as an array of strings. | . | tags_schema | If you set this option to styled, OpenSearch uses the built-in tag schema. In this schema, the pre_tags are &lt;em class=\"hlt1\"&gt;, &lt;em class=\"hlt2\"&gt;, &lt;em class=\"hlt3\"&gt;, &lt;em class=\"hlt4\"&gt;, &lt;em class=\"hlt5\"&gt;, &lt;em class=\"hlt6\"&gt;, &lt;em class=\"hlt7\"&gt;, &lt;em class=\"hlt8\"&gt;, &lt;em class=\"hlt9\"&gt;, and &lt;em class=\"hlt10\"&gt;, and the post_tags is &lt;/em&gt;. | . | boundary_chars | All boundary characters combined in a string. Default is \".,!? \\t\\n\". | . | boundary_scanner | Valid only for the unified and fvh highlighters. Specifies whether to split the highlighted fragments into sentences, words, or characters. Valid values are the following:- sentence: Split highlighted fragments at sentence boundaries, as defined by the BreakIterator. You can specify the BreakIterator’s locale in the boundary_scanner_locale option. - word: Split highlighted fragments at word boundaries, as defined by the BreakIterator. You can specify the BreakIterator’s locale in the boundary_scanner_locale option.- chars: Split highlighted fragments at any character listed in boundary_chars. Valid only for the fvh highlighter. | . | boundary_scanner_locale | Provides a locale for the boundary_scanner. Valid values are language tags (for example, \"en-US\"). Default is Locale.ROOT. | . | boundary_max_scan | Controls how far to scan for boundary characters when the boundary_scanner parameter for the fvh highlighter is set to chars. Default is 20. | . | encoder | Specifies whether the highlighted fragment should be HTML encoded before it is returned. Valid values are default (no encoding) or html (first escape the HTML text and then insert the highlighting tags). For example, if the field text is &lt;h3&gt;Hamlet&lt;/h3&gt; and the encoder is set to html, the highlighted text is \"&amp;lt;h3&amp;gt;&lt;em&gt;Hamlet&lt;/em&gt;&amp;lt;&amp;#x2F;h3&amp;gt;\". | . | fragmenter | Specifies how to split text into highlighted fragments. Valid only for the plain highlighter. Valid values are the following:- span (default): Splits text into fragments of the same size but tries not to split text between highlighted terms. - simple: Splits text into fragments of the same size. | . | fragment_offset | Specifies the character offset from which you want to start highlighting. Valid for the fvh highlighter only. | . | fragment_size | The size of a highlighted fragment, specified as the number of characters. If number_of_fragments is set to 0, fragment_size is ignored. Default is 100. | . | number_of_fragments | The maximum number of returned fragments. If number_of_fragments is set to 0, OpenSearch returns the highlighted contents of the entire field. Default is 5. | . | order | The sort order for the highlighted fragments. Set order to score to sort fragments by relevance. Each highlighter has a different algorithm for calculating relevance scores. Default is none. | . | highlight_query | Specifies that matches for a query other than the search query should be highlighted. The highlight_query option is useful when you use a faster query to get document matches and a slower query (for example, rescore_query) to refine the results. We recommend to include the search query as part of the highlight_query. | . | matched_fields | Combines matches from different fields to highlight one field. The most common use case for this functionality is highlighting text that is analyzed in different ways and kept in multi-fields. All fields in the matched_fields list must have the term_vector field set to with_positions_offsets. The field in which the matches are combined is the only loaded field, so it is beneficial to set its store option to yes. Valid only for the fvh highlighter. | . | no_match_size | Specifies the number of characters, starting from the beginning of the field, to return if there are no matching fragments to highlight. Default is 0. | . | phrase_limit | The number of matching phrases in a document that are considered. Limits the number of phrases to analyze by the fvh highlighter to avoid consuming a lot of memory. If matched_fields are used, phrase_limit specifies the number of phrases for each matched field. A higher phrase_limit leads to increased query time and more memory consumption. Valid only for the fvh highlighter. Default is 256. | . The unified highlighter’s sentence scanner splits sentences larger than fragment_size at the first word boundary after fragment_size is reached. To return whole sentences without splitting them, set fragment_size to 0. ",
    "url": "https://vagimeli.github.io/search-plugins/searching-data/highlight/#highlighting-options",
    "relUrl": "/search-plugins/searching-data/highlight/#highlighting-options"
  },"902": {
    "doc": "Highlight query matches",
    "title": "Changing the highlighting tags",
    "content": "Design your application code to parse the results from the highlight object and perform an action on the search terms, such as changing their color, bolding, italicizing, and so on. To change the default em tags, specify the new tags in the pretag and posttag parameters: . GET shakespeare/_search { \"query\": { \"match\": { \"play_name\": \"Henry IV\" } }, \"size\": 3, \"highlight\": { \"pre_tags\": [ \"&lt;strong&gt;\" ], \"post_tags\": [ \"&lt;/strong&gt;\" ], \"fields\": { \"play_name\": {} } } } . The play name is highlighted by the new tags in the response: . { \"took\" : 2, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 3205, \"relation\" : \"eq\" }, \"max_score\" : 3.548232, \"hits\" : [ { \"_index\" : \"shakespeare\", \"_id\" : \"0\", \"_score\" : 3.548232, \"_source\" : { \"type\" : \"act\", \"line_id\" : 1, \"play_name\" : \"Henry IV\", \"speech_number\" : \"\", \"line_number\" : \"\", \"speaker\" : \"\", \"text_entry\" : \"ACT I\" }, \"highlight\" : { \"play_name\" : [ \"&lt;strong&gt;Henry IV&lt;/strong&gt;\" ] } }, { \"_index\" : \"shakespeare\", \"_id\" : \"1\", \"_score\" : 3.548232, \"_source\" : { \"type\" : \"scene\", \"line_id\" : 2, \"play_name\" : \"Henry IV\", \"speech_number\" : \"\", \"line_number\" : \"\", \"speaker\" : \"\", \"text_entry\" : \"SCENE I. London. The palace.\" }, \"highlight\" : { \"play_name\" : [ \"&lt;strong&gt;Henry IV&lt;/strong&gt;\" ] } }, { \"_index\" : \"shakespeare\", \"_id\" : \"2\", \"_score\" : 3.548232, \"_source\" : { \"type\" : \"line\", \"line_id\" : 3, \"play_name\" : \"Henry IV\", \"speech_number\" : \"\", \"line_number\" : \"\", \"speaker\" : \"\", \"text_entry\" : \"Enter KING HENRY, LORD JOHN OF LANCASTER, the EARL of WESTMORELAND, SIR WALTER BLUNT, and others\" }, \"highlight\" : { \"play_name\" : [ \"&lt;strong&gt;Henry IV&lt;/strong&gt;\" ] } } ] } } . ",
    "url": "https://vagimeli.github.io/search-plugins/searching-data/highlight/#changing-the-highlighting-tags",
    "relUrl": "/search-plugins/searching-data/highlight/#changing-the-highlighting-tags"
  },"903": {
    "doc": "Highlight query matches",
    "title": "Specifying a highlight query",
    "content": "By default, OpenSearch only considers the search query for highlighting. If you use a fast query to get document matches and a slower query like rescore_query to refine the results, it is useful to highlight the refined results. You can do this by adding a highlight_query: . GET shakespeare/_search { \"query\": { \"match\": { \"text_entry\": { \"query\": \"thats my name\" } } }, \"rescore\": { \"window_size\": 20, \"query\": { \"rescore_query\": { \"match_phrase\": { \"text_entry\": { \"query\": \"thats my name\", \"slop\": 1 } } }, \"rescore_query_weight\": 5 } }, \"_source\": false, \"highlight\": { \"order\": \"score\", \"fields\": { \"text_entry\": { \"highlight_query\": { \"bool\": { \"must\": { \"match\": { \"text_entry\": { \"query\": \"thats my name\" } } }, \"should\": { \"match_phrase\": { \"text_entry\": { \"query\": \"that is my name\", \"slop\": 1, \"boost\": 10.0 } } }, \"minimum_should_match\": 0 } } } } } } . ",
    "url": "https://vagimeli.github.io/search-plugins/searching-data/highlight/#specifying-a-highlight-query",
    "relUrl": "/search-plugins/searching-data/highlight/#specifying-a-highlight-query"
  },"904": {
    "doc": "Highlight query matches",
    "title": "Combining matches from different fields to highlight one field",
    "content": "You can combine matches from different fields to highlight one field with the fvh highlighter. The most common use case for this functionality is highlighting text that is analyzed in different ways and kept in multi-fields. All fields in the matched_fields list must have the term_vector field set to with_positions_offsets. The field in which the matches are combined is the only loaded field, so it is beneficial to set its store option to yes. Example . Create a mapping for the shakespeare index where the text_entry field is analyzed with the standard analyzer and has an english subfield that is analyzed with the english analyzer: . PUT shakespeare { \"mappings\" : { \"properties\" : { \"text_entry\" : { \"type\" : \"text\", \"term_vector\": \"with_positions_offsets\", \"fields\": { \"english\": { \"type\": \"text\", \"analyzer\": \"english\", \"term_vector\": \"with_positions_offsets\" } } } } } } . The standard analyzer splits the text_entry fields into individual words. You can confirm this by using the analyze API operation: . GET shakespeare/_analyze { \"text\": \"bragging of thine\", \"field\": \"text_entry\" } . The response contains the original string split on white space: . { \"tokens\" : [ { \"token\" : \"bragging\", \"start_offset\" : 0, \"end_offset\" : 8, \"type\" : \"&lt;ALPHANUM&gt;\", \"position\" : 0 }, { \"token\" : \"of\", \"start_offset\" : 9, \"end_offset\" : 11, \"type\" : \"&lt;ALPHANUM&gt;\", \"position\" : 1 }, { \"token\" : \"thine\", \"start_offset\" : 12, \"end_offset\" : 17, \"type\" : \"&lt;ALPHANUM&gt;\", \"position\" : 2 } ] } . The english analyzer not only splits the string into words but also stems the tokens and removes stopwords. You can confirm this by using the analyze API operation with the text_entry.english field: . GET shakespeare/_analyze { \"text\": \"bragging of thine\", \"field\": \"text_entry.english\" } . The response contains the stemmed words: . { \"tokens\" : [ { \"token\" : \"brag\", \"start_offset\" : 0, \"end_offset\" : 8, \"type\" : \"&lt;ALPHANUM&gt;\", \"position\" : 0 }, { \"token\" : \"thine\", \"start_offset\" : 12, \"end_offset\" : 17, \"type\" : \"&lt;ALPHANUM&gt;\", \"position\" : 2 } ] } . To search for all forms of the word bragging, use the following query: . GET shakespeare/_search { \"query\": { \"query_string\": { \"query\": \"text_entry.english:bragging\", \"fields\": [ \"text_entry\" ] } }, \"highlight\": { \"order\": \"score\", \"fields\": { \"text_entry\": { \"matched_fields\": [ \"text_entry\", \"text_entry.english\" ], \"type\": \"fvh\" } } } } . The response highlights all versions of the word “bragging” in the text_entry field: . { \"took\" : 5, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 26, \"relation\" : \"eq\" }, \"max_score\" : 10.153671, \"hits\" : [ { \"_index\" : \"shakespeare\", \"_id\" : \"56666\", \"_score\" : 10.153671, \"_source\" : { \"type\" : \"line\", \"line_id\" : 56667, \"play_name\" : \"macbeth\", \"speech_number\" : 34, \"line_number\" : \"2.3.118\", \"speaker\" : \"MACBETH\", \"text_entry\" : \"Is left this vault to brag of.\" }, \"highlight\" : { \"text_entry\" : [ \"Is left this vault to &lt;em&gt;brag&lt;/em&gt; of.\" ] } }, { \"_index\" : \"shakespeare\", \"_id\" : \"71445\", \"_score\" : 9.284528, \"_source\" : { \"type\" : \"line\", \"line_id\" : 71446, \"play_name\" : \"Much Ado about nothing\", \"speech_number\" : 18, \"line_number\" : \"5.1.65\", \"speaker\" : \"LEONATO\", \"text_entry\" : \"As under privilege of age to brag\" }, \"highlight\" : { \"text_entry\" : [ \"As under privilege of age to &lt;em&gt;brag&lt;/em&gt;\" ] } }, { \"_index\" : \"shakespeare\", \"_id\" : \"86782\", \"_score\" : 9.284528, \"_source\" : { \"type\" : \"line\", \"line_id\" : 86783, \"play_name\" : \"Romeo and Juliet\", \"speech_number\" : 8, \"line_number\" : \"2.6.31\", \"speaker\" : \"JULIET\", \"text_entry\" : \"Brags of his substance, not of ornament:\" }, \"highlight\" : { \"text_entry\" : [ \"&lt;em&gt;Brags&lt;/em&gt; of his substance, not of ornament:\" ] } }, { \"_index\" : \"shakespeare\", \"_id\" : \"44531\", \"_score\" : 8.552448, \"_source\" : { \"type\" : \"line\", \"line_id\" : 44532, \"play_name\" : \"King John\", \"speech_number\" : 15, \"line_number\" : \"3.1.124\", \"speaker\" : \"CONSTANCE\", \"text_entry\" : \"A ramping fool, to brag and stamp and swear\" }, \"highlight\" : { \"text_entry\" : [ \"A ramping fool, to &lt;em&gt;brag&lt;/em&gt; and stamp and swear\" ] } }, { \"_index\" : \"shakespeare\", \"_id\" : \"63208\", \"_score\" : 8.552448, \"_source\" : { \"type\" : \"line\", \"line_id\" : 63209, \"play_name\" : \"Merchant of Venice\", \"speech_number\" : 11, \"line_number\" : \"3.4.79\", \"speaker\" : \"PORTIA\", \"text_entry\" : \"A thousand raw tricks of these bragging Jacks,\" }, \"highlight\" : { \"text_entry\" : [ \"A thousand raw tricks of these &lt;em&gt;bragging&lt;/em&gt; Jacks,\" ] } }, { \"_index\" : \"shakespeare\", \"_id\" : \"73026\", \"_score\" : 8.552448, \"_source\" : { \"type\" : \"line\", \"line_id\" : 73027, \"play_name\" : \"Othello\", \"speech_number\" : 75, \"line_number\" : \"2.1.242\", \"speaker\" : \"IAGO\", \"text_entry\" : \"but for bragging and telling her fantastical lies:\" }, \"highlight\" : { \"text_entry\" : [ \"but for &lt;em&gt;bragging&lt;/em&gt; and telling her fantastical lies:\" ] } }, { \"_index\" : \"shakespeare\", \"_id\" : \"85974\", \"_score\" : 8.552448, \"_source\" : { \"type\" : \"line\", \"line_id\" : 85975, \"play_name\" : \"Romeo and Juliet\", \"speech_number\" : 20, \"line_number\" : \"1.5.70\", \"speaker\" : \"CAPULET\", \"text_entry\" : \"And, to say truth, Verona brags of him\" }, \"highlight\" : { \"text_entry\" : [ \"And, to say truth, Verona &lt;em&gt;brags&lt;/em&gt; of him\" ] } }, { \"_index\" : \"shakespeare\", \"_id\" : \"96800\", \"_score\" : 8.552448, \"_source\" : { \"type\" : \"line\", \"line_id\" : 96801, \"play_name\" : \"Titus Andronicus\", \"speech_number\" : 60, \"line_number\" : \"1.1.311\", \"speaker\" : \"SATURNINUS\", \"text_entry\" : \"Agree these deeds with that proud brag of thine,\" }, \"highlight\" : { \"text_entry\" : [ \"Agree these deeds with that proud &lt;em&gt;brag&lt;/em&gt; of thine,\" ] } }, { \"_index\" : \"shakespeare\", \"_id\" : \"18189\", \"_score\" : 7.9273787, \"_source\" : { \"type\" : \"line\", \"line_id\" : 18190, \"play_name\" : \"As you like it\", \"speech_number\" : 12, \"line_number\" : \"5.2.30\", \"speaker\" : \"ROSALIND\", \"text_entry\" : \"and Caesars thrasonical brag of I came, saw, and\" }, \"highlight\" : { \"text_entry\" : [ \"and Caesars thrasonical &lt;em&gt;brag&lt;/em&gt; of I came, saw, and\" ] } }, { \"_index\" : \"shakespeare\", \"_id\" : \"32054\", \"_score\" : 7.9273787, \"_source\" : { \"type\" : \"line\", \"line_id\" : 32055, \"play_name\" : \"Cymbeline\", \"speech_number\" : 52, \"line_number\" : \"5.5.211\", \"speaker\" : \"IACHIMO\", \"text_entry\" : \"And then a mind put int, either our brags\" }, \"highlight\" : { \"text_entry\" : [ \"And then a mind put int, either our &lt;em&gt;brags&lt;/em&gt;\" ] } } ] } } . To score the original form of the word “bragging” higher, you can boost the text_entry field: . GET shakespeare/_search { \"query\": { \"query_string\": { \"query\": \"bragging\", \"fields\": [ \"text_entry^5\", \"text_entry.english\" ] } }, \"highlight\": { \"order\": \"score\", \"fields\": { \"text_entry\": { \"matched_fields\": [ \"text_entry\", \"text_entry.english\" ], \"type\": \"fvh\" } } } } . The response lists documents that contain the word “bragging” first: . { \"took\" : 17, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 26, \"relation\" : \"eq\" }, \"max_score\" : 49.746853, \"hits\" : [ { \"_index\" : \"shakespeare\", \"_id\" : \"45739\", \"_score\" : 49.746853, \"_source\" : { \"type\" : \"line\", \"line_id\" : 45740, \"play_name\" : \"King John\", \"speech_number\" : 10, \"line_number\" : \"5.1.51\", \"speaker\" : \"BASTARD\", \"text_entry\" : \"Of bragging horror: so shall inferior eyes,\" }, \"highlight\" : { \"text_entry\" : [ \"Of &lt;em&gt;bragging&lt;/em&gt; horror: so shall inferior eyes,\" ] } }, { \"_index\" : \"shakespeare\", \"_id\" : \"63208\", \"_score\" : 47.077244, \"_source\" : { \"type\" : \"line\", \"line_id\" : 63209, \"play_name\" : \"Merchant of Venice\", \"speech_number\" : 11, \"line_number\" : \"3.4.79\", \"speaker\" : \"PORTIA\", \"text_entry\" : \"A thousand raw tricks of these bragging Jacks,\" }, \"highlight\" : { \"text_entry\" : [ \"A thousand raw tricks of these &lt;em&gt;bragging&lt;/em&gt; Jacks,\" ] } }, { \"_index\" : \"shakespeare\", \"_id\" : \"68474\", \"_score\" : 47.077244, \"_source\" : { \"type\" : \"line\", \"line_id\" : 68475, \"play_name\" : \"A Midsummer nights dream\", \"speech_number\" : 101, \"line_number\" : \"3.2.427\", \"speaker\" : \"PUCK\", \"text_entry\" : \"Thou coward, art thou bragging to the stars,\" }, \"highlight\" : { \"text_entry\" : [ \"Thou coward, art thou &lt;em&gt;bragging&lt;/em&gt; to the stars,\" ] } }, { \"_index\" : \"shakespeare\", \"_id\" : \"73026\", \"_score\" : 47.077244, \"_source\" : { \"type\" : \"line\", \"line_id\" : 73027, \"play_name\" : \"Othello\", \"speech_number\" : 75, \"line_number\" : \"2.1.242\", \"speaker\" : \"IAGO\", \"text_entry\" : \"but for bragging and telling her fantastical lies:\" }, \"highlight\" : { \"text_entry\" : [ \"but for &lt;em&gt;bragging&lt;/em&gt; and telling her fantastical lies:\" ] } }, { \"_index\" : \"shakespeare\", \"_id\" : \"39816\", \"_score\" : 44.679565, \"_source\" : { \"type\" : \"line\", \"line_id\" : 39817, \"play_name\" : \"Henry V\", \"speech_number\" : 28, \"line_number\" : \"5.2.138\", \"speaker\" : \"KING HENRY V\", \"text_entry\" : \"armour on my back, under the correction of bragging\" }, \"highlight\" : { \"text_entry\" : [ \"armour on my back, under the correction of &lt;em&gt;bragging&lt;/em&gt;\" ] } }, { \"_index\" : \"shakespeare\", \"_id\" : \"63200\", \"_score\" : 44.679565, \"_source\" : { \"type\" : \"line\", \"line_id\" : 63201, \"play_name\" : \"Merchant of Venice\", \"speech_number\" : 11, \"line_number\" : \"3.4.71\", \"speaker\" : \"PORTIA\", \"text_entry\" : \"Like a fine bragging youth, and tell quaint lies,\" }, \"highlight\" : { \"text_entry\" : [ \"Like a fine &lt;em&gt;bragging&lt;/em&gt; youth, and tell quaint lies,\" ] } }, { \"_index\" : \"shakespeare\", \"_id\" : \"56666\", \"_score\" : 10.153671, \"_source\" : { \"type\" : \"line\", \"line_id\" : 56667, \"play_name\" : \"macbeth\", \"speech_number\" : 34, \"line_number\" : \"2.3.118\", \"speaker\" : \"MACBETH\", \"text_entry\" : \"Is left this vault to brag of.\" }, \"highlight\" : { \"text_entry\" : [ \"Is left this vault to &lt;em&gt;brag&lt;/em&gt; of.\" ] } }, { \"_index\" : \"shakespeare\", \"_id\" : \"71445\", \"_score\" : 9.284528, \"_source\" : { \"type\" : \"line\", \"line_id\" : 71446, \"play_name\" : \"Much Ado about nothing\", \"speech_number\" : 18, \"line_number\" : \"5.1.65\", \"speaker\" : \"LEONATO\", \"text_entry\" : \"As under privilege of age to brag\" }, \"highlight\" : { \"text_entry\" : [ \"As under privilege of age to &lt;em&gt;brag&lt;/em&gt;\" ] } }, { \"_index\" : \"shakespeare\", \"_id\" : \"86782\", \"_score\" : 9.284528, \"_source\" : { \"type\" : \"line\", \"line_id\" : 86783, \"play_name\" : \"Romeo and Juliet\", \"speech_number\" : 8, \"line_number\" : \"2.6.31\", \"speaker\" : \"JULIET\", \"text_entry\" : \"Brags of his substance, not of ornament:\" }, \"highlight\" : { \"text_entry\" : [ \"&lt;em&gt;Brags&lt;/em&gt; of his substance, not of ornament:\" ] } }, { \"_index\" : \"shakespeare\", \"_id\" : \"44531\", \"_score\" : 8.552448, \"_source\" : { \"type\" : \"line\", \"line_id\" : 44532, \"play_name\" : \"King John\", \"speech_number\" : 15, \"line_number\" : \"3.1.124\", \"speaker\" : \"CONSTANCE\", \"text_entry\" : \"A ramping fool, to brag and stamp and swear\" }, \"highlight\" : { \"text_entry\" : [ \"A ramping fool, to &lt;em&gt;brag&lt;/em&gt; and stamp and swear\" ] } } ] } } . ",
    "url": "https://vagimeli.github.io/search-plugins/searching-data/highlight/#combining-matches-from-different-fields-to-highlight-one-field",
    "relUrl": "/search-plugins/searching-data/highlight/#combining-matches-from-different-fields-to-highlight-one-field"
  },"905": {
    "doc": "Highlight query matches",
    "title": "Query limitations",
    "content": "Note the following limitations: . | When extracting terms to highlight, highlighters don’t reflect the Boolean logic of a query. Therefore, for some complex Boolean queries, such as nested Boolean queries and queries using minimum_should_match, OpenSearch may highlight terms that don’t correspond to query matches. | The fvh highlighter does not support span queries. | . ",
    "url": "https://vagimeli.github.io/search-plugins/searching-data/highlight/#query-limitations",
    "relUrl": "/search-plugins/searching-data/highlight/#query-limitations"
  },"906": {
    "doc": "Searching data",
    "title": "Searching data",
    "content": "What users expect from search engines has evolved over the years. Just returning relevant results quickly is no longer enough for most users. Now users seek methods that allow them to get even more relevant results, to sort and organize results, and to highlight their queries. OpenSearch includes many features, described in the following table, that enhance the search experience. | Feature | Description | . | Autocomplete functionality | Suggest phrases as the user types. | . | Did-you-mean functionality | Check spelling of phrases as the user types. | . | Paginate results | Rather than a single, long list, separate search results into pages. | . | Sort results | Allow sorting of results by different criteria. | . | Highlight query matches | Highlight the search term in the results. | . ",
    "url": "https://vagimeli.github.io/search-plugins/searching-data/index/",
    "relUrl": "/search-plugins/searching-data/index/"
  },"907": {
    "doc": "Paginate results",
    "title": "Paginate results",
    "content": "You can use the following methods to paginate search results in OpenSearch: . | The from and size parameters | The scroll search operation | The search_after parameter | Point in Time with search_after | . ",
    "url": "https://vagimeli.github.io/search-plugins/searching-data/paginate/",
    "relUrl": "/search-plugins/searching-data/paginate/"
  },"908": {
    "doc": "Paginate results",
    "title": "The from and size parameters",
    "content": "The from and size parameters return results one page at a time. The from parameter is the document number from which you want to start showing the results. The size parameter is the number of results that you want to show. Together, they let you return a subset of the search results. For example, if the value of size is 10 and the value of from is 0, you see the first 10 results. If you change the value of from to 10, you see the next 10 results (because the results are zero-indexed). So if you want to see results starting from result 11, from must be 10. GET shakespeare/_search { \"from\": 0, \"size\": 10, \"query\": { \"match\": { \"play_name\": \"Hamlet\" } } } . Use the following formula to calculate the from parameter relative to the page number: . from = size * (page_number - 1) . Each time the user chooses the next page of the results, your application needs to run the same search query with an incremented from value. You can also specify the from and size parameters in the search URI: . GET shakespeare/_search?from=0&amp;size=10 . If you only specify the size parameter, the from parameter defaults to 0. Querying for pages deep in your results can have a significant performance impact, so OpenSearch limits this approach to 10,000 results. The from and size parameters are stateless, so the results are based on the latest available data. This can cause inconsistent pagination. For example, assume a user stays on the first page of the results and then navigates to the second page. During that time, a new document relevant to the user’s search is indexed and shows up on the first page. In this scenario, the last result on the first page is pushed to the second page, and the user sees duplicate results (that is, the first and second pages both display that last result). Use the scroll operation for consistent pagination. The scroll operation keeps a search context open for a certain period of time. Any data changes do not affect the results during that time. ",
    "url": "https://vagimeli.github.io/search-plugins/searching-data/paginate/#the-from-and-size-parameters",
    "relUrl": "/search-plugins/searching-data/paginate/#the-from-and-size-parameters"
  },"909": {
    "doc": "Paginate results",
    "title": "Scroll search",
    "content": "The from and size parameters allow you to paginate your search results but with a limit of 10,000 results at a time. If you need to request volumes of data larger than 1 PB from, for example, a machine learning job, use the scroll operation instead. The scroll operation allows you to request an unlimited number of results. To use the scroll operation, add a scroll parameter to the request header with a search context telling OpenSearch for how long you need to keep scrolling. This search context needs to be long enough to process a single batch of results. To set the number of results that you want returned for each batch, use the size parameter: . GET shakespeare/_search?scroll=10m { \"size\": 10000 } . OpenSearch caches the results and returns a scroll ID that you can use to access them in batches: . \"_scroll_id\" : \"DXF1ZXJ5QW5kRmV0Y2gBAAAAAAAAAAUWdmpUZDhnRFBUcWFtV21nMmFwUGJEQQ==\" . Pass this scroll ID to the scroll operation to obtain the next batch of results: . GET _search/scroll { \"scroll\": \"10m\", \"scroll_id\": \"DXF1ZXJ5QW5kRmV0Y2gBAAAAAAAAAAUWdmpUZDhnRFBUcWFtV21nMmFwUGJEQQ==\" } . Using this scroll ID, you get results in batches of 10,000 as long as the search context is still open. Typically, the scroll ID does not change between requests, but it can change, so make sure to always use the latest scroll ID. If you don’t send the next scroll request within the set search context, the scroll operation does not return any results. If you expect billions of results, use a sliced scroll. Slicing allows you to perform multiple scroll operations for the same request but in parallel. Set the ID and the maximum number of slices for the scroll: . GET shakespeare/_search?scroll=10m { \"slice\": { \"id\": 0, \"max\": 10 }, \"query\": { \"match_all\": {} } } . With a single scroll ID, you receive 10 results. You can have up to 10 IDs. Perform the same command with the ID equal to 1: . GET shakespeare/_search?scroll=10m { \"slice\": { \"id\": 1, \"max\": 10 }, \"query\": { \"match_all\": {} } } . Close the search context when you’re done scrolling, because it continues to consume computing resources until the timeout: . DELETE _search/scroll/DXF1ZXJ5QW5kRmV0Y2gBAAAAAAAAAAcWdmpUZDhnRFBUcWFtV21nMmFwUGJEQQ== . Sample Response . { \"succeeded\": true, \"num_freed\": 1 } . Use the following request to close all open scroll contexts: . DELETE _search/scroll/_all . The scroll operation corresponds to a specific timestamp. It doesn’t consider documents added after that timestamp as potential results. Because open search contexts consume a lot of memory, we suggest you don’t use the scroll operation for frequent user queries that don’t need the search context to be open. Instead, use the sort parameter with the search_after parameter to scroll responses for user queries. ",
    "url": "https://vagimeli.github.io/search-plugins/searching-data/paginate/#scroll-search",
    "relUrl": "/search-plugins/searching-data/paginate/#scroll-search"
  },"910": {
    "doc": "Paginate results",
    "title": "The search_after parameter",
    "content": "The search_after parameter provides a live cursor that uses the previous page’s results to obtain the next page’s results. It is similar to the scroll operation in that it is meant to scroll many queries in parallel. For example, the following query sorts all lines from the play “Hamlet” by the speech number and then the ID and retrieves the first three results: . GET shakespeare/_search { \"size\": 3, \"query\": { \"match\": { \"play_name\": \"Hamlet\" } }, \"sort\": [ { \"speech_number\": \"asc\" }, { \"_id\": \"asc\" } ] } . The response contains the sort array of values for each document: . { \"took\" : 7, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 4244, \"relation\" : \"eq\" }, \"max_score\" : null, \"hits\" : [ { \"_index\" : \"shakespeare\", \"_id\" : \"32435\", \"_score\" : null, \"_source\" : { \"type\" : \"line\", \"line_id\" : 32436, \"play_name\" : \"Hamlet\", \"speech_number\" : 1, \"line_number\" : \"1.1.1\", \"speaker\" : \"BERNARDO\", \"text_entry\" : \"Whos there?\" }, \"sort\" : [ 1, \"32435\" ] }, { \"_index\" : \"shakespeare\", \"_id\" : \"32634\", \"_score\" : null, \"_source\" : { \"type\" : \"line\", \"line_id\" : 32635, \"play_name\" : \"Hamlet\", \"speech_number\" : 1, \"line_number\" : \"1.2.1\", \"speaker\" : \"KING CLAUDIUS\", \"text_entry\" : \"Though yet of Hamlet our dear brothers death\" }, \"sort\" : [ 1, \"32634\" ] }, { \"_index\" : \"shakespeare\", \"_id\" : \"32635\", \"_score\" : null, \"_source\" : { \"type\" : \"line\", \"line_id\" : 32636, \"play_name\" : \"Hamlet\", \"speech_number\" : 1, \"line_number\" : \"1.2.2\", \"speaker\" : \"KING CLAUDIUS\", \"text_entry\" : \"The memory be green, and that it us befitted\" }, \"sort\" : [ 1, \"32635\" ] } ] } } . You can use the last result’s sort values to retrieve the next result by using the search_after parameter: . GET shakespeare/_search { \"size\": 10, \"query\": { \"match\": { \"play_name\": \"Hamlet\" } }, \"search_after\": [ 1, \"32635\"], \"sort\": [ { \"speech_number\": \"asc\" }, { \"_id\": \"asc\" } ] } . Unlike the scroll operation, the search_after parameter is stateless, so the document order may change because of documents being indexed or deleted. ",
    "url": "https://vagimeli.github.io/search-plugins/searching-data/paginate/#the-search_after-parameter",
    "relUrl": "/search-plugins/searching-data/paginate/#the-search_after-parameter"
  },"911": {
    "doc": "Paginate results",
    "title": "Point in Time with search_after",
    "content": "Point in Time (PIT) with search_after is the preferred pagination method in OpenSearch, especially for deep pagination. It bypasses the limitations of all other methods because it operates on a dataset that is frozen in time, it is not bound to a query, and it supports consistent pagination going forward and backward. To learn more, see Point in Time. ",
    "url": "https://vagimeli.github.io/search-plugins/searching-data/paginate/#point-in-time-with-search_after",
    "relUrl": "/search-plugins/searching-data/paginate/#point-in-time-with-search_after"
  },"912": {
    "doc": "Sort results",
    "title": "Sort results",
    "content": "Sorting allows your users to sort results in a way that’s most meaningful to them. By default, full-text queries sort results by the relevance score. You can choose to sort the results by any field value in either ascending or descending order by setting the order parameter to asc or desc. For example, to sort results by descending order of a line_id value, use the following query: . GET shakespeare/_search { \"query\": { \"term\": { \"play_name\": { \"value\": \"Henry IV\" } } }, \"sort\": [ { \"line_id\": { \"order\": \"desc\" } } ] } . The results are sorted by line_id in descending order: . { \"took\" : 24, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 3205, \"relation\" : \"eq\" }, \"max_score\" : null, \"hits\" : [ { \"_index\" : \"shakespeare\", \"_id\" : \"3204\", \"_score\" : null, \"_source\" : { \"type\" : \"line\", \"line_id\" : 3205, \"play_name\" : \"Henry IV\", \"speech_number\" : 8, \"line_number\" : \"\", \"speaker\" : \"KING HENRY IV\", \"text_entry\" : \"Exeunt\" }, \"sort\" : [ 3205 ] }, { \"_index\" : \"shakespeare\", \"_id\" : \"3203\", \"_score\" : null, \"_source\" : { \"type\" : \"line\", \"line_id\" : 3204, \"play_name\" : \"Henry IV\", \"speech_number\" : 8, \"line_number\" : \"5.5.45\", \"speaker\" : \"KING HENRY IV\", \"text_entry\" : \"Let us not leave till all our own be won.\" }, \"sort\" : [ 3204 ] }, { \"_index\" : \"shakespeare\", \"_id\" : \"3202\", \"_score\" : null, \"_source\" : { \"type\" : \"line\", \"line_id\" : 3203, \"play_name\" : \"Henry IV\", \"speech_number\" : 8, \"line_number\" : \"5.5.44\", \"speaker\" : \"KING HENRY IV\", \"text_entry\" : \"And since this business so fair is done,\" }, \"sort\" : [ 3203 ] }, { \"_index\" : \"shakespeare\", \"_id\" : \"3201\", \"_score\" : null, \"_source\" : { \"type\" : \"line\", \"line_id\" : 3202, \"play_name\" : \"Henry IV\", \"speech_number\" : 8, \"line_number\" : \"5.5.43\", \"speaker\" : \"KING HENRY IV\", \"text_entry\" : \"Meeting the cheque of such another day:\" }, \"sort\" : [ 3202 ] }, { \"_index\" : \"shakespeare\", \"_id\" : \"3200\", \"_score\" : null, \"_source\" : { \"type\" : \"line\", \"line_id\" : 3201, \"play_name\" : \"Henry IV\", \"speech_number\" : 8, \"line_number\" : \"5.5.42\", \"speaker\" : \"KING HENRY IV\", \"text_entry\" : \"Rebellion in this land shall lose his sway,\" }, \"sort\" : [ 3201 ] }, { \"_index\" : \"shakespeare\", \"_id\" : \"3199\", \"_score\" : null, \"_source\" : { \"type\" : \"line\", \"line_id\" : 3200, \"play_name\" : \"Henry IV\", \"speech_number\" : 8, \"line_number\" : \"5.5.41\", \"speaker\" : \"KING HENRY IV\", \"text_entry\" : \"To fight with Glendower and the Earl of March.\" }, \"sort\" : [ 3200 ] }, { \"_index\" : \"shakespeare\", \"_id\" : \"3198\", \"_score\" : null, \"_source\" : { \"type\" : \"line\", \"line_id\" : 3199, \"play_name\" : \"Henry IV\", \"speech_number\" : 8, \"line_number\" : \"5.5.40\", \"speaker\" : \"KING HENRY IV\", \"text_entry\" : \"Myself and you, son Harry, will towards Wales,\" }, \"sort\" : [ 3199 ] }, { \"_index\" : \"shakespeare\", \"_id\" : \"3197\", \"_score\" : null, \"_source\" : { \"type\" : \"line\", \"line_id\" : 3198, \"play_name\" : \"Henry IV\", \"speech_number\" : 8, \"line_number\" : \"5.5.39\", \"speaker\" : \"KING HENRY IV\", \"text_entry\" : \"Who, as we hear, are busily in arms:\" }, \"sort\" : [ 3198 ] }, { \"_index\" : \"shakespeare\", \"_id\" : \"3196\", \"_score\" : null, \"_source\" : { \"type\" : \"line\", \"line_id\" : 3197, \"play_name\" : \"Henry IV\", \"speech_number\" : 8, \"line_number\" : \"5.5.38\", \"speaker\" : \"KING HENRY IV\", \"text_entry\" : \"To meet Northumberland and the prelate Scroop,\" }, \"sort\" : [ 3197 ] }, { \"_index\" : \"shakespeare\", \"_id\" : \"3195\", \"_score\" : null, \"_source\" : { \"type\" : \"line\", \"line_id\" : 3196, \"play_name\" : \"Henry IV\", \"speech_number\" : 8, \"line_number\" : \"5.5.37\", \"speaker\" : \"KING HENRY IV\", \"text_entry\" : \"Towards York shall bend you with your dearest speed,\" }, \"sort\" : [ 3196 ] } ] } } . The sort parameter is an array, so you can specify multiple field values in the order of their priority. If you have two fields with the same value for line_id, OpenSearch uses speech_number, which is the second option for sorting: . GET shakespeare/_search { \"query\": { \"term\": { \"play_name\": { \"value\": \"Henry IV\" } } }, \"sort\": [ { \"line_id\": { \"order\": \"desc\" } }, { \"speech_number\": { \"order\": \"desc\" } } ] } . You can continue to sort by any number of field values to get the results in just the right order. It doesn’t have to be a numerical value—you can also sort by date or timestamp fields: . \"sort\": [ { \"date\": { \"order\": \"desc\" } } ] . A text field that is analyzed cannot be used to sort documents, because the inverted index only contains the individual tokenized terms and not the entire string. So you cannot sort by the play_name, for example. To bypass this limitation, you can use a raw version of the text field mapped as a keyword type. In the following example, play_name.keyword is not analyzed and you have a copy of the full original version for sorting purposes: . GET shakespeare/_search { \"query\": { \"term\": { \"play_name\": { \"value\": \"Henry IV\" } } }, \"sort\": [ { \"play_name.keyword\": { \"order\": \"desc\" } } ] } . The results are sorted by the play_name field in alphabetical order. Use sort with the search_after parameter for more efficient scrolling. The results start with the document that comes after the sort values you specify in the search_after array. Make sure you have the same number of values in the search_after array as in the sort array, also ordered in the same way. In this case, you are requesting results starting with the document that comes after line_id = 3202 and speech_number = 8: . GET shakespeare/_search { \"query\": { \"term\": { \"play_name\": { \"value\": \"Henry IV\" } } }, \"sort\": [ { \"line_id\": { \"order\": \"desc\" } }, { \"speech_number\": { \"order\": \"desc\" } } ], \"search_after\": [ \"3202\", \"8\" ] } . ",
    "url": "https://vagimeli.github.io/search-plugins/searching-data/sort/",
    "relUrl": "/search-plugins/searching-data/sort/"
  },"913": {
    "doc": "Sort results",
    "title": "Sort mode",
    "content": "The sort mode is applicable to sorting by array or multivalued fields. It specifies what array value should be chosen for sorting the document. For numeric fields that contain an array of numbers, you can sort by the avg, sum, or median modes. To sort by the minimum or maximum values, use the min or max modes that work for both numeric and string data types. The default mode is min for ascending sort order and max for descending sort order. The following example illustrates sorting by an array field using the sort mode. Consider an index that holds student grades. Index two documents into the index: . PUT students/_doc/1 { \"name\": \"John Doe\", \"grades\": [70, 90] } PUT students/_doc/2 { \"name\": \"Mary Major\", \"grades\": [80, 100] } . Sort all students by highest grade average using the avg mode: . GET students/_search { \"query\" : { \"match_all\": {} }, \"sort\" : [ {\"grades\" : {\"order\" : \"desc\", \"mode\" : \"avg\"}} ] } . The response contains students sorted by grades in descending order: . { \"took\" : 1, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 2, \"relation\" : \"eq\" }, \"max_score\" : null, \"hits\" : [ { \"_index\" : \"students\", \"_id\" : \"2\", \"_score\" : null, \"_source\" : { \"name\" : \"Mary Major\", \"grades\" : [ 80, 100 ] }, \"sort\" : [ 90 ] }, { \"_index\" : \"students\", \"_id\" : \"1\", \"_score\" : null, \"_source\" : { \"name\" : \"John Doe\", \"grades\" : [ 70, 90 ] }, \"sort\" : [ 80 ] } ] } } . ",
    "url": "https://vagimeli.github.io/search-plugins/searching-data/sort/#sort-mode",
    "relUrl": "/search-plugins/searching-data/sort/#sort-mode"
  },"914": {
    "doc": "Sort results",
    "title": "Sorting nested objects",
    "content": "When sorting nested objects, provide the path parameter specifying the path to the field on which to sort. For example, in the index students, map the variable first_sem as nested: . PUT students { \"mappings\" : { \"properties\": { \"first_sem\": { \"type\" : \"nested\" } } } } . Index two documents with nested fields: . PUT students/_doc/1 { \"name\": \"John Doe\", \"first_sem\" : { \"grades\": [70, 90] } } PUT students/_doc/2 { \"name\": \"Mary Major\", \"first_sem\": { \"grades\": [80, 100] } } . When sorting by grade average, provide the path to the nested field: . GET students/_search { \"query\" : { \"match_all\": {} }, \"sort\" : [ {\"first_sem.grades\": { \"order\" : \"desc\", \"mode\" : \"avg\", \"nested\": { \"path\": \"first_sem\" } } } ] } . ",
    "url": "https://vagimeli.github.io/search-plugins/searching-data/sort/#sorting-nested-objects",
    "relUrl": "/search-plugins/searching-data/sort/#sorting-nested-objects"
  },"915": {
    "doc": "Sort results",
    "title": "Handling missing values",
    "content": "The missing parameter specifies the handling of missing values. The built-in valid values are _last (list the documents with the missing value last) and _first (list the documents with the missing value first). The default value is _last. You can also specify a custom value to be used for missing documents as the sort value. For example, you can index a document with an average field and another document without an average field: . PUT students/_doc/1 { \"name\": \"John Doe\", \"average\": 80 } PUT students/_doc/2 { \"name\": \"Mary Major\" } . Sort the documents, ordering the document with a missing field first: . GET students/_search { \"query\": { \"match_all\": {} }, \"sort\": [ { \"average\": { \"order\": \"desc\", \"missing\": \"_first\" } } ] } . The response lists document 2 first: . { \"took\" : 1, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 2, \"relation\" : \"eq\" }, \"max_score\" : null, \"hits\" : [ { \"_index\" : \"students\", \"_id\" : \"2\", \"_score\" : null, \"_source\" : { \"name\" : \"Mary Major\" }, \"sort\" : [ 9223372036854775807 ] }, { \"_index\" : \"students\", \"_id\" : \"1\", \"_score\" : null, \"_source\" : { \"name\" : \"John Doe\", \"average\" : 80 }, \"sort\" : [ 80 ] } ] } } . ",
    "url": "https://vagimeli.github.io/search-plugins/searching-data/sort/#handling-missing-values",
    "relUrl": "/search-plugins/searching-data/sort/#handling-missing-values"
  },"916": {
    "doc": "Sort results",
    "title": "Ignoring unmapped fields",
    "content": "If a field is not mapped, a search request that sorts by this field fails by default. To avoid this, you can use the unmapped_type parameter, which signals to OpenSearch to ignore the field. For example, if you set unmapped_type to long, the field is treated as if it were mapped as type long. Additionally, all documents in the index that have an unmapped_type field are treated as if they had no value in this field, so they are not sorted by it. For example, consider two indexes. Index a document that contains an average field in the first index: . PUT students/_doc/1 { \"name\": \"John Doe\", \"average\": 80 } . Index a document that does not contain an average field in the second index: . PUT students_no_map/_doc/2 { \"name\": \"Mary Major\" } . Search for all documents in both indexes and sort them by the average field: . GET students*/_search { \"query\": { \"match_all\": {} }, \"sort\": [ { \"average\": { \"order\": \"desc\" } } ] } . By default, the second index produces an error because the average field is not mapped: . { \"took\" : 3, \"timed_out\" : false, \"_shards\" : { \"total\" : 2, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 1, \"failures\" : [ { \"shard\" : 0, \"index\" : \"students_no_map\", \"node\" : \"cam9NWqVSV-jUIkQ3tRubw\", \"reason\" : { \"type\" : \"query_shard_exception\", \"reason\" : \"No mapping found for [average] in order to sort on\", \"index\" : \"students_no_map\", \"index_uuid\" : \"JgfRkypKSUSpyU-ZXr9kKA\" } } ] }, \"hits\" : { \"total\" : { \"value\" : 1, \"relation\" : \"eq\" }, \"max_score\" : null, \"hits\" : [ { \"_index\" : \"students\", \"_id\" : \"1\", \"_score\" : null, \"_source\" : { \"name\" : \"John Doe\", \"average\" : 80 }, \"sort\" : [ 80 ] } ] } } . You can specify the unmapped_type parameter so that the unmapped field is ignored: . GET students*/_search { \"query\": { \"match_all\": {} }, \"sort\": [ { \"average\": { \"order\": \"desc\", \"unmapped_type\": \"long\" } } ] } . The response contains both documents: . { \"took\" : 4, \"timed_out\" : false, \"_shards\" : { \"total\" : 2, \"successful\" : 2, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 2, \"relation\" : \"eq\" }, \"max_score\" : null, \"hits\" : [ { \"_index\" : \"students\", \"_id\" : \"1\", \"_score\" : null, \"_source\" : { \"name\" : \"John Doe\", \"average\" : 80 }, \"sort\" : [ 80 ] }, { \"_index\" : \"students_no_map\", \"_id\" : \"2\", \"_score\" : null, \"_source\" : { \"name\" : \"Mary Major\" }, \"sort\" : [ -9223372036854775808 ] } ] } } . ",
    "url": "https://vagimeli.github.io/search-plugins/searching-data/sort/#ignoring-unmapped-fields",
    "relUrl": "/search-plugins/searching-data/sort/#ignoring-unmapped-fields"
  },"917": {
    "doc": "Sort results",
    "title": "Tracking scores",
    "content": "By default, scores are not computed when sorting on a field. You can set track_scores to true to compute and track scores: . GET students/_search { \"query\": { \"match_all\": {} }, \"sort\": [ { \"average\": { \"order\": \"desc\" } } ], \"track_scores\": true } . ",
    "url": "https://vagimeli.github.io/search-plugins/searching-data/sort/#tracking-scores",
    "relUrl": "/search-plugins/searching-data/sort/#tracking-scores"
  },"918": {
    "doc": "Sort results",
    "title": "Sorting by geo distance",
    "content": "You can sort documents by _geo_distance. The following parameters are supported. | Parameter | Description | . | distance_type | Specifies the method of computing the distance. Valid values are arc and plane. The plane method is faster but less accurate for long distances or close to the poles. Default is arc. | . | mode | Specifies how to handle a field with several geopoints. By default, documents are sorted by the shortest distance when the sort order is ascending and by the longest distance when the sort order is descending. Valid values are min, max, median, and avg. | . | unit | Specifies the units used to compute sort values. Default is meters (m). | . | ignore_unmapped | Specifies how to treat an unmapped field. Set ignore_unmapped to true to ignore unmapped fields. Default is false (produce an error when encountering an unmapped field). | . The _geo_distance parameter does not support missing_values. The distance is always considered to be infinity when a document does not contain the field used for computing distance. For example, index two documents with geopoints: . PUT testindex1/_doc/1 { \"point\": [74.00, 40.71] } PUT testindex1/_doc/2 { \"point\": [73.77, -69.63] } . Search for all documents and sort them by the distance from the provided point: . GET testindex1/_search { \"sort\": [ { \"_geo_distance\": { \"point\": [59, -54], \"order\": \"asc\", \"unit\": \"km\", \"distance_type\": \"arc\", \"mode\": \"min\", \"ignore_unmapped\": true } } ], \"query\": { \"match_all\": {} } } . The response contains the sorted documents: . { \"took\" : 864, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 2, \"relation\" : \"eq\" }, \"max_score\" : null, \"hits\" : [ { \"_index\" : \"testindex1\", \"_id\" : \"2\", \"_score\" : null, \"_source\" : { \"point\" : [ 73.77, -69.63 ] }, \"sort\" : [ 1891.2667493895767 ] }, { \"_index\" : \"testindex1\", \"_id\" : \"1\", \"_score\" : null, \"_source\" : { \"point\" : [ 74.0, 40.71 ] }, \"sort\" : [ 10628.402240213345 ] } ] } } . You can provide coordinates in any format supported by the geopoint field type. For a description of all formats, see the geopoint field type documentation. To pass multiple geopoints to _geo_distance, use an array: . GET testindex1/_search { \"sort\": [ { \"_geo_distance\": { \"point\": [[59, -54], [60, -53]], \"order\": \"asc\", \"unit\": \"km\", \"distance_type\": \"arc\", \"mode\": \"min\", \"ignore_unmapped\": true } } ], \"query\": { \"match_all\": {} } } . For each document, the sorting distance is calculated as the minimum, maximum, or average (as specified by the mode) of the distances from all points provided in the search to all points in the document. ",
    "url": "https://vagimeli.github.io/search-plugins/searching-data/sort/#sorting-by-geo-distance",
    "relUrl": "/search-plugins/searching-data/sort/#sorting-by-geo-distance"
  },"919": {
    "doc": "Sort results",
    "title": "Performance considerations",
    "content": "Sorted field values are loaded into memory for sorting. Therefore, for minimum overhead we recommend mapping numeric types to the smallest acceptable types, like short, integer, and float. String types should not have the sorted field analyzed or tokenized. ",
    "url": "https://vagimeli.github.io/search-plugins/searching-data/sort/#performance-considerations",
    "relUrl": "/search-plugins/searching-data/sort/#performance-considerations"
  },"920": {
    "doc": "SQL and PPL CLI",
    "title": "SQL and PPL CLI",
    "content": "The SQL and PPL command line interface (CLI) is a standalone Python application that you can launch with the opensearchsql command. To use the SQL and PPL CLI, install the SQL plugin on your OpenSearch instance, run the CLI using MacOS or Linux, and connect to any valid OpenSearch endpoint. ",
    "url": "https://vagimeli.github.io/search-plugins/sql/cli/",
    "relUrl": "/search-plugins/sql/cli/"
  },"921": {
    "doc": "SQL and PPL CLI",
    "title": "Features",
    "content": "The SQL and PPL CLI has the following features: . | Multi-line input | PPL support | Autocomplete for SQL syntax and index names | Syntax highlighting | Formatted output: . | Tabular format | Field names with color | Enabled horizontal display (by default) and vertical display when output is too wide for your terminal, for better visualization | Pagination for large output | . | Works with or without security enabled | Supports loading configuration files | Supports all SQL plugin queries | . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/cli/#features",
    "relUrl": "/search-plugins/sql/cli/#features"
  },"922": {
    "doc": "SQL and PPL CLI",
    "title": "Install",
    "content": "Launch your local OpenSearch instance and make sure you have the SQL plugin installed. | Install the CLI: pip3 install opensearchsql . | . The SQL CLI only works with Python 3. | To launch the CLI, run: opensearchsql https://localhost:9200 --username admin --password admin . By default, the opensearchsql command connects to http://localhost:9200. | . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/cli/#install",
    "relUrl": "/search-plugins/sql/cli/#install"
  },"923": {
    "doc": "SQL and PPL CLI",
    "title": "Configure",
    "content": "When you first launch the SQL CLI, a configuration file is automatically created at ~/.config/opensearchsql-cli/config (for MacOS and Linux), the configuration is auto-loaded thereafter. You can configure the following connection properties: . | endpoint: You do not need to specify an option. Anything that follows the launch command opensearchsql is considered as the endpoint. If you do not provide an endpoint, by default, the SQL CLI connects to http://localhost:9200. | -u/-w: Supports username and password for HTTP basic authentication, such as with the Security plugin or fine-grained access control for Amazon OpenSearch Service. | --aws-auth: Turns on AWS sigV4 authentication to connect to an Amazon OpenSearch endpoint. Use with the AWS CLI (aws configure) to retrieve the local AWS configuration to authenticate and connect. | . For a list of all available configurations, see clirc. ",
    "url": "https://vagimeli.github.io/search-plugins/sql/cli/#configure",
    "relUrl": "/search-plugins/sql/cli/#configure"
  },"924": {
    "doc": "SQL and PPL CLI",
    "title": "Using the CLI",
    "content": ". | Run the CLI tool. If your cluster runs with the default security settings, use the following command: opensearchsql --username admin --password admin https://localhost:9200 . If your cluster runs without security, run: . opensearchsql . | Run a sample SQL command: SELECT * FROM accounts; . | . By default, you see a maximum output of 200 rows. To show more results, add a LIMIT clause with the desired value. To exit the CLI tool, select Ctrl+D. ",
    "url": "https://vagimeli.github.io/search-plugins/sql/cli/#using-the-cli",
    "relUrl": "/search-plugins/sql/cli/#using-the-cli"
  },"925": {
    "doc": "SQL and PPL CLI",
    "title": "Using the CLI with PPL",
    "content": ". | Run the CLI by specifying the query language: opensearchsql -l ppl &lt;params&gt; . | Execute a PPL query: source=accounts | fields firstname, lastname . | . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/cli/#using-the-cli-with-ppl",
    "relUrl": "/search-plugins/sql/cli/#using-the-cli-with-ppl"
  },"926": {
    "doc": "SQL and PPL CLI",
    "title": "Query options",
    "content": "Run a single query with the following command line options: . | -q: Follow by a single query | -f: Specify JDBC or raw format output | -v: Display data vertically | -e: Translate SQL to DSL | . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/cli/#query-options",
    "relUrl": "/search-plugins/sql/cli/#query-options"
  },"927": {
    "doc": "SQL and PPL CLI",
    "title": "CLI options",
    "content": ". | --help: Help page for options | -l: Query language option. Available options are sql and ppl. Default is sql | -p: Always use pager to display output | --clirc: Provide path for the configuration file | . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/cli/#cli-options",
    "relUrl": "/search-plugins/sql/cli/#cli-options"
  },"928": {
    "doc": "Data Types",
    "title": "Data types",
    "content": "The following table shows the data types supported by the SQL plugin and how each one maps to SQL and OpenSearch data types: . | OpenSearch SQL Type | OpenSearch Type | SQL Type | . | boolean | boolean | BOOLEAN | . | byte | byte | TINYINT | . | short | byte | SMALLINT | . | integer | integer | INTEGER | . | long | long | BIGINT | . | float | float | REAL | . | half_float | float | FLOAT | . | scaled_float | float | DOUBLE | . | double | double | DOUBLE | . | keyword | string | VARCHAR | . | text | text | VARCHAR | . | date | timestamp | TIMESTAMP | . | date_nanos | timestamp | TIMESTAMP | . | ip | ip | VARCHAR | . | date | timestamp | TIMESTAMP | . | binary | binary | VARBINARY | . | object | struct | STRUCT | . | nested | array | STRUCT | . In addition to this list, the SQL plugin also supports the datetime type, though it doesn’t have a corresponding mapping with OpenSearch or SQL. To use a function without a corresponding mapping, you must explicitly convert the data type to one that does. ",
    "url": "https://vagimeli.github.io/search-plugins/sql/datatypes/#data-types",
    "relUrl": "/search-plugins/sql/datatypes/#data-types"
  },"929": {
    "doc": "Data Types",
    "title": "Date and time types",
    "content": "The date and time types represent a time period: DATE, TIME, DATETIME, TIMESTAMP, and INTERVAL. By default, the OpenSearch DSL uses the date type as the only date-time related type that contains all information of an absolute time point. To integrate with SQL, each type other than the timestamp type holds part of the time period information. To use date-time functions, see datetime. Some functions might have restrictions for the input argument type. Date . The date type represents the calendar date regardless of the time zone. A given date value is a 24-hour period, but this period varies in different timezones and might have flexible hours during daylight saving programs. The date type doesn’t contain time information and it only supports a range of 1000-01-01 to 9999-12-31. | Type | Syntax | Range | . | date | yyyy-MM-dd | 0001-01-01 to 9999-12-31 | . Time . The time type represents the time of a clock regardless of its timezone. The time type doesn’t contain date information. | Type | Syntax | Range | . | time | hh:mm:ss[.fraction] | 00:00:00.0000000000 to 23:59:59.9999999999 | . Datetime . The datetime type is a combination of date and time. It doesn’t contain timezone information. For an absolute time point that contains date, time, and timezone information, see Timestamp. | Type | Syntax | Range | . | datetime | yyyy-MM-dd hh:mm:ss[.fraction] | 0001-01-01 00:00:00.0000000000 to 9999-12-31 23:59:59.9999999999 | . Timestamp . The timestamp type is an absolute instance independent of timezone or convention. For example, for a given point of time, if you change the timestamp to a different timezone, its value changes accordingly. The timestamp type is stored differently from the other types. It’s converted from its current timezone to UTC for storage and converted back to its set timezone from UTC when it’s retrieved. | Type | Syntax | Range | . | timestamp | yyyy-MM-dd hh:mm:ss[.fraction] | 0001-01-01 00:00:01.9999999999 UTC to 9999-12-31 23:59:59.9999999999 | . Interval . The interval type represents a temporal duration or a period. | Type | Syntax | . | interval | INTERVAL expr unit | . The expr unit is any expression that eventually iterates to a quantity value. It represents a unit for interpreting the quantity, including MICROSECOND, SECOND, MINUTE, HOUR, DAY, WEEK, MONTH, QUARTER, and YEAR. The INTERVAL keyword and the unit specifier are not case sensitive. The interval type has two classes of intervals: year-week intervals and day-time intervals. | Year-week intervals store years, quarters, months, and weeks. | Day-time intervals store days, hours, minutes, seconds, and microseconds. | . Convert between date and time types . Apart from the interval type, all date and time types can be converted to each other. The conversion might alter the value or cause some information loss. For example, when extracting the time value from a datetime value, or converting a date value to a datetime value, and so on. The SQL plugin supports the following conversion rules for each of the types: . Convert from date . | Because the date value doesn’t have any time information, conversion to the time type isn’t useful and always returns a zero time value of 00:00:00. | Converting from date to datetime has a data fill-up due to the lack of time information. It attaches the time 00:00:00 to the original date by default and forms a datetime instance. For example, conversion of 2020-08-17 to a datetime type is 2020-08-17 00:00:00. | Converting to timestamp type alternates both the time value and the timezone information. It attaches the zero time value 00:00:00 and the session timezone (UTC by default) to the date. For example, conversion of 2020-08-17 to a datetime type with a session timezone UTC is 2020-08-17 00:00:00 UTC. | . Convert from time . | You cannot convert the time type to any other date and time types because it doesn’t contain any date information. | . Convert from datetime . | Converting datetime to date extracts the date value from the datetime value. For example, conversion of 2020-08-17 14:09:00 to a date type is 2020-08-08. | Converting datetime to time extracts the time value from the datetime value. For example, conversion of 2020-08-17 14:09:00 to a time type is 14:09:00. | Because the datetime type doesn’t contain timezone information, converting to timestamp type fills up the timezone value with the session timezone. For example, conversion of 2020-08-17 14:09:00 (UTC) to a timestamp type is 2020-08-17 14:09:00 UTC. | . Convert from timestamp . | Converting from a timestamp type to a date type extracts the date value and converting to a time type extracts the time value. Converting from a timestamp type to datetime type extracts only the datetime value and leaves out the timezone value. For example, conversion of 2020-08-17 14:09:00 UTC to a date type is 2020-08-17, to a time type is 14:09:00, and to a datetime type is 2020-08-17 14:09:00. | . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/datatypes/#date-and-time-types",
    "relUrl": "/search-plugins/sql/datatypes/#date-and-time-types"
  },"930": {
    "doc": "Data Types",
    "title": "Data Types",
    "content": " ",
    "url": "https://vagimeli.github.io/search-plugins/sql/datatypes/",
    "relUrl": "/search-plugins/sql/datatypes/"
  },"931": {
    "doc": "Full-Text Search",
    "title": "Full-text search",
    "content": "Use SQL commands for full-text search. The SQL plugin supports a subset of full-text queries available in OpenSearch. To learn about full-text queries in OpenSearch, see Full-text queries. ",
    "url": "https://vagimeli.github.io/search-plugins/sql/full-text/#full-text-search",
    "relUrl": "/search-plugins/sql/full-text/#full-text-search"
  },"932": {
    "doc": "Full-Text Search",
    "title": "Match",
    "content": "Use the MATCH function to search documents that match a string, number, date, or boolean value for a given field. Syntax . match(field_expression, query_expression[, option=&lt;option_value&gt;]*) . You can specify the following options in any order: . | analyzer | auto_generate_synonyms_phrase | fuzziness | max_expansions | prefix_length | fuzzy_transpositions | fuzzy_rewrite | lenient | operator | minimum_should_match | zero_terms_query | boost | . Refer to the match query documentation for parameter descriptions and supported values. Example 1: Search the message field for the text “this is a test”: . GET my_index/_search { \"query\": { \"match\": { \"message\": \"this is a test\" } } } . SQL query: . SELECT message FROM my_index WHERE match(message, \"this is a test\") . PPL query: . SOURCE=my_index | WHERE match(message, \"this is a test\") | FIELDS message . Example 2: Search the message field with the operator parameter: . GET my_index/_search { \"query\": { \"match\": { \"message\": { \"query\": \"this is a test\", \"operator\": \"and\" } } } } . SQL query: . SELECT message FROM my_index WHERE match(message, \"this is a test\", operator='and') . PPL query: . SOURCE=my_index | WHERE match(message, \"this is a test\", operator='and') | FIELDS message . Example 3: Search the message field with the operator and zero_terms_query parameters: . GET my_index/_search { \"query\": { \"match\": { \"message\": { \"query\": \"to be or not to be\", \"operator\": \"and\", \"zero_terms_query\": \"all\" } } } } . SQL query: . SELECT message FROM my_index WHERE match(message, \"this is a test\", operator='and', zero_terms_query='all') . PPL query: . SOURCE=my_index | WHERE match(message, \"this is a test\", operator='and', zero_terms_query='all') | FIELDS message . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/full-text/#match",
    "relUrl": "/search-plugins/sql/full-text/#match"
  },"933": {
    "doc": "Full-Text Search",
    "title": "Multi-match",
    "content": "To search for text in multiple fields, use MULTI_MATCH function. This function maps to the multi_match query used in search engine, to returns the documents that match a provided text, number, date or boolean value with a given field or fields. Syntax . The MULTI_MATCH function lets you boost certain fields using ^ character. Boosts are multipliers that weigh matches in one field more heavily than matches in other fields. The syntax allows to specify the fields in double quotes, single quotes, surrounded by backticks, or unquoted. Use star \"*\" to search all fields. Star symbol should be quoted. multi_match([field_expression+], query_expression[, option=&lt;option_value&gt;]*) . The weight is optional and is specified after the field name. It could be delimited by the caret character – ^ or by whitespace. Please, refer to examples below: . multi_match([\"Tags\" ^ 2, 'Title' 3.4, `Body`, Comments ^ 0.3], ...) multi_match([\"*\"], ...) . You can specify the following options for MULTI_MATCH in any order: . | analyzer | auto_generate_synonyms_phrase | cutoff_frequency | fuzziness | fuzzy_transpositions | lenient | max_expansions | minimum_should_match | operator | prefix_length | tie_breaker | type | slop | zero_terms_query | boost | . Please, refer to multi_match query documentation for parameter description and supported values. For example, REST API search for Dale in either the firstname or lastname fields: . GET accounts/_search { \"query\": { \"multi_match\": { \"query\": \"Lane Street\", \"fields\": [ \"address\" ], } } } . could be called from SQL using multi_match function . SELECT firstname, lastname FROM accounts WHERE multi_match(['*name'], 'Dale') . or multi_match PPL function . SOURCE=accounts | WHERE multi_match(['*name'], 'Dale') | fields firstname, lastname . | firstname | lastname | . | Dale | Adams | . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/full-text/#multi-match",
    "relUrl": "/search-plugins/sql/full-text/#multi-match"
  },"934": {
    "doc": "Full-Text Search",
    "title": "Query string",
    "content": "To split text based on operators, use the QUERY_STRING function. The QUERY_STRING function supports logical connectives, wildcard, regex, and proximity search. This function maps to the to the query_string query used in search engine, to return the documents that match a provided text, number, date or boolean value with a given field or fields. Syntax . The QUERY_STRING function has syntax similar to MATCH_QUERY and lets you boost certain fields using ^ character. Boosts are multipliers that weigh matches in one field more heavily than matches in other fields. The syntax allows to specify the fields in double quotes, single quotes, surrounded by backticks, or unquoted. Use star \"*\" to search all fields. Star symbol should be quoted. query_string([field_expression+], query_expression[, option=&lt;option_value&gt;]*) . The weight is optional and is specified after the field name. It could be delimited by the caret character – ^ or by whitespace. Please, refer to examples below: . query_string([\"Tags\" ^ 2, 'Title' 3.4, `Body`, Comments ^ 0.3], ...) query_string([\"*\"], ...) . You can specify the following options for QUERY_STRING in any order: . | analyzer | allow_leading_wildcard | analyze_wildcard | auto_generate_synonyms_phrase_query | boost | default_operator | enable_position_increments | fuzziness | fuzzy_rewrite | escape | fuzzy_max_expansions | fuzzy_prefix_length | fuzzy_transpositions | lenient | max_determinized_states | minimum_should_match | quote_analyzer | phrase_slop | quote_field_suffix | rewrite | type | tie_breaker | time_zone | . Refer to the query_string query documentation for parameter descriptions and supported values. Example of using query_string in SQL and PPL queries: . The REST API search request . GET accounts/_search { \"query\": { \"query_string\": { \"query\": \"Lane Street\", \"fields\": [ \"address\" ], } } } . could be called from SQL . SELECT account_number, address FROM accounts WHERE query_string(['address'], 'Lane Street', default_operator='OR') . or from PPL . SOURCE=accounts | WHERE query_string(['address'], 'Lane Street', default_operator='OR') | fields account_number, address . | account_number | address | . | 1 | 880 Holmes Lane | . | 6 | 671 Bristol Street | . | 13 | 789 Madison Street | . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/full-text/#query-string",
    "relUrl": "/search-plugins/sql/full-text/#query-string"
  },"935": {
    "doc": "Full-Text Search",
    "title": "Match phrase",
    "content": "To search for exact phrases, use MATCHPHRASE or MATCH_PHRASE functions. Syntax . matchphrasequery(field_expression, query_expression) matchphrase(field_expression, query_expression[, option=&lt;option_value&gt;]*) match_phrase(field_expression, query_expression[, option=&lt;option_value&gt;]*) . The MATCHPHRASE/MATCH_PHRASE functions let you specify the following options in any order: . | analyzer | slop | zero_terms_query | boost | . Refer to the match_phrase query documentation for parameter descriptions and supported values. Example of using match_phrase in SQL and PPL queries: . The REST API search request . GET accounts/_search { \"query\": { \"match_phrase\": { \"address\": { \"query\": \"880 Holmes Lane\" } } } } . could be called from SQL . SELECT account_number, address FROM accounts WHERE match_phrase(address, '880 Holmes Lane') . or PPL . SOURCE=accounts | WHERE match_phrase(address, '880 Holmes Lane') | FIELDS account_number, address . | account_number | address | . | 1 | 880 Holmes Lane | . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/full-text/#match-phrase",
    "relUrl": "/search-plugins/sql/full-text/#match-phrase"
  },"936": {
    "doc": "Full-Text Search",
    "title": "Simple query string",
    "content": "The simple_query_string function maps to the simple_query_string query in OpenSearch. It returns the documents that match a provided text, number, date or boolean value with a given field or fields. The ^ lets you boost certain fields. Boosts are multipliers that weigh matches in one field more heavily than matches in other fields. Syntax . The syntax allows to specify the fields in double quotes, single quotes, surrounded by backticks, or unquoted. Use star \"*\" to search all fields. Star symbol should be quoted. simple_query_string([field_expression+], query_expression[, option=&lt;option_value&gt;]*) . The weight is optional and is specified after the field name. It could be delimited by the caret character – ^ or by whitespace. Please, refer to examples below: . simple_query_string([\"Tags\" ^ 2, 'Title' 3.4, `Body`, Comments ^ 0.3], ...) simple_query_string([\"*\"], ...) . You can specify the following options for SIMPLE_QUERY_STRING in any order: . | analyze_wildcard | analyzer | auto_generate_synonyms_phrase_query | boost | default_operator | flags | fuzzy_max_expansions | fuzzy_prefix_length | fuzzy_transpositions | lenient | minimum_should_match | quote_field_suffix | . Refer to the simple_query_string query documentation for parameter descriptions and supported values. Example of using simple_query_string in SQL and PPL queries: . The REST API search request . GET accounts/_search { \"query\": { \"simple_query_string\": { \"query\": \"Lane Street\", \"fields\": [ \"address\" ], } } } . could be called from SQL . SELECT account_number, address FROM accounts WHERE simple_query_string(['address'], 'Lane Street', default_operator='OR') . or from PPL . SOURCE=accounts | WHERE simple_query_string(['address'], 'Lane Street', default_operator='OR') | fields account_number, address . | account_number | address | . | 1 | 880 Holmes Lane | . | 6 | 671 Bristol Street | . | 13 | 789 Madison Street | . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/full-text/#simple-query-string",
    "relUrl": "/search-plugins/sql/full-text/#simple-query-string"
  },"937": {
    "doc": "Full-Text Search",
    "title": "Match phrase prefix",
    "content": "To search for phrases by given prefix, use MATCH_PHRASE_PREFIX function to make a prefix query out of the last term in the query string. Syntax . match_phrase_prefix(field_expression, query_expression[, option=&lt;option_value&gt;]*) . The MATCH_PHRASE_PREFIX function lets you specify the following options in any order: . | analyzer | slop | max_expansions | zero_terms_query | boost | . Refer to the match_phrase_prefix query documentation for parameter descriptions and supported values. Example of using match_phrase_prefix in SQL and PPL queries: . The REST API search request . GET accounts/_search { \"query\": { \"match_phrase_prefix\": { \"author\": { \"query\": \"Alexander Mil\" } } } } . could be called from SQL . SELECT author, title FROM books WHERE match_phrase_prefix(author, 'Alexander Mil') . or PPL . source=books | where match_phrase_prefix(author, 'Alexander Mil') | fields author, title . | author | title | . | Alan Alexander Milne | The House at Pooh Corner | . | Alan Alexander Milne | Winnie-the-Pooh | . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/full-text/#match-phrase-prefix",
    "relUrl": "/search-plugins/sql/full-text/#match-phrase-prefix"
  },"938": {
    "doc": "Full-Text Search",
    "title": "Match boolean prefix",
    "content": "Use the match_bool_prefix function to search documents that match text only for a given field prefix. Syntax . match_bool_prefix(field_expression, query_expression[, option=&lt;option_value&gt;]*) . The MATCH_BOOL_PREFIX function lets you specify the following options in any order: . | minimum_should_match | fuzziness | prefix_length | max_expansions | fuzzy_transpositions | fuzzy_rewrite | boost | analyzer | operator | . Refer to the match_bool_prefix query documentation for parameter descriptions and supported values. Example of using match_bool_prefix in SQL and PPL queries: . The REST API search request . GET accounts/_search { \"query\": { \"match_bool_prefix\": { \"address\": { \"query\": \"Bristol Stre\" } } } } . could be called from SQL . SELECT firstname, address FROM accounts WHERE match_bool_prefix(address, 'Bristol Stre') . or PPL . source=accounts | where match_bool_prefix(address, 'Bristol Stre') | fields firstname, address . | firstname | address | . | Hattie | 671 Bristol Street | . | Nanette | 789 Madison Street | . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/full-text/#match-boolean-prefix",
    "relUrl": "/search-plugins/sql/full-text/#match-boolean-prefix"
  },"939": {
    "doc": "Full-Text Search",
    "title": "Full-Text Search",
    "content": " ",
    "url": "https://vagimeli.github.io/search-plugins/sql/full-text/",
    "relUrl": "/search-plugins/sql/full-text/"
  },"940": {
    "doc": "Functions",
    "title": "Functions",
    "content": "You must enable fielddata in the document mapping for most string functions to work properly. The specification shows the return type of the function with a generic type T as the argument. For example, abs(number T) -&gt; T means that the function abs accepts a numerical argument of type T, which could be any subtype of the number type, and it returns the actual type of T as the return type. The SQL plugin supports the following common functions shared across the SQL and PPL languages. ",
    "url": "https://vagimeli.github.io/search-plugins/sql/functions/",
    "relUrl": "/search-plugins/sql/functions/"
  },"941": {
    "doc": "Functions",
    "title": "Mathematical",
    "content": "| Function | Specification | Example | . | abs | abs(number T) -&gt; T | SELECT abs(0.5) FROM my-index LIMIT 1 | . | add | add(number T, number) -&gt; T | SELECT add(1, 5) FROM my-index LIMIT 1 | . | cbrt | cbrt(number T) -&gt; T | SELECT cbrt(0.5) FROM my-index LIMIT 1 | . | ceil | ceil(number T) -&gt; T | SELECT ceil(0.5) FROM my-index LIMIT 1 | . | conv | conv(string T, int a, int b) -&gt; T | SELECT CONV('12', 10, 16), CONV('2C', 16, 10), CONV(12, 10, 2), CONV(1111, 2, 10) FROM my-index LIMIT 1 | . | crc32 | crc32(string T) -&gt; T | SELECT crc32('MySQL') FROM my-index LIMIT 1 | . | divide | divide(number T, number) -&gt; T | SELECT divide(1, 0.5) FROM my-index LIMIT 1 | . | e | e() -&gt; double | SELECT e() FROM my-index LIMIT 1 | . | exp | exp(number T) -&gt; T | SELECT exp(0.5) FROM my-index LIMIT 1 | . | expm1 | expm1(number T) -&gt; T | SELECT expm1(0.5) FROM my-index LIMIT 1 | . | floor | floor(number T) -&gt; T | SELECT floor(0.5) AS Rounded_Down FROM my-index LIMIT 1 | . | ln | ln(number T) -&gt; double | SELECT ln(10) FROM my-index LIMIT 1 | . | log | log(number T) -&gt; double or log(number T, number) -&gt; double | SELECT log(10) FROM my-index LIMIT 1 | . | log2 | log2(number T) -&gt; double | SELECT log2(10) FROM my-index LIMIT 1 | . | log10 | log10(number T) -&gt; double | SELECT log10(10) FROM my-index LIMIT 1 | . | mod | mod(number T, number) -&gt; T | SELECT modulus(2, 3) FROM my-index LIMIT 1 | . | multiply | multiply(number T, number) -&gt; number | SELECT multiply(2, 3) FROM my-index LIMIT 1 | . | pi | pi() -&gt; double | SELECT pi() FROM my-index LIMIT 1 | . | pow | pow(number T) -&gt; T or pow(number T, number) -&gt; T | SELECT pow(2, 3) FROM my-index LIMIT 1 | . | power | power(number T) -&gt; T or power(number T, number) -&gt; T | SELECT power(2, 3) FROM my-index LIMIT 1 | . | rand | rand() -&gt; number or rand(number T) -&gt; T | SELECT rand(0.5) FROM my-index LIMIT 1 | . | rint | rint(number T) -&gt; T | SELECT rint(1.5) FROM my-index LIMIT 1 | . | round | round(number T) -&gt; T | SELECT round(1.5) FROM my-index LIMIT 1 | . | sign | sign(number T) -&gt; T | SELECT sign(1.5) FROM my-index LIMIT 1 | . | signum | signum(number T) -&gt; T | SELECT signum(0.5) FROM my-index LIMIT 1 | . | sqrt | sqrt(number T) -&gt; T | SELECT sqrt(0.5) FROM my-index LIMIT 1 | . | strcmp | strcmp(string T, string T) -&gt; T | SELECT strcmp('hello', 'hello') FROM my-index LIMIT 1 | . | subtract | subtract(number T, number) -&gt; T | SELECT subtract(3, 2) FROM my-index LIMIT 1 | . | truncate | truncate(number T, number T) -&gt; T | SELECT truncate(56.78, 1) FROM my-index LIMIT 1 | . | / | number [op] number -&gt; number | SELECT 1 / 100 FROM my-index LIMIT 1 | . | % | number [op] number -&gt; number | SELECT 1 % 100 FROM my-index LIMIT 1 | . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/functions/#mathematical",
    "relUrl": "/search-plugins/sql/functions/#mathematical"
  },"942": {
    "doc": "Functions",
    "title": "Trigonometric",
    "content": "| Function | Specification | Example | . | acos | acos(number T) -&gt; double | SELECT acos(0.5) FROM my-index LIMIT 1 | . | asin | asin(number T) -&gt; double | SELECT asin(0.5) FROM my-index LIMIT 1 | . | atan | atan(number T) -&gt; double | SELECT atan(0.5) FROM my-index LIMIT 1 | . | atan2 | atan2(number T, number) -&gt; double | SELECT atan2(1, 0.5) FROM my-index LIMIT 1 | . | cos | cos(number T) -&gt; double | SELECT cos(0.5) FROM my-index LIMIT 1 | . | cosh | cosh(number T) -&gt; double | SELECT cosh(0.5) FROM my-index LIMIT 1 | . | cot | cot(number T) -&gt; double | SELECT cot(0.5) FROM my-index LIMIT 1 | . | degrees | degrees(number T) -&gt; double | SELECT degrees(0.5) FROM my-index LIMIT 1 | . | radians | radians(number T) -&gt; double | SELECT radians(0.5) FROM my-index LIMIT 1 | . | sin | sin(number T) -&gt; double | SELECT sin(0.5) FROM my-index LIMIT 1 | . | sinh | sinh(number T) -&gt; double | SELECT sinh(0.5) FROM my-index LIMIT 1 | . | tan | tan(number T) -&gt; double | SELECT tan(0.5) FROM my-index LIMIT 1 | . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/functions/#trigonometric",
    "relUrl": "/search-plugins/sql/functions/#trigonometric"
  },"943": {
    "doc": "Functions",
    "title": "Date and time",
    "content": "| Function | Specification | Example | . | adddate | adddate(date, INTERVAL expr unit) -&gt; date | SELECT adddate(date('2020-08-26'), INTERVAL 1 hour) FROM my-index LIMIT 1 | . | curdate | curdate() -&gt; date | SELECT curdate() FROM my-index LIMIT 1 | . | date | date(date) -&gt; date | SELECT date() FROM my-index LIMIT 1 | . | date_format | date_format(date, string) -&gt; string or date_format(date, string, string) -&gt; string | SELECT date_format(date, 'Y') FROM my-index LIMIT 1 | . | date_sub | date_sub(date, INTERVAL expr unit) -&gt; date | SELECT date_sub(date('2008-01-02'), INTERVAL 31 day) FROM my-index LIMIT 1 | . | dayofmonth | dayofmonth(date) -&gt; integer | SELECT dayofmonth(date) FROM my-index LIMIT 1 | . | dayname | dayname(date) -&gt; string | SELECT dayname(date('2020-08-26')) FROM my-index LIMIT 1 | . | dayofyear | dayofyear(date) -&gt; integer | SELECT dayofyear(date('2020-08-26')) FROM my-index LIMIT 1 | . | dayofweek | dayofweek(date) -&gt; integer | SELECT dayofweek(date('2020-08-26')) FROM my-index LIMIT 1 | . | from_days | from_days(N) -&gt; integer | SELECT from_days(733687) FROM my-index LIMIT 1 | . | hour | hour(time) -&gt; integer | SELECT hour((time '01:02:03')) FROM my-index LIMIT 1 | . | maketime | maketime(integer, integer, integer) -&gt; date | SELECT maketime(1, 2, 3) FROM my-index LIMIT 1 | . | microsecond | microsecond(expr) -&gt; integer | SELECT microsecond((time '01:02:03.123456')) FROM my-index LIMIT 1 | . | minute | minute(expr) -&gt; integer | SELECT minute((time '01:02:03')) FROM my-index LIMIT 1 | . | month | month(date) -&gt; integer | SELECT month(date) FROM my-index | . | monthname | monthname(date) -&gt; string | SELECT monthname(date) FROM my-index | . | now | now() -&gt; date | SELECT now() FROM my-index LIMIT 1 | . | quarter | quarter(date) -&gt; integer | SELECT quarter(date('2020-08-26')) FROM my-index LIMIT 1 | . | second | second(time) -&gt; integer | SELECT second((time '01:02:03')) FROM my-index LIMIT 1 | . | subdate | subdate(date, INTERVAL expr unit) -&gt; date, datetime | SELECT subdate(date('2008-01-02'), INTERVAL 31 day) FROM my-index LIMIT 1 | . | time | time(expr) -&gt; time | SELECT time('13:49:00') FROM my-index LIMIT 1 | . | time_to_sec | time_to_sec(time) -&gt; long | SELECT time_to_sec(time '22:23:00') FROM my-index LIMIT 1 | . | timestamp | timestamp(date) -&gt; date | SELECT timestamp(date) FROM my-index LIMIT 1 | . | to_days | to_days(date) -&gt; long | SELECT to_days(date '2008-10-07') FROM my-index LIMIT 1 | . | week | week(date[mode]) -&gt; integer | SELECT week(date('2008-02-20')) FROM my-index LIMIT 1 | . | year | year(date) -&gt; integer | SELECT year(date) FROM my-index LIMIT 1 | . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/functions/#date-and-time",
    "relUrl": "/search-plugins/sql/functions/#date-and-time"
  },"944": {
    "doc": "Functions",
    "title": "String",
    "content": "| Function | Specification | Example | . | ascii | ascii(string T) -&gt; integer | SELECT ascii(name.keyword) FROM my-index LIMIT 1 | . | concat | concat(str1, str2) -&gt; string | SELECT concat('hello', 'world') FROM my-index LIMIT 1 | . | concat_ws | concat_ws(separator, string, string…) -&gt; string | SELECT concat_ws(\"-\", \"Tutorial\", \"is\", \"fun!\") FROM my-index LIMIT 1 | . | left | left(string T, integer) -&gt; T | SELECT left('hello', 2) FROM my-index LIMIT 1 | . | length | length(string) -&gt; integer | SELECT length('hello') FROM my-index LIMIT 1 | . | locate | locate(string, string, integer) -&gt; integer or locate(string, string) -&gt; INTEGER | SELECT locate('o', 'hello') FROM my-index LIMIT 1, SELECT locate('l', 'hello', 3) FROM my-index LIMIT 1 | . | replace | replace(string T, string, string) -&gt; T | SELECT replace('hello', 'l', 'x') FROM my-index LIMIT 1 | . | right | right(string T, integer) -&gt; T | SELECT right('hello', 1) FROM my-index LIMIT 1 | . | rtrim | rtrim(string T) -&gt; T | SELECT rtrim(name.keyword) FROM my-index LIMIT 1 | . | substring | substring(string T, integer, integer) -&gt; T | SELECT substring(name.keyword, 2,5) FROM my-index LIMIT 1 | . | trim | trim(string T) -&gt; T | SELECT trim(' hello') FROM my-index LIMIT 1 | . | upper | upper(string T) -&gt; T | SELECT upper('helloworld') FROM my-index LIMIT 1 | . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/functions/#string",
    "relUrl": "/search-plugins/sql/functions/#string"
  },"945": {
    "doc": "Functions",
    "title": "Aggregate",
    "content": "| Function | Specification | Example | . | avg | avg(number T) -&gt; T | SELECT avg(2, 3) FROM my-index LIMIT 1 | . | count | count(number T) -&gt; T | SELECT count(date) FROM my-index LIMIT 1 | . | min | min(number T, number) -&gt; T | SELECT min(2, 3) FROM my-index LIMIT 1 | . | show | show(string T) -&gt; T | SHOW TABLES LIKE my-index | . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/functions/#aggregate",
    "relUrl": "/search-plugins/sql/functions/#aggregate"
  },"946": {
    "doc": "Functions",
    "title": "Advanced",
    "content": "| Function | Specification | Example | . | if | if(boolean, es_type, es_type) -&gt; es_type | SELECT if(false, 0, 1) FROM my-index LIMIT 1, SELECT if(true, 0, 1) FROM my-index LIMIT 1 | . | ifnull | ifnull(es_type, es_type) -&gt; es_type | SELECT ifnull('hello', 1) FROM my-index LIMIT 1, SELECT ifnull(null, 1) FROM my-index LIMIT 1 | . | isnull | isnull(es_type) -&gt; integer | SELECT isnull(null) FROM my-index LIMIT 1, SELECT isnull(1) FROM my-index LIMIT 1 | . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/functions/#advanced",
    "relUrl": "/search-plugins/sql/functions/#advanced"
  },"947": {
    "doc": "Functions",
    "title": "Relevance-based search (full-text search)",
    "content": "These functions are only available in the WHERE clause. For their descriptions and usage examples in SQL and PPL, see Full-text search. ",
    "url": "https://vagimeli.github.io/search-plugins/sql/functions/#relevance-based-search-full-text-search",
    "relUrl": "/search-plugins/sql/functions/#relevance-based-search-full-text-search"
  },"948": {
    "doc": "Identifiers",
    "title": "Identifiers",
    "content": "An identifier is an ID to name your database objects, such as index names, field names, aliases, and so on. OpenSearch supports two types of identifiers: regular identifiers and delimited identifiers. ",
    "url": "https://vagimeli.github.io/search-plugins/sql/identifiers/",
    "relUrl": "/search-plugins/sql/identifiers/"
  },"949": {
    "doc": "Identifiers",
    "title": "Regular identifiers",
    "content": "A regular identifier is a string of characters that starts with an ASCII letter (lower or upper case). The next character can either be a letter, digit, or underscore (_). It can’t be a reserved keyword. Whitespace and other special characters are also not allowed. OpenSearch supports the following regular identifiers: . | Identifiers prefixed by a dot . sign. Use to hide an index. For example .opensearch-dashboards. | Identifiers prefixed by an @ sign. Use for meta fields generated by Logstash ingestion. | Identifiers with hyphen - in the middle. Use for index names with date information. | Identifiers with star * present. Use for wildcard match of index patterns. | . For regular identifiers, you can use the name without any back tick or escape characters. In this example, source, fields, account_number, firstname, and lastname are all identifiers. Out of these, the source field is a reserved identifier. SELECT account_number, firstname, lastname FROM accounts; . | account_number | firstname | lastname | . | 1 | Amber | Duke | . | 6 | Hattie | Bond | . | 13 | Nanette | Bates | . | 18 | Dale | Adams | . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/identifiers/#regular-identifiers",
    "relUrl": "/search-plugins/sql/identifiers/#regular-identifiers"
  },"950": {
    "doc": "Identifiers",
    "title": "Delimited identifiers",
    "content": "A delimited identifier can contain special characters not allowed by a regular identifier. You must enclose delimited identifiers with back ticks (``). Back ticks differentiate the identifier from special characters. If the index name includes a dot (.), for example, log-2021.01.11, use delimited identifiers with back ticks to escape it `log-2021.01.11`. Typical examples of using delimited identifiers: . | Identifiers with reserved keywords. | Identifiers with a . present. Similarly, - to include date information. | Identifiers with other special characters. For example, Unicode characters. | . To quote an index name with back ticks: . source=`accounts` | fields `account_number`; . | account_number | . | 1 | . | 6 | . | 13 | . | 18 | . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/identifiers/#delimited-identifiers",
    "relUrl": "/search-plugins/sql/identifiers/#delimited-identifiers"
  },"951": {
    "doc": "Identifiers",
    "title": "Case sensitivity",
    "content": "Identifiers are case sensitive. They must be exactly the same as what’s stored in OpenSearch. For example, if you run source=Accounts, you’ll get an index not found exception because the actual index name is in lower case. ",
    "url": "https://vagimeli.github.io/search-plugins/sql/identifiers/#case-sensitivity",
    "relUrl": "/search-plugins/sql/identifiers/#case-sensitivity"
  },"952": {
    "doc": "SQL and PPL",
    "title": "SQL and PPL",
    "content": "OpenSearch SQL lets you write queries in SQL rather than the OpenSearch query domain-specific language (DSL). If you’re already familiar with SQL and don’t want to learn the query DSL, this feature is a great option. ",
    "url": "https://vagimeli.github.io/search-plugins/sql/index/",
    "relUrl": "/search-plugins/sql/index/"
  },"953": {
    "doc": "SQL and PPL",
    "title": "Contributing",
    "content": "To get involved and help us improve the SQL plugin, see the development guide for instructions on setting up your development environment and building the project. ",
    "url": "https://vagimeli.github.io/search-plugins/sql/index/#contributing",
    "relUrl": "/search-plugins/sql/index/#contributing"
  },"954": {
    "doc": "Limitations",
    "title": "Limitations",
    "content": "The SQL plugin has the following limitations: . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/limitation/",
    "relUrl": "/search-plugins/sql/limitation/"
  },"955": {
    "doc": "Limitations",
    "title": "Aggregation over expression is not supported",
    "content": "You can only apply aggregation to fields. Aggregations cannot accept an expression as a parameter. For example, avg(log(age)) is not supported. ",
    "url": "https://vagimeli.github.io/search-plugins/sql/limitation/#aggregation-over-expression-is-not-supported",
    "relUrl": "/search-plugins/sql/limitation/#aggregation-over-expression-is-not-supported"
  },"956": {
    "doc": "Limitations",
    "title": "Subquery in the FROM clause",
    "content": "Subquery in the FROM clause in this format: SELECT outer FROM (SELECT inner) is supported only when the query is merged into one query. For example, the following query is supported: . SELECT t.f, t.d FROM ( SELECT FlightNum as f, DestCountry as d FROM opensearch_dashboards_sample_data_flights WHERE OriginCountry = 'US') t . But, if the outer query has GROUP BY or ORDER BY, then it’s not supported. ",
    "url": "https://vagimeli.github.io/search-plugins/sql/limitation/#subquery-in-the-from-clause",
    "relUrl": "/search-plugins/sql/limitation/#subquery-in-the-from-clause"
  },"957": {
    "doc": "Limitations",
    "title": "JOIN does not support aggregations on the joined result",
    "content": "The join query does not support aggregations on the joined result. For example, e.g. SELECT depo.name, avg(empo.age) FROM empo JOIN depo WHERE empo.id == depo.id GROUP BY depo.name is not supported. ",
    "url": "https://vagimeli.github.io/search-plugins/sql/limitation/#join-does-not-support-aggregations-on-the-joined-result",
    "relUrl": "/search-plugins/sql/limitation/#join-does-not-support-aggregations-on-the-joined-result"
  },"958": {
    "doc": "Limitations",
    "title": "Pagination only supports basic queries",
    "content": "The pagination query enables you to get back paginated responses. Currently, the pagination only supports basic queries. For example, the following query returns the data with cursor id. POST _plugins/_sql/ { \"fetch_size\" : 5, \"query\" : \"SELECT OriginCountry, DestCountry FROM opensearch_dashboards_sample_data_flights ORDER BY OriginCountry ASC\" } . The response in JDBC format with cursor id. { \"schema\": [ { \"name\": \"OriginCountry\", \"type\": \"keyword\" }, { \"name\": \"DestCountry\", \"type\": \"keyword\" } ], \"cursor\": \"d:eyJhIjp7fSwicyI6IkRYRjFaWEo1UVc1a1JtVjBZMmdCQUFBQUFBQUFCSllXVTJKVU4yeExiWEJSUkhsNFVrdDVXVEZSYkVKSmR3PT0iLCJjIjpbeyJuYW1lIjoiT3JpZ2luQ291bnRyeSIsInR5cGUiOiJrZXl3b3JkIn0seyJuYW1lIjoiRGVzdENvdW50cnkiLCJ0eXBlIjoia2V5d29yZCJ9XSwiZiI6MSwiaSI6ImtpYmFuYV9zYW1wbGVfZGF0YV9mbGlnaHRzIiwibCI6MTMwNTh9\", \"total\": 13059, \"datarows\": [[ \"AE\", \"CN\" ]], \"size\": 1, \"status\": 200 } . The query with aggregation and join does not support pagination for now. ",
    "url": "https://vagimeli.github.io/search-plugins/sql/limitation/#pagination-only-supports-basic-queries",
    "relUrl": "/search-plugins/sql/limitation/#pagination-only-supports-basic-queries"
  },"959": {
    "doc": "Limitations",
    "title": "Query processing engines",
    "content": "The SQL plugin has two query processing engines, V1 and V2. Most of the features are supported by both engines, but only the new engine is actively being developed. A query that is first executed on the V2 engine falls back to the V1 engine in case of failure. If a query is supported in V2 but not included in V1, the query will fail with an error response. V1 engine limitations . | The select literal expression without FROM clause is not supported. For example, SELECT 1 is not supported. | The WHERE clause does not support expressions. For example, SELECT FlightNum FROM opensearch_dashboards_sample_data_flights where (AvgTicketPrice + 100) &lt;= 1000 is not supported. | Most relevancy search functions are implemented in the V2 engine only. | . Such queries are successfully executed by the V2 engine unless they have V1-specific functions. You will likely never meet these limitations. V2 engine limitations . | The cursor feature is supported by the V1 engine only. For support of cursor/pagination in the V2 engine, track GitHub issue #656. | The V2 engine does not track query execution time, so slow queries are not reported. | The V2 query engine not only runs queries in the OpenSearch engine but also supports post-processing for complicated queries. Accordingly, the explain output is no longer pure OpenSearch domain-specific language (DSL) but also includes query plan information from the V2 query engine. | The V2 engine does not support SCORE_QUERY and WILDCARD_QUERY functions. | . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/limitation/#query-processing-engines",
    "relUrl": "/search-plugins/sql/limitation/#query-processing-engines"
  },"960": {
    "doc": "Monitoring",
    "title": "Monitoring",
    "content": "By a stats endpoint, you are able to collect metrics for the plugin within the interval. Note that only node level statistics collecting is implemented for now. In other words, you only get the metrics for the node you’re accessing. Cluster level statistics have yet to be implemented. ",
    "url": "https://vagimeli.github.io/search-plugins/sql/monitoring/",
    "relUrl": "/search-plugins/sql/monitoring/"
  },"961": {
    "doc": "Monitoring",
    "title": "Node Stats",
    "content": "Description . The meaning of fields in the response is as follows: . | Field name | Description | . | request_total | Total count of request | . | request_count | Total count of request within the interval | . | failed_request_count_syserr | Count of failed request due to system error within the interval | . | failed_request_count_cuserr | Count of failed request due to bad request within the interval | . | failed_request_count_cb | Indicate if plugin is being circuit broken within the interval | . Example . SQL query: . &gt;&gt; curl -H 'Content-Type: application/json' -X GET localhost:9200/_plugins/_sql/stats . Result set: . { \"failed_request_count_cb\": 0, \"failed_request_count_cuserr\": 0, \"circuit_breaker\": 0, \"request_total\": 0, \"request_count\": 0, \"failed_request_count_syserr\": 0 } . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/monitoring/#node-stats",
    "relUrl": "/search-plugins/sql/monitoring/#node-stats"
  },"962": {
    "doc": "Commands",
    "title": "Commands",
    "content": "PPL supports all SQL common functions, including relevance search, but also introduces few more functions (called commands) which are available in PPL only. ",
    "url": "https://vagimeli.github.io/search-plugins/sql/ppl/functions/",
    "relUrl": "/search-plugins/sql/ppl/functions/"
  },"963": {
    "doc": "Commands",
    "title": "dedup",
    "content": "The dedup (data deduplication) command removes duplicate documents defined by a field from the search result. Syntax . dedup [int] &lt;field-list&gt; [keepempty=&lt;bool&gt;] [consecutive=&lt;bool&gt;] . | Field | Description | Type | Required | Default | . | int | Retain the specified number of duplicate events for each combination. The number must be greater than 0. If you do not specify a number, only the first occurring event is kept and all other duplicates are removed from the results. | string | No | 1 | . | keepempty | If true, keep the document if any field in the field list has a null value or a field missing. | nested list of objects | No | False | . | consecutive | If true, remove only consecutive events with duplicate combinations of values. | Boolean | No | False | . | field-list | Specify a comma-delimited field list. At least one field is required. | String or comma-separated list of strings | Yes | - | . Example 1: Dedup by one field . To remove duplicate documents with the same gender: . search source=accounts | dedup gender | fields account_number, gender; . | account_number | gender | . | 1 | M | . | 13 | F | . Example 2: Keep two duplicate documents . To keep two duplicate documents with the same gender: . search source=accounts | dedup 2 gender | fields account_number, gender; . | account_number | gender | . | 1 | M | . | 6 | M | . | 13 | F | . Example 3: Keep or ignore an empty field by default . To keep two duplicate documents with a null field value: . search source=accounts | dedup email keepempty=true | fields account_number, email; . | account_number | email | . | 1 | amberduke@pyrami.com | . | 6 | hattiebond@netagy.com | . | 13 | null | . | 18 | daleadams@boink.com | . To remove duplicate documents with the null field value: . search source=accounts | dedup email | fields account_number, email; . | account_number | email | . | 1 | amberduke@pyrami.com | . | 6 | hattiebond@netagy.com | . | 18 | daleadams@boink.com | . Example 4: Dedup of consecutive documents . To remove duplicates of consecutive documents: . search source=accounts | dedup gender consecutive=true | fields account_number, gender; . | account_number | gender | . | 1 | M | . | 13 | F | . | 18 | M | . Limitations . The dedup command is not rewritten to OpenSearch DSL, it is only executed on the coordination node. ",
    "url": "https://vagimeli.github.io/search-plugins/sql/ppl/functions/#dedup",
    "relUrl": "/search-plugins/sql/ppl/functions/#dedup"
  },"964": {
    "doc": "Commands",
    "title": "eval",
    "content": "The eval command evaluates an expression and appends its result to the search result. Syntax . eval &lt;field&gt;=&lt;expression&gt; [\",\" &lt;field&gt;=&lt;expression&gt; ]... | Field | Description | Required | . | field | If a field name does not exist, a new field is added. If the field name already exists, it’s overwritten. | Yes | . | expression | Specify any supported expression. | Yes | . Example 1: Create a new field . To create a new doubleAge field for each document. doubleAge is the result of age multiplied by 2: . search source=accounts | eval doubleAge = age * 2 | fields age, doubleAge; . | age | doubleAge | . | 32 | 64 | . | 36 | 72 | . | 28 | 56 | . | 33 | 66 | . Example 2: Overwrite the existing field . To overwrite the age field with age plus 1: . search source=accounts | eval age = age + 1 | fields age; . | age | . | 33 | . | 37 | . | 29 | . | 34 | . Example 3: Create a new field with a field defined with the eval command . To create a new field ddAge. ddAge is the result of doubleAge multiplied by 2, where doubleAge is defined in the eval command: . search source=accounts | eval doubleAge = age * 2, ddAge = doubleAge * 2 | fields age, doubleAge, ddAge; . | age | doubleAge | ddAge | . | 32 | 64 | 128 | . | 36 | 72 | 144 | . | 28 | 56 | 112 | . | 33 | 66 | 132 | . Limitation . The eval command is not rewritten to OpenSearch DSL, it is only executed on the coordination node. ",
    "url": "https://vagimeli.github.io/search-plugins/sql/ppl/functions/#eval",
    "relUrl": "/search-plugins/sql/ppl/functions/#eval"
  },"965": {
    "doc": "Commands",
    "title": "fields",
    "content": "Use the fields command to keep or remove fields from a search result. Syntax . fields [+|-] &lt;field-list&gt; . | Field | Description | Required | Default | . | index | Plus (+) keeps only fields specified in the field list. Minus (-) removes all fields specified in the field list. | No | + | . | field list | Specify a comma-delimited list of fields. | Yes | No default | . Example 1: Select specified fields from result . To get account_number, firstname, and lastname fields from a search result: . search source=accounts | fields account_number, firstname, lastname; . | account_number | firstname | lastname | . | 1 | Amber | Duke | . | 6 | Hattie | Bond | . | 13 | Nanette | Bates | . | 18 | Dale | Adams | . Example 2: Remove specified fields from a search result . To remove the account_number field from the search results: . search source=accounts | fields account_number, firstname, lastname | fields - account_number; . | firstname | lastname | . | Amber | Duke | . | Hattie | Bond | . | Nanette | Bates | . | Dale | Adams | . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/ppl/functions/#fields",
    "relUrl": "/search-plugins/sql/ppl/functions/#fields"
  },"966": {
    "doc": "Commands",
    "title": "parse",
    "content": "Use the parse command to parse a text field using regular expression and append the result to the search result. Syntax . parse &lt;field&gt; &lt;regular-expression&gt; . | Field | Description | Required | . | field | A text field. | Yes | . | regular-expression | The regular expression used to extract new fields from the given test field. If a new field name exists, it will replace the original field. | Yes | . The regular expression is used to match the whole text field of each document with Java regex engine. Each named capture group in the expression will become a new STRING field. Example 1: Create new field . The example shows how to create new field host for each document. host will be the hostname after @ in email field. Parsing a null field will return an empty string. os&gt; source=accounts | parse email '.+@(?&lt;host&gt;.+)' | fields email, host ; fetched rows / total rows = 4/4 . | email | host | . | amberduke@pyrami.com | pyrami.com | . | hattiebond@netagy.com | netagy.com | . | null | null | . | daleadams@boink.com | boink.com | . Example 2: Override the existing field . The example shows how to override the existing address field with street number removed. os&gt; source=accounts | parse address '\\d+ (?&lt;address&gt;.+)' | fields address ; fetched rows / total rows = 4/4 . | address | . | Holmes Lane | . | Bristol Street | . | Madison Street | . | Hutchinson Court | . Example 3: Filter and sort be casted parsed field . The example shows how to sort street numbers that are higher than 500 in address field. os&gt; source=accounts | parse address '(?&lt;streetNumber&gt;\\d+) (?&lt;street&gt;.+)' | where cast(streetNumber as int) &gt; 500 | sort num(streetNumber) | fields streetNumber, street ; fetched rows / total rows = 3/3 . | streetNumber | street | . | 671 | Bristol Street | . | 789 | Madison Street | . | 880 | Holmes Lane | . Limitations . A few limitations exist when using the parse command: . | Fields defined by parse cannot be parsed again. For example, source=accounts | parse address '\\d+ (?&lt;street&gt;.+)' | parse street '\\w+ (?&lt;road&gt;\\w+)' ; will fail to return any expressions. | Fields defined by parse cannot be overridden with other commands. For example, when entering source=accounts | parse address '\\d+ (?&lt;street&gt;.+)' | eval street='1' | where street='1' ; where will not match any documents since street cannot be overridden. | The text field used by parse cannot be overridden. For example, when entering source=accounts | parse address '\\d+ (?&lt;street&gt;.+)' | eval address='1' ; street will not be parse since address is overridden. | Fields defined by parse cannot be filtered/sorted after using them in the stats command. For example, source=accounts | parse email '.+@(?&lt;host&gt;.+)' | stats avg(age) by host | where host=pyrami.com ; where will not parse the domain listed. | . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/ppl/functions/#parse",
    "relUrl": "/search-plugins/sql/ppl/functions/#parse"
  },"967": {
    "doc": "Commands",
    "title": "rename",
    "content": "Use the rename command to rename one or more fields in the search result. Syntax . rename &lt;source-field&gt; AS &lt;target-field&gt;[\",\" &lt;source-field&gt; AS &lt;target-field&gt;]... | Field | Description | Required | . | source-field | The name of the field that you want to rename. | Yes | . | target-field | The name you want to rename to. | Yes | . Example 1: Rename one field . Rename the account_number field as an: . search source=accounts | rename account_number as an | fields an; . | an | . | 1 | . | 6 | . | 13 | . | 18 | . Example 2: Rename multiple fields . Rename the account_number field as an and employer as emp: . search source=accounts | rename account_number as an, employer as emp | fields an, emp; . | an | emp | . | 1 | Pyrami | . | 6 | Netagy | . | 13 | Quility | . | 18 | null | . Limitations . The rename command is not rewritten to OpenSearch DSL, it is only executed on the coordination node. ",
    "url": "https://vagimeli.github.io/search-plugins/sql/ppl/functions/#rename",
    "relUrl": "/search-plugins/sql/ppl/functions/#rename"
  },"968": {
    "doc": "Commands",
    "title": "sort",
    "content": "Use the sort command to sort search results by a specified field. Syntax . sort [count] &lt;[+|-] sort-field&gt;... | Field | Description | Required | Default | . | count | The maximum number results to return from the sorted result. If count=0, all results are returned. | No | 1000 | . | [+|-] | Use plus [+] to sort by ascending order and minus [-] to sort by descending order. | No | Ascending order | . | sort-field | Specify the field that you want to sort by. | Yes | - | . Example 1: Sort by one field . To sort all documents by the age field in ascending order: . search source=accounts | sort age | fields account_number, age; . | account_number | age | . | 13 | 28 | . | 1 | 32 | . | 18 | 33 | . | 6 | 36 | . Example 2: Sort by one field and return all results . To sort all documents by the age field in ascending order and specify count as 0 to get back all results: . search source=accounts | sort 0 age | fields account_number, age; . | account_number | age | . | 13 | 28 | . | 1 | 32 | . | 18 | 33 | . | 6 | 36 | . Example 3: Sort by one field in descending order . To sort all documents by the age field in descending order: . search source=accounts | sort - age | fields account_number, age; . | account_number | age | . | 6 | 36 | . | 18 | 33 | . | 1 | 32 | . | 13 | 28 | . Example 4: Specify the number of sorted documents to return . To sort all documents by the age field in ascending order and specify count as 2 to get back two results: . search source=accounts | sort 2 age | fields account_number, age; . | account_number | age | . | 13 | 28 | . | 1 | 32 | . Example 5: Sort by multiple fields . To sort all documents by the gender field in ascending order and age field in descending order: . search source=accounts | sort + gender, - age | fields account_number, gender, age; . | account_number | gender | age | . | 13 | F | 28 | . | 6 | M | 36 | . | 18 | M | 33 | . | 1 | M | 32 | . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/ppl/functions/#sort",
    "relUrl": "/search-plugins/sql/ppl/functions/#sort"
  },"969": {
    "doc": "Commands",
    "title": "stats",
    "content": "Use the stats command to aggregate from search results. The following table lists the aggregation functions and also indicates how each one handles null or missing values: . | Function | NULL | MISSING | . | COUNT | Not counted | Not counted | . | SUM | Ignore | Ignore | . | AVG | Ignore | Ignore | . | MAX | Ignore | Ignore | . | MIN | Ignore | Ignore | . Syntax . stats &lt;aggregation&gt;... [by-clause]... | Field | Description | Required | Default | . | aggregation | Specify a statistical aggregation function. The argument of this function must be a field. | Yes | 1000 | . | by-clause | Specify one or more fields to group the results by. If not specified, the stats command returns only one row, which is the aggregation over the entire result set. | No | - | . Example 1: Calculate the average value of a field . To calculate the average age of all documents: . search source=accounts | stats avg(age); . | avg(age) | . | 32.25 | . Example 2: Calculate the average value of a field by group . To calculate the average age grouped by gender: . search source=accounts | stats avg(age) by gender; . | gender | avg(age) | . | F | 28.0 | . | M | 33.666666666666664 | . Example 3: Calculate the average and sum of a field by group . To calculate the average and sum of age grouped by gender: . search source=accounts | stats avg(age), sum(age) by gender; . | gender | avg(age) | sum(age) | . | F | 28 | 28 | . | M | 33.666666666666664 | 101 | . Example 4: Calculate the maximum value of a field . To calculate the maximum age: . search source=accounts | stats max(age); . | max(age) | . | 36 | . Example 5: Calculate the maximum and minimum value of a field by group . To calculate the maximum and minimum age values grouped by gender: . search source=accounts | stats max(age), min(age) by gender; . | gender | min(age) | max(age) | . | F | 28 | 28 | . | M | 32 | 36 | . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/ppl/functions/#stats",
    "relUrl": "/search-plugins/sql/ppl/functions/#stats"
  },"970": {
    "doc": "Commands",
    "title": "where",
    "content": "Use the where command with a bool expression to filter the search result. The where command only returns the result when the bool expression evaluates to true. Syntax . where &lt;boolean-expression&gt; . | Field | Description | Required | . | bool-expression | An expression that evaluates to a boolean value. | No | . Example: Filter result set with a condition . To get all documents from the accounts index where account_number is 1 or gender is F: . search source=accounts | where account_number=1 or gender=\\\"F\\\" | fields account_number, gender; . | account_number | gender | . | 1 | M | . | 13 | F | . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/ppl/functions/#where",
    "relUrl": "/search-plugins/sql/ppl/functions/#where"
  },"971": {
    "doc": "Commands",
    "title": "head",
    "content": "Use the head command to return the first N number of results in a specified search order. Syntax . head [N] . | Field | Description | Required | Default | . | N | Specify the number of results to return. | No | 10 | . Example 1: Get the first 10 results . To get the first 10 results: . search source=accounts | fields firstname, age | head; . | firstname | age | . | Amber | 32 | . | Hattie | 36 | . | Nanette | 28 | . Example 2: Get the first N results . To get the first two results: . search source=accounts | fields firstname, age | head 2; . | firstname | age | . | Amber | 32 | . | Hattie | 36 | . Limitations . The head command is not rewritten to OpenSearch DSL, it is only executed on the coordination node. ",
    "url": "https://vagimeli.github.io/search-plugins/sql/ppl/functions/#head",
    "relUrl": "/search-plugins/sql/ppl/functions/#head"
  },"972": {
    "doc": "Commands",
    "title": "rare",
    "content": "Use the rare command to find the least common values of all fields in a field list. A maximum of 10 results are returned for each distinct set of values of the group-by fields. Syntax . rare &lt;field-list&gt; [by-clause] . | Field | Description | Required | . | field-list | Specify a comma-delimited list of field names. | No | . | by-clause | Specify one or more fields to group the results by. | No | . Example 1: Find the least common values in a field . To find the least common values of gender: . search source=accounts | rare gender; . | gender | . | F | . | M | . Example 2: Find the least common values grouped by gender . To find the least common age grouped by gender: . search source=accounts | rare age by gender; . | gender | age | . | F | 28 | . | M | 32 | . | M | 33 | . Limitations . The rare command is not rewritten to OpenSearch DSL, it is only executed on the coordination node. ",
    "url": "https://vagimeli.github.io/search-plugins/sql/ppl/functions/#rare",
    "relUrl": "/search-plugins/sql/ppl/functions/#rare"
  },"973": {
    "doc": "Commands",
    "title": "top",
    "content": "Use the top command to find the most common values of all fields in the field list. Syntax . top [N] &lt;field-list&gt; [by-clause] . | Field | Description | Default | . | N | Specify the number of results to return. | 10 | . | field-list | Specify a comma-delimited list of field names. | - | . | by-clause | Specify one or more fields to group the results by. | - | . Example 1: Find the most common values in a field . To find the most common genders: . search source=accounts | top gender; . | gender | . | M | . | F | . Example 2: Find the most common value in a field . To find the most common gender: . search source=accounts | top 1 gender; . | gender | . | M | . Example 3: Find the most common values grouped by gender . To find the most common age grouped by gender: . search source=accounts | top 1 age by gender; . | gender | age | . | F | 28 | . | M | 32 | . Limitations . The top command is not rewritten to OpenSearch DSL, it is only executed on the coordination node. ",
    "url": "https://vagimeli.github.io/search-plugins/sql/ppl/functions/#top-command",
    "relUrl": "/search-plugins/sql/ppl/functions/#top-command"
  },"974": {
    "doc": "Commands",
    "title": "ad",
    "content": "The ad command applies the Random Cut Forest (RCF) algorithm in the ML Commons plugin on the search result returned by a PPL command. Based on the input, the plugin uses two types of RCF algorithms: fixed in time RCF for processing time-series data and batch RCF for processing non-time-series data. Syntax: Fixed In Time RCF For Time-series Data Command . ad &lt;shingle_size&gt; &lt;time_decay&gt; &lt;time_field&gt; . | Field | Description | Required | . | shingle_size | A consecutive sequence of the most recent records. The default value is 8. | No | . | time_decay | Specifies how much of the recent past to consider when computing an anomaly score. The default value is 0.001. | No | . | time_field | Specifies the time filed for RCF to use as time-series data. Must be either a long value, such as the timestamp in miliseconds, or a string value in “yyyy-MM-dd HH:mm:ss”. | Yes | . Syntax: Batch RCF for Non-time-series Data Command . ad &lt;shingle_size&gt; &lt;time_decay&gt; . | Field | Description | Required | . | shingle_size | A consecutive sequence of the most recent records. The default value is 8. | No | . | time_decay | Specifies how much of the recent past to consider when computing an anomaly score. The default value is 0.001. | No | . Example 1: Detecting events in New York City from taxi ridership data with time-series data . The example trains a RCF model and use the model to detect anomalies in the time-series ridership data. PPL query: . os&gt; source=nyc_taxi | fields value, timestamp | AD time_field='timestamp' | where value=10844.0 . | value | timestamp | score | anomaly_grade | . | 10844.0 | 1404172800000 | 0.0 | 0.0 | . Example 2: Detecting events in New York City from taxi ridership data with non-time-series data . PPL query: . os&gt; source=nyc_taxi | fields value | AD | where value=10844.0 . | value | score | anomalous |   | . |   | 10844.0 | 0.0 | false | . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/ppl/functions/#ad",
    "relUrl": "/search-plugins/sql/ppl/functions/#ad"
  },"975": {
    "doc": "Commands",
    "title": "kmeans",
    "content": "The kmeans command applies the ML Commons plugin’s kmeans algorithm to the provided PPL command’s search results. Syntax . kmeans &lt;cluster-number&gt; . For cluster-number, enter the number of clusters you want to group your data points into. Example: Group Iris data . The example shows how to classify three Iris species (Iris setosa, Iris virginica and Iris versicolor) based on the combination of four features measured from each sample: the length and the width of the sepals and petals. PPL query: . os&gt; source=iris_data | fields sepal_length_in_cm, sepal_width_in_cm, petal_length_in_cm, petal_width_in_cm | kmeans 3 . | sepal_length_in_cm | sepal_width_in_cm | petal_length_in_cm | petal_width_in_cm | ClusterID |   | . |   | 5.1 | 3.5 | 1.4 | 0.2 | 1 | . |   | 5.6 | 3.0 | 4.1 | 1.3 | 0 | . |   | 6.7 | 2.5 | 5.8 | 1.8 | 2 | . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/ppl/functions/#kmeans",
    "relUrl": "/search-plugins/sql/ppl/functions/#kmeans"
  },"976": {
    "doc": "PPL &ndash; Piped Processing Language",
    "title": "PPL – Piped Processing Language",
    "content": "Piped Processing Language (PPL) is a query language that lets you use pipe (|) syntax to explore, discover, and query data stored in OpenSearch. To quickly get up and running with PPL, use Query Workbench in OpenSearch Dashboards. To learn more, see Workbench. The PPL syntax consists of commands delimited by the pipe character (|) where data flows from left to right through each pipeline. search command | command 1 | command 2 ... You can only use read-only commands like search, where, fields, rename, dedup, stats, sort, eval, head, top, and rare. ",
    "url": "https://vagimeli.github.io/search-plugins/sql/ppl/index/#ppl--piped-processing-language",
    "relUrl": "/search-plugins/sql/ppl/index/#ppl--piped-processing-language"
  },"977": {
    "doc": "PPL &ndash; Piped Processing Language",
    "title": "Quick start",
    "content": "To get started with PPL, choose Dev Tools in OpenSearch Dashboards and use the bulk operation to index some sample data: . PUT accounts/_bulk?refresh {\"index\":{\"_id\":\"1\"}} {\"account_number\":1,\"balance\":39225,\"firstname\":\"Amber\",\"lastname\":\"Duke\",\"age\":32,\"gender\":\"M\",\"address\":\"880 Holmes Lane\",\"employer\":\"Pyrami\",\"email\":\"amberduke@pyrami.com\",\"city\":\"Brogan\",\"state\":\"IL\"} {\"index\":{\"_id\":\"6\"}} {\"account_number\":6,\"balance\":5686,\"firstname\":\"Hattie\",\"lastname\":\"Bond\",\"age\":36,\"gender\":\"M\",\"address\":\"671 Bristol Street\",\"employer\":\"Netagy\",\"email\":\"hattiebond@netagy.com\",\"city\":\"Dante\",\"state\":\"TN\"} {\"index\":{\"_id\":\"13\"}} {\"account_number\":13,\"balance\":32838,\"firstname\":\"Nanette\",\"lastname\":\"Bates\",\"age\":28,\"gender\":\"F\",\"address\":\"789 Madison Street\",\"employer\":\"Quility\",\"city\":\"Nogal\",\"state\":\"VA\"} {\"index\":{\"_id\":\"18\"}} {\"account_number\":18,\"balance\":4180,\"firstname\":\"Dale\",\"lastname\":\"Adams\",\"age\":33,\"gender\":\"M\",\"address\":\"467 Hutchinson Court\",\"email\":\"daleadams@boink.com\",\"city\":\"Orick\",\"state\":\"MD\"} . Go to Query Workbench and select PPL. The following example returns firstname and lastname fields for documents in an accounts index with age greater than 18: . search source=accounts | where age &gt; 18 | fields firstname, lastname . Sample Response . | firstname | lastname | . | Amber | Duke | . | Hattie | Bond | . | Nanette | Bates | . | Dale | Adams | . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/ppl/index/#quick-start",
    "relUrl": "/search-plugins/sql/ppl/index/#quick-start"
  },"978": {
    "doc": "PPL &ndash; Piped Processing Language",
    "title": "PPL &ndash; Piped Processing Language",
    "content": " ",
    "url": "https://vagimeli.github.io/search-plugins/sql/ppl/index/",
    "relUrl": "/search-plugins/sql/ppl/index/"
  },"979": {
    "doc": "Syntax",
    "title": "PPL syntax",
    "content": "Every PPL query starts with the search command. It specifies the index to search and retrieve documents from. Subsequent commands can follow in any order. Currently, PPL supports only one search command, which can be omitted to simplify the query. { : .note} . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/ppl/syntax/#ppl-syntax",
    "relUrl": "/search-plugins/sql/ppl/syntax/#ppl-syntax"
  },"980": {
    "doc": "Syntax",
    "title": "Syntax",
    "content": "search source=&lt;index&gt; [boolean-expression] source=&lt;index&gt; [boolean-expression] . | Field | Description | Required | . | search | Specifies search keywords. | Yes | . | index | Specifies which index to query from. | No | . | bool-expression | Specifies an expression that evaluates to a Boolean value. | No | . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/ppl/syntax/",
    "relUrl": "/search-plugins/sql/ppl/syntax/"
  },"981": {
    "doc": "Syntax",
    "title": "Examples",
    "content": "Example 1: Search through accounts index . In the following example, the search command refers to an accounts index as the source and uses fields and where commands for the conditions: . search source=accounts | where age &gt; 18 | fields firstname, lastname . In the following examples, angle brackets &lt; &gt; enclose required arguments and square brackets [ ] enclose optional arguments. Example 2: Get all documents . To get all documents from the accounts index, specify it as the source: . search source=accounts; . | account_number | firstname | address | balance | gender | city | employer | state | age | email | lastname | . | 1 | Amber | 880 Holmes Lane | 39225 | M | Brogan | Pyrami | IL | 32 | amberduke@pyrami.com | Duke | . | 6 | Hattie | 671 Bristol Street | 5686 | M | Dante | Netagy | TN | 36 | hattiebond@netagy.com | Bond | . | 13 | Nanette | 789 Madison Street | 32838 | F | Nogal | Quility | VA | 28 | null | Bates | . | 18 | Dale | 467 Hutchinson Court | 4180 | M | Orick | null | MD | 33 | daleadams@boink.com | Adams | . Example 3: Get documents that match a condition . To get all documents from the accounts index that either have account_number equal to 1 or have gender as F, use the following query: . search source=accounts account_number=1 or gender=\\\"F\\\"; . | account_number | firstname | address | balance | gender | city | employer | state | age | email | lastname | . | 1 | Amber | 880 Holmes Lane | 39225 | M | Brogan | Pyrami | IL | 32 | amberduke@pyrami.com | Duke | . | 13 | Nanette | 789 Madison Street | 32838 | F | Nogal | Quility | VA | 28 | null | Bates | . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/ppl/syntax/#examples",
    "relUrl": "/search-plugins/sql/ppl/syntax/#examples"
  },"982": {
    "doc": "Response formats",
    "title": "Response formats",
    "content": "The SQL plugin provides the jdbc, csv, raw, and json response formats that are useful for different purposes. The jdbc format is widely used because it provides the schema information and adds more functionality, such as pagination. Besides the JDBC driver, various clients can benefit from a detailed and well-formatted response. ",
    "url": "https://vagimeli.github.io/search-plugins/sql/response-formats/",
    "relUrl": "/search-plugins/sql/response-formats/"
  },"983": {
    "doc": "Response formats",
    "title": "JDBC format",
    "content": "By default, the SQL plugin returns the response in the standard JDBC format. This format is provided for the JDBC driver and clients that need both the schema and the result set to be well formatted. Example request . The following query does not specify the response format, so the format is set to jdbc: . POST _plugins/_sql { \"query\" : \"SELECT firstname, lastname, age FROM accounts ORDER BY age LIMIT 2\" } . Example response . In the response, the schema contains the field names and types, and the datarows field contains the result set: . { \"schema\": [{ \"name\": \"firstname\", \"type\": \"text\" }, { \"name\": \"lastname\", \"type\": \"text\" }, { \"name\": \"age\", \"type\": \"long\" } ], \"total\": 4, \"datarows\": [ [ \"Nanette\", \"Bates\", 28 ], [ \"Amber\", \"Duke\", 32 ] ], \"size\": 2, \"status\": 200 } . If an error of any type occurs, OpenSearch returns the error message. The following query searches for a non-existent field unknown: . POST /_plugins/_sql { \"query\" : \"SELECT unknown FROM accounts\" } . The response contains the error message and the cause of the error: . { \"error\": { \"reason\": \"Invalid SQL query\", \"details\": \"Field [unknown] cannot be found or used here.\", \"type\": \"SemanticAnalysisException\" }, \"status\": 400 } . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/response-formats/#jdbc-format",
    "relUrl": "/search-plugins/sql/response-formats/#jdbc-format"
  },"984": {
    "doc": "Response formats",
    "title": "OpenSearch DSL JSON format",
    "content": "If you set the format to json, the original OpenSearch response is returned in JSON format. Because this is the native response from OpenSearch, extra effort is needed to parse and interpret it. Example request . The following query sets the response format to json: . POST _plugins/_sql?format=json { \"query\" : \"SELECT firstname, lastname, age FROM accounts ORDER BY age LIMIT 2\" } . Example response . The response is the original response from OpenSearch: . { \"_shards\": { \"total\": 5, \"failed\": 0, \"successful\": 5, \"skipped\": 0 }, \"hits\": { \"hits\": [{ \"_index\": \"accounts\", \"_type\": \"account\", \"_source\": { \"firstname\": \"Nanette\", \"age\": 28, \"lastname\": \"Bates\" }, \"_id\": \"13\", \"sort\": [ 28 ], \"_score\": null }, { \"_index\": \"accounts\", \"_type\": \"account\", \"_source\": { \"firstname\": \"Amber\", \"age\": 32, \"lastname\": \"Duke\" }, \"_id\": \"1\", \"sort\": [ 32 ], \"_score\": null } ], \"total\": { \"value\": 4, \"relation\": \"eq\" }, \"max_score\": null }, \"took\": 100, \"timed_out\": false } . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/response-formats/#opensearch-dsl-json-format",
    "relUrl": "/search-plugins/sql/response-formats/#opensearch-dsl-json-format"
  },"985": {
    "doc": "Response formats",
    "title": "CSV format",
    "content": "You can also specify to return results in CSV format. Example request . POST /_plugins/_sql?format=csv { \"query\" : \"SELECT firstname, lastname, age FROM accounts ORDER BY age\" } . Example response . firstname,lastname,age Nanette,Bates,28 Amber,Duke,32 Dale,Adams,33 Hattie,Bond,36 . Sanitizing results in CSV format . By default, OpenSearch sanitizes header cells (field names) and data cells (field contents) according to the following rules: . | If a cell starts with +, -, = , or @, the sanitizer inserts a single quote (') at the start of the cell. | If a cell contains one or more commas (,), the sanitizer surrounds the cell with double quotes (\"). | . Example . The following query indexes a document with cells that either start with special characters or contain commas: . PUT /userdata/_doc/1?refresh=true { \"+firstname\": \"-Hattie\", \"=lastname\": \"@Bond\", \"address\": \"671 Bristol Street, Dente, TN\" } . You can use the query below to request results in CSV format: . POST /_plugins/_sql?format=csv { \"query\" : \"SELECT * FROM userdata\" } . In the response, cells that start with special characters are prefixed with '. The cell that has commas is surrounded with quotation marks: . '+firstname,'=lastname,address 'Hattie,'@Bond,\"671 Bristol Street, Dente, TN\" . To skip sanitizing, set the sanitize query parameter to false: . POST /_plugins/_sql?format=csvandsanitize=false { \"query\" : \"SELECT * FROM userdata\" } . The response contains the results in the original CSV format: . =lastname,address,+firstname @Bond,\"671 Bristol Street, Dente, TN\",-Hattie . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/response-formats/#csv-format",
    "relUrl": "/search-plugins/sql/response-formats/#csv-format"
  },"986": {
    "doc": "Response formats",
    "title": "Raw format",
    "content": "You can use the raw format to pipe the results to other command line tools for post-processing. Example request . POST /_plugins/_sql?format=raw { \"query\" : \"SELECT firstname, lastname, age FROM accounts ORDER BY age\" } . Example response . Nanette|Bates|28 Amber|Duke|32 Dale|Adams|33 Hattie|Bond|36 . By default, OpenSearch sanitizes results in raw format according to the following rule: . | If a data cell contains one or more pipe characters (|), the sanitizer surrounds the cell with double quotes. | . Example . The following query indexes a document with pipe characters (|) in its fields: . PUT /userdata/_doc/1?refresh=true { \"+firstname\": \"|Hattie\", \"=lastname\": \"Bond|\", \"|address\": \"671 Bristol Street| Dente| TN\" } . You can use the query below to request results in raw format: . POST /_plugins/_sql?format=raw { \"query\" : \"SELECT * FROM userdata\" } . The query returns cells with the | character surrounded by quotation marks: . \"|address\"|=lastname|+firstname \"671 Bristol Street| Dente| TN\"|\"Bond|\"|\"|Hattie\" . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/response-formats/#raw-format",
    "relUrl": "/search-plugins/sql/response-formats/#raw-format"
  },"987": {
    "doc": "Settings",
    "title": "Settings",
    "content": "The SQL plugin adds a few settings to the standard OpenSearch cluster settings. Most are dynamic, so you can change the default behavior of the plugin without restarting your cluster. It is possible to independently disable processing of PPL or SQL queries. You can update these settings like any other cluster setting: . PUT _cluster/settings { \"transient\" : { \"plugins.sql.enabled\" : false } } . Alternatively, you can use the following request format: . PUT _cluster/settings { \"transient\": { \"plugins\": { \"ppl\": { \"enabled\": \"false\" } } } } . Similarly, you can update the settings by sending a request to the _plugins/_query/settings endpoint: . PUT _plugins/_query/settings { \"transient\" : { \"plugins.sql.enabled\" : false } } . Alternatively, you can use the following request format: . PUT _plugins/_query/settings { \"transient\": { \"plugins\": { \"ppl\": { \"enabled\": \"false\" } } } } . Requests to the _plugins/_ppl and _plugins/_sql endpoints include index names in the request body, so they have the same access policy considerations as the bulk, mget, and msearch operations. Setting the rest.action.multi.allow_explicit_index parameter to false disables both the SQL and PPL endpoints. ",
    "url": "https://vagimeli.github.io/search-plugins/sql/settings/",
    "relUrl": "/search-plugins/sql/settings/"
  },"988": {
    "doc": "Settings",
    "title": "Available settings",
    "content": "| Setting | Default | Description | . | plugins.sql.enabled | True | Change to false to disable the SQL support in the plugin. | . | plugins.ppl.enabled | True | Change to false to disable the PPL support in the plugin. | . | plugins.sql.slowlog | 2 seconds | Configures the time limit (in seconds) for slow queries. The plugin logs slow queries as Slow query: elapsed=xxx (ms) in opensearch.log. | . | plugins.sql.cursor.keep_alive | 1 minute | Configures how long the cursor context is kept open. Cursor contexts are resource resource intensive, so we recommend a low value. | . | plugins.query.memory_limit | 85% | Configures the heap memory usage limit for the circuit breaker of the query engine. | . | plugins.query.size_limit | 200 | Sets the default size of index that the query engine fetches from OpenSearch. | . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/settings/#available-settings",
    "relUrl": "/search-plugins/sql/settings/#available-settings"
  },"989": {
    "doc": "SQL/PPL API",
    "title": "SQL/PPL API",
    "content": "Use the SQL and PPL API to send queries to the SQL plugin. Use the _sql endpoint to send queries in SQL, and the _ppl endpoint to send queries in PPL. For both of these, you can also use the _explain endpoint to translate your query into OpenSearch domain-specific language (DSL) or to troubleshoot errors. . | Query API . | Query parameters | Request fields | Response fields | . | Explain API | Paginating results . | Example | . | Filtering results | Using parameters | . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/sql-ppl-api/",
    "relUrl": "/search-plugins/sql/sql-ppl-api/"
  },"990": {
    "doc": "SQL/PPL API",
    "title": "Query API",
    "content": "Introduced 1.0 . Sends an SQL/PPL query to the SQL plugin. You can pass the format for the response as a query parameter. Query parameters . | Parameter | Data Type | Description | . | format | String | The format for the response. The _sql endpoint supports jdbc, csv, raw, and json formats. The _ppl endpoint supports jdbc, csv, and raw formats. Default is jdbc. | . | sanitize | Boolean | Specifies whether to escape special characters in the results. See Response formats for more information. Default is true. | . Request fields . | Field | Data Type | Description | . | query | String | The query to be executed. Required. | . | filter | JSON object | The filter for the results. Optional. | . | fetch_size | integer | The number of results to return in one response. Used for paginating results. Default is 1,000. Optional. Only supported for the jdbc response format. | . Example request . POST /_plugins/_sql { \"query\" : \"SELECT * FROM accounts\" } . Example response . The response contains the schema and the results: . { \"schema\": [ { \"name\": \"account_number\", \"type\": \"long\" }, { \"name\": \"firstname\", \"type\": \"text\" }, { \"name\": \"address\", \"type\": \"text\" }, { \"name\": \"balance\", \"type\": \"long\" }, { \"name\": \"gender\", \"type\": \"text\" }, { \"name\": \"city\", \"type\": \"text\" }, { \"name\": \"employer\", \"type\": \"text\" }, { \"name\": \"state\", \"type\": \"text\" }, { \"name\": \"age\", \"type\": \"long\" }, { \"name\": \"email\", \"type\": \"text\" }, { \"name\": \"lastname\", \"type\": \"text\" } ], \"datarows\": [ [ 1, \"Amber\", \"880 Holmes Lane\", 39225, \"M\", \"Brogan\", \"Pyrami\", \"IL\", 32, \"amberduke@pyrami.com\", \"Duke\" ], [ 6, \"Hattie\", \"671 Bristol Street\", 5686, \"M\", \"Dante\", \"Netagy\", \"TN\", 36, \"hattiebond@netagy.com\", \"Bond\" ], [ 13, \"Nanette\", \"789 Madison Street\", 32838, \"F\", \"Nogal\", \"Quility\", \"VA\", 28, \"nanettebates@quility.com\", \"Bates\" ], [ 18, \"Dale\", \"467 Hutchinson Court\", 4180, \"M\", \"Orick\", null, \"MD\", 33, \"daleadams@boink.com\", \"Adams\" ] ], \"total\": 4, \"size\": 4, \"status\": 200 } . Response fields . | Field | Data Type | Description | . | schema | Array | Specifies the field names and types for all fields. | . | data_rows | 2D array | An array of results. Each result represents one matching row (document). | . | total | Integer | The total number of rows (documents) in the index. | . | size | Integer | The number of results to return in one response. | . | status | String | The HTTP response status OpenSearch returns after running the query. | . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/sql-ppl-api/#query-api",
    "relUrl": "/search-plugins/sql/sql-ppl-api/#query-api"
  },"991": {
    "doc": "SQL/PPL API",
    "title": "Explain API",
    "content": "The SQL plugin has an explain feature that shows how a query is executed against OpenSearch, which is useful for debugging and development. A POST request to the _plugins/_sql/_explain or _plugins/_ppl/_explain endpoint returns OpenSearch domain-specific language (DSL) in JSON format, explaining the query. You can execute the explain API operation either in command line using curl or in the Dashboards console, like in the example below. Sample explain request for an SQL query . POST _plugins/_sql/_explain { \"query\": \"SELECT firstname, lastname FROM accounts WHERE age &gt; 20\" } . Sample SQL query explain response . { \"root\": { \"name\": \"ProjectOperator\", \"description\": { \"fields\": \"[firstname, lastname]\" }, \"children\": [ { \"name\": \"OpenSearchIndexScan\", \"description\": { \"request\": \"\"\"OpenSearchQueryRequest(indexName=accounts, sourceBuilder={\"from\":0,\"size\":200,\"timeout\":\"1m\",\"query\":{\"range\":{\"age\":{\"from\":20,\"to\":null,\"include_lower\":false,\"include_upper\":true,\"boost\":1.0}}},\"_source\":{\"includes\":[\"firstname\",\"lastname\"],\"excludes\":[]},\"sort\":[{\"_doc\":{\"order\":\"asc\"}}]}, searchDone=false)\"\"\" }, \"children\": [] } ] } } . Sample explain request for a PPL query . POST _plugins/_ppl/_explain { \"query\" : \"source=accounts | fields firstname, lastname\" } . Sample PPL query explain response . { \"root\": { \"name\": \"ProjectOperator\", \"description\": { \"fields\": \"[firstname, lastname]\" }, \"children\": [ { \"name\": \"OpenSearchIndexScan\", \"description\": { \"request\": \"\"\"OpenSearchQueryRequest(indexName=accounts, sourceBuilder={\"from\":0,\"size\":200,\"timeout\":\"1m\",\"_source\":{\"includes\":[\"firstname\",\"lastname\"],\"excludes\":[]}}, searchDone=false)\"\"\" }, \"children\": [] } ] } } . For queries that require post-processing, the explain response includes a query plan in addition to the OpenSearch DSL. For those queries that don’t require post processing, you can see a complete DSL. ",
    "url": "https://vagimeli.github.io/search-plugins/sql/sql-ppl-api/#explain-api",
    "relUrl": "/search-plugins/sql/sql-ppl-api/#explain-api"
  },"992": {
    "doc": "SQL/PPL API",
    "title": "Paginating results",
    "content": "To get back a paginated response, use the fetch_size parameter. The value of fetch_size should be greater than 0. The default value is 1,000. A value of 0 will fall back to a non-paginated response. The fetch_size parameter is only supported for the jdbc response format. Example . The following request contains an SQL query and specifies to return five results at a time: . POST _plugins/_sql/ { \"fetch_size\" : 5, \"query\" : \"SELECT firstname, lastname FROM accounts WHERE age &gt; 20 ORDER BY state ASC\" } . The response contains all the fields that a query without fetch_size would contain, and a cursor field that is used to retrieve subsequent pages of results: . { \"schema\": [ { \"name\": \"firstname\", \"type\": \"text\" }, { \"name\": \"lastname\", \"type\": \"text\" } ], \"cursor\": \"d:eyJhIjp7fSwicyI6IkRYRjFaWEo1UVc1a1JtVjBZMmdCQUFBQUFBQUFBQU1XZWpkdFRFRkZUMlpTZEZkeFdsWnJkRlZoYnpaeVVRPT0iLCJjIjpbeyJuYW1lIjoiZmlyc3RuYW1lIiwidHlwZSI6InRleHQifSx7Im5hbWUiOiJsYXN0bmFtZSIsInR5cGUiOiJ0ZXh0In1dLCJmIjo1LCJpIjoiYWNjb3VudHMiLCJsIjo5NTF9\", \"total\": 956, \"datarows\": [ [ \"Cherry\", \"Carey\" ], [ \"Lindsey\", \"Hawkins\" ], [ \"Sargent\", \"Powers\" ], [ \"Campos\", \"Olsen\" ], [ \"Savannah\", \"Kirby\" ] ], \"size\": 5, \"status\": 200 } . To fetch subsequent pages, use the cursor from the previous response: . POST /_plugins/_sql { \"cursor\": \"d:eyJhIjp7fSwicyI6IkRYRjFaWEo1UVc1a1JtVjBZMmdCQUFBQUFBQUFBQU1XZWpkdFRFRkZUMlpTZEZkeFdsWnJkRlZoYnpaeVVRPT0iLCJjIjpbeyJuYW1lIjoiZmlyc3RuYW1lIiwidHlwZSI6InRleHQifSx7Im5hbWUiOiJsYXN0bmFtZSIsInR5cGUiOiJ0ZXh0In1dLCJmIjo1LCJpIjoiYWNjb3VudHMiLCJsIjo5NTF9\" } . The next response contains only the datarows of the results and a new cursor. { \"cursor\": \"d:eyJhIjp7fSwicyI6IkRYRjFaWEo1UVc1a1JtVjBZMmdCQUFBQUFBQUFBQU1XZWpkdFRFRkZUMlpTZEZkeFdsWnJkRlZoYnpaeVVRPT0iLCJjIjpbeyJuYW1lIjoiZmlyc3RuYW1lIiwidHlwZSI6InRleHQifSx7Im5hbWUiOiJsYXN0bmFtZSIsInR5cGUiOiJ0ZXh0In1dLCJmIjo1LCJpIjoiYWNjb3VudHMabcde12345\", \"datarows\": [ [ \"Abbey\", \"Karen\" ], [ \"Chen\", \"Ken\" ], [ \"Ani\", \"Jade\" ], [ \"Peng\", \"Hu\" ], [ \"John\", \"Doe\" ] ] } . The datarows can have more than the fetch_size number of records in case nested fields are flattened. The last page of results has only datarows and no cursor. The cursor context is automatically cleared on the last page. To explicitly clear the cursor context, use the _plugins/_sql/close endpoint operation: . POST /_plugins/_sql/close { \"cursor\": \"d:eyJhIjp7fSwicyI6IkRYRjFaWEo1UVc1a1JtVjBZMmdCQUFBQUFBQUFBQU1XZWpkdFRFRkZUMlpTZEZkeFdsWnJkRlZoYnpaeVVRPT0iLCJjIjpbeyJuYW1lIjoiZmlyc3RuYW1lIiwidHlwZSI6InRleHQifSx7Im5hbWUiOiJsYXN0bmFtZSIsInR5cGUiOiJ0ZXh0In1dLCJmIjo1LCJpIjoiYWNjb3VudHMiLCJsIjo5NTF9\" }' . The response is an acknowledgement from OpenSearch: . {\"succeeded\":true} . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/sql-ppl-api/#paginating-results",
    "relUrl": "/search-plugins/sql/sql-ppl-api/#paginating-results"
  },"993": {
    "doc": "SQL/PPL API",
    "title": "Filtering results",
    "content": "You can use the filter parameter to add more conditions to the OpenSearch DSL directly. The following SQL query returns the names and account balances of all customers. The results are then filtered to contain only those customers with less than $10,000 balance. POST /_plugins/_sql/ { \"query\" : \"SELECT firstname, lastname, balance FROM accounts\", \"filter\" : { \"range\" : { \"balance\" : { \"lt\" : 10000 } } } } . The response contains the matching results: . { \"schema\": [ { \"name\": \"firstname\", \"type\": \"text\" }, { \"name\": \"lastname\", \"type\": \"text\" }, { \"name\": \"balance\", \"type\": \"long\" } ], \"total\": 2, \"datarows\": [ [ \"Hattie\", \"Bond\", 5686 ], [ \"Dale\", \"Adams\", 4180 ] ], \"size\": 2, \"status\": 200 } . You can use the Explain API to see how this query is executed against OpenSearch: . POST /_plugins/_sql/_explain { \"query\" : \"SELECT firstname, lastname, balance FROM accounts\", \"filter\" : { \"range\" : { \"balance\" : { \"lt\" : 10000 } } } }' . The response contains the Boolean query in OpenSearch DSL that corresponds to the query above: . { \"from\": 0, \"size\": 200, \"query\": { \"bool\": { \"filter\": [{ \"bool\": { \"filter\": [{ \"range\": { \"balance\": { \"from\": null, \"to\": 10000, \"include_lower\": true, \"include_upper\": false, \"boost\": 1.0 } } }], \"adjust_pure_negative\": true, \"boost\": 1.0 } }], \"adjust_pure_negative\": true, \"boost\": 1.0 } }, \"_source\": { \"includes\": [ \"firstname\", \"lastname\", \"balance\" ], \"excludes\": [] } } . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/sql-ppl-api/#filtering-results",
    "relUrl": "/search-plugins/sql/sql-ppl-api/#filtering-results"
  },"994": {
    "doc": "SQL/PPL API",
    "title": "Using parameters",
    "content": "You can use the parameters field to pass parameter values to a prepared SQL query. The following explain operation uses an SQL query with an age parameter: . POST /_plugins/_sql/_explain { \"query\": \"SELECT * FROM accounts WHERE age = ?\", \"parameters\": [{ \"type\": \"integer\", \"value\": 30 }] } . The response contains the Boolean query in OpenSearch DSL that corresponds to the SQL query above: . { \"from\": 0, \"size\": 200, \"query\": { \"bool\": { \"filter\": [{ \"bool\": { \"must\": [{ \"term\": { \"age\": { \"value\": 30, \"boost\": 1.0 } } }], \"adjust_pure_negative\": true, \"boost\": 1.0 } }], \"adjust_pure_negative\": true, \"boost\": 1.0 } } } . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/sql-ppl-api/#using-parameters",
    "relUrl": "/search-plugins/sql/sql-ppl-api/#using-parameters"
  },"995": {
    "doc": "Aggregate Functions",
    "title": "Aggregate functions",
    "content": "Aggregate functions operate on subsets defined by the GROUP BY clause. In the absence of a GROUP BY clause, aggregate functions operate on all elements of the result set. You can use aggregate functions in the GROUP BY, SELECT, and HAVING clauses. OpenSearch supports the following aggregate functions. | Function | Description | . | AVG | Returns the average of the results. | . | COUNT | Returns the number of results. | . | SUM | Returns the sum of the results. | . | MIN | Returns the minimum of the results. | . | MAX | Returns the maximum of the results. | . | VAR_POP or VARIANCE | Returns the population variance of the results after discarding nulls. Returns 0 when there is only one row of results. | . | VAR_SAMP | Returns the sample variance of the results after discarding nulls. Returns null when there is only one row of results. | . | STD or STDDEV | Returns the sample standard deviation of the results. Returns 0 when there is only one row of results. | . | STDDEV_POP | Returns the population standard deviation of the results. Returns 0 when there is only one row of results. | . | STDDEV_SAMP | Returns the sample standard deviation of the results. Returns null when there is only one row of results. | . The examples below reference an employees table. You can try out the examples by indexing the following documents into OpenSearch using the bulk index operation: . PUT employees/_bulk?refresh {\"index\":{\"_id\":\"1\"}} {\"employee_id\": 1, \"department\":1, \"firstname\":\"Amber\", \"lastname\":\"Duke\", \"sales\":1356, \"sale_date\":\"2020-01-23\"} {\"index\":{\"_id\":\"2\"}} {\"employee_id\": 1, \"department\":1, \"firstname\":\"Amber\", \"lastname\":\"Duke\", \"sales\":39224, \"sale_date\":\"2021-01-06\"} {\"index\":{\"_id\":\"6\"}} {\"employee_id\":6, \"department\":1, \"firstname\":\"Hattie\", \"lastname\":\"Bond\", \"sales\":5686, \"sale_date\":\"2021-06-07\"} {\"index\":{\"_id\":\"7\"}} {\"employee_id\":6, \"department\":1, \"firstname\":\"Hattie\", \"lastname\":\"Bond\", \"sales\":12432, \"sale_date\":\"2022-05-18\"} {\"index\":{\"_id\":\"13\"}} {\"employee_id\":13,\"department\":2, \"firstname\":\"Nanette\", \"lastname\":\"Bates\", \"sales\":32838, \"sale_date\":\"2022-04-11\"} {\"index\":{\"_id\":\"18\"}} {\"employee_id\":18,\"department\":2, \"firstname\":\"Dale\", \"lastname\":\"Adams\", \"sales\":4180, \"sale_date\":\"2022-11-05\"} . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/sql/aggregations/#aggregate-functions",
    "relUrl": "/search-plugins/sql/sql/aggregations/#aggregate-functions"
  },"996": {
    "doc": "Aggregate Functions",
    "title": "GROUP BY",
    "content": "The GROUP BY clause defines subsets of a result set. Aggregate functions operate on these subsets and return one result row for each subset. You can use an identifier, ordinal, or expression in the GROUP BY clause. Using an identifier in GROUP BY . You can specify the field name (column name) to aggregate on in the GROUP BY clause. For example, the following query returns the department numbers and the total sales for each department: . SELECT department, sum(sales) FROM employees GROUP BY department; . | department | sum(sales) | . | 1 | 58700 | . | 2 | 37018 | . Using an ordinal in GROUP BY . You can specify the column number to aggregate on in the GROUP BY clause. The column number is determined by the column position in the SELECT clause. For example, the following query is equivalent to the query above. It returns the department numbers and the total sales for each department. It groups the results by the first column of the result set, which is department: . SELECT department, sum(sales) FROM employees GROUP BY 1; . | department | sum(sales) | . | 1 | 58700 | . | 2 | 37018 | . Using an expression in GROUP BY . You can use an expression in the GROUP BY clause. For example, the following query returns the average sales for each year: . SELECT year(sale_date), avg(sales) FROM employees GROUP BY year(sale_date); . | year(start_date) | avg(sales) | . | 2020 | 1356.0 | . | 2021 | 22455.0 | . | 2022 | 16484.0 | . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/sql/aggregations/#group-by",
    "relUrl": "/search-plugins/sql/sql/aggregations/#group-by"
  },"997": {
    "doc": "Aggregate Functions",
    "title": "SELECT",
    "content": "You can use aggregate expressions in the SELECT clause either directly or as part of a larger expression. In addition, you can use expressions as arguments of aggregate functions. Using aggregate expressions directly in SELECT . The following query returns the average sales for each department: . SELECT department, avg(sales) FROM employees GROUP BY department; . | department | avg(sales) | . | 1 | 14675.0 | . | 2 | 18509.0 | . Using aggregate expressions as part of larger expressions in SELECT . The following query calculates the average commission for the employees of each department as 5% of the average sales: . SELECT department, avg(sales) * 0.05 as avg_commission FROM employees GROUP BY department; . | department | avg_commission | . | 1 | 733.75 | . | 2 | 925.45 | . Using expressions as arguments to aggregate functions . The following query calculates the average commission amount for each department. First it calculates the commission amount for each sales value as 5% of the sales. Then it determines the average of all commission values: . SELECT department, avg(sales * 0.05) as avg_commission FROM employees GROUP BY department; . | department | avg_commission | . | 1 | 733.75 | . | 2 | 925.45 | . COUNT . The COUNT function accepts arguments, such as *, or literals, such as 1. The following table describes how various forms of the COUNT function operate. | Function type | Description | . | COUNT(field) | Counts the number of rows where the value of the given field (or expression) is not null. | . | COUNT(*) | Counts the total number of rows in a table. | . | COUNT(1) (same as COUNT(*)) | Counts any non-null literal. | . For example, the following query returns the count of sales for each year: . SELECT year(sale_date), count(sales) FROM employees GROUP BY year(sale_date); . | year(sale_date) | count(sales) | . | 2020 | 1 | . | 2021 | 2 | . | 2022 | 3 | . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/sql/aggregations/#select",
    "relUrl": "/search-plugins/sql/sql/aggregations/#select"
  },"998": {
    "doc": "Aggregate Functions",
    "title": "HAVING",
    "content": "Both WHERE and HAVING are used to filter results. The WHERE filter is applied before the GROUP BY phase, so you cannot use aggregate functions in a WHERE clause. However, you can use the WHERE clause to limit the rows to which the aggregate is then applied. The HAVING filter is applied after the GROUP BY phase, so you can use the HAVING clause to limit the groups that are included in the results. HAVING with GROUP BY . You can use aggregate expressions or their aliases defined in a SELECT clause in a HAVING condition. The following query uses an aggregate expression in the HAVING clause. It returns the number of sales for each employee who made more than one sale: . SELECT employee_id, count(sales) FROM employees GROUP BY employee_id HAVING count(sales) &gt; 1; . | employee_id | count(sales) | . | 1 | 2 | . | 6 | 2 | . The aggregations in a HAVING clause do not have to be the same as the aggregations in a SELECT list. The following query uses the count function in the HAVING clause but the sum function in the SELECT clause. It returns the total sales amount for each employee who made more than one sale: . SELECT employee_id, sum(sales) FROM employees GROUP BY employee_id HAVING count(sales) &gt; 1; . | employee_id | sum (sales) | . | 1 | 40580 | . | 6 | 18120 | . As an extension of the SQL standard, you are not restricted to using only identifiers in the GROUP BY clause. The following query uses an alias in the GROUP BY clause and is equivalent to the previous query: . SELECT employee_id as id, sum(sales) FROM employees GROUP BY id HAVING count(sales) &gt; 1; . | id | sum (sales) | . | 1 | 40580 | . | 6 | 18120 | . You can also use an alias for an aggregate expression in the HAVING clause. The following query returns the total sales for each department where sales exceed $40,000: . SELECT department, sum(sales) as total FROM employees GROUP BY department HAVING total &gt; 40000; . | department | total | . | 1 | 58700 | . If an identifier is ambiguous (for example, present both as a SELECT alias and as an index field), the preference is given to the alias. In the following query the identifier is replaced with the expression aliased in the SELECT clause: . SELECT department, sum(sales) as sales FROM employees GROUP BY department HAVING sales &gt; 40000; . | department | sales | . | 1 | 58700 | . HAVING without GROUP BY . You can use a HAVING clause without a GROUP BY clause. In this case, the whole set of data is to be considered one group. The following query will return True if there is more than one value in the department column: . SELECT 'True' as more_than_one_department FROM employees HAVING min(department) &lt; max(department); . | more_than_one_department | . | True | . If all employees in the employee table belonged to the same department, the result would contain zero rows: . | more_than_one_department | . |   | . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/sql/aggregations/#having",
    "relUrl": "/search-plugins/sql/sql/aggregations/#having"
  },"999": {
    "doc": "Aggregate Functions",
    "title": "Aggregate Functions",
    "content": " ",
    "url": "https://vagimeli.github.io/search-plugins/sql/sql/aggregations/",
    "relUrl": "/search-plugins/sql/sql/aggregations/"
  },"1000": {
    "doc": "Basic Queries",
    "title": "Basic queries",
    "content": "Use the SELECT clause, along with FROM, WHERE, GROUP BY, HAVING, ORDER BY, and LIMIT to search and aggregate data. Among these clauses, SELECT and FROM are required, as they specify which fields to retrieve and which indices to retrieve them from. All other clauses are optional. Use them according to your needs. Syntax . The complete syntax for searching and aggregating data is as follows: . SELECT [DISTINCT] (* | expression) [[AS] alias] [, ...] FROM index_name [WHERE predicates] [GROUP BY expression [, ...] [HAVING predicates]] [ORDER BY expression [IS [NOT] NULL] [ASC | DESC] [, ...]] [LIMIT [offset, ] size] . Fundamentals . Apart from the predefined keywords of SQL, the most basic elements are literal and identifiers. A literal is a numeric, string, date or boolean constant. An identifier is an OpenSearch index or field name. With arithmetic operators and SQL functions, use literals and identifiers to build complex expressions. Rule expressionAtom: . The expression in turn can be combined into a predicate with logical operator. Use a predicate in the WHERE and HAVING clause to filter out data by specific conditions. Rule expression: . Rule predicate: . Execution Order . These SQL clauses execute in an order different from how they appear: . FROM index WHERE predicates GROUP BY expressions HAVING predicates SELECT expressions ORDER BY expressions LIMIT size . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/sql/basic/#basic-queries",
    "relUrl": "/search-plugins/sql/sql/basic/#basic-queries"
  },"1001": {
    "doc": "Basic Queries",
    "title": "Select",
    "content": "Specify the fields to be retrieved. Syntax . Rule selectElements: . Rule selectElement: . Example 1: Use * to retrieve all fields in an index: . SELECT * FROM accounts . | account_number | firstname | gender | city | balance | employer | state | email | address | lastname | age | . | 1 | Amber | M | Brogan | 39225 | Pyrami | IL | amberduke@pyrami.com | 880 Holmes Lane | Duke | 32 | . | 16 | Hattie | M | Dante | 5686 | Netagy | TN | hattiebond@netagy.com | 671 Bristol Street | Bond | 36 | . | 13 | Nanette | F | Nogal | 32838 | Quility | VA | nanettebates@quility.com | 789 Madison Street | Bates | 28 | . | 18 | Dale | M | Orick | 4180 |   | MD | daleadams@boink.com | 467 Hutchinson Court | Adams | 33 | . Example 2: Use field name(s) to retrieve only specific fields: . SELECT firstname, lastname FROM accounts . | firstname | lastname | . | Amber | Duke | . | Hattie | Bond | . | Nanette | Bates | . | Dale | Adams | . Example 3: Use field aliases instead of field names. Field aliases are used to make field names more readable: . SELECT account_number AS num FROM accounts . | num :— | 1 | 6 | 13 | 18 . Example 4: Use the DISTINCT clause to get back only unique field values. You can specify one or more field names: . SELECT DISTINCT age FROM accounts . | age :— | 28 | 32 | 33 | 36 . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/sql/basic/#select",
    "relUrl": "/search-plugins/sql/sql/basic/#select"
  },"1002": {
    "doc": "Basic Queries",
    "title": "From",
    "content": "Specify the index that you want search. You can specify subqueries within the FROM clause. Syntax . Rule tableName: . Example 1: Use index aliases to query across indexes. To learn about index aliases, see Index Alias. In this sample query, acc is an alias for the accounts index: . SELECT account_number, accounts.age FROM accounts . or . SELECT account_number, acc.age FROM accounts acc . | account_number | age | . | 1 | 32 | . | 6 | 36 | . | 13 | 28 | . | 18 | 33 | . Example 2: Use index patterns to query indices that match a specific pattern: . SELECT account_number FROM account* . | account_number :— | 1 | 6 | 13 | 18 . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/sql/basic/#from",
    "relUrl": "/search-plugins/sql/sql/basic/#from"
  },"1003": {
    "doc": "Basic Queries",
    "title": "Where",
    "content": "Specify a condition to filter the results. | Operators | Behavior | . | = | Equal to. | . | &lt;&gt; | Not equal to. | . | &gt; | Greater than. | . | &lt; | Less than. | . | &gt;= | Greater than or equal to. | . | &lt;= | Less than or equal to. | . | IN | Specify multiple OR operators. | . | BETWEEN | Similar to a range query. For more information about range queries, see Range query. | . | LIKE | Use for full-text search. For more information about full-text queries, see Full-text queries. | . | IS NULL | Check if the field value is NULL. | . | IS NOT NULL | Check if the field value is NOT NULL. | . Combine comparison operators (=, &lt;&gt;, &gt;, &gt;=, &lt;, &lt;=) with boolean operators NOT, AND, or OR to build more complex expressions. Example 1: Use comparison operators for numbers, strings, or dates: . SELECT account_number FROM accounts WHERE account_number = 1 . | account_number | . | 1 | . Example 2: OpenSearch allows for flexible schema， so documents in an index may have different fields. Use IS NULL or IS NOT NULL to retrieve only missing fields or existing fields. We do not differentiate between missing fields and fields explicitly set to NULL: . SELECT account_number, employer FROM accounts WHERE employer IS NULL . | account_number | employer | . | 18 |   | . Example 3: Deletes a document that satisfies the predicates in the WHERE clause: . DELETE FROM accounts WHERE age &gt; 30 . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/sql/basic/#where",
    "relUrl": "/search-plugins/sql/sql/basic/#where"
  },"1004": {
    "doc": "Basic Queries",
    "title": "Group By",
    "content": "Group documents with the same field value into buckets. Example 1: Group by fields: . SELECT age FROM accounts GROUP BY age . | id | age | . | 0 | 28 | . | 1 | 32 | . | 2 | 33 | . | 3 | 36 | . Example 2: Group by field alias: . SELECT account_number AS num FROM accounts GROUP BY num . | id | num | . | 0 | 1 | . | 1 | 6 | . | 2 | 13 | . | 3 | 18 | . Example 4: Use scalar functions in the GROUP BY clause: . SELECT ABS(age) AS a FROM accounts GROUP BY ABS(age) . | id | a | . | 0 | 28.0 | . | 1 | 32.0 | . | 2 | 33.0 | . | 3 | 36.0 | . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/sql/basic/#group-by",
    "relUrl": "/search-plugins/sql/sql/basic/#group-by"
  },"1005": {
    "doc": "Basic Queries",
    "title": "Having",
    "content": "Use the HAVING clause to aggregate inside each bucket based on aggregation functions (COUNT, AVG, SUM, MIN, and MAX). The HAVING clause filters results from the GROUP BY clause: . Example 1: . SELECT age, MAX(balance) FROM accounts GROUP BY age HAVING MIN(balance) &gt; 10000 . | id | age | MAX (balance) | . | 0 | 28 | 32838 | . | 1 | 32 | 39225 | . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/sql/basic/#having",
    "relUrl": "/search-plugins/sql/sql/basic/#having"
  },"1006": {
    "doc": "Basic Queries",
    "title": "Order By",
    "content": "Use the ORDER BY clause to sort results into your desired order. Example 1: Use ORDER BY to sort by ascending or descending order. Besides regular field names, using ordinal, alias, or scalar functions are supported: . SELECT account_number FROM accounts ORDER BY account_number DESC . | account_number | . | 18 | . | 13 | . | 6 | . | 1 | . Example 2: Specify if documents with missing fields are to be put at the beginning or at the end of the results. The default behavior of OpenSearch is to return nulls or missing fields at the end. To push them before non-nulls, use the IS NOT NULL operator: . SELECT employer FROM accounts ORDER BY employer IS NOT NULL . | employer | . |   | . | Netagy | . | Pyrami | . | Quility | . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/sql/basic/#order-by",
    "relUrl": "/search-plugins/sql/sql/basic/#order-by"
  },"1007": {
    "doc": "Basic Queries",
    "title": "Limit",
    "content": "Specify the maximum number of documents that you want to retrieve. Used to prevent fetching large amounts of data into memory. Example 1: If you pass in a single argument, it’s mapped to the size parameter in OpenSearch and the from parameter is set to 0. SELECT account_number FROM accounts ORDER BY account_number LIMIT 1 . | account_number | . | 1 | . Example 2: If you pass in two arguments, the first is mapped to the from parameter and the second to the size parameter in OpenSearch. You can use this for simple pagination for small indices, as it’s inefficient for large indices. Use ORDER BY to ensure the same order between pages: . SELECT account_number FROM accounts ORDER BY account_number LIMIT 1, 1 . | account_number | . | 6 | . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/sql/basic/#limit",
    "relUrl": "/search-plugins/sql/sql/basic/#limit"
  },"1008": {
    "doc": "Basic Queries",
    "title": "Basic Queries",
    "content": " ",
    "url": "https://vagimeli.github.io/search-plugins/sql/sql/basic/",
    "relUrl": "/search-plugins/sql/sql/basic/"
  },"1009": {
    "doc": "Complex Queries",
    "title": "Complex queries",
    "content": "Besides simple SFW (SELECT-FROM-WHERE) queries, the SQL plugin supports complex queries such as subquery, join, union, and minus. These queries operate on more than one OpenSearch index. To examine how these queries execute behind the scenes, use the explain operation. ",
    "url": "https://vagimeli.github.io/search-plugins/sql/sql/complex/#complex-queries",
    "relUrl": "/search-plugins/sql/sql/complex/#complex-queries"
  },"1010": {
    "doc": "Complex Queries",
    "title": "Joins",
    "content": "OpenSearch SQL supports inner joins, cross joins, and left outer joins. Constraints . Joins have a number of constraints: . | You can only join two indices. | You must use aliases for indices (e.g. people p). | Within an ON clause, you can only use AND conditions. | In a WHERE statement, don’t combine trees that contain multiple indices. For example, the following statement works: . WHERE (a.type1 &gt; 3 OR a.type1 &lt; 0) AND (b.type2 &gt; 4 OR b.type2 &lt; -1) . The following statement does not: . WHERE (a.type1 &gt; 3 OR b.type2 &lt; 0) AND (a.type1 &gt; 4 OR b.type2 &lt; -1) . | You can’t use GROUP BY or ORDER BY for results. | LIMIT with OFFSET (e.g. LIMIT 25 OFFSET 25) is not supported. | . Description . The JOIN clause combines columns from one or more indices using values common to each. Syntax . Rule tableSource: . Rule joinPart: . Example 1: Inner join . Inner join creates a new result set by combining columns of two indices based on your join predicates. It iterates the two indices and compares each document to find the ones that satisfy the join predicates. You can optionally precede the JOIN clause with an INNER keyword. The join predicate(s) is specified by the ON clause. SQL query: . SELECT a.account_number, a.firstname, a.lastname, e.id, e.name FROM accounts a JOIN employees_nested e ON a.account_number = e.id . Explain: . The explain output is complicated, because a JOIN clause is associated with two OpenSearch DSL queries that execute in separate query planner frameworks. You can interpret it by examining the Physical Plan and Logical Plan objects. { \"Physical Plan\" : { \"Project [ columns=[a.account_number, a.firstname, a.lastname, e.name, e.id] ]\" : { \"Top [ count=200 ]\" : { \"BlockHashJoin[ conditions=( a.account_number = e.id ), type=JOIN, blockSize=[FixedBlockSize with size=10000] ]\" : { \"Scroll [ employees_nested as e, pageSize=10000 ]\" : { \"request\" : { \"size\" : 200, \"from\" : 0, \"_source\" : { \"excludes\" : [ ], \"includes\" : [ \"id\", \"name\" ] } } }, \"Scroll [ accounts as a, pageSize=10000 ]\" : { \"request\" : { \"size\" : 200, \"from\" : 0, \"_source\" : { \"excludes\" : [ ], \"includes\" : [ \"account_number\", \"firstname\", \"lastname\" ] } } }, \"useTermsFilterOptimization\" : false } } } }, \"description\" : \"Hash Join algorithm builds hash table based on result of first query, and then probes hash table to find matched rows for each row returned by second query\", \"Logical Plan\" : { \"Project [ columns=[a.account_number, a.firstname, a.lastname, e.name, e.id] ]\" : { \"Top [ count=200 ]\" : { \"Join [ conditions=( a.account_number = e.id ) type=JOIN ]\" : { \"Group\" : [ { \"Project [ columns=[a.account_number, a.firstname, a.lastname] ]\" : { \"TableScan\" : { \"tableAlias\" : \"a\", \"tableName\" : \"accounts\" } } }, { \"Project [ columns=[e.name, e.id] ]\" : { \"TableScan\" : { \"tableAlias\" : \"e\", \"tableName\" : \"employees_nested\" } } } ] } } } } } . Result set: . | a.account_number | a.firstname | a.lastname | e.id | e.name | . | 6 | Hattie | Bond | 6 | Jane Smith | . Example 2: Cross join . Cross join, also known as cartesian join, combines each document from the first index with each document from the second. The result set is the the cartesian product of documents of both indices. This operation is similar to the inner join without the ON clause that specifies the join condition. It’s risky to perform cross join on two indices of large or even medium size. It might trigger a circuit breaker that terminates the query to avoid running out of memory. SQL query: . SELECT a.account_number, a.firstname, a.lastname, e.id, e.name FROM accounts a JOIN employees_nested e . Result set: . | a.account_number | a.firstname | a.lastname | e.id | e.name | . | 1 | Amber | Duke | 3 | Bob Smith | . | 1 | Amber | Duke | 4 | Susan Smith | . | 1 | Amber | Duke | 6 | Jane Smith | . | 6 | Hattie | Bond | 3 | Bob Smith | . | 6 | Hattie | Bond | 4 | Susan Smith | . | 6 | Hattie | Bond | 6 | Jane Smith | . | 13 | Nanette | Bates | 3 | Bob Smith | . | 13 | Nanette | Bates | 4 | Susan Smith | . | 13 | Nanette | Bates | 6 | Jane Smith | . | 18 | Dale | Adams | 3 | Bob Smith | . | 18 | Dale | Adams | 4 | Susan Smith | . | 18 | Dale | Adams | 6 | Jane Smith | . Example 3: Left outer join . Use left outer join to retain rows from the first index if it does not satisfy the join predicate. The keyword OUTER is optional. SQL query: . SELECT a.account_number, a.firstname, a.lastname, e.id, e.name FROM accounts a LEFT JOIN employees_nested e ON a.account_number = e.id . Result set: . | a.account_number | a.firstname | a.lastname | e.id | e.name | . | 1 | Amber | Duke | null | null | . | 6 | Hattie | Bond | 6 | Jane Smith | . | 13 | Nanette | Bates | null | null | . | 18 | Dale | Adams | null | null | . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/sql/complex/#joins",
    "relUrl": "/search-plugins/sql/sql/complex/#joins"
  },"1011": {
    "doc": "Complex Queries",
    "title": "Subquery",
    "content": "A subquery is a complete SELECT statement used within another statement and enclosed in parenthesis. From the explain output, you can see that some subqueries are actually transformed to an equivalent join query to execute. Example 1: Table subquery . SQL query: . SELECT a1.firstname, a1.lastname, a1.balance FROM accounts a1 WHERE a1.account_number IN ( SELECT a2.account_number FROM accounts a2 WHERE a2.balance &gt; 10000 ) . Explain: . { \"Physical Plan\" : { \"Project [ columns=[a1.balance, a1.firstname, a1.lastname] ]\" : { \"Top [ count=200 ]\" : { \"BlockHashJoin[ conditions=( a1.account_number = a2.account_number ), type=JOIN, blockSize=[FixedBlockSize with size=10000] ]\" : { \"Scroll [ accounts as a2, pageSize=10000 ]\" : { \"request\" : { \"size\" : 200, \"query\" : { \"bool\" : { \"filter\" : [ { \"bool\" : { \"adjust_pure_negative\" : true, \"must\" : [ { \"bool\" : { \"adjust_pure_negative\" : true, \"must\" : [ { \"bool\" : { \"adjust_pure_negative\" : true, \"must_not\" : [ { \"bool\" : { \"adjust_pure_negative\" : true, \"must_not\" : [ { \"exists\" : { \"field\" : \"account_number\", \"boost\" : 1 } } ], \"boost\" : 1 } } ], \"boost\" : 1 } }, { \"range\" : { \"balance\" : { \"include_lower\" : false, \"include_upper\" : true, \"from\" : 10000, \"boost\" : 1, \"to\" : null } } } ], \"boost\" : 1 } } ], \"boost\" : 1 } } ], \"adjust_pure_negative\" : true, \"boost\" : 1 } }, \"from\" : 0 } }, \"Scroll [ accounts as a1, pageSize=10000 ]\" : { \"request\" : { \"size\" : 200, \"from\" : 0, \"_source\" : { \"excludes\" : [ ], \"includes\" : [ \"firstname\", \"lastname\", \"balance\", \"account_number\" ] } } }, \"useTermsFilterOptimization\" : false } } } }, \"description\" : \"Hash Join algorithm builds hash table based on result of first query, and then probes hash table to find matched rows for each row returned by second query\", \"Logical Plan\" : { \"Project [ columns=[a1.balance, a1.firstname, a1.lastname] ]\" : { \"Top [ count=200 ]\" : { \"Join [ conditions=( a1.account_number = a2.account_number ) type=JOIN ]\" : { \"Group\" : [ { \"Project [ columns=[a1.balance, a1.firstname, a1.lastname, a1.account_number] ]\" : { \"TableScan\" : { \"tableAlias\" : \"a1\", \"tableName\" : \"accounts\" } } }, { \"Project [ columns=[a2.account_number] ]\" : { \"Filter [ conditions=[AND ( AND account_number ISN null, AND balance GT 10000 ) ] ]\" : { \"TableScan\" : { \"tableAlias\" : \"a2\", \"tableName\" : \"accounts\" } } } } ] } } } } } . Result set: . | a1.firstname | a1.lastname | a1.balance | . | Amber | Duke | 39225 | . | Nanette | Bates | 32838 | . Example 2: From subquery . SQL query: . SELECT a.f, a.l, a.a FROM ( SELECT firstname AS f, lastname AS l, age AS a FROM accounts WHERE age &gt; 30 ) AS a . Explain: . { \"from\" : 0, \"size\" : 200, \"query\" : { \"bool\" : { \"filter\" : [ { \"bool\" : { \"must\" : [ { \"range\" : { \"age\" : { \"from\" : 30, \"to\" : null, \"include_lower\" : false, \"include_upper\" : true, \"boost\" : 1.0 } } } ], \"adjust_pure_negative\" : true, \"boost\" : 1.0 } } ], \"adjust_pure_negative\" : true, \"boost\" : 1.0 } }, \"_source\" : { \"includes\" : [ \"firstname\", \"lastname\", \"age\" ], \"excludes\" : [ ] } } . Result set: . | f | l | a | . | Amber | Duke | 32 | . | Dale | Adams | 33 | . | Hattie | Bond | 36 | . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/sql/complex/#subquery",
    "relUrl": "/search-plugins/sql/sql/complex/#subquery"
  },"1012": {
    "doc": "Complex Queries",
    "title": "Complex Queries",
    "content": " ",
    "url": "https://vagimeli.github.io/search-plugins/sql/sql/complex/",
    "relUrl": "/search-plugins/sql/sql/complex/"
  },"1013": {
    "doc": "Delete",
    "title": "Delete",
    "content": "The DELETE statement deletes documents that satisfy the predicates in the WHERE clause. If you don’t specify the WHERE clause, all documents are deleted. Setting . The DELETE statement is disabled by default. To enable the DELETE functionality in SQL, you need to update the configuration by sending the following request: . PUT _plugins/_query/settings { \"transient\": { \"plugins.sql.delete.enabled\": \"true\" } } . Syntax . Rule singleDeleteStatement: . Example . SQL query: . DELETE FROM accounts WHERE age &gt; 30 . Explain: . { \"size\" : 1000, \"query\" : { \"bool\" : { \"must\" : [ { \"range\" : { \"age\" : { \"from\" : 30, \"to\" : null, \"include_lower\" : false, \"include_upper\" : true, \"boost\" : 1.0 } } } ], \"adjust_pure_negative\" : true, \"boost\" : 1.0 } }, \"_source\" : false } . Result set: . { \"schema\" : [ { \"name\" : \"deleted_rows\", \"type\" : \"long\" } ], \"total\" : 1, \"datarows\" : [ [ 3 ] ], \"size\" : 1, \"status\" : 200 } . The datarows field shows the number of documents deleted. ",
    "url": "https://vagimeli.github.io/search-plugins/sql/sql/delete/",
    "relUrl": "/search-plugins/sql/sql/delete/"
  },"1014": {
    "doc": "Functions",
    "title": "Functions",
    "content": "The SQL language supports all SQL plugin common functions, including relevance search, but also introduces a few function synonyms, which are available in SQL only. These synonyms are provided by the V1 engine. For more information, see Limitations. ",
    "url": "https://vagimeli.github.io/search-plugins/sql/sql/functions/",
    "relUrl": "/search-plugins/sql/sql/functions/"
  },"1015": {
    "doc": "Functions",
    "title": "Match query",
    "content": "The MATCHQUERY and MATCH_QUERY functions are synonyms for the MATCH relevance function. They don’t accept additional arguments but provide an alternate syntax. Syntax . To use matchquery or match_query, pass in your search query and the field name that you want to search against: . match_query(field_expression, query_expression[, option=&lt;option_value&gt;]*) matchquery(field_expression, query_expression[, option=&lt;option_value&gt;]*) field_expression = match_query(query_expression[, option=&lt;option_value&gt;]*) field_expression = matchquery(query_expression[, option=&lt;option_value&gt;]*) . You can specify the following options in any order: . | analyzer | boost | . Example . You can use MATCHQUERY to replace MATCH: . SELECT account_number, address FROM accounts WHERE MATCHQUERY(address, 'Holmes') . Alternatively, you can use MATCH_QUERY to replace MATCH: . SELECT account_number, address FROM accounts WHERE address = MATCH_QUERY('Holmes') . The results contain documents in which the address contains “Holmes”: . | account_number | address | . | 1 | 880 Holmes Lane | . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/sql/functions/#match-query",
    "relUrl": "/search-plugins/sql/sql/functions/#match-query"
  },"1016": {
    "doc": "Functions",
    "title": "Multi-match",
    "content": "There are three synonyms for MULTI_MATCH, each with a slightly different syntax. They accept a query string and a fields list with weights. They can also accept additional optional parameters. Syntax . multimatch('query'=query_expression[, 'fields'=field_expression][, option=&lt;option_value&gt;]*) multi_match('query'=query_expression[, 'fields'=field_expression][, option=&lt;option_value&gt;]*) multimatchquery('query'=query_expression[, 'fields'=field_expression][, option=&lt;option_value&gt;]*) . The fields parameter is optional and can contain a single field or a comma-separated list (whitespace characters are not allowed). The weight for each field is optional and is specified after the field name. It should be delimited by the caret character – ^ – without whitespace. Example . The following queries show the fields parameter of a multi-match query with a single field and a field list: . multi_match('fields' = \"Tags^2,Title^3.4,Body,Comments^0.3\", ...) multi_match('fields' = \"Title\", ...) . You can specify the following options in any order: . | analyzer | boost | slop | type | tie_breaker | operator | . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/sql/functions/#multi-match",
    "relUrl": "/search-plugins/sql/sql/functions/#multi-match"
  },"1017": {
    "doc": "Functions",
    "title": "Query string",
    "content": "The QUERY function is a synonym for QUERY_STRING. Syntax . query('query'=query_expression[, 'fields'=field_expression][, option=&lt;option_value&gt;]*) . The fields parameter is optional and can contain a single field or a comma-separated list (whitespace characters are not allowed). The weight for each field is optional and is specified after the field name. It should be delimited by the caret character – ^ – without whitespace. Example . The following queries show the fields parameter of a multi-match query with a single field and a field list: . query('fields' = \"Tags^2,Title^3.4,Body,Comments^0.3\", ...) query('fields' = \"Tags\", ...) . You can specify the following options in any order: . | analyzer | boost | slop | default_field | . Example of using query_string in SQL and PPL queries: . The following is a sample REST API search request in OpenSearch DSL. GET accounts/_search { \"query\": { \"query_string\": { \"query\": \"Lane Street\", \"fields\": [ \"address\" ], } } } . The request above is equivalent to the following query function: . SELECT account_number, address FROM accounts WHERE query('address:Lane OR address:Street') . The results contain addresses that contain “Lane” or “Street”: . | account_number | address | . | 1 | 880 Holmes Lane | . | 6 | 671 Bristol Street | . | 13 | 789 Madison Street | . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/sql/functions/#query-string",
    "relUrl": "/search-plugins/sql/sql/functions/#query-string"
  },"1018": {
    "doc": "Functions",
    "title": "Match phrase",
    "content": "The MATCHPHRASEQUERY function is a synonym for MATCH_PHRASE. Syntax . matchphrasequery(query_expression, field_expression[, option=&lt;option_value&gt;]*) . You can specify the following options in any order: . | analyzer | boost | slop | . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/sql/functions/#match-phrase",
    "relUrl": "/search-plugins/sql/sql/functions/#match-phrase"
  },"1019": {
    "doc": "Functions",
    "title": "Score query",
    "content": "To return a relevance score along with every matching document, use the SCORE, SCOREQUERY, or SCORE_QUERY functions. Syntax . The SCORE function expects two arguments. The first argument is the MATCH_QUERY expression. The second argument is an optional floating-point number to boost the score (the default value is 1.0): . SCORE(match_query_expression, score) SCOREQUERY(match_query_expression, score) SCORE_QUERY(match_query_expression, score) . Example . The following example uses the SCORE function to boost the documents’ scores: . SELECT account_number, address, _score FROM accounts WHERE SCORE(MATCH_QUERY(address, 'Lane'), 0.5) OR SCORE(MATCH_QUERY(address, 'Street'), 100) ORDER BY _score . The results contain matches with corresponding scores: . | account_number | address | score | . | 1 | 880 Holmes Lane | 0.5 | . | 6 | 671 Bristol Street | 100 | . | 13 | 789 Madison Street | 100 | . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/sql/functions/#score-query",
    "relUrl": "/search-plugins/sql/sql/functions/#score-query"
  },"1020": {
    "doc": "Functions",
    "title": "Wildcard query",
    "content": "To search documents by a given wildcard, use the WILDCARDQUERY or WILDCARD_QUERY functions. Syntax . wildcardquery(field_expression, query_expression[, boost=&lt;value&gt;]) wildcard_query(field_expression, query_expression[, boost=&lt;value&gt;]) . Example . The following example uses a wildcard query: . SELECT account_number, address FROM accounts WHERE wildcard_query(address, '*Holmes*'); . The results contain documents that match the wildcard expression: . | account_number | address | . | 1 | 880 Holmes Lane | . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/sql/functions/#wildcard-query",
    "relUrl": "/search-plugins/sql/sql/functions/#wildcard-query"
  },"1021": {
    "doc": "SQL",
    "title": "SQL",
    "content": " ",
    "url": "https://vagimeli.github.io/search-plugins/sql/sql/index/",
    "relUrl": "/search-plugins/sql/sql/index/"
  },"1022": {
    "doc": "SQL",
    "title": "Workbench",
    "content": "The easiest way to get familiar with the SQL plugin is to use Query Workbench in OpenSearch Dashboards to test various queries. To learn more, see Workbench. ",
    "url": "https://vagimeli.github.io/search-plugins/sql/sql/index/#workbench",
    "relUrl": "/search-plugins/sql/sql/index/#workbench"
  },"1023": {
    "doc": "SQL",
    "title": "SQL and OpenSearch terminology",
    "content": "Here’s how core SQL concepts map to OpenSearch: . | SQL | OpenSearch | . | Table | Index | . | Row | Document | . | Column | Field | . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/sql/index/#sql-and-opensearch-terminology",
    "relUrl": "/search-plugins/sql/sql/index/#sql-and-opensearch-terminology"
  },"1024": {
    "doc": "SQL",
    "title": "REST API",
    "content": "For a complete REST API reference for the SQL plugin, see SQL/PPL API. To use the SQL plugin with your own applications, send requests to the _plugins/_sql endpoint: . POST _plugins/_sql { \"query\": \"SELECT * FROM my-index LIMIT 50\" } . You can query multiple indexes by using a comma-separated list: . POST _plugins/_sql { \"query\": \"SELECT * FROM my-index1,myindex2,myindex3 LIMIT 50\" } . You can also specify an index pattern with a wildcard expression: . POST _plugins/_sql { \"query\": \"SELECT * FROM my-index* LIMIT 50\" } . To run the above query in the command line, use the curl command: . curl -XPOST https://localhost:9200/_plugins/_sql -u 'admin:admin' -k -H 'Content-Type: application/json' -d '{\"query\": \"SELECT * FROM my-index* LIMIT 50\"}' . You can specify the response format as JDBC, standard OpenSearch JSON, CSV, or raw. By default, queries return data in JDBC format. The following query sets the format to JSON: . POST _plugins/_sql?format=json { \"query\": \"SELECT * FROM my-index LIMIT 50\" } . See the rest of this guide for more information about request parameters, settings, supported operations, and tools. ",
    "url": "https://vagimeli.github.io/search-plugins/sql/sql/index/#rest-api",
    "relUrl": "/search-plugins/sql/sql/index/#rest-api"
  },"1025": {
    "doc": "JDBC Driver",
    "title": "JDBC driver",
    "content": "The Java Database Connectivity (JDBC) driver lets you integrate OpenSearch with your favorite business intelligence (BI) applications. For information on downloading and using the JAR file, see the SQL repository on GitHub. ",
    "url": "https://vagimeli.github.io/search-plugins/sql/sql/jdbc/#jdbc-driver",
    "relUrl": "/search-plugins/sql/sql/jdbc/#jdbc-driver"
  },"1026": {
    "doc": "JDBC Driver",
    "title": "Connecting to Tableau",
    "content": "To connect to Tableau, follow the detailed instructions in the GitHub repository. ",
    "url": "https://vagimeli.github.io/search-plugins/sql/sql/jdbc/#connecting-to-tableau",
    "relUrl": "/search-plugins/sql/sql/jdbc/#connecting-to-tableau"
  },"1027": {
    "doc": "JDBC Driver",
    "title": "JDBC Driver",
    "content": " ",
    "url": "https://vagimeli.github.io/search-plugins/sql/sql/jdbc/",
    "relUrl": "/search-plugins/sql/sql/jdbc/"
  },"1028": {
    "doc": "Metadata Queries",
    "title": "Metadata queries",
    "content": "To see basic metadata about your indices, use the SHOW and DESCRIBE commands. Syntax . Rule showStatement: . Rule showFilter: . Example 1: See metadata for indices . To see metadata for indices that match a specific pattern, use the SHOW command. Use the wildcard % to match all indices: . SHOW TABLES LIKE % . | TABLE_CAT | TABLE_SCHEM | TABLE_NAME | TABLE_TYPE | REMARKS | TYPE_CAT | TYPE_SCHEM | TYPE_NAME | SELF_REFERENCING_COL_NAME | REF_GENERATION | . | docker-cluster | null | accounts | BASE TABLE | null | null | null | null | null | null | . | docker-cluster | null | employees_nested | BASE TABLE | null | null | null | null | null | null | . Example 2: See metadata for a specific index . To see metadata for an index name with a prefix of acc: . SHOW TABLES LIKE acc% . | TABLE_CAT | TABLE_SCHEM | TABLE_NAME | TABLE_TYPE | REMARKS | TYPE_CAT | TYPE_SCHEM | TYPE_NAME | SELF_REFERENCING_COL_NAME | REF_GENERATION | . | docker-cluster | null | accounts | BASE TABLE | null | null | null | null | null | null | . Example 3: See metadata for fields . To see metadata for field names that match a specific pattern, use the DESCRIBE command: . DESCRIBE TABLES LIKE accounts . | TABLE_CAT | TABLE_SCHEM | TABLE_NAME | COLUMN_NAME | DATA_TYPE | TYPE_NAME | COLUMN_SIZE | BUFFER_LENGTH | DECIMAL_DIGITS | NUM_PREC_RADIX | NULLABLE | REMARKS | COLUMN_DEF | SQL_DATA_TYPE | SQL_DATETIME_SUB | CHAR_OCTET_LENGTH | ORDINAL_POSITION | IS_NULLABLE | SCOPE_CATALOG | SCOPE_SCHEMA | SCOPE_TABLE | SOURCE_DATA_TYPE | IS_AUTOINCREMENT | IS_GENERATEDCOLUMN | . | docker-cluster | null | accounts | account_number | null | long | null | null | null | 10 | 2 | null | null | null | null | null | 1 |   | null | null | null | null | NO |   | . | docker-cluster | null | accounts | firstname | null | text | null | null | null | 10 | 2 | null | null | null | null | null | 2 |   | null | null | null | null | NO |   | . | docker-cluster | null | accounts | address | null | text | null | null | null | 10 | 2 | null | null | null | null | null | 3 |   | null | null | null | null | NO |   | . | docker-cluster | null | accounts | balance | null | long | null | null | null | 10 | 2 | null | null | null | null | null | 4 |   | null | null | null | null | NO |   | . | docker-cluster | null | accounts | gender | null | text | null | null | null | 10 | 2 | null | null | null | null | null | 5 |   | null | null | null | null | NO |   | . | docker-cluster | null | accounts | city | null | text | null | null | null | 10 | 2 | null | null | null | null | null | 6 |   | null | null | null | null | NO |   | . | docker-cluster | null | accounts | employer | null | text | null | null | null | 10 | 2 | null | null | null | null | null | 7 |   | null | null | null | null | NO |   | . | docker-cluster | null | accounts | state | null | text | null | null | null | 10 | 2 | null | null | null | null | null | 8 |   | null | null | null | null | NO |   | . | docker-cluster | null | accounts | age | null | long | null | null | null | 10 | 2 | null | null | null | null | null | 9 |   | null | null | null | null | NO |   | . | docker-cluster | null | accounts | email | null | text | null | null | null | 10 | 2 | null | null | null | null | null | 10 |   | null | null | null | null | NO |   | . | docker-cluster | null | accounts | lastname | null | text | null | null | null | 10 | 2 | null | null | null | null | null | 11 |   | null | null | null | null | NO |   | . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/sql/metadata/#metadata-queries",
    "relUrl": "/search-plugins/sql/sql/metadata/#metadata-queries"
  },"1029": {
    "doc": "Metadata Queries",
    "title": "Metadata Queries",
    "content": " ",
    "url": "https://vagimeli.github.io/search-plugins/sql/sql/metadata/",
    "relUrl": "/search-plugins/sql/sql/metadata/"
  },"1030": {
    "doc": "ODBC Driver",
    "title": "ODBC driver",
    "content": "The Open Database Connectivity (ODBC) driver is a read-only ODBC driver for Windows and macOS that lets you connect business intelligence (BI) and data visualization applications like Microsoft Excel and Power BI to the SQL plugin. For information on downloading and using the driver, see the SQL repository on GitHub. ",
    "url": "https://vagimeli.github.io/search-plugins/sql/sql/odbc/#odbc-driver",
    "relUrl": "/search-plugins/sql/sql/odbc/#odbc-driver"
  },"1031": {
    "doc": "ODBC Driver",
    "title": "Specifications",
    "content": "The ODBC driver is compatible with ODBC version 3.51. ",
    "url": "https://vagimeli.github.io/search-plugins/sql/sql/odbc/#specifications",
    "relUrl": "/search-plugins/sql/sql/odbc/#specifications"
  },"1032": {
    "doc": "ODBC Driver",
    "title": "Supported OS versions",
    "content": "The following operating systems are supported: . | Operating System | Version | . | Windows | Windows 10, Windows 11 | . | macOS | Catalina 10.15.4, Mojave 10.14.6, Big Sur 11.6.7, Monterey 12.4 | . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/sql/odbc/#supported-os-versions",
    "relUrl": "/search-plugins/sql/sql/odbc/#supported-os-versions"
  },"1033": {
    "doc": "ODBC Driver",
    "title": "Concepts",
    "content": "| Term | Definition | . | DSN | A DSN (Data Source Name) is used to store driver information in the system. By storing the information in the system, the information does not need to be specified each time the driver connects. | . | .tdc file | The TDC file contains configuration information that Tableau applies to any connection matching the database vendor name and driver name defined in the file. This configuration allows you to fine-tune parts of your ODBC data connection and turn on/off certain features not supported by the data source. | . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/sql/odbc/#concepts",
    "relUrl": "/search-plugins/sql/sql/odbc/#concepts"
  },"1034": {
    "doc": "ODBC Driver",
    "title": "Install driver",
    "content": "To install the driver, download the bundled distribution installer from here or by build from the source. Windows . | Open the downloaded OpenSearch SQL ODBC Driver-&lt;version&gt;-Windows.msi installer. The installer is unsigned and shows a security dialog. Choose More info and Run anyway. | Choose Next to proceed with the installation. | Accept the agreement, and choose Next. | The installer comes bundled with documentation and useful resource files to connect to various BI tools (for example, a .tdc file for Tableau). You can choose to keep or remove these resources. Choose Next. | Choose Install and Finish. | . The following connection information is set up as part of the default DSN: . Host: localhost Port: 9200 Auth: NONE . To customize the DSN, use ODBC Data Source Administrator which is pre-installed on Windows 10. macOS . Before installing the ODBC Driver on macOS, install the iODBC Driver Manager. | Open the downloaded OpenSearch SQL ODBC Driver-&lt;version&gt;-Darwin.pkg installer. The installer is unsigned and shows a security dialog. Right-click on the installer and choose Open. | Choose Continue several times to proceed with the installation. | Choose the Destination to install the driver files. | The installer comes bundled with documentation and useful resources files to connect to various BI tools (for example, a .tdc file for Tableau). You can choose to keep or remove these resources. Choose Continue. | Choose Install and Close. | . Currently, the DSN is not set up as part of the installation and needs to be configured manually. First, open iODBC Administrator: . sudo /Applications/iODBC/iODBC\\ Administrator64.app/Contents/MacOS/iODBC\\ Administrator64 . This command gives the application permissions to save the driver and DSN configurations. | Choose ODBC Drivers tab. | Choose Add a Driver and fill in the following details: . | Description of the Driver: Enter the driver name that you used for the ODBC connection (for example, OpenSearch SQL ODBC Driver). | Driver File Name: Enter the path to the driver file (default: &lt;driver-install-dir&gt;/bin/libopensearchsqlodbc.dylib). | Setup File Name: Enter the path to the setup file (default: &lt;driver-install-dir&gt;/bin/libopensearchsqlodbc.dylib). | . | Choose the user driver. | Choose OK to save the options. | Choose the User DSN tab. | Select Add. | Choose the driver that you added above. | For Data Source Name (DSN), enter the name of the DSN used to store connection options (for example, OpenSearch SQL ODBC DSN). | For Comment, add an optional comment. | Add key-value pairs by using the + button. We recommend the following options for a default local OpenSearch installation: . | Host: localhost - OpenSearch server endpoint | Port: 9200 - The server port | Auth: NONE - The authentication mode | Username: (blank) - The username used for BASIC auth | Password: (blank)- The password used for BASIC auth | ResponseTimeout: 10 - The number of seconds to wait for a response from the server | UseSSL: 0 - Do not use SSL for connections | . | Choose OK to save the DSN configuration. | Choose OK to exit the iODBC Administrator. | . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/sql/odbc/#install-driver",
    "relUrl": "/search-plugins/sql/sql/odbc/#install-driver"
  },"1035": {
    "doc": "ODBC Driver",
    "title": "Customizing the ODBC driver",
    "content": "The driver is in the form of a library file: opensearchsqlodbc.dll for Windows and libopensearchsqlodbc.dylib for macOS. If you’re using with ODBC compatible BI tools, refer to your BI tool documentation for configuring a new ODBC driver. Typically, all that’s required is to make the BI tool aware of the location of the driver library file and then use it to set up the database (i.e., OpenSearch) connection. Connection strings and other settings . The ODBC driver uses an ODBC connection string. The connection strings are semicolon-delimited strings that specify the set of options that you can use for a connection. Typically, a connection string will either: . | Specify a Data Source Name (DSN) that contains a pre-configured set of options (DSN=xxx;User=xxx;Password=xxx;). | Or, configure options explicitly using the string (Host=xxx;Port=xxx;LogLevel=ES_DEBUG;...). | . You can configure the following driver options using a DSN or connection string: . All option names are case-insensitive. Basic options . | Option | Description | Type | Default | . | DSN | Data source name that you used for configuring the connection. | string | - | . | Host / Server | Hostname or IP address for the target cluster. | string | - | . | Port | Port number on which the OpenSearch cluster’s REST interface is listening. | string | - | . Authentication Options . | Option | Description | Type | Default | . | Auth | Authentication mechanism to use. | BASIC (basic HTTP), AWS_SIGV4 (AWS auth), or NONE | NONE | . | User / UID | [Auth=BASIC] Username for the connection. | string | - | . | Password / PWD | [Auth=BASIC] Password for the connection. | string | - | . | Region | [Auth=AWS_SIGV4] Region used for signing requests. | AWS region (for example, us-west-1) | - | . Advanced options . | Option | Description | Type | Default | . | UseSSL | Whether to establish the connection over SSL/TLS. | boolean (0 or 1) | false (0) | . | HostnameVerification | Indicates whether certificate hostname verification should be performed for an SSL/TLS connection. | boolean (0 or 1) | true (1) | . | ResponseTimeout | The maximum time to wait for responses from the host, in seconds. | integer | 10 | . Logging options . | Option | Description | Type | Default | . | LogLevel | Severity level for driver logs. | LOG_OFF, LOG_FATAL, LOG_ERROR, LOG_INFO, LOG_DEBUG, LOG_TRACE, or LOG_ALL | LOG_WARNING | . | LogOutput | Location for storing driver logs. | string | WIN: C:\\, MAC: /tmp | . You need administrative privileges to change the logging options. ",
    "url": "https://vagimeli.github.io/search-plugins/sql/sql/odbc/#customizing-the-odbc-driver",
    "relUrl": "/search-plugins/sql/sql/odbc/#customizing-the-odbc-driver"
  },"1036": {
    "doc": "ODBC Driver",
    "title": "Connecting to Tableau",
    "content": "Pre-requisites: . | Make sure the DSN is already set up. | Make sure OpenSearch is running on host and port as configured in DSN. | Make sure the .tdc is copied to &lt;user_home_directory&gt;/Documents/My Tableau Repository/Datasources in both macOS and Windows. | . | Start Tableau. Under the Connect section, go to To a Server and choose Other Databases (ODBC). | In the DSN drop-down, select the OpenSearch DSN you set up in the previous set of steps. The options you added will be automatically filled in under the Connection Attributes. | Select Sign In. After a few seconds, Tableau connects to your OpenSearch server. Once connected, you will be directed to the Datasource window. The Database will be already be populated with the name of the OpenSearch cluster. To list all the indices, click the search icon under Table. | Start experimenting with data by dragging the table to the connection area. Choose Update Now or Automatically Update to populate the table data. | . See more detailed instructions in the GitHub repository. Troubleshooting . Problem . Unable to connect to server. Workaround . This is most likely due to OpenSearch server not running on host and post configured in DSN. Confirm host and post are correct and OpenSearch server is running with OpenSearch SQL plugin. Also make sure .tdc that was downloaded with the installer is copied correctly to &lt;user_home_directory&gt;/Documents/My Tableau Repository/Datasources directory. ",
    "url": "https://vagimeli.github.io/search-plugins/sql/sql/odbc/#connecting-to-tableau",
    "relUrl": "/search-plugins/sql/sql/odbc/#connecting-to-tableau"
  },"1037": {
    "doc": "ODBC Driver",
    "title": "Connecting to Microsoft Power BI",
    "content": "Follow the installation instructions and the configuration instructions published in the GitHub repository. ",
    "url": "https://vagimeli.github.io/search-plugins/sql/sql/odbc/#connecting-to-microsoft-power-bi",
    "relUrl": "/search-plugins/sql/sql/odbc/#connecting-to-microsoft-power-bi"
  },"1038": {
    "doc": "ODBC Driver",
    "title": "ODBC Driver",
    "content": " ",
    "url": "https://vagimeli.github.io/search-plugins/sql/sql/odbc/",
    "relUrl": "/search-plugins/sql/sql/odbc/"
  },"1039": {
    "doc": "JSON Support",
    "title": "JSON Support",
    "content": "SQL plugin supports JSON by following PartiQL specification, a SQL-compatible query language that lets you query semi-structured and nested data for any data format. The SQL plugin only supports a subset of the PartiQL specification. ",
    "url": "https://vagimeli.github.io/search-plugins/sql/sql/partiql/",
    "relUrl": "/search-plugins/sql/sql/partiql/"
  },"1040": {
    "doc": "JSON Support",
    "title": "Querying nested collection",
    "content": "PartiQL extends SQL to allow you to query and unnest nested collections. In OpenSearch, this is very useful to query a JSON index with nested objects or fields. To follow along, use the bulk operation to index some sample data: . POST employees_nested/_bulk?refresh {\"index\":{\"_id\":\"1\"}} {\"id\":3,\"name\":\"Bob Smith\",\"title\":null,\"projects\":[{\"name\":\"SQL Spectrum querying\",\"started_year\":1990},{\"name\":\"SQL security\",\"started_year\":1999},{\"name\":\"OpenSearch security\",\"started_year\":2015}]} {\"index\":{\"_id\":\"2\"}} {\"id\":4,\"name\":\"Susan Smith\",\"title\":\"Dev Mgr\",\"projects\":[]} {\"index\":{\"_id\":\"3\"}} {\"id\":6,\"name\":\"Jane Smith\",\"title\":\"Software Eng 2\",\"projects\":[{\"name\":\"SQL security\",\"started_year\":1998},{\"name\":\"Hello security\",\"started_year\":2015,\"address\":[{\"city\":\"Dallas\",\"state\":\"TX\"}]}]} . Example 1: Unnesting a nested collection . This example finds the nested document (projects) with a field value (name) that satisfies the predicate (contains security). Because each parent document can have more than one nested documents, the nested document that matches is flattened. In other words, the final result is the cartesian product between the parent and nested documents. SELECT e.name AS employeeName, p.name AS projectName FROM employees_nested AS e, e.projects AS p WHERE p.name LIKE '%security%' . Explain: . { \"from\" : 0, \"size\" : 200, \"query\" : { \"bool\" : { \"filter\" : [ { \"bool\" : { \"must\" : [ { \"nested\" : { \"query\" : { \"wildcard\" : { \"projects.name\" : { \"wildcard\" : \"*security*\", \"boost\" : 1.0 } } }, \"path\" : \"projects\", \"ignore_unmapped\" : false, \"score_mode\" : \"none\", \"boost\" : 1.0, \"inner_hits\" : { \"ignore_unmapped\" : false, \"from\" : 0, \"size\" : 3, \"version\" : false, \"seq_no_primary_term\" : false, \"explain\" : false, \"track_scores\" : false, \"_source\" : { \"includes\" : [ \"projects.name\" ], \"excludes\" : [ ] } } } } ], \"adjust_pure_negative\" : true, \"boost\" : 1.0 } } ], \"adjust_pure_negative\" : true, \"boost\" : 1.0 } }, \"_source\" : { \"includes\" : [ \"name\" ], \"excludes\" : [ ] } } . Result set: . | employeeName | projectName | . | Bob Smith | OpenSearch Security | . | Bob Smith | SQL security | . | Jane Smith | Hello security | . | Jane Smith | SQL security | . Example 2: Unnesting in existential subquery . To unnest a nested collection in a subquery to check if it satisfies a condition: . SELECT e.name AS employeeName FROM employees_nested AS e WHERE EXISTS ( SELECT * FROM e.projects AS p WHERE p.name LIKE '%security%' ) . Explain: . { \"from\" : 0, \"size\" : 200, \"query\" : { \"bool\" : { \"filter\" : [ { \"bool\" : { \"must\" : [ { \"nested\" : { \"query\" : { \"bool\" : { \"must\" : [ { \"bool\" : { \"must\" : [ { \"bool\" : { \"must_not\" : [ { \"bool\" : { \"must_not\" : [ { \"exists\" : { \"field\" : \"projects\", \"boost\" : 1.0 } } ], \"adjust_pure_negative\" : true, \"boost\" : 1.0 } } ], \"adjust_pure_negative\" : true, \"boost\" : 1.0 } }, { \"wildcard\" : { \"projects.name\" : { \"wildcard\" : \"*security*\", \"boost\" : 1.0 } } } ], \"adjust_pure_negative\" : true, \"boost\" : 1.0 } } ], \"adjust_pure_negative\" : true, \"boost\" : 1.0 } }, \"path\" : \"projects\", \"ignore_unmapped\" : false, \"score_mode\" : \"none\", \"boost\" : 1.0 } } ], \"adjust_pure_negative\" : true, \"boost\" : 1.0 } } ], \"adjust_pure_negative\" : true, \"boost\" : 1.0 } }, \"_source\" : { \"includes\" : [ \"name\" ], \"excludes\" : [ ] } } . Result set: . | employeeName | . | Bob Smith | . | Jane Smith | . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/sql/partiql/#querying-nested-collection",
    "relUrl": "/search-plugins/sql/sql/partiql/#querying-nested-collection"
  },"1041": {
    "doc": "Troubleshooting",
    "title": "Troubleshooting",
    "content": "The SQL plugin is stateless, so troubleshooting is mostly focused on why a particular query fails. The most common error is the dreaded null pointer exception, which can occur during parsing errors or when using the wrong HTTP method (POST vs. GET and vice versa). The POST method and HTTP request body offer the most consistent results: . POST _plugins/_sql { \"query\": \"SELECT * FROM my-index WHERE ['name.firstname']='saanvi' LIMIT 5\" } . If a query isn’t behaving the way you expect, use the _explain API to see the translated query, which you can then troubleshoot. For most operations, _explain returns OpenSearch query DSL. For UNION, MINUS, and JOIN, it returns something more akin to a SQL execution plan. Example request . POST _plugins/_sql/_explain { \"query\": \"SELECT * FROM my-index LIMIT 50\" } . Example response . { \"from\": 0, \"size\": 50 } . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/troubleshoot/",
    "relUrl": "/search-plugins/sql/troubleshoot/"
  },"1042": {
    "doc": "Troubleshooting",
    "title": "Index mapping verification exception",
    "content": "If you see the following verification exception: . { \"error\": { \"reason\": \"There was internal problem at backend\", \"details\": \"When using multiple indices, the mappings must be identical.\", \"type\": \"VerificationException\" }, \"status\": 503 } . Make sure the index in your query is not an index pattern and is not an index pattern and doesn’t have multiple types. If these steps don’t work, submit a Github issue here. ",
    "url": "https://vagimeli.github.io/search-plugins/sql/troubleshoot/#index-mapping-verification-exception",
    "relUrl": "/search-plugins/sql/troubleshoot/#index-mapping-verification-exception"
  },"1043": {
    "doc": "Query Workbench",
    "title": "Query Workbench",
    "content": "Use the Query Workbench to easily run on-demand SQL queries, translate SQL into its REST equivalent, and view and save results as text, JSON, JDBC, or CSV. ",
    "url": "https://vagimeli.github.io/search-plugins/sql/workbench/",
    "relUrl": "/search-plugins/sql/workbench/"
  },"1044": {
    "doc": "Query Workbench",
    "title": "Quick start",
    "content": "To get started with SQL Workbench, choose Dev Tools in OpenSearch Dashboards and use the bulk operation to index some sample data: . PUT accounts/_bulk?refresh {\"index\":{\"_id\":\"1\"}} {\"account_number\":1,\"balance\":39225,\"firstname\":\"Amber\",\"lastname\":\"Duke\",\"age\":32,\"gender\":\"M\",\"address\":\"880 Holmes Lane\",\"employer\":\"Pyrami\",\"email\":\"amberduke@pyrami.com\",\"city\":\"Brogan\",\"state\":\"IL\"} {\"index\":{\"_id\":\"6\"}} {\"account_number\":6,\"balance\":5686,\"firstname\":\"Hattie\",\"lastname\":\"Bond\",\"age\":36,\"gender\":\"M\",\"address\":\"671 Bristol Street\",\"employer\":\"Netagy\",\"email\":\"hattiebond@netagy.com\",\"city\":\"Dante\",\"state\":\"TN\"} {\"index\":{\"_id\":\"13\"}} {\"account_number\":13,\"balance\":32838,\"firstname\":\"Nanette\",\"lastname\":\"Bates\",\"age\":28,\"gender\":\"F\",\"address\":\"789 Madison Street\",\"employer\":\"Quility\",\"email\":\"nanettebates@quility.com\",\"city\":\"Nogal\",\"state\":\"VA\"} {\"index\":{\"_id\":\"18\"}} {\"account_number\":18,\"balance\":4180,\"firstname\":\"Dale\",\"lastname\":\"Adams\",\"age\":33,\"gender\":\"M\",\"address\":\"467 Hutchinson Court\",\"email\":\"daleadams@boink.com\",\"city\":\"Orick\",\"state\":\"MD\"} . Then return to SQL Workbench. List indices . To list all your indices: . SHOW TABLES LIKE % . | TABLE_NAME | . | accounts | . Read data . After you index a document, retrieve it using the following SQL expression: . SELECT * FROM accounts WHERE _id = 1 . | account_number | firstname | gender | city | balance | employer | state | email | address | lastname | age | . | 1 | Amber | M | Brogan | 39225 | Pyrami | IL | amberduke@pyrami.com | 880 Holmes Lane | Duke | 32 | . Delete data . To delete a document from an index, use the DELETE clause: . DELETE FROM accounts WHERE _id = 0 . | deleted_rows | . | 1 | . ",
    "url": "https://vagimeli.github.io/search-plugins/sql/workbench/#quick-start",
    "relUrl": "/search-plugins/sql/workbench/#quick-start"
  },"1045": {
    "doc": "Supported Algorithms",
    "title": "Supported Algorithms",
    "content": "ML Commons supports various algorithms to help train and predict machine learning (ML) models or test data-driven predictions without a model. This page outlines the algorithms supported by the ML Commons plugin and the API operations they support. ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/algorithms/",
    "relUrl": "/ml-commons-plugin/algorithms/"
  },"1046": {
    "doc": "Supported Algorithms",
    "title": "Common limitation",
    "content": "Except for the Localization algorithm, all of the following algorithms can only support retrieving 10,000 documents from an index as an input. ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/algorithms/#common-limitation",
    "relUrl": "/ml-commons-plugin/algorithms/#common-limitation"
  },"1047": {
    "doc": "Supported Algorithms",
    "title": "K-means",
    "content": "K-means is a simple and popular unsupervised clustering ML algorithm built on top of Tribuo library. K-means will randomly choose centroids, then calculate iteratively to optimize the position of the centroids until each observation belongs to the cluster with the nearest mean. Parameters . | Parameter | Type | Description | Default Value | . | centroids | integer | The number of clusters in which to group the generated data | 2 | . | iterations | integer | The number of iterations to perform against the data until a mean generates | 10 | . | distance_type | enum, such as EUCLIDEAN, COSINE, or L1 | The type of measurement from which to measure the distance between centroids | EUCLIDEAN | . APIs . | Train | Predict | Train and predict | . Example . The following example uses the Iris Data index to train k-means synchronously. POST /_plugins/_ml/_train/kmeans { \"parameters\": { \"centroids\": 3, \"iterations\": 10, \"distance_type\": \"COSINE\" }, \"input_query\": { \"_source\": [\"petal_length_in_cm\", \"petal_width_in_cm\"], \"size\": 10000 }, \"input_index\": [ \"iris_data\" ] } . Limitations . The training process supports multi-threads, but the number of threads should be less than half of the number of CPUs. ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/algorithms/#k-means",
    "relUrl": "/ml-commons-plugin/algorithms/#k-means"
  },"1048": {
    "doc": "Supported Algorithms",
    "title": "Linear regression",
    "content": "Linear regression maps the linear relationship between inputs and outputs. In ML Commons, the linear regression algorithm is adopted from the public machine learning library Tribuo, which offers multidimensional linear regression models. The model supports the linear optimizer in training, including popular approaches like Linear Decay, SQRT_DECAY, ADA, ADAM, and RMS_DROP. Parameters . | Parameter | Type | Description | Default Value | . | learningRate | Double | The initial step size used in an iterative optimization algorithm. | 0.01 | . | momentumFactor | Double | The extra weight factors that accelerate the rate at which the weight is adjusted. This helps move the minimization routine out of local minima. | 0 | . | epsilon | Double | The value for stabilizing gradient inversion. | 1.00E-06 | . | beta1 | Double | The exponential decay rates for the moment estimates. | 0.9 | . | beta2 | Double | The exponential decay rates for the moment estimates. | 0.99 | . | decayRate | Double | The Root Mean Squared Propagation (RMSProp). | 0.9 | . | momentumType | MomentumType | The defined Stochastic Gradient Descent (SGD) momentum type that helps accelerate gradient vectors in the right directions, leading to a fast convergence. | STANDARD | . | optimizerType | OptimizerType | The optimizer used in the model. | SIMPLE_SGD | . APIs . | Train | Predict | . Example . The following example creates a new prediction based on the previously trained linear regression model. Request . POST _plugins/_ml/_predict/LINEAR_REGRESSION/ROZs-38Br5eVE0lTsoD9 { \"parameters\": { \"target\": \"price\" }, \"input_data\": { \"column_metas\": [ { \"name\": \"A\", \"column_type\": \"DOUBLE\" }, { \"name\": \"B\", \"column_type\": \"DOUBLE\" } ], \"rows\": [ { \"values\": [ { \"column_type\": \"DOUBLE\", \"value\": 3 }, { \"column_type\": \"DOUBLE\", \"value\": 5 } ] } ] } } . Response . { \"status\": \"COMPLETED\", \"prediction_result\": { \"column_metas\": [ { \"name\": \"price\", \"column_type\": \"DOUBLE\" } ], \"rows\": [ { \"values\": [ { \"column_type\": \"DOUBLE\", \"value\": 17.25701855310131 } ] } ] } } . Limitations . ML Commons only supports the linear Stochastic gradient trainer or optimizer, which cannot effectively map the non-linear relationships in trained data. When used with complicated datasets, the linear Stochastic trainer might cause some convergence problems and inaccurate results. ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/algorithms/#linear-regression",
    "relUrl": "/ml-commons-plugin/algorithms/#linear-regression"
  },"1049": {
    "doc": "Supported Algorithms",
    "title": "RCF",
    "content": "Random Cut Forest (RCF) is a probabilistic data structure used primarily for unsupervised anomaly detection. Its use also extends to density estimation and forecasting. OpenSearch leverages RCF for anomaly detection. ML Commons supports two new variants of RCF for different use cases: . | Batch RCF: Detects anomalies in non-time series data. | Fixed in time (FIT) RCF: Detects anomalies in time series data. | . Parameters . Batch RCF . | Parameter | Type | Description | Default Value | . | number_of_trees | integer | The number of trees in the forest. | 30 | . | sample_size | integer | The same size used by the stream samplers in the forest. | 256 | . | output_after | integer | The number of points required by stream samplers before results return. | 32 | . | training_data_size | integer | The size of your training data. | Dataset size | . | anomaly_score_threshold | double | The threshold of the anomaly score. | 1.0 | . Fit RCF . All parameters are optional except time_field. | Parameter | Type | Description | Default Value | . | number_of_trees | integer | The number of trees in the forest. | 30 | . | shingle_size | integer | A shingle, or a consecutive sequence of the most recent records. | 8 | . | sample_size | integer | The sample size used by stream samplers in the forest. | 256 | . | output_after | integer | The number of points required by stream samplers before results return. | 32 | . | time_decay | double | The decay factor used by stream samplers in the forest. | 0.0001 | . | anomaly_rate | double | The anomaly rate. | 0.005 | . | time_field | string | (Required) The time field for RCF to use as time series data. | N/A | . | date_format | string | The date and time format for the time_field field. | “yyyy-MM-ddHH:mm:ss” | . | time_zone | string | The time zone for the time_field field. | “UTC” | . APIs . | Train | Predict | Train and predict | . Limitations . For FIT RCF, you can train the model with historical data and store the trained model in your index. The model will be deserialized and predict new data points when using the Predict API. However, the model in the index will not be refreshed with new data, because the model is fixed in time. ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/algorithms/#rcf",
    "relUrl": "/ml-commons-plugin/algorithms/#rcf"
  },"1050": {
    "doc": "Supported Algorithms",
    "title": "RCF Summarize",
    "content": "RCF Summarize is a clustering algorithm based on the Clustering Using Representatives (CURE) algorithm. Compared to k-means, which uses random iterations to cluster, RCF Summarize uses a hierarchical clustering technique. The algorithm starts, with a set of randomly selected centroids larger than the centroids’ ground truth distribution. During iteration, centroid pairs too close to each other automatically merge. Therefore, the number of centroids (max_k) converge to a rational number of clusters that fits ground truth, as opposed to a fixed k number of clusters. Parameters . | Parameter | Type | Description | Default Value | . | max_k | integer | The max allowed number of centroids. | 2 | . | distance_type | enum, such as EUCLIDEAN, L1, L2, or LInfinity | The type of measurement used to measure the distance between centroids. | EUCLIDEAN | . APIs . | Train | Predict | Train and predict | . Example: Train and predict . The following example estimates cluster centers and provides cluster labels for each sample in a given data frame. POST _plugins/_ml/_train_predict/RCF_SUMMARIZE { \"parameters\": { \"centroids\": 3, \"max_k\": 15, \"distance_type\": \"L2\" }, \"input_data\": { \"column_metas\": [ { \"name\": \"d0\", \"column_type\": \"DOUBLE\" }, { \"name\": \"d1\", \"column_type\": \"DOUBLE\" } ], \"rows\": [ { \"values\": [ { \"column_type\": \"DOUBLE\", \"value\": 6.2 }, { \"column_type\": \"DOUBLE\", \"value\": 3.4 } ] } ] } } . Response . The rows parameter within the prediction result has been modified for length. In your response, expect more rows and columns to be contained within the response body. { \"status\": \"COMPLETED\", \"prediction_result\": { \"column_metas\": [ { \"name\": \"ClusterID\", \"column_type\": \"INTEGER\" } ], \"rows\": [ { \"values\": [ { \"column_type\": \"DOUBLE\", \"value\": 0 } ] } ] } } . ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/algorithms/#rcf-summarize",
    "relUrl": "/ml-commons-plugin/algorithms/#rcf-summarize"
  },"1051": {
    "doc": "Supported Algorithms",
    "title": "Localization",
    "content": "The Localization algorithm finds subset-level information for aggregate data (for example, aggregated over time) that demonstrates the activity of interest, such as spikes, drops, changes, or anomalies. Localization can be applied in different scenarios, such as data exploration or root cause analysis, to expose the contributors driving the activity of interest in the aggregate data. Parameters . All parameters are required except filter_query and anomaly_start. | Parameter | Type | Description | Default Value | . | index_name | String | The data collection to analyze. | N/A | . | attribute_field_names | List | The fields for entity keys. | N/A | . | aggregations | List | The fields and aggregation for values. | N/A | . | time_field_name | String | The timestamp field. | null | . | start_time | Long | The beginning of the time range. | 0 | . | end_time | Long | The end of the time range. | 0 | . | min_time_interval | Long | The minimum time interval/scale for analysis. | 0 | . | num_outputs | integer | The maximum number of values from localization/slicing. | 0 | . | filter_query | Long | (Optional) Reduces the collection of data for analysis. | Optional.empty() | . | anomaly_star | QueryBuilder | (Optional) The time after which the data will be analyzed. | Optional.empty() | . Example: Execute localization . The following example executes Localization against an RCA index. Request . POST /_plugins/_ml/_execute/anomaly_localization { \"index_name\": \"rca-index\", \"attribute_field_names\": [ \"attribute\" ], \"aggregations\": [ { \"sum\": { \"sum\": { \"field\": \"value\" } } } ], \"time_field_name\": \"timestamp\", \"start_time\": 1620630000000, \"end_time\": 1621234800000, \"min_time_interval\": 86400000, \"num_outputs\": 10 } . Response . The API responds with the sum of the contribution and base values per aggregation, every time the algorithm executes in the specified time interval. { \"results\" : [ { \"name\" : \"sum\", \"result\" : { \"buckets\" : [ { \"start_time\" : 1620630000000, \"end_time\" : 1620716400000, \"overall_aggregate_value\" : 65.0 }, { \"start_time\" : 1620716400000, \"end_time\" : 1620802800000, \"overall_aggregate_value\" : 75.0, \"entities\" : [ { \"key\" : [ \"attr0\" ], \"contribution_value\" : 1.0, \"base_value\" : 2.0, \"new_value\" : 3.0 }, { \"key\" : [ \"attr1\" ], \"contribution_value\" : 1.0, \"base_value\" : 3.0, \"new_value\" : 4.0 }, { ... }, { \"key\" : [ \"attr8\" ], \"contribution_value\" : 6.0, \"base_value\" : 10.0, \"new_value\" : 16.0 }, { \"key\" : [ \"attr9\" ], \"contribution_value\" : 6.0, \"base_value\" : 11.0, \"new_value\" : 17.0 } ] } ] } } ] } . Limitations . The Localization algorithm can only be executed directly. Therefore, it cannot be used with the ML Commons Train and Predict APIs. ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/algorithms/#localization",
    "relUrl": "/ml-commons-plugin/algorithms/#localization"
  },"1052": {
    "doc": "Supported Algorithms",
    "title": "Logistic regression",
    "content": "A classification algorithm, logistic regression models the probability of a discrete outcome given an input variable. In ML Commons, these classifications include both binary and multi-class. The most common is the binary classification, which takes two values, such as “true/false” or “yes/no”, and predicts the outcome based on the values specified. Alternatively, a multi-class output can categorize different inputs based on type. This makes logistic regression most useful for situations where you are trying to determine how your inputs fit best into a specified category. Parameters . | Parameter | Type | Description | Default Value | . | learningRate | Double | The initial step size used in an iterative optimization algorithm. | 1 | . | momentumFactor | Double | The extra weight factors that accelerate the rate at which the weight is adjusted. This helps move the minimization routine out of local minima. | 0 | . | epsilon | Double | The value for stabilizing gradient inversion. | 0.1 | . | beta1 | Double | The exponential decay rates for the moment estimates. | 0.9 | . | beta2 | Double | The exponential decay rates for the moment estimates. | 0.99 | . | decayRate | Double | The Root Mean Squared Propagation (RMSProp). | 0.9 | . | momentumType | MomentumType | The Stochastic Gradient Descent (SGD) momentum that helps accelerate gradient vectors in the right direction, leading to faster convergence between vectors. | STANDARD | . | optimizerType | OptimizerType | The optimizer used in the model. | AdaGrad | . | target | String | The target field. | null | . | objectiveType | ObjectiveType | The objective function type. | LogMulticlass | . | epochs | Integer | The number of iterations. | 5 | . | batchSize | Integer | The size of minbatches. | 1 | . | loggingInterval | Integer | The interval of logs lost after many iterations. The interval is 1 if the algorithm contains no logs. | 1000 | . APIs . | Train | Predict | . Example: Train/Predict with Iris data . The following example creates an index in OpenSearch with the Iris dataset, then trains the data using logistic regression. Lastly, it uses the trained model to predict Iris types separated by row. Create an Iris index . Before using this request, make sure that you have downloaded Iris data. PUT /iris_data { \"mappings\": { \"properties\": { \"sepal_length_in_cm\": { \"type\": \"double\" }, \"sepal_width_in_cm\": { \"type\": \"double\" }, \"petal_length_in_cm\": { \"type\": \"double\" }, \"petal_width_in_cm\": { \"type\": \"double\" }, \"class\": { \"type\": \"keyword\" } } } } . Ingest data from IRIS_data.txt . POST _bulk { \"index\" : { \"_index\" : \"iris_data\" } } {\"sepal_length_in_cm\":5.1,\"sepal_width_in_cm\":3.5,\"petal_length_in_cm\":1.4,\"petal_width_in_cm\":0.2,\"class\":\"Iris-setosa\"} { \"index\" : { \"_index\" : \"iris_data\" } } {\"sepal_length_in_cm\":4.9,\"sepal_width_in_cm\":3.0,\"petal_length_in_cm\":1.4,\"petal_width_in_cm\":0.2,\"class\":\"Iris-setosa\"} ..... Train the logistic regression model . This example uses a multi-class logistic regression categorization methodology. Here, the inputs of sepal and petal length and width are used to train the model to categorize centroids based on the class, as indicated by the target parameter. Request . { \"parameters\": { \"target\": \"class\" }, \"input_query\": { \"query\": { \"match_all\": {} }, \"_source\": [ \"sepal_length_in_cm\", \"sepal_width_in_cm\", \"petal_length_in_cm\", \"petal_width_in_cm\", \"class\" ], \"size\": 200 }, \"input_index\": [ \"iris_data\" ] } . Response . The model_id will be used to predict the class of the Iris. { \"model_id\" : \"TOgsf4IByBqD7FK_FQGc\", \"status\" : \"COMPLETED\" } . Predict results . Using the model_id of the trained Iris dataset, logistic regression will predict the class of the Iris based on the input data. POST _plugins/_ml/_predict/logistic_regression/SsfQaoIBEoC4g4joZiyD { \"parameters\": { \"target\": \"class\" }, \"input_data\": { \"column_metas\": [ { \"name\": \"sepal_length_in_cm\", \"column_type\": \"DOUBLE\" }, { \"name\": \"sepal_width_in_cm\", \"column_type\": \"DOUBLE\" }, { \"name\": \"petal_length_in_cm\", \"column_type\": \"DOUBLE\" }, { \"name\": \"petal_width_in_cm\", \"column_type\": \"DOUBLE\" } ], \"rows\": [ { \"values\": [ { \"column_type\": \"DOUBLE\", \"value\": 6.2 }, { \"column_type\": \"DOUBLE\", \"value\": 3.4 }, { \"column_type\": \"DOUBLE\", \"value\": 5.4 }, { \"column_type\": \"DOUBLE\", \"value\": 2.3 } ] }, { \"values\": [ { \"column_type\": \"DOUBLE\", \"value\": 5.9 }, { \"column_type\": \"DOUBLE\", \"value\": 3.0 }, { \"column_type\": \"DOUBLE\", \"value\": 5.1 }, { \"column_type\": \"DOUBLE\", \"value\": 1.8 } ] } ] } } . Response . { \"status\" : \"COMPLETED\", \"prediction_result\" : { \"column_metas\" : [ { \"name\" : \"result\", \"column_type\" : \"STRING\" } ], \"rows\" : [ { \"values\" : [ { \"column_type\" : \"STRING\", \"value\" : \"Iris-virginica\" } ] }, { \"values\" : [ { \"column_type\" : \"STRING\", \"value\" : \"Iris-virginica\" } ] } ] } } . Limitations . Convergence metrics are not built into Tribuo’s trainers. Therefore, ML Commons cannot indicate the convergence status through the ML Commons API. ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/algorithms/#logistic-regression",
    "relUrl": "/ml-commons-plugin/algorithms/#logistic-regression"
  },"1053": {
    "doc": "Supported Algorithms",
    "title": "Metrics correlation",
    "content": "The metrics correlation feature is an experimental feature released in OpenSearch 2.7. It can’t be used in a production environment. To leave feedback on improving the feature, create an issue in the ML Commons repository. The metrics correlation algorithm finds events in a set of metrics data. The algorithm defines events as a window in time in which multiple metrics simultaneously display anomalous behavior. When given a set of metrics, the algorithm counts the number of events that occurred, when each event occurred, and determines which metrics were involved in each event. To enable the metrics correlation algorithm, update the following cluster setting: . PUT /_cluster/settings { \"persistent\" : { \"plugins.ml_commons.enable_inhouse_python_model\": true } } . Parameters . To use the metrics correlation algorithm, include the following parameters. | Parameter | Type | Description | Default value | . | metrics | Array | A list of metrics within the time series that can be correlated to anomalous behavior | N/A | . Input . The metrics correlation input is an $M$ x $T$ array of metrics data, where M is the number of metrics and T is the length of each individual sequence of metric values. When inputting metrics into the algorithm, assume the following: . | For each metric, the input sequence has the same length, $T$. | All input metrics should have the same corresponding set of timestamps. | The total number of data points are $M$ * $T$ &lt;= 10000. | . Example: Simple metrics correlation . The following example inputs the number of metrics ($M$) as 3 and the number of timesteps ($T$) as 128: . POST /_plugins/_ml/_execute/METRICS_CORRELATION {\"metrics\": [[-1.1635416, -1.5003631, 0.46138194, 0.5308311, -0.83149344, -3.7009873, -3.5463789, 0.22571462, -5.0380244, 0.76588845, 1.236113, 1.8460795, 1.7576948, 0.44893077, 0.7363948, 0.70440894, 0.89451003, 4.2006273, 0.3697659, 2.2458954, -2.302939, -1.7706926, 1.7445002, -1.5246059, 0.07985192, -2.7756078, 1.0002468, 1.5977372, 2.9152713, 1.4172368, -0.26551363, -2.2883027, 1.5882446, 2.0145164, 3.4862874, -1.2486862, -2.4811826, -0.17609037, -2.1095612, -1.2184235, 0.63118523, -1.8909532, 2.039797, -0.5317177, -2.2922578, -2.0179775, -0.07992507, -0.12554549, -0.2553092, 1.1450123, -0.4640453, -2.190223, -4.671612, -1.5076426, 1.635445, -1.1394824, -0.7503817, 0.98424894, -0.38896716, 1.0328646, 1.9543738, -0.5236269, 0.14298044, 3.2963762, 8.1641035, 5.717064, 7.4869685, 2.5987444, 11.018798, 9.151356, 5.7354255, 6.862203, 3.0524514, 4.431755, 5.1481285, 7.9548607, 7.4519925, 6.09533, 7.634116, 8.898271, 3.898491, 9.447067, 8.197385, 5.8284273, 5.804283, 7.7688456, 10.574343, 7.5679493, 7.1888094, 7.1107903, 8.454468, 8.066334, 8.83665, 7.11204, 4.4898267, 8.614764, 6.336754, 11.577503, 3.3998494, 9.501525, 13.17289, 6.1116023, 5.143777, 2.7813284, 3.7917604, 7.1683135, 7.627272, 7.290255, 3.1299121, 7.089733, 9.140584, 8.844729, 9.403275, 10.220029, 8.039719, 8.85549, 4.034555, 4.412663, 7.54451, 7.2116737, 4.6346946, 7.0044127, 9.7557, 10.982841, 5.897937, 6.870126, 3.5638695, 5.7872133], [1.3037996, 2.7976995, -0.12042701, 1.3688855, 1.6955005, -2.2575269, 0.080582514, 3.011721, -0.4320283, 3.2440786, -1.0321085, 1.2346085, -2.3152106, -0.9783513, 0.6837618, 1.5320586, -1.6148578, -0.94538075, 0.55978125, -4.7430468, 3.466028, 2.3792691, 1.3269067, -0.35359794, -1.5547276, 0.5202475, 1.0269136, -1.7531714, 0.43987304, -0.18845831, 2.3086758, 2.519588, 2.0116413, 0.019745048, -0.010070452, 2.496933, 1.1557871, 0.08433053, 1.375894, -1.2135965, -1.2588277, -0.31454003, 0.045949124, -1.7518936, -2.3533764, -2.0125146, 0.10255043, 1.1782314, 2.4579153, -0.8780899, -4.1442213, 3.8300152, 2.772975, 2.6803262, 0.9867382, 0.77618766, 0.46541777, 3.8959959, -2.1713195, 0.10609512, -0.26438138, -2.145317, 3.6734529, 1.4830295, -5.3445525, -10.6427765, -8.300354, -1.9608921, -6.6779685, -10.019544, -8.341513, -9.607174, -7.2441607, -3.411102, -6.180552, -8.318714, -6.060591, -7.790343, -5.9695, -7.9429936, -3.775652, -5.2827606, -3.7168224, -6.729588, -9.761094, -7.4683576, -7.2595067, -6.6790915, -9.832726, -8.352172, -6.936336, -8.252518, -6.787475, -9.091013, -11.465944, -6.712504, -8.987438, -6.946672, -8.877166, -6.7854185, -3.6417139, -6.1036086, -5.360772, -4.0435786, -4.5864973, -6.971063, -10.522461, -6.3692527, -4.387658, -9.723745, -4.7020173, -5.097396, -9.903703, -4.882414, -4.1999683, -6.7829437, -6.2555966, -8.121125, -5.334131, -9.174302, -3.9752126, -4.179469, -8.335524, -9.359406, -6.4938803, -6.794677, -8.382997, -9.879416], [1.8792984, -3.1561708, -0.8443318, -1.998743, -0.6319316, 2.4614046, -0.44511616, 0.82785237, 1.7911717, -1.8172283, 0.46574894, -1.8691323, 3.9586513, 0.8078605, 0.9049874, 5.4086914, -0.7425967, -0.20115769, -1.197923, 2.741789, 0.85432875, -1.1688408, -1.7771784, 1.615249, -4.1103697, 0.4721327, -2.75669, -0.38393462, -3.1137516, -2.2572582, 0.9580673, -3.7139492, -0.68303126, 1.6007807, 0.6313973, -2.5115106, 0.703251, 2.4844077, -1.7405633, -3.007687, 2.372802, 2.4684637, 0.6443977, -3.1433117, 0.05976736, -1.9809214, 3.514713, 2.1880944, 1.242541, 1.8236228, 0.8642841, -0.17313614, 1.7042321, 0.8298376, 4.2443194, 0.13983983, 1.1940852, 2.5076652, 39.285202, 82.73858, 44.707516, -4.267148, 0.25930226, 0.20799652, -3.7213502, 1.475217, -1.2394199, -0.0034497892, 1.1413965, 55.18923, -2.2969518, -4.1400924, -2.4707043, 43.193188, -0.19258368, 3.471275, 1.1374166, 1.2147579, 4.13017, -2.0576499, 2.1529694, -0.28360432, 0.8477302, -0.63012695, 1.2569811, 1.943168, 0.17070436, 3.2358394, -2.3737662, 0.77060974, 4.99065, 3.1079204, 3.6347675, 0.6801177, -2.2205186, 1.0961101, -2.4445753, -2.0919478, -2.895031, 2.5458927, 0.38599384, 1.0492333, -0.081834644, -7.4079595, -2.1785216, -0.7277175, -2.7413428, -3.2083786, 3.2958643, -1.1839997, 5.4849496, 2.0259023, 5.607272, -1.0125756, 3.721461, 2.5715313, 0.7741753, -0.55034757, 0.7526307, -2.6758716, -2.964664, -0.57379586, -0.28817406, -3.2334063, -0.22387607, -2.0793931, -6.4562697, 0.80134094]]} . Response . The API returns the following information: . | event_window: The event interval | event_pattern: The intensity score across the time window and the overall severity of the event | suspected_metrics: The set of metrics involved | . In the following example response, each item corresponds to an event discovered in the metrics data. The algorithm finds one event in the input data of the request, as indicated by the output in event_pattern having a length of 1. event_window shows that the event occurred between time point $t$ = 52 and $t$ = 72. Lastly, suspected_metrics shows that the event involved all three metrics. { \"function_name\": \"METRICS_CORRELATION\", \"output\": { \"inference_results\": [ { \"event_window\": [ 52, 72 ], \"event_pattern\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3.99625e-05, 0.0001052875, 0.0002605894, 0.00064648513, 0.0014303402, 0.002980127, 0.005871893, 0.010885878, 0.01904726, 0.031481907, 0.04920215, 0.07283493, 0.10219432, 0.1361888, 0.17257516, 0.20853643, 0.24082609, 0.26901975, 0.28376183, 0.29364157, 0.29541212, 0.2832976, 0.29041746, 0.2574534, 0.2610143, 0.22938538, 0.19999361, 0.18074994, 0.15539801, 0.13064545, 0.10544432, 0.081248805, 0.05965102, 0.041305058, 0.027082501, 0.01676033, 0.009760197, 0.005362286, 0.0027713624, 0.0013381141, 0.0006126331, 0.0002634901, 0.000106459476, 4.0407333e-05, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \"suspected_metrics\": [0,1,2] } ] } } . ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/algorithms/#metrics-correlation",
    "relUrl": "/ml-commons-plugin/algorithms/#metrics-correlation"
  },"1054": {
    "doc": "API",
    "title": "ML Commons API",
    "content": ". | Train model . | Request | Response | . | Getting model information | Registering a model . | Request fields | Example | Response | . | Deploying a model . | Example: Deploying to all available ML nodes | Example: Deploying to a specific node | Response | . | Undeploying a model . | Example: Undeploying model from all ML nodes | Response: Undeploying a model from all ML nodes | Example: Undeploying specific models from specific nodes | Response: Undeploying specific models from specific nodes | Response: Undeploying all models from specific nodes | Example: Undeploying specific models from all nodes | Response: Undeploying specific models from all nodes | . | Searching for a model . | Example: Querying all models | Example: Querying models with algorithm “FIT_RCF” | Response | . | Deleting a model | Profile . | Path parameters | Request fields | Example: Returning all tasks and models on a specific node | Response: Returning all tasks and models on a specific node | . | Predict . | Request | Response | . | Train and predict . | Example: Train and predict with indexed data | Example: Train and predict with data directly | Response | . | Getting task information | Searching for a task . | Example: Search task which “function_name” is “KMEANS” | Response | . | Deleting a task | Stats . | Example: Get all stats | Response | . | Execute . | Example: Execute localization | . | . The Machine Learning (ML) commons API lets you train ML algorithms synchronously and asynchronously, make predictions with that trained model, and train and predict with the same data set. In order to train tasks through the API, three inputs are required. | Algorithm name: Must be one of a FunctionName. This determines what algorithm the ML Engine runs. To add a new function, see How To Add a New Function. | Model hyper parameters: Adjust these parameters to make the model train better. | Input data: The data input that trains the ML model, or applies the ML models to predictions. You can input data in two ways, query against your index or use data frame. | . ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/api/#ml-commons-api",
    "relUrl": "/ml-commons-plugin/api/#ml-commons-api"
  },"1055": {
    "doc": "API",
    "title": "Train model",
    "content": "The train operation trains a model based on a selected algorithm. Training can occur both synchronously and asynchronously. Request . The following examples use the kmeans algorithm to train index data. Train with kmeans synchronously . POST /_plugins/_ml/_train/kmeans { \"parameters\": { \"centroids\": 3, \"iterations\": 10, \"distance_type\": \"COSINE\" }, \"input_query\": { \"_source\": [\"petal_length_in_cm\", \"petal_width_in_cm\"], \"size\": 10000 }, \"input_index\": [ \"iris_data\" ] } . Train with kmeans asynchronously . POST /_plugins/_ml/_train/kmeans?async=true { \"parameters\": { \"centroids\": 3, \"iterations\": 10, \"distance_type\": \"COSINE\" }, \"input_query\": { \"_source\": [\"petal_length_in_cm\", \"petal_width_in_cm\"], \"size\": 10000 }, \"input_index\": [ \"iris_data\" ] } . Response . Synchronously . For synchronous responses, the API returns the model_id, which can be used to get or delete a model. { \"model_id\" : \"lblVmX8BO5w8y8RaYYvN\", \"status\" : \"COMPLETED\" } . Asynchronously . For asynchronous responses, the API returns the task_id, which can be used to get or delete a task. { \"task_id\" : \"lrlamX8BO5w8y8Ra2otd\", \"status\" : \"CREATED\" } . ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/api/#train-model",
    "relUrl": "/ml-commons-plugin/api/#train-model"
  },"1056": {
    "doc": "API",
    "title": "Getting model information",
    "content": "You can retrieve information on your model using the model_id. GET /_plugins/_ml/models/&lt;model-id&gt; . The API returns information on the model, the algorithm used, and the content found within the model. { \"name\" : \"KMEANS\", \"algorithm\" : \"KMEANS\", \"version\" : 1, \"content\" : \"\" } . ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/api/#getting-model-information",
    "relUrl": "/ml-commons-plugin/api/#getting-model-information"
  },"1057": {
    "doc": "API",
    "title": "Registering a model",
    "content": "Use the register operation to register a custom model to a model index. ML Commons splits the model into smaller chunks and saves those chunks in the model’s index. POST /_plugins/_ml/models/_register . Request fields . All request fields are required. | Field | Data type | Description | . | name | string | The name of the model. | . | version | integer | The version number of the model. | . | model_format | string | The portable format of the model file. Currently only supports TORCH_SCRIPT. | . | model_config | json object | The model’s configuration, including the model_type, embedding_dimension, and framework_type. all_config is an optional JSON string which contains all model configurations. | . | url | string | The URL which contains the model. | . Example . The following example request registers a version 1.0.0 of an NLP sentence transformation model named all-MiniLM-L6-v2. POST /_plugins/_ml/models/_register { \"name\": \"all-MiniLM-L6-v2\", \"version\": \"1.0.0\", \"description\": \"test model\", \"model_format\": \"TORCH_SCRIPT\", \"model_config\": { \"model_type\": \"bert\", \"embedding_dimension\": 384, \"framework_type\": \"sentence_transformers\", }, \"url\": \"https://github.com/opensearch-project/ml-commons/raw/2.x/ml-algorithms/src/test/resources/org/opensearch/ml/engine/algorithms/text_embedding/all-MiniLM-L6-v2_torchscript_sentence-transformer.zip?raw=true\" } . Response . OpenSearch responds with the task_id and task status. { \"task_id\" : \"ew8I44MBhyWuIwnfvDIH\", \"status\" : \"CREATED\" } . To see the status of your model registration, enter the task_id in the [task API] … . { \"model_id\" : \"WWQI44MBbzI2oUKAvNUt\", \"task_type\" : \"UPLOAD_MODEL\", \"function_name\" : \"TEXT_EMBEDDING\", \"state\" : \"REGISTERED\", \"worker_node\" : \"KzONM8c8T4Od-NoUANQNGg\", \"create_time\" : 1665961344003, \"last_update_time\" : 1665961373047, \"is_async\" : true } . ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/api/#registering-a-model",
    "relUrl": "/ml-commons-plugin/api/#registering-a-model"
  },"1058": {
    "doc": "API",
    "title": "Deploying a model",
    "content": "The deploy model operation reads the model’s chunks from the model index and then creates an instance of the model to cache into memory. This operation requires the model_id. POST /_plugins/_ml/models/&lt;model_id&gt;/_deploy . Example: Deploying to all available ML nodes . In this example request, OpenSearch deploys the model to any available OpenSearch ML node: . POST /_plugins/_ml/models/WWQI44MBbzI2oUKAvNUt/_deploy . Example: Deploying to a specific node . If you want to reserve the memory of other ML nodes within your cluster, you can deploy your model to a specific node(s) by specifying the node_ids in the request body: . POST /_plugins/_ml/models/WWQI44MBbzI2oUKAvNUt/_deploy { \"node_ids\": [\"4PLK7KJWReyX0oWKnBA8nA\"] } . Response . { \"task_id\" : \"hA8P44MBhyWuIwnfvTKP\", \"status\" : \"DEPLOYING\" } . ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/api/#deploying-a-model",
    "relUrl": "/ml-commons-plugin/api/#deploying-a-model"
  },"1059": {
    "doc": "API",
    "title": "Undeploying a model",
    "content": "To undeploy a model from memory, use the undeploy operation: . POST /_plugins/_ml/models/&lt;model_id&gt;/_undeploy . Example: Undeploying model from all ML nodes . POST /_plugins/_ml/models/MGqJhYMBbbh0ushjm8p_/_undeploy . Response: Undeploying a model from all ML nodes . { \"s5JwjZRqTY6nOT0EvFwVdA\": { \"stats\": { \"MGqJhYMBbbh0ushjm8p_\": \"UNDEPLOYED\" } } } . Example: Undeploying specific models from specific nodes . POST /_plugins/_ml/models/_undeploy { \"node_ids\": [\"sv7-3CbwQW-4PiIsDOfLxQ\"], \"model_ids\": [\"KDo2ZYQB-v9VEDwdjkZ4\"] } . Response: Undeploying specific models from specific nodes . { \"sv7-3CbwQW-4PiIsDOfLxQ\" : { \"stats\" : { \"KDo2ZYQB-v9VEDwdjkZ4\" : \"UNDEPLOYED\" } } } . Response: Undeploying all models from specific nodes . { \"sv7-3CbwQW-4PiIsDOfLxQ\" : { \"stats\" : { \"KDo2ZYQB-v9VEDwdjkZ4\" : \"UNDEPLOYED\", \"-8o8ZYQBvrLMaN0vtwzN\" : \"UNDEPLOYED\" } } } . Example: Undeploying specific models from all nodes . { \"model_ids\": [\"KDo2ZYQB-v9VEDwdjkZ4\"] } . Response: Undeploying specific models from all nodes . { \"sv7-3CbwQW-4PiIsDOfLxQ\" : { \"stats\" : { \"KDo2ZYQB-v9VEDwdjkZ4\" : \"UNDEPLOYED\" } } } . ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/api/#undeploying-a-model",
    "relUrl": "/ml-commons-plugin/api/#undeploying-a-model"
  },"1060": {
    "doc": "API",
    "title": "Searching for a model",
    "content": "Use this command to search models you’ve already created. POST /_plugins/_ml/models/_search {query} . Example: Querying all models . POST /_plugins/_ml/models/_search { \"query\": { \"match_all\": {} }, \"size\": 1000 } . Example: Querying models with algorithm “FIT_RCF” . POST /_plugins/_ml/models/_search { \"query\": { \"term\": { \"algorithm\": { \"value\": \"FIT_RCF\" } } } } . Response . { \"took\" : 8, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 2, \"relation\" : \"eq\" }, \"max_score\" : 2.4159138, \"hits\" : [ { \"_index\" : \".plugins-ml-model\", \"_id\" : \"-QkKJX8BvytMh9aUeuLD\", \"_version\" : 1, \"_seq_no\" : 12, \"_primary_term\" : 15, \"_score\" : 2.4159138, \"_source\" : { \"name\" : \"FIT_RCF\", \"version\" : 1, \"content\" : \"xxx\", \"algorithm\" : \"FIT_RCF\" } }, { \"_index\" : \".plugins-ml-model\", \"_id\" : \"OxkvHn8BNJ65KnIpck8x\", \"_version\" : 1, \"_seq_no\" : 2, \"_primary_term\" : 8, \"_score\" : 2.4159138, \"_source\" : { \"name\" : \"FIT_RCF\", \"version\" : 1, \"content\" : \"xxx\", \"algorithm\" : \"FIT_RCF\" } } ] } } . ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/api/#searching-for-a-model",
    "relUrl": "/ml-commons-plugin/api/#searching-for-a-model"
  },"1061": {
    "doc": "API",
    "title": "Deleting a model",
    "content": "Deletes a model based on the model_id. DELETE /_plugins/_ml/models/&lt;model_id&gt; . The API returns the following: . { \"_index\" : \".plugins-ml-model\", \"_id\" : \"MzcIJX8BA7mbufL6DOwl\", \"_version\" : 2, \"result\" : \"deleted\", \"_shards\" : { \"total\" : 2, \"successful\" : 2, \"failed\" : 0 }, \"_seq_no\" : 27, \"_primary_term\" : 18 } . ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/api/#deleting-a-model",
    "relUrl": "/ml-commons-plugin/api/#deleting-a-model"
  },"1062": {
    "doc": "API",
    "title": "Profile",
    "content": "The profile operation returns runtime information on ML tasks and models. The profile operation can help debug issues with models at runtime. GET /_plugins/_ml/profile GET /_plugins/_ml/profile/models GET /_plugins/_ml/profile/tasks . Path parameters . | Parameter | Data type | Description | . | model_id | string | Returns runtime data for a specific model. You can string together multiple model_ids to return multiple model profiles. | . | tasks | string | Returns runtime data for a specific task. You can string together multiple task_ids to return multiple task profiles. | . Request fields . All profile body request fields are optional. | Field | Data type | Description | . | node_ids | string | Returns all tasks and profiles from a specific node. | . | model_ids | string | Returns runtime data for a specific model. You can string together multiple model_ids to return multiple model profiles. | . | task_ids | string | Returns runtime data for a specific task. You can string together multiple task_ids to return multiple task profiles. | . | return_all_tasks | boolean | Determines whether or not a request returns all tasks. When set to false task profiles are left out of the response. | . | return_all_models | boolean | Determines whether or not a profile request returns all models. When set to false model profiles are left out of the response. | . Example: Returning all tasks and models on a specific node . GET /_plugins/_ml/profile { \"node_ids\": [\"KzONM8c8T4Od-NoUANQNGg\"], \"return_all_tasks\": true, \"return_all_models\": true } . Response: Returning all tasks and models on a specific node . { \"nodes\" : { \"qTduw0FJTrmGrqMrxH0dcA\" : { # node id \"models\" : { \"WWQI44MBbzI2oUKAvNUt\" : { # model id \"worker_nodes\" : [ # routing table \"KzONM8c8T4Od-NoUANQNGg\" ] } } }, ... \"KzONM8c8T4Od-NoUANQNGg\" : { # node id \"models\" : { \"WWQI44MBbzI2oUKAvNUt\" : { # model id \"model_state\" : \"DEPLOYED\", # model status \"predictor\" : \"org.opensearch.ml.engine.algorithms.text_embedding.TextEmbeddingModel@592814c9\", \"worker_nodes\" : [ # routing table \"KzONM8c8T4Od-NoUANQNGg\" ], \"predict_request_stats\" : { # predict request stats on this node \"count\" : 2, # total predict requests on this node \"max\" : 89.978681, # max latency in milliseconds \"min\" : 5.402, \"average\" : 47.6903405, \"p50\" : 47.6903405, \"p90\" : 81.5210129, \"p99\" : 89.13291418999998 } } } }, ... } . ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/api/#profile",
    "relUrl": "/ml-commons-plugin/api/#profile"
  },"1063": {
    "doc": "API",
    "title": "Predict",
    "content": "ML Commons can predict new data with your trained model either from indexed data or a data frame. To use the Predict API, the model_id is required. POST /_plugins/_ml/_predict/&lt;algorithm_name&gt;/&lt;model_id&gt; . Request . POST /_plugins/_ml/_predict/kmeans/&lt;model-id&gt; { \"input_query\": { \"_source\": [\"petal_length_in_cm\", \"petal_width_in_cm\"], \"size\": 10000 }, \"input_index\": [ \"iris_data\" ] } . Response . { \"status\" : \"COMPLETED\", \"prediction_result\" : { \"column_metas\" : [ { \"name\" : \"ClusterID\", \"column_type\" : \"INTEGER\" } ], \"rows\" : [ { \"values\" : [ { \"column_type\" : \"INTEGER\", \"value\" : 1 } ] }, { \"values\" : [ { \"column_type\" : \"INTEGER\", \"value\" : 1 } ] }, { \"values\" : [ { \"column_type\" : \"INTEGER\", \"value\" : 0 } ] }, { \"values\" : [ { \"column_type\" : \"INTEGER\", \"value\" : 0 } ] }, { \"values\" : [ { \"column_type\" : \"INTEGER\", \"value\" : 0 } ] }, { \"values\" : [ { \"column_type\" : \"INTEGER\", \"value\" : 0 } ] } ] } . ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/api/#predict",
    "relUrl": "/ml-commons-plugin/api/#predict"
  },"1064": {
    "doc": "API",
    "title": "Train and predict",
    "content": "Use to train and then immediately predict against the same training data set. Can only be used with unsupervised learning models and the following algorithms: . | BATCH_RCF | FIT_RCF | kmeans | . Example: Train and predict with indexed data . POST /_plugins/_ml/_train_predict/kmeans { \"parameters\": { \"centroids\": 2, \"iterations\": 10, \"distance_type\": \"COSINE\" }, \"input_query\": { \"query\": { \"bool\": { \"filter\": [ { \"range\": { \"k1\": { \"gte\": 0 } } } ] } }, \"size\": 10 }, \"input_index\": [ \"test_data\" ] } . Example: Train and predict with data directly . POST /_plugins/_ml/_train_predict/kmeans { \"parameters\": { \"centroids\": 2, \"iterations\": 1, \"distance_type\": \"EUCLIDEAN\" }, \"input_data\": { \"column_metas\": [ { \"name\": \"k1\", \"column_type\": \"DOUBLE\" }, { \"name\": \"k2\", \"column_type\": \"DOUBLE\" } ], \"rows\": [ { \"values\": [ { \"column_type\": \"DOUBLE\", \"value\": 1.00 }, { \"column_type\": \"DOUBLE\", \"value\": 2.00 } ] }, { \"values\": [ { \"column_type\": \"DOUBLE\", \"value\": 1.00 }, { \"column_type\": \"DOUBLE\", \"value\": 4.00 } ] }, { \"values\": [ { \"column_type\": \"DOUBLE\", \"value\": 1.00 }, { \"column_type\": \"DOUBLE\", \"value\": 0.00 } ] }, { \"values\": [ { \"column_type\": \"DOUBLE\", \"value\": 10.00 }, { \"column_type\": \"DOUBLE\", \"value\": 2.00 } ] }, { \"values\": [ { \"column_type\": \"DOUBLE\", \"value\": 10.00 }, { \"column_type\": \"DOUBLE\", \"value\": 4.00 } ] }, { \"values\": [ { \"column_type\": \"DOUBLE\", \"value\": 10.00 }, { \"column_type\": \"DOUBLE\", \"value\": 0.00 } ] } ] } } . Response . { \"status\" : \"COMPLETED\", \"prediction_result\" : { \"column_metas\" : [ { \"name\" : \"ClusterID\", \"column_type\" : \"INTEGER\" } ], \"rows\" : [ { \"values\" : [ { \"column_type\" : \"INTEGER\", \"value\" : 1 } ] }, { \"values\" : [ { \"column_type\" : \"INTEGER\", \"value\" : 1 } ] }, { \"values\" : [ { \"column_type\" : \"INTEGER\", \"value\" : 1 } ] }, { \"values\" : [ { \"column_type\" : \"INTEGER\", \"value\" : 0 } ] }, { \"values\" : [ { \"column_type\" : \"INTEGER\", \"value\" : 0 } ] }, { \"values\" : [ { \"column_type\" : \"INTEGER\", \"value\" : 0 } ] } ] } } . ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/api/#train-and-predict",
    "relUrl": "/ml-commons-plugin/api/#train-and-predict"
  },"1065": {
    "doc": "API",
    "title": "Getting task information",
    "content": "You can retrieve information about a task using the task_id. GET /_plugins/_ml/tasks/&lt;task_id&gt; . The response includes information about the task. { \"model_id\" : \"l7lamX8BO5w8y8Ra2oty\", \"task_type\" : \"TRAINING\", \"function_name\" : \"KMEANS\", \"state\" : \"COMPLETED\", \"input_type\" : \"SEARCH_QUERY\", \"worker_node\" : \"54xOe0w8Qjyze00UuLDfdA\", \"create_time\" : 1647545342556, \"last_update_time\" : 1647545342587, \"is_async\" : true } . ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/api/#getting-task-information",
    "relUrl": "/ml-commons-plugin/api/#getting-task-information"
  },"1066": {
    "doc": "API",
    "title": "Searching for a task",
    "content": "Search tasks based on parameters indicated in the request body. GET /_plugins/_ml/tasks/_search {query body} . Example: Search task which “function_name” is “KMEANS” . GET /_plugins/_ml/tasks/_search { \"query\": { \"bool\": { \"filter\": [ { \"term\": { \"function_name\": \"KMEANS\" } } ] } } } . Response . { \"took\" : 12, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 2, \"relation\" : \"eq\" }, \"max_score\" : 0.0, \"hits\" : [ { \"_index\" : \".plugins-ml-task\", \"_id\" : \"_wnLJ38BvytMh9aUi-Ia\", \"_version\" : 4, \"_seq_no\" : 29, \"_primary_term\" : 4, \"_score\" : 0.0, \"_source\" : { \"last_update_time\" : 1645640125267, \"create_time\" : 1645640125209, \"is_async\" : true, \"function_name\" : \"KMEANS\", \"input_type\" : \"SEARCH_QUERY\", \"worker_node\" : \"jjqFrlW7QWmni1tRnb_7Dg\", \"state\" : \"COMPLETED\", \"model_id\" : \"AAnLJ38BvytMh9aUi-M2\", \"task_type\" : \"TRAINING\" } }, { \"_index\" : \".plugins-ml-task\", \"_id\" : \"wwRRLX8BydmmU1x6I-AI\", \"_version\" : 3, \"_seq_no\" : 38, \"_primary_term\" : 7, \"_score\" : 0.0, \"_source\" : { \"last_update_time\" : 1645732766656, \"create_time\" : 1645732766472, \"is_async\" : true, \"function_name\" : \"KMEANS\", \"input_type\" : \"SEARCH_QUERY\", \"worker_node\" : \"A_IiqoloTDK01uZvCjREaA\", \"state\" : \"COMPLETED\", \"model_id\" : \"xARRLX8BydmmU1x6I-CG\", \"task_type\" : \"TRAINING\" } } ] } } . ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/api/#searching-for-a-task",
    "relUrl": "/ml-commons-plugin/api/#searching-for-a-task"
  },"1067": {
    "doc": "API",
    "title": "Deleting a task",
    "content": "Delete a task based on the task_id. ML Commons does not check the task status when running the Delete request. There is a risk that a currently running task could be deleted before the task completes. To check the status of a task, run GET /_plugins/_ml/tasks/&lt;task_id&gt; before task deletion. DELETE /_plugins/_ml/tasks/{task_id} . The API returns the following: . { \"_index\" : \".plugins-ml-task\", \"_id\" : \"xQRYLX8BydmmU1x6nuD3\", \"_version\" : 4, \"result\" : \"deleted\", \"_shards\" : { \"total\" : 2, \"successful\" : 2, \"failed\" : 0 }, \"_seq_no\" : 42, \"_primary_term\" : 7 } . ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/api/#deleting-a-task",
    "relUrl": "/ml-commons-plugin/api/#deleting-a-task"
  },"1068": {
    "doc": "API",
    "title": "Stats",
    "content": "Get statistics related to the number of tasks. To receive all stats, use: . GET /_plugins/_ml/stats . To receive stats for a specific node, use: . GET /_plugins/_ml/&lt;nodeId&gt;/stats/ . To receive stats for a specific node and return a specified stat, use: . GET /_plugins/_ml/&lt;nodeId&gt;/stats/&lt;stat&gt; . To receive information on a specific stat from all nodes, use: . GET /_plugins/_ml/stats/&lt;stat&gt; . Example: Get all stats . GET /_plugins/_ml/stats . Response . { \"zbduvgCCSOeu6cfbQhTpnQ\" : { \"ml_executing_task_count\" : 0 }, \"54xOe0w8Qjyze00UuLDfdA\" : { \"ml_executing_task_count\" : 0 }, \"UJiykI7bTKiCpR-rqLYHyw\" : { \"ml_executing_task_count\" : 0 }, \"zj2_NgIbTP-StNlGZJlxdg\" : { \"ml_executing_task_count\" : 0 }, \"jjqFrlW7QWmni1tRnb_7Dg\" : { \"ml_executing_task_count\" : 0 }, \"3pSSjl5PSVqzv5-hBdFqyA\" : { \"ml_executing_task_count\" : 0 }, \"A_IiqoloTDK01uZvCjREaA\" : { \"ml_executing_task_count\" : 0 } } . ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/api/#stats",
    "relUrl": "/ml-commons-plugin/api/#stats"
  },"1069": {
    "doc": "API",
    "title": "Execute",
    "content": "Some algorithms, such as Localization, don’t require trained models. You can run no-model-based algorithms using the execute API. POST _plugins/_ml/_execute/&lt;algorithm_name&gt; . Example: Execute localization . The following example uses the Localization algorithm to find subset-level information for aggregate data (for example, aggregated over time) that demonstrates the activity of interest, such as spikes, drops, changes, or anomalies. POST /_plugins/_ml/_execute/anomaly_localization { \"index_name\": \"rca-index\", \"attribute_field_names\": [ \"attribute\" ], \"aggregations\": [ { \"sum\": { \"sum\": { \"field\": \"value\" } } } ], \"time_field_name\": \"timestamp\", \"start_time\": 1620630000000, \"end_time\": 1621234800000, \"min_time_interval\": 86400000, \"num_outputs\": 10 } . Upon execution, the API returns the following: . \"results\" : [ { \"name\" : \"sum\", \"result\" : { \"buckets\" : [ { \"start_time\" : 1620630000000, \"end_time\" : 1620716400000, \"overall_aggregate_value\" : 65.0 }, { \"start_time\" : 1620716400000, \"end_time\" : 1620802800000, \"overall_aggregate_value\" : 75.0, \"entities\" : [ { \"key\" : [ \"attr0\" ], \"contribution_value\" : 1.0, \"base_value\" : 2.0, \"new_value\" : 3.0 }, { \"key\" : [ \"attr1\" ], \"contribution_value\" : 1.0, \"base_value\" : 3.0, \"new_value\" : 4.0 }, { \"key\" : [ \"attr2\" ], \"contribution_value\" : 1.0, \"base_value\" : 4.0, \"new_value\" : 5.0 }, { \"key\" : [ \"attr3\" ], \"contribution_value\" : 1.0, \"base_value\" : 5.0, \"new_value\" : 6.0 }, { \"key\" : [ \"attr4\" ], \"contribution_value\" : 1.0, \"base_value\" : 6.0, \"new_value\" : 7.0 }, { \"key\" : [ \"attr5\" ], \"contribution_value\" : 1.0, \"base_value\" : 7.0, \"new_value\" : 8.0 }, { \"key\" : [ \"attr6\" ], \"contribution_value\" : 1.0, \"base_value\" : 8.0, \"new_value\" : 9.0 }, { \"key\" : [ \"attr7\" ], \"contribution_value\" : 1.0, \"base_value\" : 9.0, \"new_value\" : 10.0 }, { \"key\" : [ \"attr8\" ], \"contribution_value\" : 1.0, \"base_value\" : 10.0, \"new_value\" : 11.0 }, { \"key\" : [ \"attr9\" ], \"contribution_value\" : 1.0, \"base_value\" : 11.0, \"new_value\" : 12.0 } ] }, ... ] } } ] } . ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/api/#execute",
    "relUrl": "/ml-commons-plugin/api/#execute"
  },"1070": {
    "doc": "API",
    "title": "API",
    "content": " ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/api/",
    "relUrl": "/ml-commons-plugin/api/"
  },"1071": {
    "doc": "ML Commons cluster settings",
    "title": "ML Commons cluster settings",
    "content": "To enhance and customize your OpenSearch cluster for machine learning (ML), you can add and modify several configuration settings for the ML Commons plugin in your ‘opensearch.yml’ file. ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/cluster-settings/",
    "relUrl": "/ml-commons-plugin/cluster-settings/"
  },"1072": {
    "doc": "ML Commons cluster settings",
    "title": "Run tasks and models on ML nodes only",
    "content": "If true, ML Commons tasks and models run machine learning (ML) tasks on ML nodes only. If false, tasks and models run on ML nodes first. If no ML nodes exist, tasks and models run on data nodes. We recommend that you do not set this value to “false” on production clusters. Setting . plugins.ml_commons.only_run_on_ml_node: true . Values . | Default value: true | Value range: true or false | . ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/cluster-settings/#run-tasks-and-models-on-ml-nodes-only",
    "relUrl": "/ml-commons-plugin/cluster-settings/#run-tasks-and-models-on-ml-nodes-only"
  },"1073": {
    "doc": "ML Commons cluster settings",
    "title": "Dispatch tasks to ML node",
    "content": "round_robin dispatches ML tasks to ML nodes using round robin routing. least_load gathers runtime information from all ML nodes, like JVM heap memory usage and running tasks, and then dispatches the tasks to the ML node with the lowest load. Setting . plugins.ml_commons.task_dispatch_policy: round_robin . Values . | Default value: round_robin | Value range: round_robin or least_load | . ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/cluster-settings/#dispatch-tasks-to-ml-node",
    "relUrl": "/ml-commons-plugin/cluster-settings/#dispatch-tasks-to-ml-node"
  },"1074": {
    "doc": "ML Commons cluster settings",
    "title": "Set number of ML tasks per node",
    "content": "Sets the number of ML tasks that can run on each ML node. When set to 0, no ML tasks run on any nodes. Setting . plugins.ml_commons.max_ml_task_per_node: 10 . Values . | Default value: 10 | Value range: [0, 10,000] | . ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/cluster-settings/#set-number-of-ml-tasks-per-node",
    "relUrl": "/ml-commons-plugin/cluster-settings/#set-number-of-ml-tasks-per-node"
  },"1075": {
    "doc": "ML Commons cluster settings",
    "title": "Set number of ML models per node",
    "content": "Sets the number of ML models that can be deployed to each ML node. When set to 0, no ML models can deploy on any node. Setting . plugins.ml_commons.max_model_on_node: 10 . Values . | Default value: 10 | Value range: [0, 10,000] | . ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/cluster-settings/#set-number-of-ml-models-per-node",
    "relUrl": "/ml-commons-plugin/cluster-settings/#set-number-of-ml-models-per-node"
  },"1076": {
    "doc": "ML Commons cluster settings",
    "title": "Set sync job intervals",
    "content": "When returning runtime information with the Profile API, ML Commons will run a regular job to sync newly deployed or undeployed models on each node. When set to 0, ML Commons immediately stops sync-up jobs. Setting . plugins.ml_commons.sync_up_job_interval_in_seconds: 3 . Values . | Default value: 3 | Value range: [0, 86,400] | . ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/cluster-settings/#set-sync-job-intervals",
    "relUrl": "/ml-commons-plugin/cluster-settings/#set-sync-job-intervals"
  },"1077": {
    "doc": "ML Commons cluster settings",
    "title": "Predict monitoring requests",
    "content": "Controls how many predict requests are monitored on one node. If set to 0, OpenSearch clears all monitoring predict requests in cache and does not monitor for new predict requests. Setting . plugins.ml_commons.monitoring_request_count: 100 . Value range . | Default value: 100 | Value range: [0, 10,000,000] | . ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/cluster-settings/#predict-monitoring-requests",
    "relUrl": "/ml-commons-plugin/cluster-settings/#predict-monitoring-requests"
  },"1078": {
    "doc": "ML Commons cluster settings",
    "title": "Upload model tasks per node",
    "content": "Controls how many upload model tasks can run in parallel on one node. If set to 0, you cannot upload models to any node. Setting . plugins.ml_commons.max_upload_model_tasks_per_node: 10 . Values . | Default value: 10 | Value range: [0, 10] | . ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/cluster-settings/#upload-model-tasks-per-node",
    "relUrl": "/ml-commons-plugin/cluster-settings/#upload-model-tasks-per-node"
  },"1079": {
    "doc": "ML Commons cluster settings",
    "title": "Load model tasks per node",
    "content": "Controls how many load model tasks can run in parallel on one node. If set to 0, you cannot load models to any node. Setting . plugins.ml_commons.max_load_model_tasks_per_node: 10 . Values . | Default value: 10 | Value range: [0, 10] | . ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/cluster-settings/#load-model-tasks-per-node",
    "relUrl": "/ml-commons-plugin/cluster-settings/#load-model-tasks-per-node"
  },"1080": {
    "doc": "ML Commons cluster settings",
    "title": "Add trusted URL",
    "content": "The default value allows you to upload a model file from any http/https/ftp/local file. You can change this value to restrict trusted model URLs. Setting . The default URL value for this trusted URL setting is not secure. To ensure the security, please use you own regex string to the trusted repository that contains your models, for example https://github.com/opensearch-project/ml-commons/blob/2.x/ml-algorithms/src/test/resources/org/opensearch/ml/engine/algorithms/text_embedding/*. plugins.ml_commons.trusted_url_regex: &lt;model-repository-url&gt; . Values . | Default value: \"^(https?|ftp|file)://[-a-zA-Z0-9+&amp;@#/%?=~_|!:,.;]*[-a-zA-Z0-9+&amp;@#/%=~_|]\" | Value range: Java regular expression (regex) string | . ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/cluster-settings/#add-trusted-url",
    "relUrl": "/ml-commons-plugin/cluster-settings/#add-trusted-url"
  },"1081": {
    "doc": "ML Commons cluster settings",
    "title": "Assign task timeout",
    "content": "Assigns how long in seconds an ML task will live. After the timeout, the task will fail. Setting . plugins.ml_commons.ml_task_timeout_in_seconds: 600 . Values . | Default value: 600 | Value range: [1, 86,400] | . ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/cluster-settings/#assign-task-timeout",
    "relUrl": "/ml-commons-plugin/cluster-settings/#assign-task-timeout"
  },"1082": {
    "doc": "ML Commons cluster settings",
    "title": "Set native memory threshold",
    "content": "Sets a circuit breaker that checks all system memory usage before running an ML task. If the native memory exceeds the threshold, OpenSearch throws an exception and stops running any ML task. Values are based on the percentage of memory available. When set to 0, no ML tasks will run. When set to 100, the circuit breaker closes and no threshold exists. Setting . plugins.ml_commons.native_memory_threshold: 90 . Values . | Default value: 90 | Value range: [0, 100] | . ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/cluster-settings/#set-native-memory-threshold",
    "relUrl": "/ml-commons-plugin/cluster-settings/#set-native-memory-threshold"
  },"1083": {
    "doc": "ML Commons cluster settings",
    "title": "Allow custom deployment plans",
    "content": "When enabled, this setting grants users the ability to deploy models to specific ML nodes according to that user’s permissions. Setting . plugins.ml_commons.allow_custom_deployment_plan: false . Values . | Default value: false | Value range: [false, true] | . ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/cluster-settings/#allow-custom-deployment-plans",
    "relUrl": "/ml-commons-plugin/cluster-settings/#allow-custom-deployment-plans"
  },"1084": {
    "doc": "ML Commons cluster settings",
    "title": "Enable auto redeploy",
    "content": "This setting automatically redeploys deployed or partially deployed models upon cluster failure. If all ML nodes inside a cluster crash, the model switches to the DEPLOYED_FAILED state, and the model must be deployed manually. Setting . plugins.ml_commons.model_auto_redeploy.enable: false . Values . | Default value: false | Value range: [false, true] | . ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/cluster-settings/#enable-auto-redeploy",
    "relUrl": "/ml-commons-plugin/cluster-settings/#enable-auto-redeploy"
  },"1085": {
    "doc": "ML Commons cluster settings",
    "title": "Set retires for auto redeploy",
    "content": "This setting sets the limit for the number of times a deployed or partially deployed model will try and redeploy when ML nodes in a cluster fail or new ML nodes join the cluster. Setting . plugins.ml_commons.model_auto_redeploy.lifetime_retry_times: 3 . Values . | Default value: 3 | Value range: [0, 100] | . ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/cluster-settings/#set-retires-for-auto-redeploy",
    "relUrl": "/ml-commons-plugin/cluster-settings/#set-retires-for-auto-redeploy"
  },"1086": {
    "doc": "ML Commons cluster settings",
    "title": "Set auto redeploy success ratio",
    "content": "This setting sets the ratio of success for the auto-redeployment of a model based on the available ML nodes in a cluster. For example, if ML nodes crash inside a cluster, the auto redeploy protocol adds another node or retires a crashed node. If the ratio is 0.7 and 70% of all ML nodes successfully redeploy the model on auto-redeploy activation, the redeployment is a success. If the model redeploys on fewer than 70% of available ML nodes, the auto-redeploy retries until the redeployment succeeds or OpenSearch reaches the maximum number of retries. Setting . plugins.ml_commons.model_auto_redeploy_success_ratio: 0.8 . Values . | Default value: 0.8 | Value range: [0, 1] | . ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/cluster-settings/#set-auto-redeploy-success-ratio",
    "relUrl": "/ml-commons-plugin/cluster-settings/#set-auto-redeploy-success-ratio"
  },"1087": {
    "doc": "GPU acceleration",
    "title": "GPU acceleration",
    "content": "GPU acceleration is an experimental feature. For updates on the progress of GPU acceleration, or if you want to leave feedback that could help improve the feature, join the discussion in the OpenSearch forum. When running a natural language processing (NLP) model in your OpenSearch cluster with a machine learning (ML) node, you can achieve better performance on the ML node using graphics processing unit (GPU) acceleration. GPUs can work in tandem with the CPU of your cluster to speed up the model upload and training. ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/gpu-acceleration/",
    "relUrl": "/ml-commons-plugin/gpu-acceleration/"
  },"1088": {
    "doc": "GPU acceleration",
    "title": "Supported GPUs",
    "content": "Currently, ML nodes following GPU instances: . | NVIDIA instances with CUDA 11.6 | AWS Inferentia | . If you need GPU power, you can provision GPU instances through Amazon Elastic Compute Cloud (Amazon EC2). For more information on how to provision a GPU instance, see Recommended GPU Instances. ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/gpu-acceleration/#supported-gpus",
    "relUrl": "/ml-commons-plugin/gpu-acceleration/#supported-gpus"
  },"1089": {
    "doc": "GPU acceleration",
    "title": "Supported images",
    "content": "You can use GPU acceleration with both Docker images with CUDA 11.6 and Amazon Machine Images (AMIs). ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/gpu-acceleration/#supported-images",
    "relUrl": "/ml-commons-plugin/gpu-acceleration/#supported-images"
  },"1090": {
    "doc": "GPU acceleration",
    "title": "PyTorch",
    "content": "GPU-accelerated ML nodes require PyTorch 1.12.1 work with ML models. ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/gpu-acceleration/#pytorch",
    "relUrl": "/ml-commons-plugin/gpu-acceleration/#pytorch"
  },"1091": {
    "doc": "GPU acceleration",
    "title": "Setting up a GPU-accelerated ML node",
    "content": "Depending on the GPU, you can provision a GPU-accelerated ML node manually or by using automated initialization scripts. Preparing an NVIDIA ML node . NVIDIA uses CUDA to increase node performance. In order to take advantage of CUDA, you need to make sure that your drivers include the nvidia-uvm kernel inside the /dev directory. To check for the kernel, enter ls -al /dev | grep nvidia-uvm. If the nvidia-uvm kernel does not exist, run nvidia-uvm-init.sh: . #!/bin/bash ## Script to initialize nvidia device nodes. ## https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#runfile-verifications /sbin/modprobe nvidia if [ \"$?\" -eq 0 ]; then # Count the number of NVIDIA controllers found. NVDEVS=`lspci | grep -i NVIDIA` N3D=`echo \"$NVDEVS\" | grep \"3D controller\" | wc -l` NVGA=`echo \"$NVDEVS\" | grep \"VGA compatible controller\" | wc -l` N=`expr $N3D + $NVGA - 1` for i in `seq 0 $N`; do mknod -m 666 /dev/nvidia$i c 195 $i done mknod -m 666 /dev/nvidiactl c 195 255 else exit 1 fi /sbin/modprobe nvidia-uvm if [ \"$?\" -eq 0 ]; then # Find out the major device number used by the nvidia-uvm driver D=`grep nvidia-uvm /proc/devices | awk '{print $1}'` mknod -m 666 /dev/nvidia-uvm c $D 0 mknod -m 666 /dev/nvidia-uvm-tools c $D 0 else exit 1 fi . After verifying that nvidia-uvm exists under /dev, you can start OpenSearch inside your cluster. Preparing AWS Inferentia ML node . Depending on the Linux operating system running on AWS Inferentia, you can use the following commands and scripts to provision an ML node and run OpenSearch inside your cluster. To start, download and install OpenSearch on your cluster. Then export OpenSearch and set up your environment variables. This example exports OpenSearch into the directory opensearch-2.5.0, so OPENSEARCH_HOME = opensearch-2.5.0: . echo \"export OPENSEARCH_HOME=~/opensearch-2.5.0\" | tee -a ~/.bash_profile echo \"export PYTORCH_VERSION=1.12.1\" | tee -a ~/.bash_profile source ~/.bash_profile . Next, create a shell script file called prepare_torch_neuron.sh. You can copy and customize one of the following examples based on your Linux operating system: . | Ubuntu 20.04 | Amazon Linux 2 | . After you’ve run the scripts, exit your current terminal and open a new terminal to start OpenSearch. GPU acceleration has only been tested on Ubuntu 20.04 and Amazon Linux 2. However, you can use other Linux operating systems. Ubuntu 20.04 . /etc/os-release sudo tee /etc/apt/sources.list.d/neuron.list &gt; /dev/null &lt;&lt;EOF deb https://apt.repos.neuron.amazonaws.com ${VERSION_CODENAME} main EOF wget -qO - https://apt.repos.neuron.amazonaws.com/GPG-PUB-KEY-AMAZON-AWS-NEURON.PUB | sudo apt-key add - # Update OS packages sudo apt-get update -y ################################################################################################################ # To install or update to Neuron versions 1.19.1 and newer from previous releases: # - DO NOT skip 'aws-neuron-dkms' install or upgrade step, you MUST install or upgrade to latest Neuron driver ################################################################################################################ # Install OS headers sudo apt-get install linux-headers-$(uname -r) -y # Install Neuron Driver sudo apt-get install aws-neuronx-dkms -y #################################################################################### # Warning: If Linux kernel is updated as a result of OS package update # Neuron driver (aws-neuron-dkms) should be re-installed after reboot #################################################################################### # Install Neuron Tools sudo apt-get install aws-neuronx-tools -y ###################################################### # Only for Ubuntu 20 - Install Python3.7 sudo add-apt-repository ppa:deadsnakes/ppa sudo apt-get install python3.7 ###################################################### # Install Python venv and activate Python virtual environment to install # Neuron pip packages. cd ~ sudo apt-get install -y python3.7-venv g++ python3.7 -m venv pytorch_venv source pytorch_venv/bin/activate pip install -U pip # Set pip repository to point to the Neuron repository pip config set global.extra-index-url https://pip.repos.neuron.amazonaws.com #Install Neuron PyTorch pip install torch-neuron torchvision # If you need to trace the neuron model, install torch neuron with this command # pip install torch-neuron neuron-cc[tensorflow] \"protobuf==3.20.1\" torchvision # If you need to trace neuron model, install the transformers for tracing the Huggingface model. # pip install transformers # Copy torch neuron lib to OpenSearch PYTORCH_NEURON_LIB_PATH=~/pytorch_venv/lib/python3.7/site-packages/torch_neuron/lib/ mkdir -p $OPENSEARCH_HOME/lib/torch_neuron; cp -r $PYTORCH_NEURON_LIB_PATH/ $OPENSEARCH_HOME/lib/torch_neuron export PYTORCH_EXTRA_LIBRARY_PATH=$OPENSEARCH_HOME/lib/torch_neuron/lib/libtorchneuron.so echo \"export PYTORCH_EXTRA_LIBRARY_PATH=$OPENSEARCH_HOME/lib/torch_neuron/lib/libtorchneuron.so\" | tee -a ~/.bash_profile # Increase JVm stack size to &gt;=2MB echo \"-Xss2m\" | tee -a $OPENSEARCH_HOME/config/jvm.options # Increase max file descriptors to 65535 echo \"$(whoami) - nofile 65535\" | sudo tee -a /etc/security/limits.conf # max virtual memory areas vm.max_map_count to 262144 sudo sysctl -w vm.max_map_count=262144 . Amazon Linux 2 . # Configure Linux for Neuron repository updates sudo tee /etc/yum.repos.d/neuron.repo &gt; /dev/null &lt;&lt;EOF [neuron] name=Neuron YUM Repository baseurl=https://yum.repos.neuron.amazonaws.com enabled=1 metadata_expire=0 EOF sudo rpm --import https://yum.repos.neuron.amazonaws.com/GPG-PUB-KEY-AMAZON-AWS-NEURON.PUB # Update OS packages sudo yum update -y ################################################################################################################ # To install or update to Neuron versions 1.19.1 and newer from previous releases: # - DO NOT skip 'aws-neuron-dkms' install or upgrade step, you MUST install or upgrade to latest Neuron driver ################################################################################################################ # Install OS headers sudo yum install kernel-devel-$(uname -r) kernel-headers-$(uname -r) -y # Install Neuron Driver #################################################################################### # Warning: If Linux kernel is updated as a result of OS package update # Neuron driver (aws-neuron-dkms) should be re-installed after reboot #################################################################################### sudo yum install aws-neuronx-dkms -y # Install Neuron Tools sudo yum install aws-neuronx-tools -y # Install Python venv and activate Python virtual environment to install # Neuron pip packages. cd ~ sudo yum install -y python3.7-venv gcc-c++ python3.7 -m venv pytorch_venv source pytorch_venv/bin/activate pip install -U pip # Set Pip repository to point to the Neuron repository pip config set global.extra-index-url https://pip.repos.neuron.amazonaws.com # Install Neuron PyTorch pip install torch-neuron torchvision # If you need to trace the neuron model, install torch neuron with this command # pip install torch-neuron neuron-cc[tensorflow] \"protobuf&lt;4\" torchvision # If you need to run the trace neuron model, install transformers for tracing Huggingface model. # pip install transformers # Copy torch neuron lib to OpenSearch PYTORCH_NEURON_LIB_PATH=~/pytorch_venv/lib/python3.7/site-packages/torch_neuron/lib/ mkdir -p $OPENSEARCH_HOME/lib/torch_neuron; cp -r $PYTORCH_NEURON_LIB_PATH/ $OPENSEARCH_HOME/lib/torch_neuron export PYTORCH_EXTRA_LIBRARY_PATH=$OPENSEARCH_HOME/lib/torch_neuron/lib/libtorchneuron.so echo \"export PYTORCH_EXTRA_LIBRARY_PATH=$OPENSEARCH_HOME/lib/torch_neuron/lib/libtorchneuron.so\" | tee -a ~/.bash_profile # Increase JVm stack size to &gt;=2MB echo \"-Xss2m\" | tee -a $OPENSEARCH_HOME/config/jvm.options # Increase max file descriptors to 65535 echo \"$(whoami) - nofile 65535\" | sudo tee -a /etc/security/limits.conf # max virtual memory areas vm.max_map_count to 262144 sudo sysctl -w vm.max_map_count=262144 . When the script completes running, open a new terminal for the settings to take effect. Then, start OpenSearch. OpenSearch should now be running inside your GPU-accelerated cluster. However, if any errors occur during provisioning, you can install the GPU accelerator drivers manually. Prepare ML node manually . If the previous two scripts do not provision your GPU-accelerated node properly, you can install the drivers for AWS Inferentia manually: . | Deploy an AWS accelerator instance based on your chosen Linux operating system. For instructions, see Deploy on AWS accelerator instance. | Copy the Neuron library into OpenSearch. The following command uses a directory named opensearch-2.5.0: . OPENSEARCH_HOME=~/opensearch-2.5.0 . | Set the PYTORCH_EXTRA_LIBRARY_PATH path. In this example, we create a pytorch virtual environment in the OPENSEARCH_HOME folder: . PYTORCH_NEURON_LIB_PATH=~/pytorch_venv/lib/python3.7/site-packages/torch_neuron/lib/ mkdir -p $OPENSEARCH_HOME/lib/torch_neuron; cp -r $PYTORCH_NEURON_LIB_PATH/ $OPENSEARCH_HOME/lib/torch_neuron export PYTORCH_EXTRA_LIBRARY_PATH=$OPENSEARCH_HOME/lib/torch_neuron/lib/libtorchneuron.so . | (Optional) To monitor the GPU usage of your accelerator instance, install Neuron tools, which allows models to be used inside your instance: . # Install Neuron Tools sudo apt-get install aws-neuronx-tools -y . # Add Neuron tools your PATH export PATH=/opt/aws/neuron/bin:$PATH . # Test Neuron tools neuron-top . | To make sure you have enough memory to upload a model, increase the JVM stack size to &gt;+2MB: . echo \"-Xss2m\" | sudo tee -a $OPENSEARCH_HOME/config/jvm.options . | Start OpenSearch. | . ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/gpu-acceleration/#setting-up-a-gpu-accelerated-ml-node",
    "relUrl": "/ml-commons-plugin/gpu-acceleration/#setting-up-a-gpu-accelerated-ml-node"
  },"1092": {
    "doc": "GPU acceleration",
    "title": "Troubleshooting",
    "content": "Due to the amount of data required to work with ML models, you might encounter the following max file descriptors or vm.max_map_count errors when trying to run OpenSearch in a your cluster: . [1]: max file descriptors [8192] for opensearch process is too low, increase to at least [65535] [2]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144] . To troubleshoot the max file descriptors error, run the following command: . echo \"$(whoami) - nofile 65535\" | sudo tee -a /etc/security/limits.conf . To fix the vm.max_map_count error, run this command to increase the count to 262114: . sudo sysctl -w vm.max_map_count=262144 . ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/gpu-acceleration/#troubleshooting",
    "relUrl": "/ml-commons-plugin/gpu-acceleration/#troubleshooting"
  },"1093": {
    "doc": "GPU acceleration",
    "title": "Next steps",
    "content": "If you want to try a GPU-accelerated cluster using AWS Inferentia with a pretrained HuggingFace model, see Compiling and Deploying HuggingFace Pretrained BERT. ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/gpu-acceleration/#next-steps",
    "relUrl": "/ml-commons-plugin/gpu-acceleration/#next-steps"
  },"1094": {
    "doc": "About ML Commons",
    "title": "ML Commons plugin",
    "content": "ML Commons for OpenSearch eases the development of machine learning features by providing a set of common machine learning (ML) algorithms through transport and REST API calls. Those calls choose the right nodes and resources for each ML request and monitors ML tasks to ensure uptime. This allows you to leverage existing open-source ML algorithms and reduce the effort required to develop new ML features. Interaction with the ML Commons plugin occurs through either the REST API or ad and kmeans Piped Processing Language (PPL) commands. Models trained through the ML Commons plugin support model-based algorithms such as kmeans. After you’ve trained a model enough so that it meets your precision requirements, you can apply the model to predict new data safely. Should you not want to use a model, you can use the Train and Predict API to test your model without having to evaluate the model’s performance. ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/index/#ml-commons-plugin",
    "relUrl": "/ml-commons-plugin/index/#ml-commons-plugin"
  },"1095": {
    "doc": "About ML Commons",
    "title": "Permissions",
    "content": "There are two reserved user roles that can use of the ML Commons plugin. | ml_full_access: Full access to all ML features, including starting new ML tasks and reading or deleting models. | ml_readonly_access: Can only read ML tasks, trained models and statistics relevant to the model’s cluster. Cannot start nor delete ML tasks or models. | . ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/index/#permissions",
    "relUrl": "/ml-commons-plugin/index/#permissions"
  },"1096": {
    "doc": "About ML Commons",
    "title": "ML node",
    "content": "To prevent your cluster from failing when running ML tasks, you configure a node with the ml node role. When configuring without the data node role, ML nodes will not store any shards and will calculate resource requirements at runtime. To use an ML node, create a node in your opensearch.yml file. Give your node a custom name and define the node role as ml: . node.name: ml-node node.roles: [ ml ] . ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/index/#ml-node",
    "relUrl": "/ml-commons-plugin/index/#ml-node"
  },"1097": {
    "doc": "About ML Commons",
    "title": "About ML Commons",
    "content": " ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/index/",
    "relUrl": "/ml-commons-plugin/index/"
  },"1098": {
    "doc": "Managing ML models in OpenSearch Dashboards",
    "title": "Enabling ML in Dashboards",
    "content": "In OpenSearch 2.6, ML functionality is disabled by default. To enable it, you need to edit the configuration in opensearch_dashboards.yml and then restart your cluster. To enable the feature: . | In your OpenSearch cluster, navigate to your Dashboards home directory; for example, in Docker, /usr/share/opensearch-dashboards. | Open your local copy of the Dashboards configuration file opensearch_dashboards.yml. If you don’t have a copy, get one from GitHub: opensearch_dashboards.yml. | Add the setting ml_commons_dashboards.enabled: to opensearch_dashboards.yml. Then, set it to ml_commons_dashboards.enabled: true and save the configuration file. | Restart the Dashboards container. | Verify that the feature configuration settings were created and configured properly by launching OpenSearch Dashboards. The Machine Learning section should appear under OpenSearch plugins. | . ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/ml-dashboard/#enabling-ml-in-dashboards",
    "relUrl": "/ml-commons-plugin/ml-dashboard/#enabling-ml-in-dashboards"
  },"1099": {
    "doc": "Managing ML models in OpenSearch Dashboards",
    "title": "Accessing ML functionality in Dashboards",
    "content": "To access ML functionality in OpenSearch Dashboards,select OpenSearch plugins &gt; Machine Learning. In the Machine Learning section, you now have access to the Deployed models dashboard. ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/ml-dashboard/#accessing-ml-functionality-in-dashboards",
    "relUrl": "/ml-commons-plugin/ml-dashboard/#accessing-ml-functionality-in-dashboards"
  },"1100": {
    "doc": "Managing ML models in OpenSearch Dashboards",
    "title": "Deployed models dashboard",
    "content": "The deployed models dashboard gives admins the ability to check the status of any models stored inside your OpenSearch cluster. The dashboard includes the following information about the model: . | Name: The name of the model given upon upload. | Status: The number of nodes for which the model responds. | When all nodes are responsive, the status is Green. | When some nodes are responsive,the status is Yellow. | When all nodes are unresponsive,the status is Red. | . | Model ID: The model ID. | Action: What actions you can take with the model. | . As of OpenSearch 2.6, the only action available is View Status Details, shown in the following image. When selected, the Status Details panel appears. The panel provides the following details inside the panel: . | Model ID | Model status by node: The number of nodes for which the model is responsive. | . A list of nodes gives you a view of each node the model is running on, including each node’s Node ID and status, as shown in the following image. This is useful if you want to use the node’s Node ID to determine why a node is unresponsive. ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/ml-dashboard/#deployed-models-dashboard",
    "relUrl": "/ml-commons-plugin/ml-dashboard/#deployed-models-dashboard"
  },"1101": {
    "doc": "Managing ML models in OpenSearch Dashboards",
    "title": "Next steps",
    "content": "For more information about how to manage ML models in OpenSearch, see Model-serving framework. ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/ml-dashboard/#next-steps",
    "relUrl": "/ml-commons-plugin/ml-dashboard/#next-steps"
  },"1102": {
    "doc": "Managing ML models in OpenSearch Dashboards",
    "title": "Managing ML models in OpenSearch Dashboards",
    "content": "Released in OpenSearch 2.6, the machine learning (ML) functionality in OpenSearch Dashboards is experimental and can’t be used in a production environment. For updates or to leave feedback, see the OpenSearch Forum discussion. Administrators of machine learning (ML) clusters can use OpenSearch Dashboards to manage and check the status of ML models running inside a cluster. This can help ML developers provision nodes to ensure their models run efficiently. As of OpenSearch 2.6, you can only upload models using the API. For more information about how to upload a model to your cluster, see Upload model to OpenSearch. ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/ml-dashboard/",
    "relUrl": "/ml-commons-plugin/ml-dashboard/"
  },"1103": {
    "doc": "Model-serving framework",
    "title": "Model-serving framework",
    "content": "The model-serving framework is an experimental feature. For updates on the progress of the model-serving framework, or if you want to leave feedback that could help improve the feature, join the discussion in the Model-serving framework forum. ML Commons allows you to serve custom models and use those models to make inferences. For those who want to run their PyTorch deep learning model inside an OpenSearch cluster, you can upload and run that model with the ML Commons REST API. This page outlines the steps required to upload a custom model and run it with the ML Commons plugin. ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/model-serving-framework/",
    "relUrl": "/ml-commons-plugin/model-serving-framework/"
  },"1104": {
    "doc": "Model-serving framework",
    "title": "Prerequisites",
    "content": "To upload a custom model to OpenSearch, you need to prepare it outside of your OpenSearch cluster. You can use a pretrained model, like one from Huggingface, or train a new model in accordance with your needs. Model support . As of OpenSearch 2.6, the model-serving framework supports text embedding models. Model format . To use a model in OpenSearch, you’ll need to export the model into a portable format. As of Version 2.5, OpenSearch only supports the TorchScript and ONNX formats. Furthermore, files must be saved as zip files before upload. Therefore, to ensure that ML Commons can upload your model, compress your TorchScript file before uploading. You can download an example file here. Model size . Most deep learning models are more than 100 MB, making it difficult to fit them into a single document. OpenSearch splits the model file into smaller chunks to be stored in a model index. When allocating machine learning (ML) or data nodes for your OpenSearch cluster, make sure you correctly size your ML nodes so that you have enough memory when making ML inferences. ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/model-serving-framework/#prerequisites",
    "relUrl": "/ml-commons-plugin/model-serving-framework/#prerequisites"
  },"1105": {
    "doc": "Model-serving framework",
    "title": "GPU acceleration",
    "content": "To achieve better performance within the model-serving framework, you can take advantage of GPU acceleration on your ML node. For more information, see GPU acceleration. ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/model-serving-framework/#gpu-acceleration",
    "relUrl": "/ml-commons-plugin/model-serving-framework/#gpu-acceleration"
  },"1106": {
    "doc": "Model-serving framework",
    "title": "Upload model to OpenSearch",
    "content": "Use the URL upload operation for models that already exist on another server, such as GitHub or S3. POST /_plugins/_ml/models/_upload . The URL upload method requires the following request fields. | Field | Data type | Description | . | name | String | The name of the model. | . | version | String | The version number of the model. Since OpenSearch does not enforce a specific version schema for models, you can choose any number or format that makes sense for your models. | . | model_format | String | The portable format of the model file. Currently only supports TORCH_SCRIPT. | . | model_config | JSON object | The model’s configuration, including the model_type, embedding_dimension, and framework_type. | . | url | string | The URL where the model is located. | . The model_config object . | Field | Data type | Description | . | model_type | String | The model type, such as bert. For a Huggingface model, the model type is specified in config.json. For an example, see the all-MiniLM-L6-v2 Huggingface model config.json. | . | embedding_dimension | Integer | The dimension of the model-generated dense vector. For a Huggingface model, the dimension is specified in the model card. For example, in the all-MiniLM-L6-v2 Huggingface model card, the statement 384 dimensional dense vector space specifies 384 as the embedding dimension. | . | framework_type | String | The framework the model is using. Currently, we support sentence_transformers and huggingface_transformers frameworks. The sentence_transformers model outputs text embeddings directly, so ML Commons does not perform any post processing. For huggingface_transformers, ML Commons performs post processing by applying mean pooling to get text embeddings. See the example all-MiniLM-L6-v2 Huggingface model for more details. | . | all_config (Optional) | String | This field is used for reference purposes. You can specify all model configurations in this field. For example, if you are using a Huggingface model, you can minify the config.json file to one line and save its contents in the all_config field. Once the model is uploaded, you can use the get model API operation to get all model configurations stored in this field. | . You can further customize a pre-trained sentence transformer model’s post-processing logic with the following optional fields in the model_config object. | Field | Data type | Description | . | pooling_mode | String | The post-process model output, either mean, mean_sqrt_len, max, weightedmean, or cls. | . | normalize_result | Boolean | When set to true, normalizes the model output in order to scale to a standard range for the model. | . Example request . The following example request uploads version 1.0.0 of a natural language processing (NLP) sentence transformation model named all-MiniLM-L6-v2: . POST /_plugins/_ml/models/_upload { \"name\": \"all-MiniLM-L6-v2\", \"version\": \"1.0.0\", \"description\": \"test model\", \"model_format\": \"TORCH_SCRIPT\", \"model_config\": { \"model_type\": \"bert\", \"embedding_dimension\": 384, \"framework_type\": \"sentence_transformers\" }, \"url\": \"https://github.com/opensearch-project/ml-commons/raw/2.x/ml-algorithms/src/test/resources/org/opensearch/ml/engine/algorithms/text_embedding/all-MiniLM-L6-v2_torchscript_sentence-transformer.zip?raw=true\" } . Example response . OpenSearch responds with the task_id and task status: . { \"task_id\" : \"ew8I44MBhyWuIwnfvDIH\", \"status\" : \"CREATED\" } . To see the status of your model upload, pass the task_id into the task API. ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/model-serving-framework/#upload-model-to-opensearch",
    "relUrl": "/ml-commons-plugin/model-serving-framework/#upload-model-to-opensearch"
  },"1107": {
    "doc": "Model-serving framework",
    "title": "Load the model",
    "content": "The load model operation reads the model’s chunks from the model index and then creates an instance of the model to load into memory. The bigger the model, the more chunks the model is split into. The more chunks a model index contains, the longer it takes for the model to load into memory. Get the model_id . To load a model, you need the model_id. To find the model_id, take the task_id from the model’s upload operations API response and use the GET _ml/tasks API. This example request uses the task_id from the upload example. GET /_plugins/_ml/tasks/ew8I44MBhyWuIwnfvDIH . OpenSearch responds with the model_id: . { \"model_id\" : \"WWQI44MBbzI2oUKAvNUt\", \"task_type\" : \"UPLOAD_MODEL\", \"function_name\" : \"TEXT_EMBEDDING\", \"state\" : \"COMPLETED\", \"worker_node\" : \"KzONM8c8T4Od-NoUANQNGg\", \"create_time\" : 3455961564003, \"last_update_time\" : 3216361373241, \"is_async\" : true } . Load the model from the model index . With the model_id, you can now load the model from the model’s index in order to deploy the model to ML nodes. The load API reads model chunks from the model index, creates an instance of that model, and saves the model instance in the ML node’s cache. Add the model_id to the load API: . POST /_plugins/_ml/models/&lt;model_id&gt;/_load . By default, the ML Commons setting plugins.ml_commons.only_run_on_ml_node is set to false. When false, models load on ML nodes first. If no ML nodes exist, models load on data nodes. When running ML models in production, set plugins.ml_commons.only_run_on_ml_node to true so that models only load on ML nodes. Example request: Load into any available ML node . In this example request, OpenSearch loads the model into all available OpenSearch node: . POST /_plugins/_ml/models/WWQI44MBbzI2oUKAvNUt/_load . Example request: Load into a specific node . If you want to reserve the memory of other ML nodes within your cluster, you can load your model into a specific node(s) by specifying each node’s ID in the request body: . POST /_plugins/_ml/models/WWQI44MBbzI2oUKAvNUt/_load { \"node_ids\": [\"4PLK7KJWReyX0oWKnBA8nA\"] } . Example response . All models load asynchronously. Therefore, the load API responds with a new task_id based on the load and responds with a new status for the task. { \"task_id\" : \"hA8P44MBhyWuIwnfvTKP\", \"status\" : \"CREATED\" } . Check the model load status . With your task_id from the load response, you can use the GET _ml/tasks API to see the load status of your model. Before a loaded model can be used for inferences, the load task’s state must be COMPLETED. Example request . GET /_plugins/_ml/tasks/hA8P44MBhyWuIwnfvTKP . Example response . { \"model_id\" : \"WWQI44MBbzI2oUKAvNUt\", \"task_type\" : \"LOAD_MODEL\", \"function_name\" : \"TEXT_EMBEDDING\", \"state\" : \"COMPLETED\", \"worker_node\" : \"KzONM8c8T4Od-NoUANQNGg\", \"create_time\" : 1665961803150, \"last_update_time\" : 1665961815959, \"is_async\" : true } . ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/model-serving-framework/#load-the-model",
    "relUrl": "/ml-commons-plugin/model-serving-framework/#load-the-model"
  },"1108": {
    "doc": "Model-serving framework",
    "title": "Use the loaded model for inferences",
    "content": "After the model has been loaded, you can enter the model_id into the predict API to perform inferences. POST /_plugins/_ml/models/&lt;model_id&gt;/_predict . Example request . POST /_plugins/_ml/_predict/text_embedding/WWQI44MBbzI2oUKAvNUt { \"text_docs\":[ \"today is sunny\"], \"return_number\": true, \"target_response\": [\"sentence_embedding\"] } . Example response . { \"inference_results\" : [ { \"output\" : [ { \"name\" : \"sentence_embedding\", \"data_type\" : \"FLOAT32\", \"shape\" : [ 384 ], \"data\" : [ -0.023315024, 0.08975691, 0.078479774, ... ] } ] } ] } . ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/model-serving-framework/#use-the-loaded-model-for-inferences",
    "relUrl": "/ml-commons-plugin/model-serving-framework/#use-the-loaded-model-for-inferences"
  },"1109": {
    "doc": "Model-serving framework",
    "title": "Unload the model",
    "content": "If you’re done making predictions with your model, use the unload operation to remove the model from your memory cache. The model will remain accessible in the model index. POST /_plugins/_ml/models/&lt;model_id&gt;/_unload . Example request . POST /_plugins/_ml/models/MGqJhYMBbbh0ushjm8p_/_unload . Example response . { \"s5JwjZRqTY6nOT0EvFwVdA\": { \"stats\": { \"MGqJhYMBbbh0ushjm8p_\": \"deleted\" } } } . ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/model-serving-framework/#unload-the-model",
    "relUrl": "/ml-commons-plugin/model-serving-framework/#unload-the-model"
  },"1110": {
    "doc": "Pretrained models",
    "title": "Pretrained models",
    "content": "The model-serving framework supports a variety of open-source pretrained models that can assist with a range of machine learning (ML) search and analytics use cases. ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/pretrained-models/#pretrained-models",
    "relUrl": "/ml-commons-plugin/pretrained-models/#pretrained-models"
  },"1111": {
    "doc": "Pretrained models",
    "title": "Uploading pretrained models",
    "content": "To use a pretrained model in your OpenSearch cluster: . | Select the model you want to upload. For a list of pretrained models, see supported pretrained models. | Upload the model using the upload API. Because a pretrained model originates from the ML Commons model repository, you only need to provide the name, version, and model_format in the upload API request. | . POST /_plugins/_ml/models/_upload { \"name\": \"huggingface/sentence-transformers/all-MiniLM-L12-v2\", \"version\": \"1.0.1\", \"model_format\": \"TORCH_SCRIPT\" } . For more information on how to upload and use ML models, see Model-serving framework. ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/pretrained-models/#uploading-pretrained-models",
    "relUrl": "/ml-commons-plugin/pretrained-models/#uploading-pretrained-models"
  },"1112": {
    "doc": "Pretrained models",
    "title": "Supported pretrained models",
    "content": "The model-serving framework supports the following models, categorized by type. All models are traced from Hugging Face. Although models with the same type will have similar use cases, each model has a different model size and performs differently depending on your cluster. For a comparison of the performances of some pretrained models, see the sbert documentation. Sentence transformers . Sentence transformer models map sentences and paragraphs across a dimensional dense vector space. The number of vectors depends on the model. Use these models for use cases such as clustering and semantic search. The following table provides a list of sentence transformer models and artifact links to download them. As of OpenSearch 2.6, all artifacts are set to version 1.0.1. | Model name | Vector dimensions | Auto-truncation | Torchscript artifact | ONNX artifact | . | sentence-transformers/all-distilroberta-v1 | 768-dimensional dense vector space. | Yes | - model_url- config_url | - model_url- config_url | . | sentence-transformers/all-MiniLM-L6-v2 | 384-dimensional dense vector space. | Yes | - model_url- config_url | - model_url- config_url | . | sentence-transformers/all-MiniLM-L12-v2 | 384-dimensional dense vector space. | Yes | - model_url- config_url | - model_url- config_url | . | sentence-transformers/all-mpnet-base-v2 | 768-dimensional dense vector space. | Yes | - model_url- config_url | - model_url- config_url | . | sentence-transformers/msmarco-distilbert-base-tas-b | 768-dimensional dense vector space. Optimized for semantic search. | No | - model_url- config_url | - model_url- config_url | . | sentence-transformers/multi-qa-MiniLM-L6-cos-v1 | 384 dimensional dense vector space. Designed for semantic search and trained on 215 million question/answer pairs. | Yes | - model_url- config_url | - model_url- config_url | . | sentence-transformers/multi-qa-mpnet-base-dot-v1 | 384 dimensional dense vector space. | Yes | - model_url- config_url | - model_url- config_url | . | sentence-transformers/paraphrase-MiniLM-L3-v2 | 384-dimensional dense vector space. | Yes | - model_url- config_url | - model_url- config_url | . | sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 | 384-dimensional dense vector space. | Yes | - model_url- config_url | - model_url- config_url | . ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/pretrained-models/#supported-pretrained-models",
    "relUrl": "/ml-commons-plugin/pretrained-models/#supported-pretrained-models"
  },"1113": {
    "doc": "Pretrained models",
    "title": "Pretrained models",
    "content": "The model-serving framework is an experimental feature. For updates on the progress of the model-serving framework, or if you want to leave feedback that could help improve the feature, join the discussion in the Model-serving framework forum. ",
    "url": "https://vagimeli.github.io/ml-commons-plugin/pretrained-models/",
    "relUrl": "/ml-commons-plugin/pretrained-models/"
  },"1114": {
    "doc": "Monitoring your cluster",
    "title": "Monitoring your cluster",
    "content": "OpenSearch provides several ways for you to monitor your cluster health and performance and automate common tasks: . | The OpenSearch logs include valuable information for monitoring cluster operations and troubleshooting issues. | Performance analyzer is an agent and REST API that allows you to query numerous performance metrics for your cluster, including aggregations of those metrics. | OpenSearch Job Scheduler plugin provides a framework that you can use to build schedules for common cluster management tasks. | . ",
    "url": "https://vagimeli.github.io/monitoring-your-cluster/index/",
    "relUrl": "/monitoring-your-cluster/index/"
  },"1115": {
    "doc": "Job Scheduler",
    "title": "Job Scheduler",
    "content": "The OpenSearch Job Scheduler plugin provides a framework that can be used to build schedules for common tasks performed on your cluster. You can use Job Scheduler’s Service Provider Interface (SPI) to define schedules for cluster management tasks such as taking snapshots, managing your data’s lifecycle, and running periodic jobs. Job Scheduler has a sweeper that listens for updated events on the OpenSearch cluster and a scheduler that manages when jobs run. You can install the Job Scheduler plugin by following the standard OpenSearch plugin installation process. The sample-extension-plugin example provided in the Job Scheduler GitHub repository provides a complete example of utilizing Job Scheduler when building a plugin. To define schedules, you build a plugin that implements the interfaces provided in the Job Scheduler library. You can schedule jobs by specifying an interval, or you can use a Unix cron expression such as 0 12 * * ?, which runs at noon every day, to define a more flexible schedule. ",
    "url": "https://vagimeli.github.io/monitoring-your-cluster/job-scheduler/index/",
    "relUrl": "/monitoring-your-cluster/job-scheduler/index/"
  },"1116": {
    "doc": "Job Scheduler",
    "title": "Building a plugin for Job Scheduler",
    "content": "OpenSearch plugin developers can extend the Job Scheduler plugin to schedule jobs to perform on the cluster. Jobs you can schedule include running aggregation queries against raw data, saving the aggregated data to a new index every hour, or continuing to monitor the shard allocation by calling the OpenSearch API and then posting the output to a webhook. For examples of building a plugin that uses the Job Scheduler plugin, see the Job Scheduler README. ",
    "url": "https://vagimeli.github.io/monitoring-your-cluster/job-scheduler/index/#building-a-plugin-for-job-scheduler",
    "relUrl": "/monitoring-your-cluster/job-scheduler/index/#building-a-plugin-for-job-scheduler"
  },"1117": {
    "doc": "Job Scheduler",
    "title": "Defining an endpoint",
    "content": "You can configure your plugin’s API endpoint by referencing the example SampleExtensionRestHandler.java file. Set the endpoint URL that your plugin will expose with WATCH_INDEX_URI: . public class SampleExtensionRestHandler extends BaseRestHandler { public static final String WATCH_INDEX_URI = \"/_plugins/scheduler_sample/watch\"; . You can define the job configuration by extending ScheduledJobParameter. You can also define the fields used by your plugin, like indexToWatch, as shown in the example SampleJobParameter file. This job configuration will be saved as a document in an index you define, as shown in this example. ",
    "url": "https://vagimeli.github.io/monitoring-your-cluster/job-scheduler/index/#defining-an-endpoint",
    "relUrl": "/monitoring-your-cluster/job-scheduler/index/#defining-an-endpoint"
  },"1118": {
    "doc": "Job Scheduler",
    "title": "Configuring parameters",
    "content": "You can configure your plugin’s parameters by referencing the example SampleJobParameter.java file and modifying it to fit your needs: . /** * A sample job parameter. * &lt;p&gt; * It adds an additional \"indexToWatch\" field to {@link ScheduledJobParameter}, which stores the index * the job runner will watch. */ public class SampleJobParameter implements ScheduledJobParameter { public static final String NAME_FIELD = \"name\"; public static final String ENABLED_FILED = \"enabled\"; public static final String LAST_UPDATE_TIME_FIELD = \"last_update_time\"; public static final String LAST_UPDATE_TIME_FIELD_READABLE = \"last_update_time_field\"; public static final String SCHEDULE_FIELD = \"schedule\"; public static final String ENABLED_TIME_FILED = \"enabled_time\"; public static final String ENABLED_TIME_FILED_READABLE = \"enabled_time_field\"; public static final String INDEX_NAME_FIELD = \"index_name_to_watch\"; public static final String LOCK_DURATION_SECONDS = \"lock_duration_seconds\"; public static final String JITTER = \"jitter\"; private String jobName; private Instant lastUpdateTime; private Instant enabledTime; private boolean isEnabled; private Schedule schedule; private String indexToWatch; private Long lockDurationSeconds; private Double jitter; . Next, configure the request parameters you would like your plugin to use with Job Scheduler. These will be based on the variables you declare when configuring your plugin. The following example shows the request parameters you set when building your plugin: . public SampleJobParameter(String id, String name, String indexToWatch, Schedule schedule, Long lockDurationSeconds, Double jitter) { this.jobName = name; this.indexToWatch = indexToWatch; this.schedule = schedule; Instant now = Instant.now(); this.isEnabled = true; this.enabledTime = now; this.lastUpdateTime = now; this.lockDurationSeconds = lockDurationSeconds; this.jitter = jitter; } @Override public String getName() { return this.jobName; } @Override public Instant getLastUpdateTime() { return this.lastUpdateTime; } @Override public Instant getEnabledTime() { return this.enabledTime; } @Override public Schedule getSchedule() { return this.schedule; } @Override public boolean isEnabled() { return this.isEnabled; } @Override public Long getLockDurationSeconds() { return this.lockDurationSeconds; } @Override public Double getJitter() { return jitter; } . The following table describes the request parameters configured in the previous example. All the request parameters shown are required. | Field | Data type | Description | . | getName | String | Returns the name of the job. | . | getLastUpdateTime | Time unit | Returns the time that the job was last run. | . | getEnabledTime | Time unit | Returns the time that the job was enabled. | . | getSchedule | Unix cron | Returns the job schedule formatted in Unix cron syntax. | . | isEnabled | Boolean | Indicates whether or not the job is enabled. | . | getLockDurationSeconds | Integer | Returns the duration of time for which the job is locked. | . | getJitter | Integer | Returns the defined jitter value. | . The logic used by your job should be defined by a class extended from ScheduledJobRunner in the SampleJobParameter.java sample file, such as SampleJobRunner. While the job is running, there is a locking mechanism you can use to prevent other nodes from running the same job. First, acquire the lock. Then make sure to release the lock before the job finishes. For more information, see the Job Scheduler sample extension directory in the Job Scheduler GitHub repo. ",
    "url": "https://vagimeli.github.io/monitoring-your-cluster/job-scheduler/index/#configuring-parameters",
    "relUrl": "/monitoring-your-cluster/job-scheduler/index/#configuring-parameters"
  },"1119": {
    "doc": "Logs",
    "title": "Logs",
    "content": "The OpenSearch logs include valuable information for monitoring cluster operations and troubleshooting issues. The location of the logs differs based on the installation type: . | On Docker, OpenSearch writes most logs to the console and stores the remainder in opensearch/logs/. The tarball installation also uses opensearch/logs/. | On most Linux installations, OpenSearch writes logs to /var/log/opensearch/. | . Logs are available as .log (plain text) and .json files. Permissions for the OpenSearch logs are -rw-r--r-- by default, meaning that any user account on the node can read them. You can change this behavior for each log type in log4j2.properties using the filePermissions option. For example, you might add appender.rolling.filePermissions = rw-r----- to change permissions for the JSON server log. For details, see the Log4j 2 documentation. ",
    "url": "https://vagimeli.github.io/monitoring-your-cluster/logs/",
    "relUrl": "/monitoring-your-cluster/logs/"
  },"1120": {
    "doc": "Logs",
    "title": "Application logs",
    "content": "For its application logs, OpenSearch uses Apache Log4j 2 and its built-in log levels (from least to most severe) of TRACE, DEBUG, INFO, WARN, ERROR, and FATAL. The default OpenSearch log level is INFO. Rather than changing the default log level (logger.level), you change the log level for individual OpenSearch modules: . PUT /_cluster/settings { \"persistent\" : { \"logger.org.opensearch.index.reindex\" : \"DEBUG\" } } . The easiest way to identify modules is not from the logs, which abbreviate the path (for example, o.o.i.r), but from the OpenSearch source code. After this sample change, OpenSearch emits much more detailed logs during reindex operations: . [2019-10-18T16:52:51,184][DEBUG][o.o.i.r.TransportReindexAction] [node1] [1626]: starting [2019-10-18T16:52:51,186][DEBUG][o.o.i.r.TransportReindexAction] [node1] executing initial scroll against [some-index] [2019-10-18T16:52:51,291][DEBUG][o.o.i.r.TransportReindexAction] [node1] scroll returned [3] documents with a scroll id of [DXF1Z==] [2019-10-18T16:52:51,292][DEBUG][o.o.i.r.TransportReindexAction] [node1] [1626]: got scroll response with [3] hits [2019-10-18T16:52:51,294][DEBUG][o.o.i.r.WorkerBulkByScrollTaskState] [node1] [1626]: preparing bulk request for [0s] [2019-10-18T16:52:51,297][DEBUG][o.o.i.r.TransportReindexAction] [node1] [1626]: preparing bulk request [2019-10-18T16:52:51,299][DEBUG][o.o.i.r.TransportReindexAction] [node1] [1626]: sending [3] entry, [222b] bulk request [2019-10-18T16:52:51,310][INFO ][o.e.c.m.MetaDataMappingService] [node1] [some-new-index/R-j3adc6QTmEAEb-eAie9g] create_mapping [_doc] [2019-10-18T16:52:51,383][DEBUG][o.o.i.r.TransportReindexAction] [node1] [1626]: got scroll response with [0] hits [2019-10-18T16:52:51,384][DEBUG][o.o.i.r.WorkerBulkByScrollTaskState] [node1] [1626]: preparing bulk request for [0s] [2019-10-18T16:52:51,385][DEBUG][o.o.i.r.TransportReindexAction] [node1] [1626]: preparing bulk request [2019-10-18T16:52:51,386][DEBUG][o.o.i.r.TransportReindexAction] [node1] [1626]: finishing without any catastrophic failures [2019-10-18T16:52:51,395][DEBUG][o.o.i.r.TransportReindexAction] [node1] Freed [1] contexts . The DEBUG and TRACE levels are extremely verbose. If you enable either one to troubleshoot a problem, disable it after you finish. There are other ways to change log levels: . | Add lines to opensearch.yml: . logger.org.opensearch.index.reindex: debug . Modifying opensearch.yml makes the most sense if you want to reuse your logging configuration across multiple clusters or debug startup issues with a single node. | Modify log4j2.properties: . # Define a new logger with unique ID of reindex logger.reindex.name = org.opensearch.index.reindex # Set the log level for that ID logger.reindex.level = debug . This approach is extremely flexible, but requires familiarity with the Log4j 2 property file syntax. In general, the other options offer a simpler configuration experience. If you examine the default log4j2.properties file in the configuration directory, you can see a few OpenSearch-specific variables: . appender.console.layout.pattern = [%d{ISO8601}][%-5p][%-25c{1.}] [%node_name]%marker %m%n appender.rolling_old.fileName = ${sys:os.logs.base_path}${sys:file.separator}${sys:os.logs.cluster_name}.log . | ${sys:os.logs.base_path} is the directory for logs (for example, /var/log/opensearch/). | ${sys:os.logs.cluster_name} is the name of the cluster. | [%node_name] is the name of the node. | . | . ",
    "url": "https://vagimeli.github.io/monitoring-your-cluster/logs/#application-logs",
    "relUrl": "/monitoring-your-cluster/logs/#application-logs"
  },"1121": {
    "doc": "Logs",
    "title": "Slow logs",
    "content": "OpenSearch has two slow logs, logs that help you identify performance issues: the search slow log and the indexing slow log. These logs rely on thresholds to define what qualifies as a “slow” search or indexing operation. For example, you might decide that a query is slow if it takes more than 15 seconds to complete. Unlike application logs, which you configure for modules, you configure slow logs for indexes. By default, both logs are disabled (all thresholds are set to -1): . GET &lt;some-index&gt;/_settings?include_defaults=true { \"indexing\": { \"slowlog\": { \"reformat\": \"true\", \"threshold\": { \"index\": { \"warn\": \"-1\", \"trace\": \"-1\", \"debug\": \"-1\", \"info\": \"-1\" } }, \"source\": \"1000\", \"level\": \"TRACE\" } }, \"search\": { \"slowlog\": { \"level\": \"TRACE\", \"threshold\": { \"fetch\": { \"warn\": \"-1\", \"trace\": \"-1\", \"debug\": \"-1\", \"info\": \"-1\" }, \"query\": { \"warn\": \"-1\", \"trace\": \"-1\", \"debug\": \"-1\", \"info\": \"-1\" } } } } } . To enable these logs, increase one or more thresholds: . PUT &lt;some-index&gt;/_settings { \"indexing\": { \"slowlog\": { \"threshold\": { \"index\": { \"warn\": \"15s\", \"trace\": \"750ms\", \"debug\": \"3s\", \"info\": \"10s\" } }, \"source\": \"500\", \"level\": \"INFO\" } } } . In this example, OpenSearch logs indexing operations that take 15 seconds or longer at the WARN level and operations that take between 10 and 14.x seconds at the INFO level. If you set a threshold to 0 seconds, OpenSearch logs all operations, which can be useful for testing whether slow logs are indeed enabled. | reformat specifies whether to log the document _source field as a single line (true) or let it span multiple lines (false). | source is the number of characters of the document _source field to log. | level is the minimum log level to include. | . A line from opensearch_index_indexing_slowlog.log might look like this: . node1 | [2019-10-24T19:48:51,012][WARN][i.i.s.index] [node1] [some-index/i86iF5kyTyy-PS8zrdDeAA] took[3.4ms], took_millis[3], type[_doc], id[1], routing[], source[{\"title\":\"Your Name\", \"Director\":\"Makoto Shinkai\"}] . Slow logs can consume considerable disk space if you set thresholds or levels too low. Consider enabling them temporarily for troubleshooting or performance tuning. To disable slow logs, return all thresholds to -1. ",
    "url": "https://vagimeli.github.io/monitoring-your-cluster/logs/#slow-logs",
    "relUrl": "/monitoring-your-cluster/logs/#slow-logs"
  },"1122": {
    "doc": "Logs",
    "title": "Task logs",
    "content": "OpenSearch can log CPU time and memory utilization for the top N memory expensive search tasks when task resource consumers are enabled. By default, task resource consumers will log the top 10 search tasks at 60 second intervals. These values can be configured in opensearch.yml. Task logging is enabled dynamically through the cluster settings API: . PUT _cluster/settings { \"persistent\" : { \"task_resource_consumers.enabled\" : \"true\" } } . Enabling task resource consumers can have an impact on search latency. Once enabled, logs will be written to logs/opensearch_task_detailslog.json and logs/opensearch_task_detailslog.log. To configure the logging interval and the number of search tasks logged, add the following lines to opensearch.yml: . # Number of expensive search tasks to log cluster.task.consumers.top_n.size:100 # Logging interval cluster.task.consumers.top_n.frequency:30s . ",
    "url": "https://vagimeli.github.io/monitoring-your-cluster/logs/#task-logs",
    "relUrl": "/monitoring-your-cluster/logs/#task-logs"
  },"1123": {
    "doc": "Logs",
    "title": "Deprecation logs",
    "content": "Deprecation logs record when clients make deprecated API calls to your cluster. These logs can help you identify and fix issues prior to upgrading to a new major version. By default, OpenSearch logs deprecated API calls at the WARN level, which works well for almost all use cases. If desired, configure logger.deprecation.level using _cluster/settings, opensearch.yml, or log4j2.properties. ",
    "url": "https://vagimeli.github.io/monitoring-your-cluster/logs/#deprecation-logs",
    "relUrl": "/monitoring-your-cluster/logs/#deprecation-logs"
  },"1124": {
    "doc": "API",
    "title": "Performance Analyzer API",
    "content": "Introduced 1.0 . Performance Analyzer uses a single HTTP method and URI for most requests: . GET &lt;endpoint&gt;:9600/_plugins/_performanceanalyzer/metrics . Note the use of port 9600. Provide parameters for metrics, aggregations, dimensions, and nodes (optional): . ?metrics=&lt;metrics&gt;&amp;agg=&lt;aggregations&gt;&amp;dim=&lt;dimensions&gt;&amp;nodes=all\" . For a full list of metrics, see Metrics reference. Performance Analyzer updates its data every five seconds. If you create a custom client, we recommend using that same interval for calls to the API. Example request . GET localhost:9600/_plugins/_performanceanalyzer/metrics?metrics=Latency,CPU_Utilization&amp;agg=avg,max&amp;dim=ShardID&amp;nodes=all . Example response . { \"keHlhQbbTpm1BYicficEQg\": { \"timestamp\": 1554940530000, \"data\": { \"fields\": [{ \"name\": \"ShardID\", \"type\": \"VARCHAR\" }, { \"name\": \"Latency\", \"type\": \"DOUBLE\" }, { \"name\": \"CPU_Utilization\", \"type\": \"DOUBLE\" } ], \"records\": [ [ null, null, 0.012552206029147535 ], [ \"1\", 4.8, 0.0009780939762972104 ] ] } }, \"bHdpbMJZTs-TKtZro2SmYA\": { \"timestamp\": 1554940530000, \"data\": { \"fields\": [{ \"name\": \"ShardID\", \"type\": \"VARCHAR\" }, { \"name\": \"Latency\", \"type\": \"DOUBLE\" }, { \"name\": \"CPU_Utilization\", \"type\": \"DOUBLE\" } ], \"records\": [ [ null, 18.2, 0.011966493817311527 ], [ \"1\", 14.8, 0.0007670829370071493 ] ] } } } . In this case, each top-level object represents a node. The API returns names and data types for the metrics and dimensions that you specified, along with values from five seconds ago and current values (if different). Null values represent inactivity during that time period. Performance Analyzer has one additional URI that returns the unit for each metric. Example request . GET localhost:9600/_plugins/_performanceanalyzer/metrics/units . Example response . { \"Disk_Utilization\": \"%\", \"Cache_Request_Hit\": \"count\", \"HTTP_RequestDocs\": \"count\", \"Net_TCP_Lost\": \"segments/flow\", \"Refresh_Time\": \"ms\", \"GC_Collection_Event\": \"count\", \"Merge_Time\": \"ms\", \"Sched_CtxRate\": \"count/s\", \"Cache_Request_Size\": \"B\", \"ThreadPool_QueueSize\": \"count\", \"Sched_Runtime\": \"s/ctxswitch\", \"Disk_ServiceRate\": \"MB/s\", \"Heap_AllocRate\": \"B/s\", \"Heap_Max\": \"B\", \"Sched_Waittime\": \"s/ctxswitch\", \"ShardBulkDocs\": \"count\", \"Thread_Blocked_Time\": \"s/event\", \"VersionMap_Memory\": \"B\", \"Master_Task_Queue_Time\": \"ms\", \"Merge_CurrentEvent\": \"count\", \"Indexing_Buffer\": \"B\", \"Bitset_Memory\": \"B\", \"Net_PacketDropRate4\": \"packets/s\", \"Heap_Committed\": \"B\", \"Net_PacketDropRate6\": \"packets/s\", \"Thread_Blocked_Event\": \"count\", \"GC_Collection_Time\": \"ms\", \"Cache_Query_Miss\": \"count\", \"IO_TotThroughput\": \"B/s\", \"Latency\": \"ms\", \"Net_PacketRate6\": \"packets/s\", \"Cache_Query_Hit\": \"count\", \"IO_ReadSyscallRate\": \"count/s\", \"Net_PacketRate4\": \"packets/s\", \"Cache_Request_Miss\": \"count\", \"CB_ConfiguredSize\": \"B\", \"CB_TrippedEvents\": \"count\", \"ThreadPool_RejectedReqs\": \"count\", \"Disk_WaitTime\": \"ms\", \"Net_TCP_TxQ\": \"segments/flow\", \"Master_Task_Run_Time\": \"ms\", \"IO_WriteSyscallRate\": \"count/s\", \"IO_WriteThroughput\": \"B/s\", \"Flush_Event\": \"count\", \"Net_TCP_RxQ\": \"segments/flow\", \"Refresh_Event\": \"count\", \"Flush_Time\": \"ms\", \"Heap_Init\": \"B\", \"CPU_Utilization\": \"cores\", \"HTTP_TotalRequests\": \"count\", \"ThreadPool_ActiveThreads\": \"count\", \"Cache_Query_Size\": \"B\", \"Paging_MinfltRate\": \"count/s\", \"Merge_Event\": \"count\", \"Net_TCP_SendCWND\": \"B/flow\", \"Cache_Request_Eviction\": \"count\", \"Segments_Total\": \"count\", \"Heap_Used\": \"B\", \"Cache_FieldData_Eviction\": \"count\", \"IO_TotalSyscallRate\": \"count/s\", \"CB_EstimatedSize\": \"B\", \"Net_Throughput\": \"B/s\", \"Paging_RSS\": \"pages\", \"Indexing_ThrottleTime\": \"ms\", \"IndexWriter_Memory\": \"B\", \"Master_PendingQueueSize\": \"count\", \"Net_TCP_SSThresh\": \"B/flow\", \"Cache_FieldData_Size\": \"B\", \"Paging_MajfltRate\": \"count/s\", \"ThreadPool_TotalThreads\": \"count\", \"IO_ReadThroughput\": \"B/s\", \"ShardEvents\": \"count\", \"Net_TCP_NumFlows\": \"count\" } . ",
    "url": "https://vagimeli.github.io/monitoring-your-cluster/pa/api/#performance-analyzer-api",
    "relUrl": "/monitoring-your-cluster/pa/api/#performance-analyzer-api"
  },"1125": {
    "doc": "API",
    "title": "API",
    "content": " ",
    "url": "https://vagimeli.github.io/monitoring-your-cluster/pa/api/",
    "relUrl": "/monitoring-your-cluster/pa/api/"
  },"1126": {
    "doc": "Create PerfTop Dashboards",
    "title": "PerfTop dashboards",
    "content": "Dashboards are defined in JSON and composed of three main elements: tables, line graphs, and bar graphs. You define a grid of rows and columns and then place elements within that grid, with each element spanning as many rows and columns as you specify. The best way to get started with building custom dashboards is to duplicate and modify one of the existing JSON files in the dashboards directory. . | Summary of elements | Position elements | Add queries | Add options . | All elements | Tables | Bars | Lines | . | . ",
    "url": "https://vagimeli.github.io/monitoring-your-cluster/pa/dashboards/#perftop-dashboards",
    "relUrl": "/monitoring-your-cluster/pa/dashboards/#perftop-dashboards"
  },"1127": {
    "doc": "Create PerfTop Dashboards",
    "title": "Summary of elements",
    "content": ". | Tables show metrics per dimension. For example, if your metric is CPU_Utilization and your dimension ShardID, a PerfTop table shows a row for each shard on each node. | Bar graphs are aggregated for the cluster, unless you add nodeName to the dashboard. See the options for all elements. | Line graphs are aggregated for each node. Each line represents a node. | . ",
    "url": "https://vagimeli.github.io/monitoring-your-cluster/pa/dashboards/#summary-of-elements",
    "relUrl": "/monitoring-your-cluster/pa/dashboards/#summary-of-elements"
  },"1128": {
    "doc": "Create PerfTop Dashboards",
    "title": "Position elements",
    "content": "PerfTop positions elements within a grid. For example, consider this 12 * 12 grid. The upper-left of the grid represents row 0, column 0, so the starting positions for the three boxes are: . | Orange: row 0, column 0 | Purple: row 2, column 2 | Green: row 1, column 6 | . These boxes span a number of rows and columns. In this case: . | Orange: 2 rows, 4 columns | Purple: 1 row, 4 columns | Green: 3 rows, 2 columns | . In JSON form, we have the following: . { \"gridOptions\": { \"rows\": 12, \"cols\": 12 }, \"graphs\": { \"tables\": [{ \"options\": { \"gridPosition\": { \"row\": 0, \"col\": 0, \"rowSpan\": 2, \"colSpan\": 4 } } }, { \"options\": { \"gridPosition\": { \"row\": 2, \"col\": 2, \"rowSpan\": 1, \"colSpan\": 4 } } }, { \"options\": { \"gridPosition\": { \"row\": 1, \"col\": 6, \"rowSpan\": 3, \"colSpan\": 2 } } } ] } } . At this point, however, all the JSON does is define the size and position of three tables. To fill elements with data, you specify a query. ",
    "url": "https://vagimeli.github.io/monitoring-your-cluster/pa/dashboards/#position-elements",
    "relUrl": "/monitoring-your-cluster/pa/dashboards/#position-elements"
  },"1129": {
    "doc": "Create PerfTop Dashboards",
    "title": "Add queries",
    "content": "Queries use the same elements as the REST API, just in JSON form: . { \"queryParams\": { \"metrics\": \"estimated,limitConfigured\", \"aggregates\": \"avg,avg\", \"dimensions\": \"type\", \"sortBy\": \"estimated\" } } . For details on available metrics, see Metrics reference. ",
    "url": "https://vagimeli.github.io/monitoring-your-cluster/pa/dashboards/#add-queries",
    "relUrl": "/monitoring-your-cluster/pa/dashboards/#add-queries"
  },"1130": {
    "doc": "Create PerfTop Dashboards",
    "title": "Add options",
    "content": "Options include labels, colors, and a refresh interval. Different elements types have different options. Dashboards support the 16 ANSI colors: black, red, green, yellow, blue, magenta, cyan, and white. For the “bright” variants of these colors, use the numbers 8–15. If your terminal supports 256 colors, you can also use hex codes (e.g. #6D40ED). All elements . | Option | Type | Description | . | label | String or integer | The text in the upper-left corner of the box. | . | labelColor | String or integer | The color of the label. | . | refreshInterval | Integer | The number of milliseconds between calls to the Performance Analyzer API for new data. Minimum value is 5000. | . | dimensionFilters | String array | The dimension value to display for the graph. For example, if you query for metric=Net_Throughput&amp;agg=sum&amp;dim=Direction and the possible dimension values are in and out, you can define dimensionFilters: [\"in\"] to only display the metric data for in dimension | . | nodeName | String | If non-null, lets you restrict elements to individual nodes. You can specify the node name directly in the dashboard file, but the better approach is to use \"nodeName\": \"#nodeName\" in the dashboard and include the --nodename &lt;node_name&gt; argument when starting PerfTop. | . Tables . | Option | Type | Description | . | bg | String or integer | The background color. | . | fg | String or integer | The text color. | . | selectedFg | String or integer | The text color for focused text. | . | selectedBg | String or integer | The background color for focused text. | . | columnSpacing | Integer | The amount of space (measured in characters) between columns. | . | keys | Boolean | Has no impact at this time. | . Bars . | Option | Type | Description | . | barWidth | Integer | The width of each bar (measured in characters) in the graph. | . | xOffset | Integer | The amount of space (measured in characters) between the y-axis and the first bar in the graph. | . | maxHeight | Integer | The maximum height of each bar (measured in characters) in the graph. | . Lines . | Option | Type | Description | . | showNthLabel | Integer | Which of the xAxis labels to show. For example, \"showNthLabel\": 2 shows every other label. | . | showLegend | Boolean | Whether or not to display a legend for the line graph. | . | legend.width | Integer | The width of the legend (measured in characters) in the graph. | . | xAxis | String array | Array of labels for the x-axis. For example, [\"0:00\", \"0:10\", \"0:20\", \"0:30\", \"0:40\", \"0:50\"]. | . | colors | String array | Array of line colors to choose from. For example, [\"magenta\", \"cyan\"]. If you don’t provide this value, PerfTop chooses random colors for each line. | . ",
    "url": "https://vagimeli.github.io/monitoring-your-cluster/pa/dashboards/#add-options",
    "relUrl": "/monitoring-your-cluster/pa/dashboards/#add-options"
  },"1131": {
    "doc": "Create PerfTop Dashboards",
    "title": "Create PerfTop Dashboards",
    "content": " ",
    "url": "https://vagimeli.github.io/monitoring-your-cluster/pa/dashboards/",
    "relUrl": "/monitoring-your-cluster/pa/dashboards/"
  },"1132": {
    "doc": "Performance Analyzer",
    "title": "Performance analyzer",
    "content": "Performance analyzer is an agent and REST API that allows you to query numerous performance metrics for your cluster, including aggregations of those metrics. The performance analyzer plugin is installed by default in OpenSearch version 2.0 and higher. ",
    "url": "https://vagimeli.github.io/monitoring-your-cluster/pa/index/#performance-analyzer",
    "relUrl": "/monitoring-your-cluster/pa/index/#performance-analyzer"
  },"1133": {
    "doc": "Performance Analyzer",
    "title": "Performance analyzer installation and configuration",
    "content": "The following sections provide the steps for installing and configuring the performance analyzer plugin. Install performance analyzer . The performance analyzer plugin is included in the installation for Docker and tarball. If you need to install the performance analyzer plugin manually, download the plugin from Maven and install the plugin using the standard plugins install process. Performance analyzer will run on each node in a cluster. To start the performance analyzer root cause analysis (RCA) agent on a tarball installation, run the following command: . OPENSEARCH_HOME=~/opensearch-2.2.1 OPENSEARCH_JAVA_HOME=~/opensearch-2.2.1/jdk OPENSEARCH_PATH_CONF=~/opensearch-2.2.1/bin ./performance-analyzer-agent-cli . The following command enables the performance analyzer plugin and performance analyzer RCA agent: . curl -XPOST localhost:9200/_plugins/_performanceanalyzer/cluster/config -H 'Content-Type: application/json' -d '{\"enabled\": true}' . To shut down the performance analyzer RCA agent, run the following command: . kill $(ps aux | grep -i 'PerformanceAnalyzerApp' | grep -v grep | awk '{print $2}') . To disable the performance analyzer plugin, run the following command: . curl -XPOST localhost:9200/_plugins/_performanceanalyzer/cluster/config -H 'Content-Type: application/json' -d '{\"enabled\": false}' . To uninstall the performance analyzer plugin, run the following command: . bin/opensearch-plugin remove opensearch-performance-analyzer . Configure performance analyzer . To configure the performance analyzer plugin, you will need to edit the performance-analyzer.properties configuration file in the config/opensearch-performance-analyzer/ directory. Make sure to uncomment the line #webservice-bind-host and set it to 0.0.0.0. You can reference the following example configuration. # ======================== OpenSearch performance analyzer plugin config ========================= # NOTE: this is an example for Linux. Please modify the config accordingly if you are using it under other OS. # WebService bind host; default to all interfaces webservice-bind-host = 0.0.0.0 # Metrics data location metrics-location = /dev/shm/performanceanalyzer/ # Metrics deletion interval (minutes) for metrics data. # Interval should be between 1 to 60. metrics-deletion-interval = 1 # If set to true, the system cleans up the files behind it. So at any point, we should expect only 2 # metrics-db-file-prefix-path files. If set to false, no files are cleaned up. This can be useful, if you are archiving # the files and wouldn't like for them to be cleaned up. cleanup-metrics-db-files = true # WebService exposed by App's port webservice-listener-port = 9600 # Metric DB File Prefix Path location metrics-db-file-prefix-path = /tmp/metricsdb_ https-enabled = false #Setup the correct path for certificates #certificate-file-path = specify_path #private-key-file-path = specify_path # Plugin Stats Metadata file name, expected to be in the same location plugin-stats-metadata = plugin-stats-metadata # Agent Stats Metadata file name, expected to be in the same location agent-stats-metadata = agent-stats-metadata . To start the performance analyzer RCA agent, run the following command. OPENSEARCH_HOME=~/opensearch-2.2.1 OPENSEARCH_JAVA_HOME=~/opensearch-2.2.1/jdk OPENSEARCH_PATH_CONF=~/opensearch-2.2.1/bin ./performance-analyzer-agent-cli . Storage . Performance analyzer uses /dev/shm for temporary storage. During heavy workloads on a cluster, performance analyzer can use up to 1 GB of space. Docker, however, has a default /dev/shm size of 64 MB. To change this value, you can use the docker run --shm-size 1gb flag or a similar setting in Docker Compose. If you’re not using Docker, check the size of /dev/shm using df -h. The default value is probably plenty, but if you need to change its size, add the following line to /etc/fstab: . tmpfs /dev/shm tmpfs defaults,noexec,nosuid,size=1G 0 0 . Then remount the file system: . mount -o remount /dev/shm . Security . Performance analyzer supports encryption in transit for requests. It currently does not support client or server authentication for requests. To enable encryption in transit, edit performance-analyzer.properties in your $OPENSEARCH_HOME directory. vi $OPENSEARCH_HOME/config/opensearch-performance-analyzer/performance-analyzer.properties . Change the following lines to configure encryption in transit. Note that certificate-file-path must be a certificate for the server, not a root certificate authority (CA). https-enabled = true #Setup the correct path for certificates certificate-file-path = specify_path private-key-file-path = specify_path . Enable performance analyzer for RPM/YUM installations . If you installed OpenSearch from an RPM distribution, you can start and stop performance analyzer with systemctl. # Start OpenSearch Performance Analyzer sudo systemctl start opensearch-performance-analyzer.service # Stop OpenSearch Performance Analyzer sudo systemctl stop opensearch-performance-analyzer.service . ",
    "url": "https://vagimeli.github.io/monitoring-your-cluster/pa/index/#performance-analyzer-installation-and-configuration",
    "relUrl": "/monitoring-your-cluster/pa/index/#performance-analyzer-installation-and-configuration"
  },"1134": {
    "doc": "Performance Analyzer",
    "title": "Example API query and response",
    "content": "The following is an example API query: . GET localhost:9600/_plugins/_performanceanalyzer/metrics/units . The following is an example response: . {\"Disk_Utilization\":\"%\",\"Cache_Request_Hit\":\"count\", \"Refresh_Time\":\"ms\",\"ThreadPool_QueueLatency\":\"count\", \"Merge_Time\":\"ms\",\"ClusterApplierService_Latency\":\"ms\", \"PublishClusterState_Latency\":\"ms\", \"Cache_Request_Size\":\"B\",\"LeaderCheck_Failure\":\"count\", \"ThreadPool_QueueSize\":\"count\",\"Sched_Runtime\":\"s/ctxswitch\",\"Disk_ServiceRate\":\"MB/s\",\"Heap_AllocRate\":\"B/s\",\"Indexing_Pressure_Current_Limits\":\"B\", \"Sched_Waittime\":\"s/ctxswitch\",\"ShardBulkDocs\":\"count\", \"Thread_Blocked_Time\":\"s/event\",\"VersionMap_Memory\":\"B\", \"Master_Task_Queue_Time\":\"ms\",\"IO_TotThroughput\":\"B/s\", \"Indexing_Pressure_Current_Bytes\":\"B\", \"Indexing_Pressure_Last_Successful_Timestamp\":\"ms\", \"Net_PacketRate6\":\"packets/s\",\"Cache_Query_Hit\":\"count\", \"IO_ReadSyscallRate\":\"count/s\",\"Net_PacketRate4\":\"packets/s\",\"Cache_Request_Miss\":\"count\", \"ThreadPool_RejectedReqs\":\"count\",\"Net_TCP_TxQ\":\"segments/flow\",\"Master_Task_Run_Time\":\"ms\", \"IO_WriteSyscallRate\":\"count/s\",\"IO_WriteThroughput\":\"B/s\", \"Refresh_Event\":\"count\",\"Flush_Time\":\"ms\",\"Heap_Init\":\"B\", \"Indexing_Pressure_Rejection_Count\":\"count\", \"CPU_Utilization\":\"cores\",\"Cache_Query_Size\":\"B\", \"Merge_Event\":\"count\",\"Cache_FieldData_Eviction\":\"count\", \"IO_TotalSyscallRate\":\"count/s\",\"Net_Throughput\":\"B/s\", \"Paging_RSS\":\"pages\", \"AdmissionControl_ThresholdValue\":\"count\", \"Indexing_Pressure_Average_Window_Throughput\":\"count/s\", \"Cache_MaxSize\":\"B\",\"IndexWriter_Memory\":\"B\", \"Net_TCP_SSThresh\":\"B/flow\",\"IO_ReadThroughput\":\"B/s\", \"LeaderCheck_Latency\":\"ms\",\"FollowerCheck_Failure\":\"count\", \"HTTP_RequestDocs\":\"count\",\"Net_TCP_Lost\":\"segments/flow\", \"GC_Collection_Event\":\"count\",\"Sched_CtxRate\":\"count/s\", \"AdmissionControl_RejectionCount\":\"count\",\"Heap_Max\":\"B\", \"ClusterApplierService_Failure\":\"count\", \"PublishClusterState_Failure\":\"count\", \"Merge_CurrentEvent\":\"count\",\"Indexing_Buffer\":\"B\", \"Bitset_Memory\":\"B\",\"Net_PacketDropRate4\":\"packets/s\", \"Heap_Committed\":\"B\",\"Net_PacketDropRate6\":\"packets/s\", \"Thread_Blocked_Event\":\"count\",\"GC_Collection_Time\":\"ms\", \"Cache_Query_Miss\":\"count\",\"Latency\":\"ms\", \"Shard_State\":\"count\",\"Thread_Waited_Event\":\"count\", \"CB_ConfiguredSize\":\"B\",\"ThreadPool_QueueCapacity\":\"count\", \"CB_TrippedEvents\":\"count\",\"Disk_WaitTime\":\"ms\", \"Data_RetryingPendingTasksCount\":\"count\", \"AdmissionControl_CurrentValue\":\"count\", \"Flush_Event\":\"count\",\"Net_TCP_RxQ\":\"segments/flow\", \"Shard_Size_In_Bytes\":\"B\",\"Thread_Waited_Time\":\"s/event\", \"HTTP_TotalRequests\":\"count\", \"ThreadPool_ActiveThreads\":\"count\", \"Paging_MinfltRate\":\"count/s\",\"Net_TCP_SendCWND\":\"B/flow\", \"Cache_Request_Eviction\":\"count\",\"Segments_Total\":\"count\", \"FollowerCheck_Latency\":\"ms\",\"Heap_Used\":\"B\", \"Master_ThrottledPendingTasksCount\":\"count\", \"CB_EstimatedSize\":\"B\",\"Indexing_ThrottleTime\":\"ms\", \"Master_PendingQueueSize\":\"count\", \"Cache_FieldData_Size\":\"B\",\"Paging_MajfltRate\":\"count/s\", \"ThreadPool_TotalThreads\":\"count\",\"ShardEvents\":\"count\", \"Net_TCP_NumFlows\":\"count\",\"Election_Term\":\"count\"} . ",
    "url": "https://vagimeli.github.io/monitoring-your-cluster/pa/index/#example-api-query-and-response",
    "relUrl": "/monitoring-your-cluster/pa/index/#example-api-query-and-response"
  },"1135": {
    "doc": "Performance Analyzer",
    "title": "Root cause analysis",
    "content": "The root cause analysis (RCA) framework uses the information from performance analyzer to inform administrators of the root cause of performance and availability issues that their clusters might be experiencing. Enable the RCA framework . To enable the RCA framework, run the following command: . curl -XPOST http://localhost:9200/_plugins/_performanceanalyzer/rca/cluster/config -H 'Content-Type: application/json' -d '{\"enabled\": true}' . If you encounter the curl: (52) Empty reply from server response, run the following command to enable RCA: . curl -XPOST https://localhost:9200/_plugins/_performanceanalyzer/rca/cluster/config -H 'Content-Type: application/json' -d '{\"enabled\": true}' -u 'admin:admin' -k . Example API query and response . To request all available RCAs, run the following command: . GET localhost:9600/_plugins/_performanceanalyzer/rca . To request a specific RCA, run the following command: . GET localhost:9600/_plugins/_performanceanalyzer/rca?name=HighHeapUsageClusterRCA . The following is an example response: . { \"HighHeapUsageClusterRCA\": [{ \"RCA_name\": \"HighHeapUsageClusterRCA\", \"state\": \"unhealthy\", \"timestamp\": 1587426650942, \"HotClusterSummary\": [{ \"number_of_nodes\": 2, \"number_of_unhealthy_nodes\": 1, \"HotNodeSummary\": [{ \"host_address\": \"192.168.144.2\", \"node_id\": \"JtlEoRowSI6iNpzpjlbp_Q\", \"HotResourceSummary\": [{ \"resource_type\": \"old gen\", \"threshold\": 0.65, \"value\": 0.81827232588145373, \"avg\": NaN, \"max\": NaN, \"min\": NaN, \"unit_type\": \"heap usage in percentage\", \"time_period_seconds\": 600, \"TopConsumerSummary\": [{ \"name\": \"CACHE_FIELDDATA_SIZE\", \"value\": 590702564 }, { \"name\": \"CACHE_REQUEST_SIZE\", \"value\": 28375 }, { \"name\": \"CACHE_QUERY_SIZE\", \"value\": 12687 } ], }] }] }] }] } . ",
    "url": "https://vagimeli.github.io/monitoring-your-cluster/pa/index/#root-cause-analysis",
    "relUrl": "/monitoring-your-cluster/pa/index/#root-cause-analysis"
  },"1136": {
    "doc": "Performance Analyzer",
    "title": "Performance analyzer and RCA API references",
    "content": "Related links . Further documentation on the use of performance analyzer and RCA can be found at the following links: . | Performance analyzer API guide. | RCA. | RCA API guide. | RFC: Root cause analysis. | . ",
    "url": "https://vagimeli.github.io/monitoring-your-cluster/pa/index/#performance-analyzer-and-rca-api-references",
    "relUrl": "/monitoring-your-cluster/pa/index/#performance-analyzer-and-rca-api-references"
  },"1137": {
    "doc": "Performance Analyzer",
    "title": "Performance Analyzer",
    "content": " ",
    "url": "https://vagimeli.github.io/monitoring-your-cluster/pa/index/",
    "relUrl": "/monitoring-your-cluster/pa/index/"
  },"1138": {
    "doc": "API",
    "title": "RCA API",
    "content": " ",
    "url": "https://vagimeli.github.io/monitoring-your-cluster/pa/rca/api/#rca-api",
    "relUrl": "/monitoring-your-cluster/pa/rca/api/#rca-api"
  },"1139": {
    "doc": "API",
    "title": "Example request",
    "content": "# Request all available RCAs GET localhost:9600/_plugins/_performanceanalyzer/rca # Request a specific RCA GET localhost:9600/_plugins/_performanceanalyzer/rca?name=HighHeapUsageClusterRca . ",
    "url": "https://vagimeli.github.io/monitoring-your-cluster/pa/rca/api/#example-request",
    "relUrl": "/monitoring-your-cluster/pa/rca/api/#example-request"
  },"1140": {
    "doc": "API",
    "title": "Example response",
    "content": "{ \"HighHeapUsageClusterRca\": [{ \"rca_name\": \"HighHeapUsageClusterRca\", \"state\": \"unhealthy\", \"timestamp\": 1587426650942, \"HotClusterSummary\": [{ \"number_of_nodes\": 2, \"number_of_unhealthy_nodes\": 1, \"HotNodeSummary\": [{ \"host_address\": \"192.168.144.2\", \"node_id\": \"JtlEoRowSI6iNpzpjlbp_Q\", \"HotResourceSummary\": [{ \"resource_type\": \"old gen\", \"threshold\": 0.65, \"value\": 0.81827232588145373, \"avg\": NaN, \"max\": NaN, \"min\": NaN, \"unit_type\": \"heap usage in percentage\", \"time_period_seconds\": 600, \"TopConsumerSummary\": [{ \"name\": \"CACHE_FIELDDATA_SIZE\", \"value\": 590702564 }, { \"name\": \"CACHE_REQUEST_SIZE\", \"value\": 28375 }, { \"name\": \"CACHE_QUERY_SIZE\", \"value\": 12687 } ], }] }] }] }] } . ",
    "url": "https://vagimeli.github.io/monitoring-your-cluster/pa/rca/api/#example-response",
    "relUrl": "/monitoring-your-cluster/pa/rca/api/#example-response"
  },"1141": {
    "doc": "API",
    "title": "API",
    "content": " ",
    "url": "https://vagimeli.github.io/monitoring-your-cluster/pa/rca/api/",
    "relUrl": "/monitoring-your-cluster/pa/rca/api/"
  },"1142": {
    "doc": "Root Cause Analysis",
    "title": "Root Cause Analysis",
    "content": "The OpenSearch Performance Analyzer plugin (PA) captures OpenSearch and JVM activity, plus their lower-level resource usage (e.g. disk, network, CPU, and memory). Based on this instrumentation, Performance Analyzer computes and exposes diagnostic metrics so that administrators can measure and understand the bottlenecks in their OpenSearch clusters. The Root Cause Analysis framework (RCA) uses the information from PA to alert administrators about the root cause of performance and availability issues that their clusters might be experiencing. In broad strokes, the framework helps you access data streams from OpenSearch nodes running Performance Analyzer. You write snippets of Java to choose the streams that matter to you and evaluate the streams’ PA metrics against certain thresholds. As RCA runs, you can access the state of each analysis using the REST API. To learn more about Root Cause Analysis, see its repository on GitHub. ",
    "url": "https://vagimeli.github.io/monitoring-your-cluster/pa/rca/index/",
    "relUrl": "/monitoring-your-cluster/pa/rca/index/"
  },"1143": {
    "doc": "RCA Reference",
    "title": "RCA reference",
    "content": "You can find a reference of available RCAs and their purposes on GitHub. ",
    "url": "https://vagimeli.github.io/monitoring-your-cluster/pa/rca/reference/#rca-reference",
    "relUrl": "/monitoring-your-cluster/pa/rca/reference/#rca-reference"
  },"1144": {
    "doc": "RCA Reference",
    "title": "RCA Reference",
    "content": " ",
    "url": "https://vagimeli.github.io/monitoring-your-cluster/pa/rca/reference/",
    "relUrl": "/monitoring-your-cluster/pa/rca/reference/"
  },"1145": {
    "doc": "Hot shard identification",
    "title": "Hot shard identification",
    "content": "Hot shard identification root cause analysis (RCA) lets you identify a hot shard within an index. A hot shard is an outlier that consumes more resources than other shards and may lead to poor indexing and search performance. The hot shard identification RCA monitors the following metrics: . | CPU utilization | Heap allocation rate | . Shards may become hot because of the nature of your workload. When you use a _routing parameter or a custom document ID, a specific shard or several shards within the cluster receive frequent updates, consuming more CPU and heap resources than other shards. The hot shard identification RCA compares the CPU utilization and heap allocation rates against their threshold values. If the usage for either metric is greater than the threshold, the shard is considered to be hot. For more information about the hot shard identification RCA implementation, see Hot Shard RCA. Example request . The following query requests hot shard identification: . GET _plugins/_performanceanalyzer/rca?name=HotShardClusterRca . copy . Example Response . The response contains a list of unhealthy shards: . \"HotShardClusterRca\": [{ \"rca_name\": \"HotShardClusterRca\", \"timestamp\": 1680721367563, \"state\": \"unhealthy\", \"HotClusterSummary\": [ { \"number_of_nodes\": 3, \"number_of_unhealthy_nodes\": 1, \"HotNodeSummary\": [ { \"node_id\": \"7kosAbpASsqBoHmHkVXxmw\", \"host_address\": \"192.168.80.4\", \"HotResourceSummary\": [ { \"resource_type\": \"cpu usage\", \"resource_metric\": \"cpu usage(num of cores)\", \"threshold\": 0.027397981341796683, \"value\": 0.034449630200405396, \"time_period_seconds\": 60, \"meta_data\": \"ssZw1WRUSHS5DZCW73BOJQ index9 4\" }, { \"resource_type\": \"heap\", \"resource_metric\": \"heap alloc rate(heap alloc rate in bytes per second)\", \"threshold\": 7605441.367010161, \"value\": 10872119.748328414, \"time_period_seconds\": 60, \"meta_data\": \"ssZw1WRUSHS5DZCW73BOJQ index9 4\" }, { \"resource_type\": \"heap\", \"resource_metric\": \"heap alloc rate(heap alloc rate in bytes per second)\", \"threshold\": 7605441.367010161, \"value\": 8019622.354388569, \"time_period_seconds\": 60, \"meta_data\": \"QRF4rBM7SNCDr1g3KU6HyA index9 0\" } ] } ] } ] }] . ",
    "url": "https://vagimeli.github.io/monitoring-your-cluster/pa/rca/shard-hotspot/",
    "relUrl": "/monitoring-your-cluster/pa/rca/shard-hotspot/"
  },"1146": {
    "doc": "Hot shard identification",
    "title": "Response fields",
    "content": "The following table lists the response fields. | Field | Type | Description | . | rca_name | String | The name of the RCA. In this case, “HotShardClusterRca”. | . | timestamp | Integer | The timestamp of the RCA. | . | state | Object | The state of the cluster determined by the RCA. The state can be healthy, unhealthy, or unknown. | . | HotClusterSummary.HotNodeSummary.number_of_nodes | Integer | The number of nodes in the cluster. | . | HotClusterSummary.HotNodeSummary.number_of_unhealthy_nodes | Integer | The number of nodes found to be in an unhealthy state. | . | HotClusterSummary.HotNodeSummary.HotResourceSummary.resource_type | Object | The type of resource causing the unhealthy state, either “cpu usage” or “heap”. | . | HotClusterSummary.HotNodeSummary.HotResourceSummary.resource_metric | String | The definition of the resource_type. Either “cpu usage(num of cores)” or “heap alloc rate(heap alloc rate in bytes per second)”. | . | HotClusterSummary.HotNodeSummary.HotResourceSummary.threshold | Float | The value that determines whether a resource is contended. | . | HotClusterSummary.HotNodeSummary.HotResourceSummary.value | Float | The current value of the resource. | . | HotClusterSummary.HotNodeSummary.HotResourceSummary.time_period_seconds | Time | The amount of time that a shard was monitored before its state was declared to be healthy or unhealthy. | . | HotClusterSummary.HotNodeSummary.HotResourceSummary.meta_data | String | The metadata associated with the resource_type. | . In the preceding example response, meta_data is QRF4rBM7SNCDr1g3KU6HyA index9 0. The meta_data string consists of three fields: . | Node name: QRF4rBM7SNCDr1g3KU6HyA | Index name: index9 | Shard ID: 0 | . This means that shard 0 of index index9 on node QRF4rBM7SNCDr1g3KU6HyA is hot. ",
    "url": "https://vagimeli.github.io/monitoring-your-cluster/pa/rca/shard-hotspot/#response-fields",
    "relUrl": "/monitoring-your-cluster/pa/rca/shard-hotspot/#response-fields"
  },"1147": {
    "doc": "Metrics Reference",
    "title": "Metrics reference",
    "content": "This page contains all Performance Analyzer metrics. All metrics support the avg, sum, min, and max aggregations, although certain metrics measure only one thing, making the choice of aggregation irrelevant. For information on dimensions, see the dimensions reference. This list is extensive. We recommend using Ctrl/Cmd + F to find what you’re looking for. | Metric | Dimensions | Description | . | CPU_Utilization | ShardID, IndexName, Operation, ShardRole | CPU usage ratio. CPU time (in milliseconds) used by the associated thread(s) in the past five seconds, divided by 5000 milliseconds. | . | Paging_MajfltRate | The number of major faults per second in the past five seconds. A major fault requires the process to load a memory page from disk. | . | Paging_MinfltRate | The number of minor faults per second in the past five seconds. A minor fault does not requires the process to load a memory page from disk. | . | Paging_RSS | The number of pages the process has in real memory---the pages that count towards text, data, or stack space. This number does not include pages that have not been demand-loaded in or swapped out. | . | Sched_Runtime | Time (seconds) spent executing on the CPU per context switch. | . | Sched_Waittime | Time (seconds) spent waiting on a run queue per context switch. | . | Sched_CtxRate | Number of times run on the CPU per second in the past five seconds. | . | Heap_AllocRate | An approximation of the heap memory allocated, in bytes, per second in the past five seconds | . | IO_ReadThroughput | Number of bytes read per second in the last five seconds. | . | IO_WriteThroughput | Number of bytes written per second in the last five seconds. | . | IO_TotThroughput | Number of bytes read or written per second in the last five seconds. | . | IO_ReadSyscallRate | Read system calls per second in the last five seconds. | . | IO_WriteSyscallRate | Write system calls per second in the last five seconds. | . | IO_TotalSyscallRate | Read and write system calls per second in the last five seconds. | . | Thread_Blocked_Time | Average time (seconds) that the associated thread(s) blocked to enter or reenter a monitor. | . | Thread_Blocked_Event | The total number of times that the associated thread(s) blocked to enter or reenter a monitor (i.e. the number of times a thread has been in the blocked state). | . | Thread_Waited_Time | Average time (seconds) that the associated thread(s) waited to enter or reenter a monitor in WAITING or TIMED_WAITING state. | . | Thread_Waited_Event | The total number of times that the associated thread(s) waited to enter or reenter a monitor (i.e. the number of times a thread has been in the WAITING or TIMED_WAITING state). | . | ShardEvents | The total number of events executed on a shard in the past five seconds. | . | ShardBulkDocs | The total number of documents indexed in the past five seconds. | . | Indexing_ThrottleTime | ShardID, IndexName | Time (milliseconds) that the index has been under merge throttling control in the past five seconds. | . | Cache_Query_Hit | The number of successful lookups in the query cache in the past five seconds. | . | Cache_Query_Miss | The number of lookups in the query cache that failed to retrieve a `DocIdSet` in the past five seconds. `DocIdSet` is a set of document IDs in Lucene. | . | Cache_Query_Size | Query cache memory size in bytes. | . | Cache_FieldData_Eviction | The number of times OpenSearch has evicted data from the fielddata heap space (occurs when the heap space is full) in the past five seconds. | . | Cache_FieldData_Size | Fielddata memory size in bytes. | . | Cache_Request_Hit | The number of successful lookups in the shard request cache in the past five seconds. | . | Cache_Request_Miss | The number of lookups in the request cache that failed to retrieve the results of search requests in the past five seconds. | . | Cache_Request_Eviction | The number of times OpenSearch evicts data from shard request cache (occurs when the request cache is full) in the past five seconds. | . | Cache_Request_Size | Shard request cache memory size in bytes. | . | Refresh_Event | The total number of refreshes executed in the past five seconds. | . | Refresh_Time | The total time (milliseconds) spent executing refreshes in the past five seconds | . | Flush_Event | The total number of flushes executed in the past five seconds. | . | Flush_Time | The total time (milliseconds) spent executing flushes in the past five seconds. | . | Merge_Event | The total number of merges executed in the past five seconds. | . | Merge_Time | The total time (milliseconds) spent executing merges in the past five seconds. | . | Merge_CurrentEvent | The current number of merges executing. | . | Indexing_Buffer | Index buffer memory size in bytes. | . | Segments_Total | The number of segments. | . | IndexWriter_Memory | Estimated memory usage by the index writer in bytes. | . | Bitset_Memory | Estimated memory usage for the cached bit sets in bytes. | . | VersionMap_Memory | Estimated memory usage of the version map in bytes. | . | Shard_Size_In_Bytes | Estimated disk usage of the shard in bytes. | . | Indexing_Pressure_Current_Limits | ShardID, IndexName, IndexingStage | Total heap size (in bytes) that is available for utilization by a shard of an index in a particular indexing stage (Coordinating, Primary or Replica). | . | Indexing_Pressure_Current_Bytes | Total heap size (in bytes) occupied by a shard of an index in a particular indexing stage (Coordinating, Primary or Replica). | . | Indexing_Pressure_Last_Successful_Timestamp | Timestamp of a request that was successful for a shard of an index in a particular indexing stage (Coordinating, Primary or Replica). | . | Indexing_Pressure_Rejection_Count | Total rejections performed by OpenSearch for a shard of an index in a particular indexing stage (Coordinating, Primary or Replica). | . | Indexing_Pressure_Average_Window_Throughput | Average throughput of the last n requests (The value of n is determined by `shard_indexing_pressure.secondary_parameter.throughput.request_size_window` setting) for a shard of an index in a particular indexing stage (Coordinating, Primary or Replica). | . | Latency | Operation, Exception, Indices, HTTPRespCode, ShardID, IndexName, ShardRole | Latency (milliseconds) of a request. | . | GC_Collection_Event | MemType | The number of garbage collections that have occurred in the past five seconds. | . | GC_Collection_Time | The approximate accumulated time (milliseconds) of all garbage collections that have occurred in the past five seconds. | . | Heap_Committed | The amount of memory (bytes) that is committed for the JVM to use. | . | Heap_Init | The amount of memory (bytes) that the JVM initially requests from the operating system for memory management. | . | Heap_Max | The maximum amount of memory (bytes) that can be used for memory management. | . | Heap_Used | The amount of used memory in bytes. | . | Disk_Utilization | DiskName | Disk utilization rate: percentage of disk time spent reading and writing by the OpenSearch process in the past five seconds. | . | Disk_WaitTime | Average duration (milliseconds) of read and write operations in the past five seconds. | . | Disk_ServiceRate | Service rate: MB read or written per second in the past five seconds. This metric assumes that each disk sector stores 512 bytes. | . | Net_TCP_NumFlows | DestAddr | Number of samples collected. Performance Analyzer collects one sample every five seconds. | . | Net_TCP_TxQ | Average number of TCP packets in the send buffer. | . | Net_TCP_RxQ | Average number of TCP packets in the receive buffer. | . | Net_TCP_Lost | Average number of unrecovered recurring timeouts. This number is reset when the recovery finishes or `SND.UNA` is advanced. `SND.UNA` is the sequence number of the first byte of data that has been sent, but not yet acknowledged. | . | Net_TCP_SendCWND | Average size (bytes) of the sending congestion window. | . | Net_TCP_SSThresh | Average size (bytes) of the slow start size threshold. | . | Net_PacketRate4 | Direction | The total number of IPv4 datagrams transmitted/received from/by interfaces per second, including those transmitted or received in error. | . | Net_PacketDropRate4 | The total number of IPv4 datagrams transmitted or received in error per second. | . | Net_PacketRate6 | The total number of IPv6 datagrams transmitted or received from or by interfaces per second, including those transmitted or received in error. | . | Net_PacketDropRate6 | The total number of IPv6 datagrams transmitted or received in error per second. | . | Net_Throughput | The number of bits transmitted or received per second by all network interfaces. | . | ThreadPool_QueueSize | ThreadPoolType | The size of the task queue. | . | ThreadPool_RejectedReqs | The number of rejected executions. | . | ThreadPool_TotalThreads | The current number of threads in the pool. | . | ThreadPool_ActiveThreads | The approximate number of threads that are actively executing tasks. | . | ThreadPool_QueueLatency | The latency of the task queue. | . | ThreadPool_QueueCapacity | The current capacity of the task queue. | . | Master_PendingQueueSize | Master_PendingTaskType | The current number of pending tasks in the cluster state update thread. Each node has a cluster state update thread that submits cluster state update tasks (create index, update mapping, allocate shard, fail shard, etc.). | . | HTTP_RequestDocs | Operation, Exception, Indices, HTTPRespCode | The number of items in the request (only for `_bulk` request type). | . | HTTP_TotalRequests | The number of finished requests in the past five seconds. | . | CB_EstimatedSize | CBType | The current number of estimated bytes. | . | CB_TrippedEvents | The number of times the circuit breaker has tripped. | . | CB_ConfiguredSize | The limit (bytes) for how much memory operations can use. | . | Master_Task_Queue_Time | MasterTaskInsertOrder, MasterTaskPriority, MasterTaskType, MasterTaskMetadata | The time (milliseconds) that a master task spent in the queue. | . | Master_Task_Run_Time | The time (milliseconds) that a master task has been executed. | . | Cache_MaxSize | CacheType | The max size of the cache in bytes. | . | AdmissionControl_RejectionCount (WIP) | ControllerName | Total rejections performed by a Controller of Admission Control. | . | AdmissionControl_CurrentValue (WIP) | Current value for Controller of Admission Control. | . | AdmissionControl_ThresholdValue (WIP) | Threshold value for Controller of Admission Control. | . | Data_RetryingPendingTasksCount (WIP) | NodeID | Number of throttled pending tasks on which data node is actively performing retries. It will be an absolute metric at that point of time. | . | Master_ThrottledPendingTasksCount (WIP) | Sum of total pending tasks which got throttled by node (master node). It is a cumulative metric so look at the max aggregation. | . | Election_Term (WIP) | N/A | Monotonically increasing number with every master election. | . | PublishClusterState_Latency (WIP) | The time taken by quorum of nodes to publish new cluster state. This metric is available for current master. | . | PublishClusterState_Failure (WIP) | The number of times publish new cluster state action failed on master node. | . | ClusterApplierService_Latency (WIP) | The time taken by each node to apply cluster state sent by master. | . | ClusterApplierService_Failure (WIP) | The number of times apply cluster state action failed on each node. | . | Shard_State (WIP) | IndexName, NodeName, ShardType, ShardID | The state of each shard - whether it is STARTED, UNASSIGNED, RELOCATING etc. | . | LeaderCheck_Latency (WIP) | WIP | WIP | . | FollowerCheck_Failure (WIP) | . | LeaderCheck_Failure (WIP) | . | FollowerCheck_Latency (WIP) | . ",
    "url": "https://vagimeli.github.io/monitoring-your-cluster/pa/reference/#metrics-reference",
    "relUrl": "/monitoring-your-cluster/pa/reference/#metrics-reference"
  },"1148": {
    "doc": "Metrics Reference",
    "title": "Dimensions reference",
    "content": "| Dimension | Return values | . | ShardID | ID for the shard (e.g. 1). | . | IndexName | Name of the index (e.g. my-index). | . | Operation | Type of operation (e.g. shardbulk). | . | ShardRole | primary, replica | . | Exception | OpenSearch exceptions (e.g. org.opensearch.index_not_found_exception). | . | Indices | The list of indices in the request URI. | . | HTTPRespCode | Response code from OpenSearch (e.g. 200). | . | MemType | totYoungGC, totFullGC, Survivor, PermGen, OldGen, Eden, NonHeap, Heap | . | DiskName | Name of the disk (e.g. sda1). | . | DestAddr | Destination address (e.g. 010015AC). | . | Direction | in, out | . | ThreadPoolType | The OpenSearch thread pools (e.g. index, search,snapshot). | . | CBType | accounting, fielddata, in_flight_requests, parent, request | . | MasterTaskInsertOrder | The order in which the task was inserted (e.g. 3691). | . | MasterTaskPriority | Priority of the task (e.g. URGENT). OpenSearch executes higher priority tasks before lower priority ones, regardless of insert_order. | . | MasterTaskType | shard-started, create-index, delete-index, refresh-mapping, put-mapping, CleanupSnapshotRestoreState, Update snapshot state | . | MasterTaskMetadata | Metadata for the task (if any). | . | CacheType | Field_Data_Cache, Shard_Request_Cache, Node_Query_Cache | . ",
    "url": "https://vagimeli.github.io/monitoring-your-cluster/pa/reference/#dimensions-reference",
    "relUrl": "/monitoring-your-cluster/pa/reference/#dimensions-reference"
  },"1149": {
    "doc": "Metrics Reference",
    "title": "Metrics Reference",
    "content": " ",
    "url": "https://vagimeli.github.io/monitoring-your-cluster/pa/reference/",
    "relUrl": "/monitoring-your-cluster/pa/reference/"
  },"1150": {
    "doc": "Anomaly detection API",
    "title": "Anomaly detection API",
    "content": "Use these anomaly detection operations to programmatically create and manage detectors. . | Create anomaly detector | Validate detector | Get detector | Update detector | Delete detector | Preview detector | Start detector job | Stop detector job | Search detector | Search detector tasks | Search detector result | Search top anomalies | Get detector stats | Profile detector | Delete detector results | Create monitor | . ",
    "url": "https://vagimeli.github.io/observing-your-data/ad/api/",
    "relUrl": "/observing-your-data/ad/api/"
  },"1151": {
    "doc": "Anomaly detection API",
    "title": "Create anomaly detector",
    "content": "Introduced 1.0 . Creates an anomaly detector. This command creates a single-entity detector named test-detector that finds anomalies based on the sum of the value field and stores the result in a custom opensearch-ad-plugin-result-test index: . Request . POST _plugins/_anomaly_detection/detectors { \"name\": \"test-detector\", \"description\": \"Test detector\", \"time_field\": \"timestamp\", \"indices\": [ \"server_log*\" ], \"feature_attributes\": [ { \"feature_name\": \"test\", \"feature_enabled\": true, \"aggregation_query\": { \"test\": { \"sum\": { \"field\": \"value\" } } } } ], \"filter_query\": { \"bool\": { \"filter\": [ { \"range\": { \"value\": { \"gt\": 1 } } } ], \"adjust_pure_negative\": true, \"boost\": 1 } }, \"detection_interval\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"window_delay\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"result_index\" : \"opensearch-ad-plugin-result-test\" } . Example response . { \"_id\": \"VEHKTXwBwf_U8gjUXY2s\", \"_version\": 1, \"_seq_no\": 5, \"anomaly_detector\": { \"name\": \"test-detector\", \"description\": \"Test detector\", \"time_field\": \"timestamp\", \"indices\": [ \"server_log*\" ], \"filter_query\": { \"bool\": { \"filter\": [ { \"range\": { \"value\": { \"from\": 1, \"to\": null, \"include_lower\": false, \"include_upper\": true, \"boost\": 1 } } } ], \"adjust_pure_negative\": true, \"boost\": 1 } }, \"detection_interval\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"window_delay\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"shingle_size\": 8, \"schema_version\": 0, \"feature_attributes\": [ { \"feature_id\": \"U0HKTXwBwf_U8gjUXY2m\", \"feature_name\": \"test\", \"feature_enabled\": true, \"aggregation_query\": { \"test\": { \"sum\": { \"field\": \"value\" } } } } ], \"last_update_time\": 1633392680364, \"user\": { \"name\": \"admin\", \"backend_roles\": [ \"admin\" ], \"roles\": [ \"own_index\", \"all_access\" ], \"custom_attribute_names\": [], \"user_requested_tenant\": \"__user__\" }, \"detector_type\": \"SINGLE_ENTITY\" }, \"_primary_term\": 1 } . To create a high cardinality detector by specifying a category field: . Request . POST _plugins/_anomaly_detection/detectors { \"name\": \"test-hc-detector\", \"description\": \"Test detector\", \"time_field\": \"timestamp\", \"indices\": [ \"server_log*\" ], \"feature_attributes\": [ { \"feature_name\": \"test\", \"feature_enabled\": true, \"aggregation_query\": { \"test\": { \"sum\": { \"field\": \"value\" } } } } ], \"filter_query\": { \"bool\": { \"filter\": [ { \"range\": { \"value\": { \"gt\": 1 } } } ], \"adjust_pure_negative\": true, \"boost\": 1 } }, \"detection_interval\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"window_delay\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"category_field\": [ \"ip\" ] } . Example response . { \"_id\": \"b0HRTXwBwf_U8gjUw43R\", \"_version\": 1, \"_seq_no\": 6, \"anomaly_detector\": { \"name\": \"test-hc-detector\", \"description\": \"Test detector\", \"time_field\": \"timestamp\", \"indices\": [ \"server_log*\" ], \"filter_query\": { \"bool\": { \"filter\": [ { \"range\": { \"value\": { \"from\": 1, \"to\": null, \"include_lower\": false, \"include_upper\": true, \"boost\": 1 } } } ], \"adjust_pure_negative\": true, \"boost\": 1 } }, \"detection_interval\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"window_delay\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"shingle_size\": 8, \"schema_version\": 0, \"feature_attributes\": [ { \"feature_id\": \"bkHRTXwBwf_U8gjUw43K\", \"feature_name\": \"test\", \"feature_enabled\": true, \"aggregation_query\": { \"test\": { \"sum\": { \"field\": \"value\" } } } } ], \"last_update_time\": 1633393165265, \"category_field\": [ \"ip\" ], \"user\": { \"name\": \"admin\", \"backend_roles\": [ \"admin\" ], \"roles\": [ \"own_index\", \"all_access\" ], \"custom_attribute_names\": [], \"user_requested_tenant\": \"__user__\" }, \"detector_type\": \"MULTI_ENTITY\" }, \"_primary_term\": 1 } . You can specify a maximum of two category fields: . \"category_field\": [ \"ip\" ] . \"category_field\": [ \"ip\", \"error_type\" ] . You can specify the following options. | Options | Description | Type | Required | . | name | The name of the detector. | string | Yes | . | description | A description of the detector. | string | No | . | time_field | The name of the time field. | string | Yes | . | indices | A list of indices to use as the data source. | list | Yes | . | feature_attributes | Specify a feature_name, set the enabled parameter to true, and specify an aggregation query. | list | Yes | . | filter_query | Provide an optional filter query for your feature. | object | No | . | detection_interval | The time interval for your anomaly detector. | object | Yes | . | window_delay | Add extra processing time for data collection. | object | No | . | category_field | Categorizes or slices data with a dimension. Similar to GROUP BY in SQL. | list | No | . ",
    "url": "https://vagimeli.github.io/observing-your-data/ad/api/#create-anomaly-detector",
    "relUrl": "/observing-your-data/ad/api/#create-anomaly-detector"
  },"1152": {
    "doc": "Anomaly detection API",
    "title": "Validate detector",
    "content": "Introduced 1.2 . Returns whether the detector configuration has any issues that might prevent OpenSearch from creating the detector. You can use the validate detector API operation to identify issues in your detector configuration before creating the detector. The request body consists of the detector configuration and follows the same format as the request body of the create detector API. You have the following validation options: . | Only validate against the detector configuration and find any issues that would completely block detector creation: | . POST _plugins/_anomaly_detection/detectors/_validate POST _plugins/_anomaly_detection/detectors/_validate/detector . | Validate against the source data to see how likely the detector would complete model training. | . POST _plugins/_anomaly_detection/detectors/_validate/model . Responses from this API operation return either blocking issues as detector type responses or a response indicating a field that could be revised to increase likelihood of model training completing successfully. Model type issues don’t need to be fixed for detector creation to succeed, but the detector would likely not train successfully if they aren’t addressed. Request . POST _plugins/_anomaly_detection/detectors/_validate POST _plugins/_anomaly_detection/detectors/_validate/detector { \"name\": \"test-detector\", \"description\": \"Test detector\", \"time_field\": \"timestamp\", \"indices\": [ \"server_log*\" ], \"feature_attributes\": [ { \"feature_name\": \"test\", \"feature_enabled\": true, \"aggregation_query\": { \"test\": { \"sum\": { \"field\": \"value\" } } } } ], \"filter_query\": { \"bool\": { \"filter\": [ { \"range\": { \"value\": { \"gt\": 1 } } } ], \"adjust_pure_negative\": true, \"boost\": 1 } }, \"detection_interval\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"window_delay\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } } } . If the validate detector API doesn’t find any issue in the detector configuration, it returns an empty response: . Example response . {} . If the validate detector API finds an issue, it returns a message explaining what’s wrong with the configuration. In this example, the feature query aggregates over a field that doesn’t exist in the data source: . Example response . { \"detector\": { \"feature_attributes\": { \"message\": \"Feature has invalid query returning empty aggregated data: average_total_rev\", \"sub_issues\": { \"average_total_rev\": \"Feature has invalid query returning empty aggregated data\" } } } } . The following request validates against the source data to see if model training might succeed. In this example, the data is ingested at a rate of every 5 minutes, and detector interval is set to 1 minute. POST _plugins/_anomaly_detection/detectors/_validate/model { \"name\": \"test-detector\", \"description\": \"Test detector\", \"time_field\": \"timestamp\", \"indices\": [ \"server_log*\" ], \"feature_attributes\": [ { \"feature_name\": \"test\", \"feature_enabled\": true, \"aggregation_query\": { \"test\": { \"sum\": { \"field\": \"value\" } } } } ], \"filter_query\": { \"bool\": { \"filter\": [ { \"range\": { \"value\": { \"gt\": 1 } } } ], \"adjust_pure_negative\": true, \"boost\": 1 } }, \"detection_interval\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"window_delay\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } } } . If the validate detector API finds areas of improvement with your configuration, it returns a response with suggestions about how you can change your configuration to improve model training. Sample Responses . In this example, the validate detector API returns a response indicating that changing the detector interval length to at least four minutes can increase the chances of successful model training. { \"model\": { \"detection_interval\": { \"message\": \"The selected detector interval might collect sparse data. Consider changing interval length to: 4\", \"suggested_value\": { \"period\": { \"interval\": 4, \"unit\": \"Minutes\" } } } } } . Another response might indicate that you can change filter_query (data filter) because the currently filtered data is too sparse for the model to train correctly, which can happen because the index is also ingesting data that falls outside the chosen filter. Using another filter_query can make your data more dense. { \"model\": { \"filter_query\": { \"message\": \"Data is too sparse after data filter is applied. Consider changing the data filter\" } } } . ",
    "url": "https://vagimeli.github.io/observing-your-data/ad/api/#validate-detector",
    "relUrl": "/observing-your-data/ad/api/#validate-detector"
  },"1153": {
    "doc": "Anomaly detection API",
    "title": "Get detector",
    "content": "Introduced 1.0 . Returns all information about a detector based on the detector_id. Request . GET _plugins/_anomaly_detection/detectors/&lt;detectorId&gt; . Example response . { \"_id\": \"VEHKTXwBwf_U8gjUXY2s\", \"_version\": 1, \"_primary_term\": 1, \"_seq_no\": 5, \"anomaly_detector\": { \"name\": \"test-detector\", \"description\": \"Test detector\", \"time_field\": \"timestamp\", \"indices\": [ \"server_log*\" ], \"filter_query\": { \"bool\": { \"filter\": [ { \"range\": { \"value\": { \"from\": 1, \"to\": null, \"include_lower\": false, \"include_upper\": true, \"boost\": 1 } } } ], \"adjust_pure_negative\": true, \"boost\": 1 } }, \"detection_interval\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"window_delay\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"shingle_size\": 8, \"schema_version\": 0, \"feature_attributes\": [ { \"feature_id\": \"U0HKTXwBwf_U8gjUXY2m\", \"feature_name\": \"test\", \"feature_enabled\": true, \"aggregation_query\": { \"test\": { \"sum\": { \"field\": \"value\" } } } } ], \"last_update_time\": 1633392680364, \"user\": { \"name\": \"admin\", \"backend_roles\": [ \"admin\" ], \"roles\": [ \"own_index\", \"all_access\" ], \"custom_attribute_names\": [], \"user_requested_tenant\": \"__user__\" }, \"detector_type\": \"SINGLE_ENTITY\" } } . A “job” is something that you schedule to run periodically, so it’s only applicable for real-time anomaly detection and not historical analysis that you run just one time. When you start a real-time detector, the anomaly detection plugin creates a job or if the job already exists updates it. When you start or a restart a real-time detector, the plugin creates a new real-time task that records run-time information like detector configuration snapshot, real-time job states (initializing/running/stopped), init progress, and so on. A single detector can only have one real-time job (job ID is the same as detector ID), but it can have multiple real-time tasks because each restart of a real-time job creates a new real-time task. You can limit the number of real-time tasks with the plugins.anomaly_detection.max_old_ad_task_docs_per_detector setting. Historical analysis doesn’t have an associated job. When you start or rerun historical analysis for a detector, the anomaly detection plugin creates a new historical batch task that tracks the historical analysis runtime information like state, coordinating/worker node, task progress, and so on. You can limit the historical task number with the plugins.anomaly_detection.max_old_ad_task_docs_per_detector setting. Use job=true to get real-time analysis task information. Request . GET _plugins/_anomaly_detection/detectors/&lt;detectorId&gt;?job=true . Example response . { \"_id\": \"VEHKTXwBwf_U8gjUXY2s\", \"_version\": 1, \"_primary_term\": 1, \"_seq_no\": 5, \"anomaly_detector\": { \"name\": \"test-detector\", \"description\": \"Test detector\", \"time_field\": \"timestamp\", \"indices\": [ \"server_log*\" ], \"filter_query\": { \"bool\": { \"filter\": [ { \"range\": { \"value\": { \"from\": 1, \"to\": null, \"include_lower\": false, \"include_upper\": true, \"boost\": 1 } } } ], \"adjust_pure_negative\": true, \"boost\": 1 } }, \"detection_interval\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"window_delay\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"shingle_size\": 8, \"schema_version\": 0, \"feature_attributes\": [ { \"feature_id\": \"U0HKTXwBwf_U8gjUXY2m\", \"feature_name\": \"test\", \"feature_enabled\": true, \"aggregation_query\": { \"test\": { \"sum\": { \"field\": \"value\" } } } } ], \"last_update_time\": 1633392680364, \"user\": { \"name\": \"admin\", \"backend_roles\": [ \"admin\" ], \"roles\": [ \"own_index\", \"all_access\" ], \"custom_attribute_names\": [], \"user_requested_tenant\": \"__user__\" }, \"detector_type\": \"SINGLE_ENTITY\" }, \"anomaly_detector_job\": { \"name\": \"VEHKTXwBwf_U8gjUXY2s\", \"schedule\": { \"interval\": { \"start_time\": 1633393656357, \"period\": 1, \"unit\": \"Minutes\" } }, \"window_delay\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"enabled\": true, \"enabled_time\": 1633393656357, \"last_update_time\": 1633393656357, \"lock_duration_seconds\": 60, \"user\": { \"name\": \"admin\", \"backend_roles\": [ \"admin\" ], \"roles\": [ \"own_index\", \"all_access\" ], \"custom_attribute_names\": [], \"user_requested_tenant\": \"__user__\" } } } . Use task=true to get information for both real-time and historical analysis task information. Request . GET _plugins/_anomaly_detection/detectors/&lt;detectorId&gt;?task=true . Example response . { \"_id\": \"VEHKTXwBwf_U8gjUXY2s\", \"_version\": 1, \"_primary_term\": 1, \"_seq_no\": 5, \"anomaly_detector\": { \"name\": \"test-detector\", \"description\": \"Test detector\", \"time_field\": \"timestamp\", \"indices\": [ \"server_log*\" ], \"filter_query\": { \"bool\": { \"filter\": [ { \"range\": { \"value\": { \"from\": 1, \"to\": null, \"include_lower\": false, \"include_upper\": true, \"boost\": 1 } } } ], \"adjust_pure_negative\": true, \"boost\": 1 } }, \"detection_interval\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"window_delay\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"shingle_size\": 8, \"schema_version\": 0, \"feature_attributes\": [ { \"feature_id\": \"U0HKTXwBwf_U8gjUXY2m\", \"feature_name\": \"test\", \"feature_enabled\": true, \"aggregation_query\": { \"test\": { \"sum\": { \"field\": \"value\" } } } } ], \"last_update_time\": 1633392680364, \"user\": { \"name\": \"admin\", \"backend_roles\": [ \"admin\" ], \"roles\": [ \"own_index\", \"all_access\" ], \"custom_attribute_names\": [], \"user_requested_tenant\": \"__user__\" }, \"detector_type\": \"SINGLE_ENTITY\" }, \"realtime_detection_task\": { \"task_id\": \"nkTZTXwBjd8s6RK4QlMq\", \"last_update_time\": 1633393776375, \"started_by\": \"admin\", \"error\": \"\", \"state\": \"RUNNING\", \"detector_id\": \"VEHKTXwBwf_U8gjUXY2s\", \"task_progress\": 0, \"init_progress\": 1, \"execution_start_time\": 1633393656362, \"is_latest\": true, \"task_type\": \"REALTIME_SINGLE_ENTITY\", \"coordinating_node\": \"SWD7ihu9TaaW1zKwFZNVNg\", \"detector\": { \"name\": \"test-detector\", \"description\": \"Test detector\", \"time_field\": \"timestamp\", \"indices\": [ \"server_log*\" ], \"filter_query\": { \"bool\": { \"filter\": [ { \"range\": { \"value\": { \"from\": 1, \"to\": null, \"include_lower\": false, \"include_upper\": true, \"boost\": 1 } } } ], \"adjust_pure_negative\": true, \"boost\": 1 } }, \"detection_interval\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"window_delay\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"shingle_size\": 8, \"schema_version\": 0, \"feature_attributes\": [ { \"feature_id\": \"U0HKTXwBwf_U8gjUXY2m\", \"feature_name\": \"test\", \"feature_enabled\": true, \"aggregation_query\": { \"test\": { \"sum\": { \"field\": \"value\" } } } } ], \"last_update_time\": 1633392680364, \"user\": { \"name\": \"admin\", \"backend_roles\": [ \"admin\" ], \"roles\": [ \"own_index\", \"all_access\" ], \"custom_attribute_names\": [], \"user_requested_tenant\": \"__user__\" }, \"detector_type\": \"SINGLE_ENTITY\" }, \"estimated_minutes_left\": 0, \"user\": { \"name\": \"admin\", \"backend_roles\": [ \"admin\" ], \"roles\": [ \"own_index\", \"all_access\" ], \"custom_attribute_names\": [], \"user_requested_tenant\": \"__user__\" } }, \"historical_analysis_task\": { \"task_id\": \"99DaTXwB6HknB84StRN1\", \"last_update_time\": 1633393797040, \"started_by\": \"admin\", \"state\": \"RUNNING\", \"detector_id\": \"VEHKTXwBwf_U8gjUXY2s\", \"task_progress\": 0.89285713, \"init_progress\": 1, \"current_piece\": 1633328940000, \"execution_start_time\": 1633393751412, \"is_latest\": true, \"task_type\": \"HISTORICAL_SINGLE_ENTITY\", \"coordinating_node\": \"SWD7ihu9TaaW1zKwFZNVNg\", \"worker_node\": \"2Z4q22BySEyzakYt_A0A2A\", \"detector\": { \"name\": \"test-detector\", \"description\": \"Test detector\", \"time_field\": \"timestamp\", \"indices\": [ \"server_log*\" ], \"filter_query\": { \"bool\": { \"filter\": [ { \"range\": { \"value\": { \"from\": 1, \"to\": null, \"include_lower\": false, \"include_upper\": true, \"boost\": 1 } } } ], \"adjust_pure_negative\": true, \"boost\": 1 } }, \"detection_interval\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"window_delay\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"shingle_size\": 8, \"schema_version\": 0, \"feature_attributes\": [ { \"feature_id\": \"U0HKTXwBwf_U8gjUXY2m\", \"feature_name\": \"test\", \"feature_enabled\": true, \"aggregation_query\": { \"test\": { \"sum\": { \"field\": \"value\" } } } } ], \"last_update_time\": 1633392680364, \"user\": { \"name\": \"admin\", \"backend_roles\": [ \"admin\" ], \"roles\": [ \"own_index\", \"all_access\" ], \"custom_attribute_names\": [], \"user_requested_tenant\": \"__user__\" }, \"detector_type\": \"SINGLE_ENTITY\" }, \"detection_date_range\": { \"start_time\": 1632788951329, \"end_time\": 1633393751329 }, \"user\": { \"name\": \"admin\", \"backend_roles\": [ \"admin\" ], \"roles\": [ \"own_index\", \"all_access\" ], \"custom_attribute_names\": [], \"user_requested_tenant\": \"__user__\" } } } . ",
    "url": "https://vagimeli.github.io/observing-your-data/ad/api/#get-detector",
    "relUrl": "/observing-your-data/ad/api/#get-detector"
  },"1154": {
    "doc": "Anomaly detection API",
    "title": "Update detector",
    "content": "Introduced 1.0 . Updates a detector with any changes, including the description or adding or removing of features. To update a detector, you need to first stop both real-time detection and historical analysis. You can’t update a category field. Request . PUT _plugins/_anomaly_detection/detectors/&lt;detectorId&gt; { \"name\": \"test-detector\", \"description\": \"Test update detector\", \"time_field\": \"timestamp\", \"indices\": [ \"server_log*\" ], \"feature_attributes\": [ { \"feature_name\": \"test\", \"feature_enabled\": true, \"aggregation_query\": { \"test\": { \"sum\": { \"field\": \"value\" } } } } ], \"filter_query\": { \"bool\": { \"filter\": [ { \"range\": { \"value\": { \"gt\": 1 } } } ], \"adjust_pure_negative\": true, \"boost\": 1 } }, \"detection_interval\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"window_delay\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } } } . Example response . { \"_id\": \"VEHKTXwBwf_U8gjUXY2s\", \"_version\": 2, \"_seq_no\": 7, \"anomaly_detector\": { \"name\": \"test-detector\", \"description\": \"Test update detector\", \"time_field\": \"timestamp\", \"indices\": [ \"server_log*\" ], \"filter_query\": { \"bool\": { \"filter\": [ { \"range\": { \"value\": { \"from\": 1, \"to\": null, \"include_lower\": false, \"include_upper\": true, \"boost\": 1 } } } ], \"adjust_pure_negative\": true, \"boost\": 1 } }, \"detection_interval\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"window_delay\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"shingle_size\": 8, \"schema_version\": 0, \"feature_attributes\": [ { \"feature_id\": \"3kHiTXwBwf_U8gjUlY15\", \"feature_name\": \"test\", \"feature_enabled\": true, \"aggregation_query\": { \"test\": { \"sum\": { \"field\": \"value\" } } } } ], \"last_update_time\": 1633394267522, \"user\": { \"name\": \"admin\", \"backend_roles\": [ \"admin\" ], \"roles\": [ \"own_index\", \"all_access\" ], \"custom_attribute_names\": [], \"user_requested_tenant\": \"__user__\" }, \"detector_type\": \"SINGLE_ENTITY\" }, \"_primary_term\": 1 } . ",
    "url": "https://vagimeli.github.io/observing-your-data/ad/api/#update-detector",
    "relUrl": "/observing-your-data/ad/api/#update-detector"
  },"1155": {
    "doc": "Anomaly detection API",
    "title": "Delete detector",
    "content": "Introduced 1.0 . Deletes a detector based on the detector_id. To delete a detector, you need to first stop both real-time detection and historical analysis. Request . DELETE _plugins/_anomaly_detection/detectors/&lt;detectorId&gt; . Example response . { \"_index\": \".opensearch-anomaly-detectors\", \"_id\": \"70TxTXwBjd8s6RK4j1Pj\", \"_version\": 2, \"result\": \"deleted\", \"forced_refresh\": true, \"_shards\": { \"total\": 2, \"successful\": 2, \"failed\": 0 }, \"_seq_no\": 9, \"_primary_term\": 1 } . ",
    "url": "https://vagimeli.github.io/observing-your-data/ad/api/#delete-detector",
    "relUrl": "/observing-your-data/ad/api/#delete-detector"
  },"1156": {
    "doc": "Anomaly detection API",
    "title": "Preview detector",
    "content": "Introduced 1.0 . Passes a date range to the anomaly detector to return any anomalies within that date range. To preview a single-entity detector: . Request . POST _plugins/_anomaly_detection/detectors/_preview { \"period_start\": 1633048868000, \"period_end\": 1633394468000, \"detector\": { \"name\": \"test-detector\", \"description\": \"Test update detector\", \"time_field\": \"timestamp\", \"indices\": [ \"server_log*\" ], \"feature_attributes\": [ { \"feature_name\": \"test\", \"feature_enabled\": true, \"aggregation_query\": { \"test\": { \"sum\": { \"field\": \"value\" } } } } ], \"filter_query\": { \"bool\": { \"filter\": [ { \"range\": { \"value\": { \"gt\": 1 } } } ], \"adjust_pure_negative\": true, \"boost\": 1 } }, \"detection_interval\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"window_delay\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } } } } . Example response . { \"anomaly_result\": [ { \"detector_id\": null, \"data_start_time\": 1633049280000, \"data_end_time\": 1633049340000, \"schema_version\": 0, \"feature_data\": [ { \"feature_id\": \"8EHmTXwBwf_U8gjU0Y0u\", \"feature_name\": \"test\", \"data\": 0 } ], \"anomaly_grade\": 0, \"confidence\": 0 }, ... ], \"anomaly_detector\": { \"name\": \"test-detector\", \"description\": \"Test update detector\", \"time_field\": \"timestamp\", \"indices\": [ \"server_log*\" ], \"filter_query\": { \"bool\": { \"filter\": [ { \"range\": { \"value\": { \"from\": 1, \"to\": null, \"include_lower\": false, \"include_upper\": true, \"boost\": 1 } } } ], \"adjust_pure_negative\": true, \"boost\": 1 } }, \"detection_interval\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"window_delay\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"shingle_size\": 8, \"schema_version\": 0, \"feature_attributes\": [ { \"feature_id\": \"8EHmTXwBwf_U8gjU0Y0u\", \"feature_name\": \"test\", \"feature_enabled\": true, \"aggregation_query\": { \"test\": { \"sum\": { \"field\": \"value\" } } } } ], \"detector_type\": \"SINGLE_ENTITY\" } } . If you specify a category field, each result is associated with an entity: . Request . POST _plugins/_anomaly_detection/detectors/_preview { \"period_start\": 1633048868000, \"period_end\": 1633394468000, \"detector\": { \"name\": \"test-detector\", \"description\": \"Test update detector\", \"time_field\": \"timestamp\", \"indices\": [ \"server_log*\" ], \"feature_attributes\": [ { \"feature_name\": \"test\", \"feature_enabled\": true, \"aggregation_query\": { \"test\": { \"sum\": { \"field\": \"value\" } } } } ], \"filter_query\": { \"bool\": { \"filter\": [ { \"range\": { \"value\": { \"gt\": 1 } } } ], \"adjust_pure_negative\": true, \"boost\": 1 } }, \"detection_interval\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"window_delay\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"category_field\": [ \"error_type\" ] } } . Example response . { \"anomaly_result\": [ { \"detector_id\": null, \"data_start_time\": 1633049280000, \"data_end_time\": 1633049340000, \"schema_version\": 0, \"feature_data\": [ { \"feature_id\": \"tkTpTXwBjd8s6RK4DlOZ\", \"feature_name\": \"test\", \"data\": 0 } ], \"anomaly_grade\": 0, \"confidence\": 0, \"entity\": [ { \"name\": \"error_type\", \"value\": \"error1\" } ] }, ... ], \"anomaly_detector\": { \"name\": \"test-detector\", \"description\": \"Test update detector\", \"time_field\": \"timestamp\", \"indices\": [ \"server_log*\" ], \"filter_query\": { \"bool\": { \"filter\": [ { \"range\": { \"value\": { \"from\": 1, \"to\": null, \"include_lower\": false, \"include_upper\": true, \"boost\": 1 } } } ], \"adjust_pure_negative\": true, \"boost\": 1 } }, \"detection_interval\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"window_delay\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"shingle_size\": 8, \"schema_version\": 0, \"feature_attributes\": [ { \"feature_id\": \"tkTpTXwBjd8s6RK4DlOZ\", \"feature_name\": \"test\", \"feature_enabled\": true, \"aggregation_query\": { \"test\": { \"sum\": { \"field\": \"value\" } } } } ], \"category_field\": [ \"error_type\" ], \"detector_type\": \"MULTI_ENTITY\" } } . You can preview a detector with the detector ID: . POST _plugins/_anomaly_detection/detectors/_preview { \"detector_id\": \"VEHKTXwBwf_U8gjUXY2s\", \"period_start\": 1633048868000, \"period_end\": 1633394468000 } . Or: . POST _opendistro/_anomaly_detection/detectors/VEHKTXwBwf_U8gjUXY2s/_preview { \"period_start\": 1633048868000, \"period_end\": 1633394468000 } . Example response . { \"anomaly_result\": [ { \"detector_id\": \"VEHKTXwBwf_U8gjUXY2s\", \"data_start_time\": 1633049280000, \"data_end_time\": 1633049340000, \"schema_version\": 0, \"feature_data\": [ { \"feature_id\": \"3kHiTXwBwf_U8gjUlY15\", \"feature_name\": \"test\", \"data\": 0 } ], \"anomaly_grade\": 0, \"confidence\": 0, \"user\": { \"name\": \"admin\", \"backend_roles\": [ \"admin\" ], \"roles\": [ \"own_index\", \"all_access\" ], \"custom_attribute_names\": [], \"user_requested_tenant\": \"__user__\" } }, ... ], \"anomaly_detector\": { \"name\": \"test-detector\", \"description\": \"Test update detector\", \"time_field\": \"timestamp\", \"indices\": [ \"server_log*\" ], \"filter_query\": { \"bool\": { \"filter\": [ { \"range\": { \"value\": { \"from\": 1, \"to\": null, \"include_lower\": false, \"include_upper\": true, \"boost\": 1 } } } ], \"adjust_pure_negative\": true, \"boost\": 1 } }, \"detection_interval\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"window_delay\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"shingle_size\": 8, \"schema_version\": 0, \"feature_attributes\": [ { \"feature_id\": \"3kHiTXwBwf_U8gjUlY15\", \"feature_name\": \"test\", \"feature_enabled\": true, \"aggregation_query\": { \"test\": { \"sum\": { \"field\": \"value\" } } } } ], \"last_update_time\": 1633394267522, \"user\": { \"name\": \"admin\", \"backend_roles\": [ \"admin\" ], \"roles\": [ \"own_index\", \"all_access\" ], \"custom_attribute_names\": [], \"user_requested_tenant\": \"__user__\" }, \"detector_type\": \"SINGLE_ENTITY\" } } . ",
    "url": "https://vagimeli.github.io/observing-your-data/ad/api/#preview-detector",
    "relUrl": "/observing-your-data/ad/api/#preview-detector"
  },"1157": {
    "doc": "Anomaly detection API",
    "title": "Start detector job",
    "content": "Introduced 1.0 . Starts a real-time or historical anomaly detector job. To start a real-time detector job: . Request . POST _plugins/_anomaly_detection/detectors/&lt;detectorId&gt;/_start . Example response . { \"_id\": \"VEHKTXwBwf_U8gjUXY2s\", \"_version\": 3, \"_seq_no\": 6, \"_primary_term\": 1 } . The _id represents the real-time job ID, which is the same as the detector ID. To start historical analysis: . POST _plugins/_anomaly_detection/detectors/&lt;detectorId&gt;/_start { \"start_time\": 1633048868000, \"end_time\": 1633394468000 } . Example response . { \"_id\": \"f9DsTXwB6HknB84SoRTY\", \"_version\": 1, \"_seq_no\": 958, \"_primary_term\": 1 } . The _id represents the historical batch task ID, which is a random universally unique identifier (UUID). ",
    "url": "https://vagimeli.github.io/observing-your-data/ad/api/#start-detector-job",
    "relUrl": "/observing-your-data/ad/api/#start-detector-job"
  },"1158": {
    "doc": "Anomaly detection API",
    "title": "Stop detector job",
    "content": "Introduced 1.0 . Stops a real-time or historical anomaly detector job. To stop a real-time detector job: . Request . POST _plugins/_anomaly_detection/detectors/&lt;detectorId&gt;/_stop . Example response . { \"_id\": \"VEHKTXwBwf_U8gjUXY2s\", \"_version\": 0, \"_seq_no\": 0, \"_primary_term\": 0 } . To stop historical analysis: . Introduced 1.1 . POST _plugins/_anomaly_detection/detectors/&lt;detectorId&gt;/_stop?historical=true . Example response . { \"_id\": \"f9DsTXwB6HknB84SoRTY\", \"_version\": 0, \"_seq_no\": 0, \"_primary_term\": 0 } . ",
    "url": "https://vagimeli.github.io/observing-your-data/ad/api/#stop-detector-job",
    "relUrl": "/observing-your-data/ad/api/#stop-detector-job"
  },"1159": {
    "doc": "Anomaly detection API",
    "title": "Search detector",
    "content": "Introduced 1.0 . Returns all anomaly detectors for a search query. To search detectors using the server_log* index: . Request . GET _plugins/_anomaly_detection/detectors/_search POST _plugins/_anomaly_detection/detectors/_search { \"query\": { \"wildcard\": { \"indices\": { \"value\": \"server_log*\" } } } } . Example response . { \"took\": 2, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 4, \"relation\": \"eq\" }, \"max_score\": 1, \"hits\": [ { \"_index\": \".opensearch-anomaly-detectors\", \"_id\": \"Zi5zTXwBwf_U8gjUTfJG\", \"_version\": 1, \"_seq_no\": 1, \"_primary_term\": 1, \"_score\": 1, \"_source\": { \"name\": \"test\", \"description\": \"test\", \"time_field\": \"timestamp\", \"indices\": [ \"server_log\" ], \"filter_query\": { \"match_all\": { \"boost\": 1 } }, \"detection_interval\": { \"period\": { \"interval\": 5, \"unit\": \"Minutes\" } }, \"window_delay\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"shingle_size\": 8, \"schema_version\": 0, \"feature_attributes\": [ { \"feature_id\": \"ZS5zTXwBwf_U8gjUTfIn\", \"feature_name\": \"test_feature\", \"feature_enabled\": true, \"aggregation_query\": { \"test_feature\": { \"sum\": { \"field\": \"value\" } } } } ], \"last_update_time\": 1633386974533, \"category_field\": [ \"error_type\" ], \"user\": { \"name\": \"admin\", \"backend_roles\": [ \"admin\" ], \"roles\": [ \"own_index\", \"all_access\" ], \"custom_attribute_names\": [], \"user_requested_tenant\": \"__user__\" }, \"detector_type\": \"MULTI_ENTITY\" } }, ... ] } } . ",
    "url": "https://vagimeli.github.io/observing-your-data/ad/api/#search-detector",
    "relUrl": "/observing-your-data/ad/api/#search-detector"
  },"1160": {
    "doc": "Anomaly detection API",
    "title": "Search detector tasks",
    "content": "Introduced 1.1 . Searches detector tasks. To search for the latest detector level historical analysis task for a high cardinality detector . Request . GET _plugins/_anomaly_detection/detectors/tasks/_search POST _plugins/_anomaly_detection/detectors/tasks/_search { \"query\": { \"bool\": { \"filter\": [ { \"term\": { \"detector_id\": \"Zi5zTXwBwf_U8gjUTfJG\" } }, { \"term\": { \"task_type\": \"HISTORICAL_HC_DETECTOR\" } }, { \"term\": { \"is_latest\": \"true\" } } ] } } } . Example response . { \"took\": 1, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 1, \"relation\": \"eq\" }, \"max_score\": 0, \"hits\": [ { \"_index\": \".opensearch-anomaly-detection-state\", \"_id\": \"fm-RTXwBYwCbWecgB753\", \"_version\": 34, \"_seq_no\": 928, \"_primary_term\": 1, \"_score\": 0, \"_source\": { \"detector_id\": \"Zi5zTXwBwf_U8gjUTfJG\", \"error\": \"\", \"detection_date_range\": { \"start_time\": 1630794960000, \"end_time\": 1633386960000 }, \"task_progress\": 1, \"last_update_time\": 1633389090738, \"execution_start_time\": 1633388922742, \"state\": \"FINISHED\", \"coordinating_node\": \"2Z4q22BySEyzakYt_A0A2A\", \"task_type\": \"HISTORICAL_HC_DETECTOR\", \"execution_end_time\": 1633389090738, \"started_by\": \"admin\", \"init_progress\": 0, \"is_latest\": true, \"detector\": { \"category_field\": [ \"error_type\" ], \"description\": \"test\", \"ui_metadata\": { \"features\": { \"test_feature\": { \"aggregationBy\": \"sum\", \"aggregationOf\": \"value\", \"featureType\": \"simple_aggs\" } }, \"filters\": [] }, \"feature_attributes\": [ { \"feature_id\": \"ZS5zTXwBwf_U8gjUTfIn\", \"feature_enabled\": true, \"feature_name\": \"test_feature\", \"aggregation_query\": { \"test_feature\": { \"sum\": { \"field\": \"value\" } } } } ], \"schema_version\": 0, \"time_field\": \"timestamp\", \"last_update_time\": 1633386974533, \"indices\": [ \"server_log\" ], \"window_delay\": { \"period\": { \"unit\": \"Minutes\", \"interval\": 1 } }, \"detection_interval\": { \"period\": { \"unit\": \"Minutes\", \"interval\": 5 } }, \"name\": \"testhc\", \"filter_query\": { \"match_all\": { \"boost\": 1 } }, \"shingle_size\": 8, \"user\": { \"backend_roles\": [ \"admin\" ], \"custom_attribute_names\": [], \"roles\": [ \"own_index\", \"all_access\" ], \"name\": \"admin\", \"user_requested_tenant\": \"__user__\" }, \"detector_type\": \"MULTI_ENTITY\" }, \"user\": { \"backend_roles\": [ \"admin\" ], \"custom_attribute_names\": [], \"roles\": [ \"own_index\", \"all_access\" ], \"name\": \"admin\", \"user_requested_tenant\": \"__user__\" } } } ] } } . To search for the latest entity-level tasks for the historical analysis of a high cardinality detector: . Request . GET _plugins/_anomaly_detection/detectors/tasks/_search POST _plugins/_anomaly_detection/detectors/tasks/_search { \"query\": { \"bool\": { \"filter\": [ { \"term\": { \"detector_id\": \"Zi5zTXwBwf_U8gjUTfJG\" } }, { \"term\": { \"task_type\": \"HISTORICAL_HC_ENTITY\" } }, { \"term\": { \"is_latest\": \"true\" } } ] } }, \"sort\": [ { \"execution_start_time\": { \"order\": \"desc\" } } ], \"size\": 100 } . To search and aggregate states for all entity-level historical tasks: . The parent_task_id is the same as the task ID that you can get with the profile detector API: GET _plugins/_anomaly_detection/detectors/&lt;detector_ID&gt;/_profile/ad_task. Request . GET _plugins/_anomaly_detection/detectors/tasks/_search POST _plugins/_anomaly_detection/detectors/tasks/_search { \"size\": 0, \"query\": { \"bool\": { \"filter\": [ { \"term\": { \"detector_id\": { \"value\": \"Zi5zTXwBwf_U8gjUTfJG\", \"boost\": 1 } } }, { \"term\": { \"parent_task_id\": { \"value\": \"fm-RTXwBYwCbWecgB753\", \"boost\": 1 } } }, { \"terms\": { \"task_type\": [ \"HISTORICAL_HC_ENTITY\" ], \"boost\": 1 } } ] } }, \"aggs\": { \"test\": { \"terms\": { \"field\": \"state\", \"size\": 100 } } } } . Example response . { \"took\": 2, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 32, \"relation\": \"eq\" }, \"max_score\": null, \"hits\": [] }, \"aggregations\": { \"test\": { \"doc_count_error_upper_bound\": 0, \"sum_other_doc_count\": 0, \"buckets\": [ { \"key\": \"FINISHED\", \"doc_count\": 32 } ] } } } . ",
    "url": "https://vagimeli.github.io/observing-your-data/ad/api/#search-detector-tasks",
    "relUrl": "/observing-your-data/ad/api/#search-detector-tasks"
  },"1161": {
    "doc": "Anomaly detection API",
    "title": "Search detector result",
    "content": "Introduced 1.0 . Returns all results for a search query. You have the following search options: . | To search only the default result index, simply use the search API: . POST _plugins/_anomaly_detection/detectors/results/_search/ . | To search both the custom result index and default result index, you can either add the custom result index to the search API: . POST _plugins/_anomaly_detection/detectors/results/_search/&lt;custom_result_index&gt; . Or, add the custom result index and set the only_query_custom_result_index parameter to false: . POST _plugins/_anomaly_detection/detectors/results/_search/&lt;custom_result_index&gt;?only_query_custom_result_index=false . | To search only the custom result index, add the custom result index to the search API and set the only_query_custom_result_index parameter to true: . POST _plugins/_anomaly_detection/detectors/results/_search/&lt;custom_result_index&gt;?only_query_custom_result_index=true . | . The following example searches anomaly results for grade greater than 0 for real-time analysis: . Request . GET _plugins/_anomaly_detection/detectors/results/_search/opensearch-ad-plugin-result-test POST _plugins/_anomaly_detection/detectors/results/_search/opensearch-ad-plugin-result-test { \"query\": { \"bool\": { \"filter\": [ { \"term\": { \"detector_id\": \"EWy02nwBm38sXcF2AiFJ\" } }, { \"range\": { \"anomaly_grade\": { \"gt\": 0 } } } ], \"must_not\": [ { \"exists\": { \"field\": \"task_id\" } } ] } } } . If you specify the custom result index like in this example, the search results API searches both the default result indices and custom result indices. If you don’t specify the custom result index and you just use the _plugins/_anomaly_detection/detectors/results/_search URL, the anomaly detection plugin searches only the default result indices. Real-time detection doesn’t persist the task ID in the anomaly result, so the task ID will be null. For information about the response body fields, see Anomaly result mapping. Example response . { \"took\": 4, \"timed_out\": false, \"_shards\": { \"total\": 3, \"successful\": 3, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 90, \"relation\": \"eq\" }, \"max_score\": 0, \"hits\": [ { \"_index\": \".opensearch-anomaly-results-history-2021.10.04-1\", \"_id\": \"686KTXwB6HknB84SMr6G\", \"_version\": 1, \"_seq_no\": 103622, \"_primary_term\": 1, \"_score\": 0, \"_source\": { \"detector_id\": \"EWy02nwBm38sXcF2AiFJ\", \"confidence\": 0.918886275269358, \"model_id\": \"EWy02nwBm38sXcF2AiFJ_entity_error16\", \"schema_version\": 4, \"anomaly_score\": 1.1093755891885446, \"execution_start_time\": 1633388475001, \"data_end_time\": 1633388414989, \"data_start_time\": 1633388114989, \"feature_data\": [ { \"feature_id\": \"ZS5zTXwBwf_U8gjUTfIn\", \"feature_name\": \"test_feature\", \"data\": 0.532 } ], \"relevant_attribution\": [ { \"feature_id\": \"ZS5zTXwBwf_U8gjUTfIn\", \"data\": 1.0 } ], \"expected_values\": [ { \"likelihood\": 1, \"value_list\": [ { \"feature_id\": \"ZS5zTXwBwf_U8gjUTfIn\", \"data\": 2 } ] } ], \"execution_end_time\": 1633388475014, \"user\": { \"backend_roles\": [ \"admin\" ], \"custom_attribute_names\": [], \"roles\": [ \"own_index\", \"all_access\" ], \"name\": \"admin\", \"user_requested_tenant\": \"__user__\" }, \"anomaly_grade\": 0.031023547546561225, \"entity\": [ { \"name\": \"error_type\", \"value\": \"error16\" } ] } }, ... ] } } . You can run historical analysis as many times as you like. So, multiple tasks might exist for the same detector. You can search for the latest historical batch task first and then search the historical batch task results. To search anomaly results for grade greater than 0 for historical analysis with the task_id: . Request . GET _plugins/_anomaly_detection/detectors/results/_search POST _plugins/_anomaly_detection/detectors/results/_search { \"query\": { \"bool\": { \"filter\": [ { \"term\": { \"detector_id\": \"Zi5zTXwBwf_U8gjUTfJG\" } }, { \"range\": { \"anomaly_grade\": { \"gt\": 0 } } }, { \"term\": { \"task_id\": \"fm-RTXwBYwCbWecgB753\" } } ] } } } . Example response . { \"took\": 915, \"timed_out\": false, \"_shards\": { \"total\": 3, \"successful\": 3, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 4115, \"relation\": \"eq\" }, \"max_score\": 0, \"hits\": [ { \"_index\": \".opensearch-anomaly-results-history-2021.10.04-1\", \"_id\": \"VRyRTXwBDx7vzPBV8jYC\", \"_version\": 1, \"_seq_no\": 149657, \"_primary_term\": 1, \"_score\": 0, \"_source\": { \"detector_id\": \"Zi5zTXwBwf_U8gjUTfJG\", \"confidence\": 0.9642989263957601, \"task_id\": \"fm-RTXwBYwCbWecgB753\", \"model_id\": \"Zi5zTXwBwf_U8gjUTfJG_entity_error24\", \"schema_version\": 4, \"anomaly_score\": 1.2260712437521946, \"execution_start_time\": 1633388982692, \"data_end_time\": 1631721300000, \"data_start_time\": 1631721000000, \"feature_data\": [ { \"feature_id\": \"ZS5zTXwBwf_U8gjUTfIn\", \"feature_name\": \"test_feature\", \"data\": 10 } ], \"execution_end_time\": 1633388982709, \"user\": { \"backend_roles\": [ \"admin\" ], \"custom_attribute_names\": [], \"roles\": [ \"own_index\", \"all_access\" ], \"name\": \"admin\", \"user_requested_tenant\": \"__user__\" }, \"anomaly_grade\": 0.14249628345655782, \"entity\": [ { \"name\": \"error_type\", \"value\": \"error1\" } ] } }, ... ] } } . ",
    "url": "https://vagimeli.github.io/observing-your-data/ad/api/#search-detector-result",
    "relUrl": "/observing-your-data/ad/api/#search-detector-result"
  },"1162": {
    "doc": "Anomaly detection API",
    "title": "Search top anomalies",
    "content": "Introduced 1.2 . Returns the top anomaly results for a high-cardinality detector, bucketed by categorical field values. You can pass a historical boolean parameter to specify whether you want to analyze real-time or historical results. Request . GET _plugins/_anomaly_detection/detectors/&lt;detectorId&gt;/results/_topAnomalies?historical=false { \"size\": 3, \"category_field\": [ \"ip\" ], \"order\": \"severity\", \"task_id\": \"example-task-id\", \"start_time_ms\": 123456789000, \"end_time_ms\": 987654321000 } . Example response . { \"buckets\": [ { \"key\": { \"ip\": \"1.2.3.4\" }, \"doc_count\": 10, \"max_anomaly_grade\": 0.8 }, { \"key\": { \"ip\": \"5.6.7.8\" }, \"doc_count\": 12, \"max_anomaly_grade\": 0.6 }, { \"key\": { \"ip\": \"9.10.11.12\" }, \"doc_count\": 3, \"max_anomaly_grade\": 0.5 } ] } . You can specify the following options. | Options | Description | Type | Required | . | size | Specify the number of top buckets that you want to see. Default is 10. The maximum number is 10,000. | integer | No | . | category_field | Specify the set of category fields that you want to aggregate on. Defaults to all category fields for the detector. | list | No | . | order | Specify severity (anomaly grade) or occurrence (number of anomalies). Default is severity. | string | No | . | task_id | Specify a historical task ID to see results only from that specific task. Use only when historical=true, otherwise the anomaly detection plugin ignores this parameter. | string | No | . | start_time_ms | Specify the time to start analyzing results, in Epoch milliseconds. | long | Yes | . | end_time_ms | Specify the time to end analyzing results, in Epoch milliseconds. | long | Yes | . ",
    "url": "https://vagimeli.github.io/observing-your-data/ad/api/#search-top-anomalies",
    "relUrl": "/observing-your-data/ad/api/#search-top-anomalies"
  },"1163": {
    "doc": "Anomaly detection API",
    "title": "Get detector stats",
    "content": "Introduced 1.0 . Provides information about how the plugin is performing. To get all stats: . Request . GET _plugins/_anomaly_detection/stats . Example response . { \"anomaly_detectors_index_status\": \"green\", \"anomaly_detection_state_status\": \"green\", \"single_entity_detector_count\": 2, \"detector_count\": 5, \"multi_entity_detector_count\": 3, \"anomaly_detection_job_index_status\": \"green\", \"models_checkpoint_index_status\": \"green\", \"anomaly_results_index_status\": \"green\", \"nodes\": { \"2Z4q22BySEyzakYt_A0A2A\": { \"ad_execute_request_count\": 95, \"models\": [ { \"detector_id\": \"WTBnTXwBjd8s6RK4b1Sz\", \"model_type\": \"rcf\", \"last_used_time\": 1633398197185, \"model_id\": \"WTBnTXwBjd8s6RK4b1Sz_model_rcf_0\", \"last_checkpoint_time\": 1633396573679 }, ... ], \"ad_canceled_batch_task_count\": 0, \"ad_hc_execute_request_count\": 75, \"ad_hc_execute_failure_count\": 0, \"model_count\": 28, \"ad_execute_failure_count\": 1, \"ad_batch_task_failure_count\": 0, \"ad_total_batch_task_execution_count\": 27, \"ad_executing_batch_task_count\": 3 }, \"SWD7ihu9TaaW1zKwFZNVNg\": { \"ad_execute_request_count\": 12, \"models\": [ { \"detector_id\": \"Zi5zTXwBwf_U8gjUTfJG\", \"model_type\": \"entity\", \"last_used_time\": 1633398375008, \"model_id\": \"Zi5zTXwBwf_U8gjUTfJG_entity_error13\", \"last_checkpoint_time\": 1633392973682, \"entity\": [ { \"name\": \"error_type\", \"value\": \"error13\" } ] }, ... ], \"ad_canceled_batch_task_count\": 1, \"ad_hc_execute_request_count\": 0, \"ad_hc_execute_failure_count\": 0, \"model_count\": 15, \"ad_execute_failure_count\": 2, \"ad_batch_task_failure_count\": 0, \"ad_total_batch_task_execution_count\": 27, \"ad_executing_batch_task_count\": 4 }, \"TQDUXEzyTJyV0H6_T4hYUw\": { \"ad_execute_request_count\": 0, \"models\": [ { \"detector_id\": \"Zi5zTXwBwf_U8gjUTfJG\", \"model_type\": \"entity\", \"last_used_time\": 1633398375004, \"model_id\": \"Zi5zTXwBwf_U8gjUTfJG_entity_error24\", \"last_checkpoint_time\": 1633388177359, \"entity\": [ { \"name\": \"error_type\", \"value\": \"error24\" } ] }, ... ], \"ad_canceled_batch_task_count\": 0, \"ad_hc_execute_request_count\": 0, \"ad_hc_execute_failure_count\": 0, \"model_count\": 22, \"ad_execute_failure_count\": 0, \"ad_batch_task_failure_count\": 0, \"ad_total_batch_task_execution_count\": 28, \"ad_executing_batch_task_count\": 3 } } } . The model_count parameter shows the total number of models running on each node’s memory. For historical analysis, you see the values for the following fields: . | ad_total_batch_task_execution_count | ad_executing_batch_task_count | ad_canceled_batch_task_count | ad_batch_task_failure_count | . If haven’t run any historical analysis, these values show up as 0. To get all stats for a specific node: . Request . GET _plugins/_anomaly_detection/&lt;nodeId&gt;/stats . To get specific stats for a node: . Request . GET _plugins/_anomaly_detection/&lt;nodeId&gt;/stats/&lt;stat&gt; . For example, to get the ad_execute_request_count value for node SWD7ihu9TaaW1zKwFZNVNg: . GET _plugins/_anomaly_detection/SWD7ihu9TaaW1zKwFZNVNg/stats/ad_execute_request_count . Example response . { \"nodes\": { \"SWD7ihu9TaaW1zKwFZNVNg\": { \"ad_execute_request_count\": 12 } } } . To get a specific type of stats: . Request . GET _plugins/_anomaly_detection/stats/&lt;stat&gt; . For example: . GET _plugins/_anomaly_detection/stats/ad_executing_batch_task_count . Example response . { \"nodes\": { \"2Z4q22BySEyzakYt_A0A2A\": { \"ad_executing_batch_task_count\": 3 }, \"SWD7ihu9TaaW1zKwFZNVNg\": { \"ad_executing_batch_task_count\": 3 }, \"TQDUXEzyTJyV0H6_T4hYUw\": { \"ad_executing_batch_task_count\": 4 } } } . ",
    "url": "https://vagimeli.github.io/observing-your-data/ad/api/#get-detector-stats",
    "relUrl": "/observing-your-data/ad/api/#get-detector-stats"
  },"1164": {
    "doc": "Anomaly detection API",
    "title": "Profile detector",
    "content": "Introduced 1.0 . Returns information related to the current state of the detector and memory usage, including current errors and shingle size, to help troubleshoot the detector. This command helps locate logs by identifying the nodes that run the anomaly detector job for each detector. It also helps track the initialization percentage, the required shingles, and the estimated time left. Request . GET _plugins/_anomaly_detection/detectors/&lt;detectorId&gt;/_profile/ GET _plugins/_anomaly_detection/detectors/&lt;detectorId&gt;/_profile?_all=true GET _plugins/_anomaly_detection/detectors/&lt;detectorId&gt;/_profile/&lt;type&gt; GET _plugins/_anomaly_detection/detectors/&lt;detectorId&gt;/_profile/&lt;type1&gt;,&lt;type2&gt; . Sample Responses . GET _plugins/_anomaly_detection/detectors/&lt;detectorId&gt;/_profile { \"state\": \"DISABLED\", \"error\": \"Stopped detector: AD models memory usage exceeds our limit.\" } GET _plugins/_anomaly_detection/detectors/&lt;detectorId&gt;/_profile?_all=true&amp;pretty { \"state\": \"RUNNING\", \"error\": \"\", \"models\": [ { \"model_id\": \"3Dh6TXwBwf_U8gjURE0F_entity_KSLSh0Wv05RQXiBAQHTEZg\", \"entity\": [ { \"name\": \"ip\", \"value\": \"192.168.1.1\" }, { \"name\": \"error_type\", \"value\": \"error8\" } ], \"model_size_in_bytes\": 403491, \"node_id\": \"2Z4q22BySEyzakYt_A0A2A\" }, ... ], \"total_size_in_bytes\": 12911712, \"init_progress\": { \"percentage\": \"100%\" }, \"total_entities\": 33, \"active_entities\": 32, \"ad_task\": { \"ad_task\": { \"task_id\": \"D3I5TnwBYwCbWecg7lN9\", \"last_update_time\": 1633399993685, \"started_by\": \"admin\", \"state\": \"RUNNING\", \"detector_id\": \"3Dh6TXwBwf_U8gjURE0F\", \"task_progress\": 0, \"init_progress\": 0, \"execution_start_time\": 1633399991933, \"is_latest\": true, \"task_type\": \"HISTORICAL_HC_DETECTOR\", \"coordinating_node\": \"2Z4q22BySEyzakYt_A0A2A\", \"detector\": { \"name\": \"testhc-mc\", \"description\": \"test\", \"time_field\": \"timestamp\", \"indices\": [ \"server_log\" ], \"filter_query\": { \"match_all\": { \"boost\": 1 } }, \"detection_interval\": { \"period\": { \"interval\": 5, \"unit\": \"Minutes\" } }, \"window_delay\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"shingle_size\": 8, \"schema_version\": 0, \"feature_attributes\": [ { \"feature_id\": \"2zh6TXwBwf_U8gjUQ039\", \"feature_name\": \"test\", \"feature_enabled\": true, \"aggregation_query\": { \"test\": { \"sum\": { \"field\": \"value\" } } } } ], \"ui_metadata\": { \"features\": { \"test\": { \"aggregationBy\": \"sum\", \"aggregationOf\": \"value\", \"featureType\": \"simple_aggs\" } }, \"filters\": [] }, \"last_update_time\": 1633387430916, \"category_field\": [ \"ip\", \"error_type\" ], \"user\": { \"name\": \"admin\", \"backend_roles\": [ \"admin\" ], \"roles\": [ \"own_index\", \"all_access\" ], \"custom_attribute_names\": [], \"user_requested_tenant\": \"__user__\" }, \"detector_type\": \"MULTI_ENTITY\" }, \"detection_date_range\": { \"start_time\": 1632793800000, \"end_time\": 1633398600000 }, \"user\": { \"name\": \"admin\", \"backend_roles\": [ \"admin\" ], \"roles\": [ \"own_index\", \"all_access\" ], \"custom_attribute_names\": [], \"user_requested_tenant\": \"__user__\" } }, \"node_id\": \"2Z4q22BySEyzakYt_A0A2A\", \"task_id\": \"D3I5TnwBYwCbWecg7lN9\", \"task_type\": \"HISTORICAL_HC_DETECTOR\", \"detector_task_slots\": 10, \"total_entities_count\": 32, \"pending_entities_count\": 22, \"running_entities_count\": 10, \"running_entities\": [ \"\"\"[{\"name\":\"ip\",\"value\":\"192.168.1.1\"},{\"name\":\"error_type\",\"value\":\"error9\"}]\"\"\", ...], \"entity_task_profiles\": [ { \"shingle_size\": 8, \"rcf_total_updates\": 1994, \"threshold_model_trained\": true, \"threshold_model_training_data_size\": 0, \"model_size_in_bytes\": 1593240, \"node_id\": \"2Z4q22BySEyzakYt_A0A2A\", \"entity\": [ { \"name\": \"ip\", \"value\": \"192.168.1.1\" }, { \"name\": \"error_type\", \"value\": \"error7\" } ], \"task_id\": \"E3I5TnwBYwCbWecg9FMm\", \"task_type\": \"HISTORICAL_HC_ENTITY\" }, ... ] }, \"model_count\": 32 } GET _plugins/_anomaly_detection/detectors/&lt;detectorId&gt;/_profile/total_size_in_bytes { \"total_size_in_bytes\": 13369344 } . You can see the ad_task field only for historical analysis. The model_count parameter shows the total number of models that a detector runs on each node’s memory. This is useful if you have several models running on your cluster and want to know the count. If you configured the category field, you can see the number of unique values in the field and all active entities with models running in memory. You can use this data to estimate how much memory is required for anomaly detection so you can decide how to size your cluster. For example, if a detector has one million entities and only 10 of them are active in memory, you need to scale your cluster up or out. For a single-entity detector: . Example response . { \"state\": \"INIT\", \"total_size_in_bytes\": 0, \"init_progress\": { \"percentage\": \"0%\", \"needed_shingles\": 128 }, \"ad_task\": { \"ad_task\": { \"task_id\": \"cfUNOXwBFLNqSEcxAlde\", \"last_update_time\": 1633044731640, \"started_by\": \"admin\", \"state\": \"RUNNING\", \"detector_id\": \"qL4NOXwB__6eNorTAKtJ\", \"task_progress\": 0.49603173, \"init_progress\": 1, \"current_piece\": 1632739800000, \"execution_start_time\": 1633044726365, \"is_latest\": true, \"task_type\": \"HISTORICAL_SINGLE_ENTITY\", \"coordinating_node\": \"bCtWtxWPThq0BIn5P5I4Xw\", \"worker_node\": \"dIyavWhmSYWGz65b4u-lpQ\", \"detector\": { \"name\": \"detector1\", \"description\": \"test\", \"time_field\": \"timestamp\", \"indices\": [ \"server_log\" ], \"filter_query\": { \"match_all\": { \"boost\": 1 } }, \"detection_interval\": { \"period\": { \"interval\": 5, \"unit\": \"Minutes\" } }, \"window_delay\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"shingle_size\": 8, \"schema_version\": 0, \"feature_attributes\": [ { \"feature_id\": \"p74NOXwB__6eNorTAKss\", \"feature_name\": \"test-feature\", \"feature_enabled\": true, \"aggregation_query\": { \"test_feature\": { \"sum\": { \"field\": \"value\" } } } } ], \"ui_metadata\": { \"features\": { \"test-feature\": { \"aggregationBy\": \"sum\", \"aggregationOf\": \"value\", \"featureType\": \"simple_aggs\" } }, \"filters\": [] }, \"last_update_time\": 1633044725832, \"user\": { \"name\": \"admin\", \"backend_roles\": [ \"admin\" ], \"roles\": [ \"own_index\", \"all_access\" ], \"custom_attribute_names\": [], \"user_requested_tenant\": \"__user__\" }, \"detector_type\": \"SINGLE_ENTITY\" }, \"detection_date_range\": { \"start_time\": 1632439925885, \"end_time\": 1633044725885 }, \"user\": { \"name\": \"admin\", \"backend_roles\": [ \"admin\" ], \"roles\": [ \"own_index\", \"all_access\" ], \"custom_attribute_names\": [], \"user_requested_tenant\": \"__user__\" } }, \"shingle_size\": 8, \"rcf_total_updates\": 1994, \"threshold_model_trained\": true, \"threshold_model_training_data_size\": 0, \"model_size_in_bytes\": 1593240, \"node_id\": \"dIyavWhmSYWGz65b4u-lpQ\", \"detector_task_slots\": 1 } } . The total_entities parameter shows you the total number of entities including the number of category fields for a detector. Getting the total count of entities is an expensive operation for real-time analysis of a detector with more than one category field. By default, for a real-time detection profile, a detector counts the number of entities up to a value of 10,000. For historical analysis, the anomaly detection plugin only detects the top 1,000 entities by default and caches the top entities in memory, so it doesn’t cost much to get the total count of entities for historical analysis. The profile operation also provides information about each entity, such as the entity’s last_sample_timestamp and last_active_timestamp. last_sample_timestamp shows the last document in the input data source index containing the entity, while last_active_timestamp shows the timestamp when the entity’s model was last seen in the model cache. If there are no anomaly results for an entity, either the entity doesn’t have any sample data or resources such as memory and disk IO are constrained relative to the number of entities. Request . GET _plugins/_anomaly_detection/detectors/&lt;detectorId&gt;/_profile?_all=true { \"entity\": [ { \"name\": \"host\", \"value\": \"i-00f28ec1eb8997686\" } ] } . Sample Responses . { \"is_active\": true, \"last_active_timestamp\": 1604026394879, \"last_sample_timestamp\": 1604026394879, \"init_progress\": { \"percentage\": \"100%\" }, \"model\": { \"model_id\": \"TFUdd3UBBwIAGQeRh5IS_entity_i-00f28ec1eb8997686\", \"model_size_in_bytes\": 712480, \"node_id\": \"MQ-bTBW3Q2uU_2zX3pyEQg\" }, \"state\": \"RUNNING\" } . To get profile information for only historical analysis, specify ad_task. Specifying _all is an expensive operation for multi-category high cardinality detectors. Request . GET _plugins/_anomaly_detection/detectors/&lt;detectorId&gt;/_profile?_all GET _plugins/_anomaly_detection/detectors/&lt;detectorId&gt;/_profile/ad_task . Sample Responses . { \"ad_task\": { \"ad_task\": { \"task_id\": \"CHI0TnwBYwCbWecgqgRA\", \"last_update_time\": 1633399648413, \"started_by\": \"admin\", \"state\": \"RUNNING\", \"detector_id\": \"3Dh6TXwBwf_U8gjURE0F\", \"task_progress\": 0, \"init_progress\": 0, \"execution_start_time\": 1633399646784, \"is_latest\": true, \"task_type\": \"HISTORICAL_HC_DETECTOR\", \"coordinating_node\": \"2Z4q22BySEyzakYt_A0A2A\", \"detector\": { \"name\": \"testhc-mc\", \"description\": \"test\", \"time_field\": \"timestamp\", \"indices\": [ \"server_log\" ], \"filter_query\": { \"match_all\": { \"boost\": 1 } }, \"detection_interval\": { \"period\": { \"interval\": 5, \"unit\": \"Minutes\" } }, \"window_delay\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"shingle_size\": 8, \"schema_version\": 0, \"feature_attributes\": [ { \"feature_id\": \"2zh6TXwBwf_U8gjUQ039\", \"feature_name\": \"test\", \"feature_enabled\": true, \"aggregation_query\": { \"test\": { \"sum\": { \"field\": \"value\" } } } } ], \"ui_metadata\": { \"features\": { \"test\": { \"aggregationBy\": \"sum\", \"aggregationOf\": \"value\", \"featureType\": \"simple_aggs\" } }, \"filters\": [] }, \"last_update_time\": 1633387430916, \"category_field\": [ \"ip\", \"error_type\" ], \"user\": { \"name\": \"admin\", \"backend_roles\": [ \"admin\" ], \"roles\": [ \"own_index\", \"all_access\" ], \"custom_attribute_names\": [], \"user_requested_tenant\": \"__user__\" }, \"detector_type\": \"MULTI_ENTITY\" }, \"detection_date_range\": { \"start_time\": 1632793800000, \"end_time\": 1633398600000 }, \"user\": { \"name\": \"admin\", \"backend_roles\": [ \"admin\" ], \"roles\": [ \"own_index\", \"all_access\" ], \"custom_attribute_names\": [], \"user_requested_tenant\": \"__user__\" } }, \"node_id\": \"2Z4q22BySEyzakYt_A0A2A\", \"task_id\": \"CHI0TnwBYwCbWecgqgRA\", \"task_type\": \"HISTORICAL_HC_DETECTOR\", \"detector_task_slots\": 10, \"total_entities_count\": 32, \"pending_entities_count\": 22, \"running_entities_count\": 10, \"running_entities\" : [ \"\"\"[{\"name\":\"ip\",\"value\":\"192.168.1.1\"},{\"name\":\"error_type\",\"value\":\"error9\"}]\"\"\", ... ], \"entity_task_profiles\": [ { \"shingle_size\": 8, \"rcf_total_updates\": 994, \"threshold_model_trained\": true, \"threshold_model_training_data_size\": 0, \"model_size_in_bytes\": 1593240, \"node_id\": \"2Z4q22BySEyzakYt_A0A2A\", \"entity\": [ { \"name\": \"ip\", \"value\": \"192.168.1.1\" }, { \"name\": \"error_type\", \"value\": \"error6\" } ], \"task_id\": \"9XI0TnwBYwCbWecgsAd6\", \"task_type\": \"HISTORICAL_HC_ENTITY\" }, ... ] } } . ",
    "url": "https://vagimeli.github.io/observing-your-data/ad/api/#profile-detector",
    "relUrl": "/observing-your-data/ad/api/#profile-detector"
  },"1165": {
    "doc": "Anomaly detection API",
    "title": "Delete detector results",
    "content": "Introduced 1.1 . Deletes the results of a detector based on a query. The delete detector results API only deletes anomaly result documents in the default result index. It doesn’t support deleting anomaly result documents stored in any custom result indices. You need to manually delete anomaly result documents that you don’t need from custom result indices. Request . DELETE _plugins/_anomaly_detection/detectors/results { \"query\": { \"bool\": { \"filter\": [ { \"term\": { \"detector_id\": { \"value\": \"rlDtOHwBD5tpxlbyW7Nt\" } } }, { \"term\": { \"task_id\": { \"value\": \"TM3tOHwBCi2h__AOXlyQ\" } } }, { \"range\": { \"data_start_time\": { \"lte\": 1632441600000 } } } ] } } } . Example response . { \"took\": 48, \"timed_out\": false, \"total\": 28, \"updated\": 0, \"created\": 0, \"deleted\": 28, \"batches\": 1, \"version_conflicts\": 0, \"noops\": 0, \"retries\": { \"bulk\": 0, \"search\": 0 }, \"throttled_millis\": 0, \"requests_per_second\": -1, \"throttled_until_millis\": 0, \"failures\": [] } . ",
    "url": "https://vagimeli.github.io/observing-your-data/ad/api/#delete-detector-results",
    "relUrl": "/observing-your-data/ad/api/#delete-detector-results"
  },"1166": {
    "doc": "Anomaly detection API",
    "title": "Create monitor",
    "content": "Introduced 1.0 . Create a monitor to set up alerts for the detector. Request . POST _plugins/_alerting/monitors { \"type\": \"monitor\", \"name\": \"test-monitor\", \"enabled\": true, \"schedule\": { \"period\": { \"interval\": 20, \"unit\": \"MINUTES\" } }, \"inputs\": [ { \"search\": { \"indices\": [ \".opensearch-anomaly-results*\" ], \"query\": { \"size\": 1, \"query\": { \"bool\": { \"filter\": [ { \"range\": { \"data_end_time\": { \"from\": \"||-20m\", \"to\": \"\", \"include_lower\": true, \"include_upper\": true, \"boost\": 1 } } }, { \"term\": { \"detector_id\": { \"value\": \"m4ccEnIBTXsGi3mvMt9p\", \"boost\": 1 } } } ], \"adjust_pure_negative\": true, \"boost\": 1 } }, \"sort\": [ { \"anomaly_grade\": { \"order\": \"desc\" } }, { \"confidence\": { \"order\": \"desc\" } } ], \"aggregations\": { \"max_anomaly_grade\": { \"max\": { \"field\": \"anomaly_grade\" } } } } } } ], \"triggers\": [ { \"name\": \"test-trigger\", \"severity\": \"1\", \"condition\": { \"script\": { \"source\": \"return ctx.results[0].aggregations.max_anomaly_grade.value != null &amp;&amp; ctx.results[0].aggregations.max_anomaly_grade.value &gt; 0.7 &amp;&amp; ctx.results[0].hits.hits[0]._source.confidence &gt; 0.7\", \"lang\": \"painless\" } }, \"actions\": [ { \"name\": \"test-action\", \"destination_id\": \"ld7912sBlQ5JUWWFThoW\", \"message_template\": { \"source\": \"This is my message body.\" }, \"throttle_enabled\": false, \"subject_template\": { \"source\": \"TheSubject\" } } ] } ] } . Example response . { \"_id\": \"OClTEnIBmSf7y6LP11Jz\", \"_version\": 1, \"_seq_no\": 10, \"_primary_term\": 1, \"monitor\": { \"type\": \"monitor\", \"schema_version\": 1, \"name\": \"test-monitor\", \"enabled\": true, \"enabled_time\": 1589445384043, \"schedule\": { \"period\": { \"interval\": 20, \"unit\": \"MINUTES\" } }, \"inputs\": [ { \"search\": { \"indices\": [ \".opensearch-anomaly-results*\" ], \"query\": { \"size\": 1, \"query\": { \"bool\": { \"filter\": [ { \"range\": { \"data_end_time\": { \"from\": \"||-20m\", \"to\": \"\", \"include_lower\": true, \"include_upper\": true, \"boost\": 1 } } }, { \"term\": { \"detector_id\": { \"value\": \"m4ccEnIBTXsGi3mvMt9p\", \"boost\": 1 } } } ], \"adjust_pure_negative\": true, \"boost\": 1 } }, \"sort\": [ { \"anomaly_grade\": { \"order\": \"desc\" } }, { \"confidence\": { \"order\": \"desc\" } } ], \"aggregations\": { \"max_anomaly_grade\": { \"max\": { \"field\": \"anomaly_grade\" } } } } } } ], \"triggers\": [ { \"id\": \"NilTEnIBmSf7y6LP11Jr\", \"name\": \"test-trigger\", \"severity\": \"1\", \"condition\": { \"script\": { \"source\": \"return ctx.results[0].aggregations.max_anomaly_grade.value != null &amp;&amp; ctx.results[0].aggregations.max_anomaly_grade.value &gt; 0.7 &amp;&amp; ctx.results[0].hits.hits[0]._source.confidence &gt; 0.7\", \"lang\": \"painless\" } }, \"actions\": [ { \"id\": \"NylTEnIBmSf7y6LP11Jr\", \"name\": \"test-action\", \"destination_id\": \"ld7912sBlQ5JUWWFThoW\", \"message_template\": { \"source\": \"This is my message body.\", \"lang\": \"mustache\" }, \"throttle_enabled\": false, \"subject_template\": { \"source\": \"TheSubject\", \"lang\": \"mustache\" } } ] } ], \"last_update_time\": 1589445384043 } } . ",
    "url": "https://vagimeli.github.io/observing-your-data/ad/api/#create-monitor",
    "relUrl": "/observing-your-data/ad/api/#create-monitor"
  },"1167": {
    "doc": "Anomaly detection",
    "title": "Anomaly detection",
    "content": "An anomaly in OpenSearch is any unusual behavior change in your time-series data. Anomalies can provide valuable insights into your data. For example, for IT infrastructure data, an anomaly in the memory usage metric might help you uncover early signs of a system failure. It can be challenging to discover anomalies using conventional methods such as creating visualizations and dashboards. You could configure an alert based on a static threshold, but this requires prior domain knowledge and isn’t adaptive to data that exhibits organic growth or seasonal behavior. Anomaly detection automatically detects anomalies in your OpenSearch data in near real-time using the Random Cut Forest (RCF) algorithm. RCF is an unsupervised machine learning algorithm that models a sketch of your incoming data stream to compute an anomaly grade and confidence score value for each incoming data point. These values are used to differentiate an anomaly from normal variations. For more information about how RCF works, see Random Cut Forests. You can pair the anomaly detection plugin with the alerting plugin to notify you as soon as an anomaly is detected. To get started, choose Anomaly Detection in OpenSearch Dashboards. To first test with sample streaming data, you can try out one of the preconfigured detectors with one of the sample datasets. ",
    "url": "https://vagimeli.github.io/observing-your-data/ad/index/",
    "relUrl": "/observing-your-data/ad/index/"
  },"1168": {
    "doc": "Anomaly detection",
    "title": "Step 1: Define a detector",
    "content": "A detector is an individual anomaly detection task. You can define multiple detectors, and all the detectors can run simultaneously, with each analyzing data from different sources. | Choose Create detector. | Add in the detector details. | Enter a name and brief description. Make sure the name is unique and descriptive enough to help you to identify the purpose of the detector. | . | Specify the data source. | For Data source, choose the index you want to use as the data source. You can optionally use index patterns to choose multiple indices. | (Optional) For Data filter, filter the index you chose as the data source. From the Data filter menu, choose Add data filter, and then design your filter query by selecting Field, Operator, and Value, or choose Use query DSL and add your own JSON filter query. | . | Specify a timestamp. | Select the Timestamp field in your index. | . | Define operation settings. | For Operation settings, define the Detector interval, which is the time interval at which the detector collects data. | The detector aggregates the data in this interval, then feeds the aggregated result into the anomaly detection model. The shorter you set this interval, the fewer data points the detector aggregates. The anomaly detection model uses a shingling process, a technique that uses consecutive data points to create a sample for the model. This process needs a certain number of aggregated data points from contiguous intervals. | We recommend setting the detector interval based on your actual data. If it’s too long it might delay the results, and if it’s too short it might miss some data. It also won’t have a sufficient number of consecutive data points for the shingle process. | . | (Optional) To add extra processing time for data collection, specify a Window delay value. | This value tells the detector that the data is not ingested into OpenSearch in real time but with a certain delay. Set the window delay to shift the detector interval to account for this delay. | For example, say the detector interval is 10 minutes and data is ingested into your cluster with a general delay of 1 minute. Assume the detector runs at 2:00. The detector attempts to get the last 10 minutes of data from 1:50 to 2:00, but because of the 1-minute delay, it only gets 9 minutes of data and misses the data from 1:59 to 2:00. Setting the window delay to 1 minute shifts the interval window to 1:49 - 1:59, so the detector accounts for all 10 minutes of the detector interval time. | . | . | Specify custom result index. | If you want to store the anomaly detection results in your own index, choose Enable custom result index and specify the custom index to store the result. The anomaly detection plugin adds an opensearch-ad-plugin-result- prefix to the index name that you input. For example, if you input abc as the result index name, the final index name is opensearch-ad-plugin-result-abc. | . You can use the dash “-” sign to separate the namespace to manage custom result index permissions. For example, if you use opensearch-ad-plugin-result-financial-us-group1 as the result index, you can create a permission role based on the pattern opensearch-ad-plugin-result-financial-us-* to represent the “financial” department at a granular level for the “us” area. | If the custom index you specify doesn’t already exist, the anomaly detection plugin creates this index when you create the detector and start your real-time or historical analysis. | If the custom index already exists, the plugin checks if the index mapping of the custom index matches the anomaly result file. You need to make sure the custom index has valid mapping as shown here: anomaly-results.json. | To use the custom result index option, you need the following permissions: . | indices:admin/create - If the custom index already exists, you don’t need this. | indices:data/write/index - You need the write permission for the anomaly detection plugin to write results into the custom index for a single-entity detector. | indices:data/read/search - You need the search permission because the anomaly detection plugin needs to search custom result indices to show results on the anomaly detection UI. | indices:data/write/delete - Because the detector might generate a large number of anomaly results, you need the delete permission to delete old data and save disk space. | indices:data/write/bulk* - You need the bulk* permission because the anomaly detection plugin uses the bulk API to write results into the custom index. | . | Managing the custom result index: . | The anomaly detection dashboard queries all detectors’ results from all custom result indices. Having too many custom result indices might impact the performance of the anomaly detection plugin. | You can use Index State Management to rollover old result indices. You can also manually delete or archive any old result indices. We recommend reusing a custom result index for multiple detectors. | . | . | Choose Next. | . After you define the detector, the next step is to configure the model. ",
    "url": "https://vagimeli.github.io/observing-your-data/ad/index/#step-1-define-a-detector",
    "relUrl": "/observing-your-data/ad/index/#step-1-define-a-detector"
  },"1169": {
    "doc": "Anomaly detection",
    "title": "Step 2: Configure the model",
    "content": "Add features to your detector . A feature is the field in your index that you want to check for anomalies. A detector can discover anomalies across one or more features. You must choose an aggregation method for each feature: average(), count(), sum(), min(), or max(). The aggregation method determines what constitutes an anomaly. For example, if you choose min(), the detector focuses on finding anomalies based on the minimum values of your feature. If you choose average(), the detector finds anomalies based on the average values of your feature. A multi-feature model correlates anomalies across all its features. The curse of dimensionality makes it less likely for multi-feature models to identify smaller anomalies as compared to a single-feature model. Adding more features might negatively impact the precision and recall of a model. A higher proportion of noise in your data might further amplify this negative impact. Selecting the optimal feature set is usually an iterative process. By default, the maximum number of features for a detector is 5. You can adjust this limit with the plugins.anomaly_detection.max_anomaly_features setting. To configure an anomaly detection model based on an aggregation method, follow these steps: . | On the Configure Model page, enter the Feature name and check Enable feature. | For Find anomalies based on, select Field Value. | For aggregation method, select either average(), count(), sum(), min(), or max(). | For Field, select from the available options. | . To configure an anomaly detection model based on a JSON aggregation query, follow these steps: . | On the Configure Model page, enter the Feature name and check Enable feature. | For Find anomalies based on, select Custom expression. You will see the JSON editor window open up. | Enter your JSON aggregation query in the editor. | . For acceptable JSON query syntax, see OpenSearch Query DSL . (Optional) Set category fields for high cardinality . You can categorize anomalies based on a keyword or IP field type. The category field categorizes or slices the source time series with a dimension like IP addresses, product IDs, country codes, and so on. This helps to see a granular view of anomalies within each entity of the category field to isolate and debug issues. To set a category field, choose Enable a category field and select a field. You can’t change the category fields after you create the detector. Only a certain number of unique entities are supported in the category field. Use the following equation to calculate the recommended total number of entities supported in a cluster: . (data nodes * heap size * anomaly detection maximum memory percentage) / (entity model size of a detector) . To get the entity model size of a detector, use the profile detector API. You can adjust the maximum memory percentage with the plugins.anomaly_detection.model_max_size_percent setting. This formula provides a good starting point, but make sure to test with a representative workload. For example, for a cluster with three data nodes, each with 8 GB of JVM heap size, a maximum memory percentage of 10% (default), and the entity model size of the detector as 1MB: the total number of unique entities supported is (8.096 * 10^9 * 0.1 / 1 MB ) * 3 = 2429. If the actual total number of unique entities higher than this number that you calculate (in this case: 2429), the anomaly detector makes its best effort to model the extra entities. The detector prioritizes entities that occur more often and are more recent. (Advanced settings) Set a shingle size . Set the number of aggregation intervals from your data stream to consider in a detection window. It’s best to choose this value based on your actual data to see which one leads to the best results for your use case. The anomaly detector expects the shingle size to be in the range of 1 and 60. The default shingle size is 8. We recommend that you don’t choose 1 unless you have two or more features. Smaller values might increase recall but also false positives. Larger values might be useful for ignoring noise in a signal. Preview sample anomalies . Preview sample anomalies and adjust the feature settings if needed. For sample previews, the anomaly detection plugin selects a small number of data samples—for example, one data point every 30 minutes—and uses interpolation to estimate the remaining data points to approximate the actual feature data. It loads this sample dataset into the detector. The detector uses this sample dataset to generate a sample preview of anomaly results. Examine the sample preview and use it to fine-tune your feature configurations (for example, enable or disable features) to get more accurate results. | Choose Preview sample anomalies. | If you don’t see any sample anomaly result, check the detector interval and make sure you have more than 400 data points for some entities during the preview date range. | . | Choose Next. | . ",
    "url": "https://vagimeli.github.io/observing-your-data/ad/index/#step-2-configure-the-model",
    "relUrl": "/observing-your-data/ad/index/#step-2-configure-the-model"
  },"1170": {
    "doc": "Anomaly detection",
    "title": "Step 3: Set up detector jobs",
    "content": "To start a real-time detector to find anomalies in your data in near real-time, check Start real-time detector automatically (recommended). Alternatively, if you want to perform historical analysis and find patterns in long historical data windows (weeks or months), check Run historical analysis detection and select a date range (at least 128 detection intervals). Analyzing historical data helps you get familiar with the anomaly detection plugin. You can also evaluate the performance of a detector with historical data to further fine-tune it. We recommend experimenting with historical analysis with different feature sets and checking the precision before moving on to real-time detectors. ",
    "url": "https://vagimeli.github.io/observing-your-data/ad/index/#step-3-set-up-detector-jobs",
    "relUrl": "/observing-your-data/ad/index/#step-3-set-up-detector-jobs"
  },"1171": {
    "doc": "Anomaly detection",
    "title": "Step 4: Review and create",
    "content": "Review your detector settings and model configurations to make sure that they’re valid and then select Create detector. If you see any validation errors, edit the settings to fix the errors and then return back to this page. ",
    "url": "https://vagimeli.github.io/observing-your-data/ad/index/#step-4-review-and-create",
    "relUrl": "/observing-your-data/ad/index/#step-4-review-and-create"
  },"1172": {
    "doc": "Anomaly detection",
    "title": "Step 5: Observe the results",
    "content": "Choose the Real-time results or Historical analysis tab. For real-time results, you need to wait for some time to see the anomaly results. If the detector interval is 10 minutes, the detector might take more than an hour to start, because its waiting for sufficient data to generate anomalies. A shorter interval means the model passes the shingle process more quickly and starts to generate the anomaly results sooner. Use the profile detector operation to make sure you have sufficient data points. If you see the detector pending in “initialization” for longer than a day, aggregate your existing data using the detector interval to check for any missing data points. If you find a lot of missing data points from the aggregated data, consider increasing the detector interval. Choose and drag over the anomaly line chart to zoom in and see a more detailed view of an anomaly. Analyze anomalies with the following visualizations: . | Live anomalies (for real-time results) displays live anomaly results for the last 60 intervals. For example, if the interval is 10, it shows results for the last 600 minutes. The chart refreshes every 30 seconds. | Anomaly overview (for real-time results) / Anomaly history (for historical analysis in the Historical analysis tab) plots the anomaly grade with the corresponding measure of confidence. This pane includes: . | The number of anomaly occurrences based on the given data-time range. | The Average anomaly grade, a number between 0 and 1 that indicates how anomalous a data point is. An anomaly grade of 0 represents “not an anomaly,” and a non-zero value represents the relative severity of the anomaly. | Confidence estimate of the probability that the reported anomaly grade matches the expected anomaly grade. Confidence increases as the model observes more data and learns the data behavior and trends. Note that confidence is distinct from model accuracy. | Last anomaly occurrence is the time at which the last anomaly occurred. | . | . Underneath Anomaly overview/Anomaly history are: . | Feature breakdown plots the features based on the aggregation method. You can vary the date-time range of the detector. Selecting a point on the feature line chart shows the Feature output, the number of times a field appears in your index, and the Expected value, a predicted value for the feature output. Where there is no anomaly, the output and expected values are equal. | Anomaly occurrences shows the Start time, End time, Data confidence, and Anomaly grade for each detected anomaly. | . Selecting a point on the anomaly line chart shows Feature Contribution, the percentage of a feature that contributes to the anomaly . If you set the category field, you see an additional Heat map chart. The heat map correlates results for anomalous entities. This chart is empty until you select an anomalous entity. You also see the anomaly and feature line chart for the time period of the anomaly (anomaly_grade &gt; 0). If you have set multiple category fields, you can select a subset of fields to filter and sort the fields by. Selecting a subset of fields lets you see the top values of one field that share a common value with another field. For example, if you have a detector with the category fields ip and endpoint, you can select endpoint in the View by dropdown menu. Then, select a specific cell to overlay the top 20 values of ip on the charts. The anomaly detection plugin selects the top ip by default. You can see a maximum of 5 individual time-series values at the same time. ",
    "url": "https://vagimeli.github.io/observing-your-data/ad/index/#step-5-observe-the-results",
    "relUrl": "/observing-your-data/ad/index/#step-5-observe-the-results"
  },"1173": {
    "doc": "Anomaly detection",
    "title": "Step 6: Set up alerts",
    "content": "Under Real-time results, choose Set up alerts and configure a monitor to notify you when anomalies are detected. For steps to create a monitor and set up notifications based on your anomaly detector, see Monitors. If you stop or delete a detector, make sure to delete any monitors associated with it. ",
    "url": "https://vagimeli.github.io/observing-your-data/ad/index/#step-6-set-up-alerts",
    "relUrl": "/observing-your-data/ad/index/#step-6-set-up-alerts"
  },"1174": {
    "doc": "Anomaly detection",
    "title": "Step 7: Adjust the model",
    "content": "To see all the configuration settings for a detector, choose the Detector configuration tab. | To make any changes to the detector configuration, or fine tune the time interval to minimize any false positives, go to the Detector configuration section and choose Edit. | You need to stop real-time and historical analysis to change its configuration. Confirm that you want to stop the detector and proceed. | . | To enable or disable features, in the Features section, choose Edit and adjust the feature settings as needed. After you make your changes, choose Save and start detector. | . ",
    "url": "https://vagimeli.github.io/observing-your-data/ad/index/#step-7-adjust-the-model",
    "relUrl": "/observing-your-data/ad/index/#step-7-adjust-the-model"
  },"1175": {
    "doc": "Anomaly detection",
    "title": "Step 8: Manage your detectors",
    "content": "To start, stop, or delete a detector, go to the Detectors page. | Choose the detector name. | Choose Actions and select Start real-time detectors, Stop real-time detectors, or Delete detectors. | . ",
    "url": "https://vagimeli.github.io/observing-your-data/ad/index/#step-8-manage-your-detectors",
    "relUrl": "/observing-your-data/ad/index/#step-8-manage-your-detectors"
  },"1176": {
    "doc": "Anomaly result mapping",
    "title": "Anomaly result mapping",
    "content": "If you enabled custom result index, the anomaly detection plugin stores the results in your own index. If the anomaly detector doesn’t detect an anomaly, the result has the following format: . { \"detector_id\": \"kzcZ43wBgEQAbjDnhzGF\", \"schema_version\": 5, \"data_start_time\": 1635898161367, \"data_end_time\": 1635898221367, \"feature_data\": [ { \"feature_id\": \"processing_bytes_max\", \"feature_name\": \"processing bytes max\", \"data\": 2322 }, { \"feature_id\": \"processing_bytes_avg\", \"feature_name\": \"processing bytes avg\", \"data\": 1718.6666666666667 }, { \"feature_id\": \"processing_bytes_min\", \"feature_name\": \"processing bytes min\", \"data\": 1375 }, { \"feature_id\": \"processing_bytes_sum\", \"feature_name\": \"processing bytes sum\", \"data\": 5156 }, { \"feature_id\": \"processing_time_max\", \"feature_name\": \"processing time max\", \"data\": 31198 } ], \"execution_start_time\": 1635898231577, \"execution_end_time\": 1635898231622, \"anomaly_score\": 1.8124904404395776, \"anomaly_grade\": 0, \"confidence\": 0.9802940756605277, \"entity\": [ { \"name\": \"process_name\", \"value\": \"process_3\" } ], \"model_id\": \"kzcZ43wBgEQAbjDnhzGF_entity_process_3\", \"threshold\": 1.2368549346675202 } . ",
    "url": "https://vagimeli.github.io/observing-your-data/ad/result-mapping/",
    "relUrl": "/observing-your-data/ad/result-mapping/"
  },"1177": {
    "doc": "Anomaly result mapping",
    "title": "Response body fields",
    "content": "| Field | Description | . | detector_id | A unique ID for identifying a detector. | . | schema_version | The mapping version of the result index. | . | data_start_time | The start of the detection range of the aggregated data. | . | data_end_time | The end of the detection range of the aggregated data. | . | feature_data | An array of the aggregated data points between the data_start_time and data_end_time. | . | execution_start_time | The actual start time of the detector for a specific run that produces the anomaly result. This start time includes the window delay parameter that you can set to delay data collection. Window delay is the difference between the execution_start_time and data_start_time. | . | execution_end_time | The actual end time of the detector for a specific run that produces the anomaly result. | . | anomaly_score | Indicates relative severity of an anomaly. The higher the score, the more anomalous a data point is. | . | anomaly_grade | A normalized version of the anomaly_score on a scale between 0 and 1. | . | confidence | The probability of the accuracy of the anomaly_score. The closer this number is to 1, the higher the accuracy. During the probation period of a running detector, the confidence is low (&lt; 0.9) because of its exposure to limited data. | . | entity | An entity is a combination of specific category fields’ values. It includes the name and value of the category field. In the previous example, process_name is the category field and one of the processes such as process_3 is the field’s value. The entity field is only present for a high-cardinality detector (where you’ve selected a category field). | . | model_id | A unique ID that identifies a model. If a detector is a single-stream detector (with no category field), it has only one model. If a detector is a high-cardinality detector (with one or more category fields), it might have multiple models, one for each entity. | . | threshold | One of the criteria for a detector to classify a data point as an anomaly is that its anomaly_score must surpass a dynamic threshold. This field records the current threshold. | . If an anomaly detector detects an anomaly, the result has the following format: . { \"detector_id\": \"fylE53wBc9MCt6q12tKp\", \"schema_version\": 0, \"data_start_time\": 1635927900000, \"data_end_time\": 1635927960000, \"feature_data\": [ { \"feature_id\": \"processing_bytes_max\", \"feature_name\": \"processing bytes max\", \"data\": 2291 }, { \"feature_id\": \"processing_bytes_avg\", \"feature_name\": \"processing bytes avg\", \"data\": 1677.3333333333333 }, { \"feature_id\": \"processing_bytes_min\", \"feature_name\": \"processing bytes min\", \"data\": 1054 }, { \"feature_id\": \"processing_bytes_sum\", \"feature_name\": \"processing bytes sum\", \"data\": 5032 }, { \"feature_id\": \"processing_time_max\", \"feature_name\": \"processing time max\", \"data\": 11422 } ], \"anomaly_score\": 1.1986675882872033, \"anomaly_grade\": 0.26806225550178464, \"confidence\": 0.9607519742565531, \"entity\": [ { \"name\": \"process_name\", \"value\": \"process_3\" } ], \"approx_anomaly_start_time\": 1635927900000, \"relevant_attribution\": [ { \"feature_id\": \"processing_bytes_max\", \"data\": 0.03628638020431366 }, { \"feature_id\": \"processing_bytes_avg\", \"data\": 0.03384479053991436 }, { \"feature_id\": \"processing_bytes_min\", \"data\": 0.058812549572819096 }, { \"feature_id\": \"processing_bytes_sum\", \"data\": 0.10154576265526988 }, { \"feature_id\": \"processing_time_max\", \"data\": 0.7695105170276828 } ], \"expected_values\": [ { \"likelihood\": 1, \"value_list\": [ { \"feature_id\": \"processing_bytes_max\", \"data\": 2291 }, { \"feature_id\": \"processing_bytes_avg\", \"data\": 1677.3333333333333 }, { \"feature_id\": \"processing_bytes_min\", \"data\": 1054 }, { \"feature_id\": \"processing_bytes_sum\", \"data\": 6062 }, { \"feature_id\": \"processing_time_max\", \"data\": 23379 } ] } ], \"threshold\": 1.0993584705913992, \"execution_end_time\": 1635898427895, \"execution_start_time\": 1635898427803 } . You can see the following additional fields: . | Field | Description | . | relevant_attribution | Represents the contribution of each input variable. The sum of the attributions is normalized to 1. | . | expected_values | The expected value for each feature. | . At times, the detector might detect an anomaly late. Let’s say the detector sees a random mix of the triples {1, 2, 3} and {2, 4, 5} that correspond to slow weeks and busy weeks, respectively. For example 1, 2, 3, 1, 2, 3, 2, 4, 5, 1, 2, 3, 2, 4, 5, … and so on. If the detector comes across a pattern {2, 2, X} and it’s yet to see X, the detector infers that the pattern is anomalous, but it can’t determine at this point which of the 2’s is the cause. If X = 3, then the detector knows it’s the first 2 in that unfinished triple, and if X = 5, then it’s the second 2. If it’s the first 2, then the detector detects the anomaly late. If a detector detects an anomaly late, the result has the following additional fields: . | Field | Description | . | past_values | The actual input that triggered an anomaly. If past_values is null, the attributions or expected values are from the current input. If past_values is not null, the attributions or expected values are from a past input (for example, the previous two steps of the data [1,2,3]). | . | approx_anomaly_start_time | The approximate time of the actual input that triggers an anomaly. This field helps you understand when a detector flags an anomaly. Both single-stream and high-cardinality detectors don’t query previous anomaly results because these queries are expensive operations. The cost is especially high for high-cardinality detectors that might have a lot of entities. If the data is not continuous, the accuracy of this field is low and the actual time that the detector detects an anomaly can be earlier. | . { \"detector_id\": \"kzcZ43wBgEQAbjDnhzGF\", \"confidence\": 0.9746820962328963, \"relevant_attribution\": [ { \"feature_id\": \"deny_max1\", \"data\": 0.07339452532666227 }, { \"feature_id\": \"deny_avg\", \"data\": 0.04934972719948845 }, { \"feature_id\": \"deny_min\", \"data\": 0.01803003656061806 }, { \"feature_id\": \"deny_sum\", \"data\": 0.14804918212089874 }, { \"feature_id\": \"accept_max5\", \"data\": 0.7111765287923325 } ], \"task_id\": \"9Dck43wBgEQAbjDn4zEe\", \"threshold\": 1, \"model_id\": \"kzcZ43wBgEQAbjDnhzGF_entity_app_0\", \"schema_version\": 5, \"anomaly_score\": 1.141419389056506, \"execution_start_time\": 1635898427803, \"past_values\": [ { \"feature_id\": \"processing_bytes_max\", \"data\": 905 }, { \"feature_id\": \"processing_bytes_avg\", \"data\": 479 }, { \"feature_id\": \"processing_bytes_min\", \"data\": 128 }, { \"feature_id\": \"processing_bytes_sum\", \"data\": 1437 }, { \"feature_id\": \"processing_time_max\", \"data\": 8440 } ], \"data_end_time\": 1635883920000, \"data_start_time\": 1635883860000, \"feature_data\": [ { \"feature_id\": \"processing_bytes_max\", \"feature_name\": \"processing bytes max\", \"data\": 1360 }, { \"feature_id\": \"processing_bytes_avg\", \"feature_name\": \"processing bytes avg\", \"data\": 990 }, { \"feature_id\": \"processing_bytes_min\", \"feature_name\": \"processing bytes min\", \"data\": 608 }, { \"feature_id\": \"processing_bytes_sum\", \"feature_name\": \"processing bytes sum\", \"data\": 2970 }, { \"feature_id\": \"processing_time_max\", \"feature_name\": \"processing time max\", \"data\": 9670 } ], \"expected_values\": [ { \"likelihood\": 1, \"value_list\": [ { \"feature_id\": \"processing_bytes_max\", \"data\": 905 }, { \"feature_id\": \"processing_bytes_avg\", \"data\": 479 }, { \"feature_id\": \"processing_bytes_min\", \"data\": 128 }, { \"feature_id\": \"processing_bytes_sum\", \"data\": 4847 }, { \"feature_id\": \"processing_time_max\", \"data\": 15713 } ] } ], \"execution_end_time\": 1635898427895, \"anomaly_grade\": 0.5514172746375128, \"entity\": [ { \"name\": \"process_name\", \"value\": \"process_3\" } ], \"approx_anomaly_start_time\": 1635883620000 } . ",
    "url": "https://vagimeli.github.io/observing-your-data/ad/result-mapping/#response-body-fields",
    "relUrl": "/observing-your-data/ad/result-mapping/#response-body-fields"
  },"1178": {
    "doc": "Anomaly detection security",
    "title": "Anomaly detection security",
    "content": "You can use the Security plugin with anomaly detection in OpenSearch to limit non-admin users to specific actions. For example, you might want some users to only be able to create, update, or delete detectors, while others to only view detectors. All anomaly detection indices are protected as system indices. Only a super admin user or an admin user with a TLS certificate can access system indices. For more information, see System indices. Security for anomaly detection works the same as security for alerting. ",
    "url": "https://vagimeli.github.io/observing-your-data/ad/security/",
    "relUrl": "/observing-your-data/ad/security/"
  },"1179": {
    "doc": "Anomaly detection security",
    "title": "Basic permissions",
    "content": "As an admin user, you can use the Security plugin to assign specific permissions to users based on which APIs they need access to. For a list of supported APIs, see Anomaly detection API. The Security plugin has two built-in roles that cover most anomaly detection use cases: anomaly_full_access and anomaly_read_access. For descriptions of each, see Predefined roles. If these roles don’t meet your needs, mix and match individual anomaly detection permissions to suit your use case. Each action corresponds to an operation in the REST API. For example, the cluster:admin/opensearch/ad/detector/delete permission lets you delete detectors. A note on alerts and fine-grained access control . When a trigger generates an alert, the detector and monitor configurations, the alert itself, and any notification that is sent to a channel may include metadata describing the index being queried. By design, the plugin must extract the data and store it as metadata outside of the index. Document-level security (DLS) and field-level security (FLS) access controls are designed to protect the data in the index. But once the data is stored outside the index as metadata, users with access to the detector and monitor configurations, alerts, and their notifications will be able to view this metadata and possibly infer the contents and quality of data in the index, which would otherwise be concealed by DLS and FLS access control. To reduce the chances of unintended users viewing metadata that could describe an index, we recommend that administrators enable role-based access control and keep these kinds of design elements in mind when assigning permissions to the intended group of users. See Limit access by backend role for details. ",
    "url": "https://vagimeli.github.io/observing-your-data/ad/security/#basic-permissions",
    "relUrl": "/observing-your-data/ad/security/#basic-permissions"
  },"1180": {
    "doc": "Anomaly detection security",
    "title": "(Advanced) Limit access by backend role",
    "content": "Use backend roles to configure fine-grained access to individual detectors based on roles. For example, users of different departments in an organization can view detectors owned by their own department. First, make sure your users have the appropriate backend roles. Backend roles usually come from an LDAP server or SAML provider, but if you use the internal user database, you can use the REST API to add them manually. Next, enable the following setting: . PUT _cluster/settings { \"transient\": { \"plugins.anomaly_detection.filter_by_backend_roles\": \"true\" } } . Now when users view anomaly detection resources in OpenSearch Dashboards (or make REST API calls), they only see detectors created by users who share at least one backend role. For example, consider two users: alice and bob. alice has an analyst backend role: . PUT _plugins/_security/api/internalusers/alice { \"password\": \"alice\", \"backend_roles\": [ \"analyst\" ], \"attributes\": {} } . bob has a human-resources backend role: . PUT _plugins/_security/api/internalusers/bob { \"password\": \"bob\", \"backend_roles\": [ \"human-resources\" ], \"attributes\": {} } . Both alice and bob have full access to anomaly detection: . PUT _plugins/_security/api/rolesmapping/anomaly_full_access { \"backend_roles\": [], \"hosts\": [], \"users\": [ \"alice\", \"bob\" ] } . Because they have different backend roles, alice and bob cannot view each other’s detectors or their results. ",
    "url": "https://vagimeli.github.io/observing-your-data/ad/security/#advanced-limit-access-by-backend-role",
    "relUrl": "/observing-your-data/ad/security/#advanced-limit-access-by-backend-role"
  },"1181": {
    "doc": "Settings",
    "title": "Settings",
    "content": "The anomaly detection plugin adds several settings to the standard OpenSearch cluster settings. The settings are dynamic, so you can change the default behavior of the plugin without restarting your cluster. You can mark settings as persistent or transient. For example, to update the retention period of the result index: . PUT _cluster/settings { \"transient\": { \"plugins.anomaly_detection.ad_result_history_retention_period\": \"5m\" } } . | Setting | Default | Description | . | plugins.anomaly_detection.enabled | True | Whether the anomaly detection plugin is enabled or not. If disabled, all detectors immediately stop running. | . | plugins.anomaly_detection.max_anomaly_detectors | 1,000 | The maximum number of non-high cardinality detectors (no category field) users can create. | . | plugins.anomaly_detection.max_multi_entity_anomaly_detectors | 10 | The maximum number of high cardinality detectors (with category field) in a cluster. | . | plugins.anomaly_detection.max_anomaly_features | 5 | The maximum number of features for a detector. | . | plugins.anomaly_detection.ad_result_history_rollover_period | 12h | How often the rollover condition is checked. If true, the anomaly detection plugin rolls over the result index to a new index. | . | plugins.anomaly_detection.ad_result_history_max_docs_per_shard | 1,350,000,000 | The maximum number of documents in a single shard of the result index. The anomaly detection plugin only counts the refreshed documents in the primary shards. | . | plugins.anomaly_detection.max_entities_per_query | 1,000,000 | The maximum unique values per detection interval for high cardinality detectors. By default, if the category field(s) have more than the configured unique values in a detector interval, the anomaly detection plugin orders them by the natural ordering of categorical values (for example, entity ab comes before bc) and then selects the top values. | . | plugins.anomaly_detection.max_entities_for_preview | 5 | The maximum unique category field values displayed with the preview operation for high cardinality detectors. By default, if the category field(s) have more than the configured unique values in a detector interval, the anomaly detection plugin orders them by the natural ordering of categorical values (for example, entity ab comes before bc) and then selects the top values. | . | plugins.anomaly_detection.max_primary_shards | 10 | The maximum number of primary shards an anomaly detection index can have. | . | plugins.anomaly_detection.filter_by_backend_roles | False | When you enable the Security plugin and set this to true, the anomaly detection plugin filters results based on the user’s backend role(s). | . | plugins.anomaly_detection.max_batch_task_per_node | 10 | Starting a historical analysis triggers a batch task. This setting is the number of batch tasks that you can run per data node. You can tune this setting from 1 to 1,000. If the data nodes can’t support all batch tasks and you’re not sure if the data nodes are capable of running more historical analysis, add more data nodes instead of changing this setting to a higher value. Increasing this value might bring more load on each data node. | . | plugins.anomaly_detection.max_old_ad_task_docs_per_detector | 1 | You can run historical analysis for the same detector many times. For each run, the anomaly detection plugin creates a new task. This setting is the number of previous tasks the plugin keeps. Set this value to at least 1 to track its last run. You can keep a maximum of 1,000 old tasks to avoid overwhelming the cluster. | . | plugins.anomaly_detection.batch_task_piece_size | 1,000 | The date range for a historical task is split into smaller pieces and the anomaly detection plugin runs the task piece by piece. Each piece contains 1,000 detection intervals by default. For example, if detector interval is 1 minute and one piece is 1,000 minutes, the feature data is queried every 1,000 minutes. You can change this setting from 1 to 10,000. | . | plugins.anomaly_detection.batch_task_piece_interval_seconds | 5 | Add a time interval between two pieces of the same historical analysis task. This interval prevents the task from consuming too much of the available resources and starving other operations like search and bulk index. You can change this setting from 1 to 600 seconds. | . | plugins.anomaly_detection.max_top_entities_for_historical_analysis | 1,000 | The maximum number of top entities that you run for a high cardinality detector historical analysis. The range is from 1 to 10,000. | . | plugins.anomaly_detection.max_running_entities_per_detector_for_historical_analysis | 10 | The number of entity tasks that you can run in parallel for a high cardinality detector analysis. The task slots available on your cluster also impact how many entities run in parallel. If a cluster has 3 data nodes, each data node has 10 task slots by default. Say you already have two high cardinality detectors and each of them run 10 entities. If you start a single-entity detector that takes 1 task slot, the number of task slots available is 10 * 3 - 10 * 2 - 1 = 9. If you now start a new high cardinality detector, the detector can only run 9 entities in parallel and not 10. You can tune this value from 1 to 1,000 based on your cluster’s capability. If you set a higher value, the anomaly detection plugin runs historical analysis faster but also consumes more resources. | . | plugins.anomaly_detection.max_cached_deleted_tasks | 1,000 | You can rerun historical analysis for a single detector as many times as you like. The anomaly detection plugin only keeps a limited number of old tasks, by default 1 old task. If you run historical analysis three times for a detector, the oldest task is deleted. Because historical analysis generates a number of anomaly results in a short span of time, it’s necessary to clean up anomaly results for a deleted task. With this field, you can configure how many deleted tasks you can cache at most. The plugin cleans up a task’s results when it’s deleted. If the plugin fails to do this cleanup, it adds the task’s results into a cache and an hourly cron job performs the cleanup. You can use this setting to limit how many old tasks are put into cache to avoid a DDoS attack. After an hour, if still you find an old task result in the cache, use the delete detector results API to delete the task result manually. You can tune this setting from 1 to 10,000. | . | plugins.anomaly_detection.delete_anomaly_result_when_delete_detector | False | Whether the anomaly detection plugin deletes the anomaly result when you delete a detector. If you want to save some disk space, especially if you’ve high cardinality detectors generating a lot of results, set this field to true. Alternatively, you can use the delete detector results API to manually delete the results. | . | plugins.anomaly_detection.dedicated_cache_size | 10 | If the real-time analysis of a high cardinality detector starts successfully, the anomaly detection plugin guarantees keeping 10 (dynamically adjustable via this setting) entities’ models in memory per node. If the number of entities exceeds this limit, the plugin puts the extra entities’ models in a memory space shared by all detectors. The actual number of entities varies based on the memory that you’ve available and the frequencies of the entities. If you’d like the plugin to guarantee keeping more entities’ models in memory and if you’re cluster has sufficient memory, you can increase this setting value. | . | plugins.anomaly_detection.max_concurrent_preview | 2 | The maximum number of concurrent previews. You can use this setting to limit resource usage. | . | plugins.anomaly_detection.model_max_size_percent | 0.1 | The upper bound of the memory percentage for a model. | . | plugins.anomaly_detection.door_keeper_in_cache.enabled | False | When set to true, OpenSearch places a bloom filter in front of an inactive entity cache to filter out items that are not likely to appear more than once. | . | plugins.anomaly_detection.hcad_cold_start_interpolation.enabled | False | When set to true, enables interpolation in high-cardinality anomaly detection (HCAD) cold start. | . ",
    "url": "https://vagimeli.github.io/observing-your-data/ad/settings/",
    "relUrl": "/observing-your-data/ad/settings/"
  },"1182": {
    "doc": "API",
    "title": "Alerting API",
    "content": "Use the Alerting API to programmatically create, update, and manage monitors and alerts. . | Create a query-level monitor | Create a bucket-level monitor | Document-level monitors . | Search for monitor findings | Create a document-level monitor | Limitations | . | Update monitor | Get monitor | Monitor stats | Delete monitor | Search monitors | Run monitor | Get alerts | Acknowledge alert | Create destination | Update destination | Get destination | Get destinations | Delete destination | Create email account | Update email account | Get email account | Delete email account | Search email account | Create email group | Update email group | Get email group | Delete email group | Search email group | . ",
    "url": "https://vagimeli.github.io/observing-your-data/alerting/api/#alerting-api",
    "relUrl": "/observing-your-data/alerting/api/#alerting-api"
  },"1183": {
    "doc": "API",
    "title": "Create a query-level monitor",
    "content": "Introduced 1.0 . Query-level monitors run the query and check whether or not the results should trigger an alert. Query-level monitors can only trigger one alert at a time. For more information about query-level monitors and bucket-level monitors, see Create monitors. Sample Request . POST _plugins/_alerting/monitors { \"type\": \"monitor\", \"name\": \"test-monitor\", \"monitor_type\": \"query_level_monitor\", \"enabled\": true, \"schedule\": { \"period\": { \"interval\": 1, \"unit\": \"MINUTES\" } }, \"inputs\": [{ \"search\": { \"indices\": [\"movies\"], \"query\": { \"size\": 0, \"aggregations\": {}, \"query\": { \"bool\": { \"filter\": { \"range\": { \"@timestamp\": { \"gte\": \"{{period_end}}||-1h\", \"lte\": \"{{period_end}}\", \"format\": \"epoch_millis\" } } } } } } } }], \"triggers\": [{ \"name\": \"test-trigger\", \"severity\": \"1\", \"condition\": { \"script\": { \"source\": \"ctx.results[0].hits.total.value &gt; 0\", \"lang\": \"painless\" } }, \"actions\": [{ \"name\": \"test-action\", \"destination_id\": \"ld7912sBlQ5JUWWFThoW\", \"message_template\": { \"source\": \"This is my message body.\" }, \"throttle_enabled\": true, \"throttle\": { \"value\": 27, \"unit\": \"MINUTES\" }, \"subject_template\": { \"source\": \"TheSubject\" } }] }] } . If you use a custom webhook for your destination and need to embed JSON in the message body, be sure to escape your quotes: . { \"message_template\": { \"source\": \"{ \\\"text\\\": \\\"Monitor {{ctx.monitor.name}} just entered alert status. Please investigate the issue. - Trigger: {{ctx.trigger.name}} - Severity: {{ctx.trigger.severity}} - Period start: {{ctx.periodStart}} - Period end: {{ctx.periodEnd}}\\\" }\" } } . Optionally, to specify a backend role, you can add the rbac_roles parameter and backend role names to the bottom of your create monitor request. Example request . The following request creates a query-level monitor and provides two backend roles, role1 and role2. The section at the bottom of the request shows the line that specifies the roles with this syntax: \"rbac_roles\": [\"role1\", \"role2\"]. POST _plugins/_alerting/monitors { \"type\": \"monitor\", \"name\": \"test-monitor\", \"monitor_type\": \"query_level_monitor\", \"enabled\": true, \"schedule\": { \"period\": { \"interval\": 1, \"unit\": \"MINUTES\" } }, \"inputs\": [{ \"search\": { \"indices\": [\"movies\"], \"query\": { \"size\": 0, \"aggregations\": {}, \"query\": { \"bool\": { \"filter\": { \"range\": { \"@timestamp\": { \"gte\": \"||-1h\", \"lte\": \"\", \"format\": \"epoch_millis\" } } } } } } } }], \"triggers\": [{ \"name\": \"test-trigger\", \"severity\": \"1\", \"condition\": { \"script\": { \"source\": \"ctx.results[0].hits.total.value &gt; 0\", \"lang\": \"painless\" } }, \"actions\": [{ \"name\": \"test-action\", \"destination_id\": \"ld7912sBlQ5JUWWFThoW\", \"message_template\": { \"source\": \"This is my message body.\" }, \"throttle_enabled\": true, \"throttle\": { \"value\": 27, \"unit\": \"MINUTES\" }, \"subject_template\": { \"source\": \"TheSubject\" } }] }], \"rbac_roles\": [\"role1\", \"role2\"] } . To learn more about using backend roles to limit access, see (Advanced) Limit access by backend role. Example response . { \"_id\": \"vd5k2GsBlQ5JUWWFxhsP\", \"_version\": 1, \"_seq_no\": 7, \"_primary_term\": 1, \"monitor\": { \"type\": \"monitor\", \"schema_version\": 1, \"name\": \"test-monitor\", \"enabled\": true, \"enabled_time\": 1562703611363, \"schedule\": { \"period\": { \"interval\": 1, \"unit\": \"MINUTES\" } }, \"inputs\": [{ \"search\": { \"indices\": [ \"movies\" ], \"query\": { \"size\": 0, \"query\": { \"bool\": { \"filter\": [{ \"range\": { \"@timestamp\": { \"from\": \"{{period_end}}||-1h\", \"to\": \"{{period_end}}\", \"include_lower\": true, \"include_upper\": true, \"format\": \"epoch_millis\", \"boost\": 1 } } }], \"adjust_pure_negative\": true, \"boost\": 1 } }, \"aggregations\": {} } } }], \"triggers\": [{ \"id\": \"ud5k2GsBlQ5JUWWFxRvi\", \"name\": \"test-trigger\", \"severity\": \"1\", \"condition\": { \"script\": { \"source\": \"ctx.results[0].hits.total.value &gt; 0\", \"lang\": \"painless\" } }, \"actions\": [{ \"id\": \"ut5k2GsBlQ5JUWWFxRvj\", \"name\": \"test-action\", \"destination_id\": \"ld7912sBlQ5JUWWFThoW\", \"message_template\": { \"source\": \"This is my message body.\", \"lang\": \"mustache\" }, \"throttle_enabled\": false, \"subject_template\": { \"source\": \"Subject\", \"lang\": \"mustache\" } }] }], \"last_update_time\": 1562703611363 } } . If you want to specify a timezone, you can do so by including a cron expression with a timezone name in the schedule section of your request. The following example creates a monitor that runs at 12:10 PM Pacific Time on the 1st day of every month. Request . { \"type\": \"monitor\", \"name\": \"test-monitor\", \"monitor_type\": \"query_level_monitor\", \"enabled\": true, \"schedule\": { \"cron\" : { \"expression\": \"10 12 1 * *\", \"timezone\": \"America/Los_Angeles\" } }, \"inputs\": [{ \"search\": { \"indices\": [\"movies\"], \"query\": { \"size\": 0, \"aggregations\": {}, \"query\": { \"bool\": { \"filter\": { \"range\": { \"@timestamp\": { \"gte\": \"{{period_end}}||-1h\", \"lte\": \"{{period_end}}\", \"format\": \"epoch_millis\" } } } } } } } }], \"triggers\": [{ \"name\": \"test-trigger\", \"severity\": \"1\", \"condition\": { \"script\": { \"source\": \"ctx.results[0].hits.total.value &gt; 0\", \"lang\": \"painless\" } }, \"actions\": [{ \"name\": \"test-action\", \"destination_id\": \"ld7912sBlQ5JUWWFThoW\", \"message_template\": { \"source\": \"This is a message body.\" }, \"throttle_enabled\": true, \"throttle\": { \"value\": 27, \"unit\": \"MINUTES\" }, \"subject_template\": { \"source\": \"Subject\" } }] }] } . For a full list of timezone names, refer to Wikipedia. The alerting plugin uses the Java TimeZone class to convert a ZoneId to a valid timezone. ",
    "url": "https://vagimeli.github.io/observing-your-data/alerting/api/#create-a-query-level-monitor",
    "relUrl": "/observing-your-data/alerting/api/#create-a-query-level-monitor"
  },"1184": {
    "doc": "API",
    "title": "Create a bucket-level monitor",
    "content": "Bucket-level monitors categorize results into buckets separated by fields. The monitor then runs your script with each bucket’s results and evaluates whether to trigger an alert. For more information about bucket-level and query-level monitors, see Create monitors. POST _plugins/_alerting/monitors { \"type\": \"monitor\", \"name\": \"Demo bucket-level monitor\", \"monitor_type\": \"bucket_level_monitor\", \"enabled\": true, \"schedule\": { \"period\": { \"interval\": 1, \"unit\": \"MINUTES\" } }, \"inputs\": [ { \"search\": { \"indices\": [ \"movies\" ], \"query\": { \"size\": 0, \"query\": { \"bool\": { \"filter\": [ { \"range\": { \"order_date\": { \"from\": \"{{period_end}}||-1h\", \"to\": \"{{period_end}}\", \"include_lower\": true, \"include_upper\": true, \"format\": \"epoch_millis\" } } } ] } }, \"aggregations\": { \"composite_agg\": { \"composite\": { \"sources\": [ { \"user\": { \"terms\": { \"field\": \"user\" } } } ] }, \"aggregations\": { \"avg_products_base_price\": { \"avg\": { \"field\": \"products.base_price\" } } } } } } } } ], \"triggers\": [ { \"bucket_level_trigger\": { \"name\": \"test-trigger\", \"severity\": \"1\", \"condition\": { \"buckets_path\": { \"_count\": \"_count\", \"avg_products_base_price\": \"avg_products_base_price\" }, \"parent_bucket_path\": \"composite_agg\", \"script\": { \"source\": \"params._count &gt; 50 || params.avg_products_base_price &lt; 35\", \"lang\": \"painless\" } }, \"actions\": [ { \"name\": \"test-action\", \"destination_id\": \"E4o5hnsB6KjPKmHtpfCA\", \"message_template\": { \"source\": \"\"\"Monitor {{ctx.monitor.name}} just entered alert status. Please investigate the issue. - Trigger: {{ctx.trigger.name}} - Severity: {{ctx.trigger.severity}} - Period start: {{ctx.periodStart}} - Period end: {{ctx.periodEnd}} - Deduped Alerts: {{ctx.dedupedAlerts}} * {{id}} : {{bucket_keys}} {{ctx.dedupedAlerts}} - New Alerts: {{ctx.newAlerts}} * {{id}} : {{bucket_keys}} {{ctx.newAlerts}} - Completed Alerts: {{ctx.completedAlerts}} * {{id}} : {{bucket_keys}} {{ctx.completedAlerts}}\"\"\", \"lang\": \"mustache\" }, \"throttle_enabled\": false, \"throttle\": { \"value\": 10, \"unit\": \"MINUTES\" }, \"action_execution_policy\": { \"action_execution_scope\": { \"per_alert\": { \"actionable_alerts\": [ \"DEDUPED\", \"NEW\" ] } } }, \"subject_template\": { \"source\": \"The Subject\", \"lang\": \"mustache\" } } ] } } ] } . Example response . { \"_id\" : \"Dfxr63sBwex6DxEhHV5N\", \"_version\" : 1, \"_seq_no\" : 3, \"_primary_term\" : 1, \"monitor\" : { \"type\" : \"monitor\", \"schema_version\" : 4, \"name\" : \"Demo a bucket-level monitor\", \"monitor_type\" : \"bucket_level_monitor\", \"user\" : { \"name\" : \"\", \"backend_roles\" : [ ], \"roles\" : [ ], \"custom_attribute_names\" : [ ], \"user_requested_tenant\" : null }, \"enabled\" : true, \"enabled_time\" : 1631742270785, \"schedule\" : { \"period\" : { \"interval\" : 1, \"unit\" : \"MINUTES\" } }, \"inputs\" : [ { \"search\" : { \"indices\" : [ \"opensearch_dashboards_sample_data_flights\" ], \"query\" : { \"size\" : 0, \"query\" : { \"bool\" : { \"filter\" : [ { \"range\" : { \"order_date\" : { \"from\" : \"{{period_end}}||-1h\", \"to\" : \"{{period_end}}\", \"include_lower\" : true, \"include_upper\" : true, \"format\" : \"epoch_millis\", \"boost\" : 1.0 } } } ], \"adjust_pure_negative\" : true, \"boost\" : 1.0 } }, \"aggregations\" : { \"composite_agg\" : { \"composite\" : { \"size\" : 10, \"sources\" : [ { \"user\" : { \"terms\" : { \"field\" : \"user\", \"missing_bucket\" : false, \"order\" : \"asc\" } } } ] }, \"aggregations\" : { \"avg_products_base_price\" : { \"avg\" : { \"field\" : \"products.base_price\" } } } } } } } } ], \"triggers\" : [ { \"bucket_level_trigger\" : { \"id\" : \"C_xr63sBwex6DxEhHV5B\", \"name\" : \"test-trigger\", \"severity\" : \"1\", \"condition\" : { \"buckets_path\" : { \"_count\" : \"_count\", \"avg_products_base_price\" : \"avg_products_base_price\" }, \"parent_bucket_path\" : \"composite_agg\", \"script\" : { \"source\" : \"params._count &gt; 50 || params.avg_products_base_price &lt; 35\", \"lang\" : \"painless\" }, \"gap_policy\" : \"skip\" }, \"actions\" : [ { \"id\" : \"DPxr63sBwex6DxEhHV5B\", \"name\" : \"test-action\", \"destination_id\" : \"E4o5hnsB6KjPKmHtpfCA\", \"message_template\" : { \"source\" : \"Monitor {{ctx.monitor.name}} just entered alert status. Please investigate the issue. - Trigger: {{ctx.trigger.name}} - Severity: {{ctx.trigger.severity}} - Period start: {{ctx.periodStart}} - Period end: {{ctx.periodEnd}} - Deduped Alerts: {{ctx.dedupedAlerts}} * {{id}} : {{bucket_keys}} {{ctx.dedupedAlerts}} - New Alerts: {{ctx.newAlerts}} * {{id}} : {{bucket_keys}} {{ctx.newAlerts}} - Completed Alerts: {{ctx.completedAlerts}} * {{id}} : {{bucket_keys}} {{ctx.completedAlerts}}\", \"lang\" : \"mustache\" }, \"throttle_enabled\" : false, \"subject_template\" : { \"source\" : \"The Subject\", \"lang\" : \"mustache\" }, \"throttle\" : { \"value\" : 10, \"unit\" : \"MINUTES\" }, \"action_execution_policy\" : { \"action_execution_scope\" : { \"per_alert\" : { \"actionable_alerts\" : [ \"DEDUPED\", \"NEW\" ] } } } } ] } } ], \"last_update_time\" : 1631742270785 } } . ",
    "url": "https://vagimeli.github.io/observing-your-data/alerting/api/#create-a-bucket-level-monitor",
    "relUrl": "/observing-your-data/alerting/api/#create-a-bucket-level-monitor"
  },"1185": {
    "doc": "API",
    "title": "Document-level monitors",
    "content": "Introduced 2.0 . Document-level monitors check whether individual documents in an index match trigger conditions. If so, the monitor generates an alert notification. When you run a query with a document-level monitor, the results are returned for each document that matches the trigger condition. You can create trigger conditions based on query names, query IDs, or tags that combine multiple queries. To learn more about per document monitors that function similarly to the document-level monitor API, see Monitor types. Search for monitor findings . You can use the Alerting search API operation to search the findings index .opensearch-alerting-finding* for available document findings with a GET request. By default, a GET request without path parameters returns all available findings. To learn more about monitor findings, see Document findings. To retrieve any available findings, send a GET request without any path parameters as follows: . GET /_plugins/_alerting/findings/_search? . To retrieve metadata for an individual document finding entry, you can search for the finding by its findingId as follows: . GET /_plugins/_alerting/findings/_search?findingId=gKQhj8WJit3BxjGfiOXC . The response returns the number of individual finding entries in the total_findings field. To get more specific results in a findings search, you can use any of the optional path parameters that are defined in the following table: . | Path parameter | Description | Usage | . | findingId | The identifier for the finding entry. | The finding ID is returned in the initial query response. | . | sortString | This field specifies which string the Alerting plugin uses to sort the findings. | The default value is id. | . | sortOrder | The order to sort the list of findings, either ascending or descending. | Use sortOrder=asc to indicate ascending, or sortOrder=desc for descending sort order. | . | size | An optional limit for the maximum number of results returned in the response. | There is no minimum or maximum values. | . | startIndex | The pagination indicator. | Default is 0. | . | searchString | The finding attribute you want returned in the search. | To search in a specific index, specify the index name in the request path. For example, to search findings in the indexABC index, use `searchString=indexABC’. | . Create a document-level monitor . You can create a document-level monitor with a POST request that provides the monitor details in the request body. At a minimum, you need to provide the following details: specify the queries or combinations by tag with the inputs field, a valid trigger condition, and provide the notification message in the action field. The following table shows the syntax to use for each trigger option: . | Trigger options | Definition | Syntax | . | Tag | Creates alerts for documents that match a multiple query with this tag applied. If you group multiple queries by a single tag, then you can set it to trigger an alert if the results are returned by this tag name. | query[tag=&lt;tag-name&gt;] | . | Query by name | Creates alerts for documents matched or returned by the named query. | query[name=&lt;query-name&gt;] | . | Query by ID | Creates alerts for documents that were returned by the identified query. | query[id=&lt;query-id&gt;] | . Sample Request . The following sample shows how to create a document-level monitor: . POST _plugins/_alerting/monitors { \"type\": \"monitor\", \"monitor_type\": \"doc_level_monitor\", \"name\": \"Example document-level monitor\", \"enabled\": true, \"schedule\": { \"period\": { \"interval\": 1, \"unit\": \"MINUTES\" } }, \"inputs\": [ { \"doc_level_input\": { \"description\": \"Example document-level monitor for audit logs\", \"indices\": [ \"audit-logs\" ], \"queries\": [ { \"id\": \"nKQnFYABit3BxjGfiOXC\", \"name\": \"sigma-123\", \"query\": \"region:\\\"us-west-2\\\"\", \"tags\": [ \"tag1\" ] }, { \"id\": \"gKQnABEJit3BxjGfiOXC\", \"name\": \"sigma-456\", \"query\": \"region:\\\"us-east-1\\\"\", \"tags\": [ \"tag2\" ] }, { \"id\": \"h4J2ABEFNW3vxjGfiOXC\", \"name\": \"sigma-789\", \"query\": \"message:\\\"This is a SEPARATE error from IAD region\\\"\", \"tags\": [ \"tag3\" ] } ] } } ], \"triggers\": [ { \"document_level_trigger\": { \"name\": \"test-trigger\", \"severity\": \"1\", \"condition\": { \"script\": { \"source\": \"(query[name=sigma-123] || query[tag=tag3]) &amp;&amp; query[name=sigma-789]\", \"lang\": \"painless\" } }, \"actions\": [ { \"name\": \"test-action\", \"destination_id\": \"E4o5hnsB6KjPKmHtpfCA\", \"message_template\": { \"source\": \"\"\"Monitor just entered alert status. Please investigate the issue. Related Finding Ids: {{ctx.alerts.0.finding_ids}}, Related Document Ids: {{ctx.alerts.0.related_doc_ids}}\"\"\", \"lang\": \"mustache\" }, \"action_execution_policy\": { \"action_execution_scope\": { \"per_alert\": { \"actionable_alerts\": [] } } }, \"subject_template\": { \"source\": \"The Subject\", \"lang\": \"mustache\" } } ] }}] } . Limitations . If you run a document-level query while the index is getting reindexed, the API response will not return the reindexed results. To get updates, wait until the reindexing process completes, then rerun the query. ",
    "url": "https://vagimeli.github.io/observing-your-data/alerting/api/#document-level-monitors",
    "relUrl": "/observing-your-data/alerting/api/#document-level-monitors"
  },"1186": {
    "doc": "API",
    "title": "Update monitor",
    "content": "Introduced 1.0 . When updating a monitor, you can optionally include seq_no and primary_term as URL parameters. If these numbers don’t match the existing monitor or the monitor doesn’t exist, the alerting plugin throws an error. OpenSearch increments the version number and the sequence number automatically (see the example response). Request . PUT _plugins/_alerting/monitors/&lt;monitor_id&gt; { \"type\": \"monitor\", \"name\": \"test-monitor\", \"enabled\": true, \"enabled_time\": 1551466220455, \"schedule\": { \"period\": { \"interval\": 1, \"unit\": \"MINUTES\" } }, \"inputs\": [{ \"search\": { \"indices\": [ \"*\" ], \"query\": { \"query\": { \"match_all\": { \"boost\": 1 } } } } }], \"triggers\": [{ \"id\": \"StaeOmkBC25HCRGmL_y-\", \"name\": \"test-trigger\", \"severity\": \"1\", \"condition\": { \"script\": { \"source\": \"return true\", \"lang\": \"painless\" } }, \"actions\": [{ \"name\": \"test-action\", \"destination_id\": \"RtaaOmkBC25HCRGm0fxi\", \"subject_template\": { \"source\": \"My Message Subject\", \"lang\": \"mustache\" }, \"message_template\": { \"source\": \"This is my message body.\", \"lang\": \"mustache\" } }] }], \"last_update_time\": 1551466639295 } PUT _plugins/_alerting/monitors/&lt;monitor_id&gt;?if_seq_no=3&amp;if_primary_term=1 { \"type\": \"monitor\", \"name\": \"test-monitor\", \"enabled\": true, \"enabled_time\": 1551466220455, \"schedule\": { \"period\": { \"interval\": 1, \"unit\": \"MINUTES\" } }, \"inputs\": [{ \"search\": { \"indices\": [ \"*\" ], \"query\": { \"query\": { \"match_all\": { \"boost\": 1 } } } } }], \"triggers\": [{ \"id\": \"StaeOmkBC25HCRGmL_y-\", \"name\": \"test-trigger\", \"severity\": \"1\", \"condition\": { \"script\": { \"source\": \"return true\", \"lang\": \"painless\" } }, \"actions\": [{ \"name\": \"test-action\", \"destination_id\": \"RtaaOmkBC25HCRGm0fxi\", \"subject_template\": { \"source\": \"My Message Subject\", \"lang\": \"mustache\" }, \"message_template\": { \"source\": \"This is my message body.\", \"lang\": \"mustache\" } }] }], \"last_update_time\": 1551466639295 } . Example response . { \"_id\": \"Q9aXOmkBC25HCRGmzfw-\", \"_version\": 4, \"_seq_no\": 4, \"_primary_term\": 1, \"monitor\": { \"type\": \"monitor\", \"name\": \"test-monitor\", \"enabled\": true, \"enabled_time\": 1551466220455, \"schedule\": { \"period\": { \"interval\": 1, \"unit\": \"MINUTES\" } }, \"inputs\": [{ \"search\": { \"indices\": [ \"*\" ], \"query\": { \"query\": { \"match_all\": { \"boost\": 1 } } } } }], \"triggers\": [{ \"id\": \"StaeOmkBC25HCRGmL_y-\", \"name\": \"test-trigger\", \"severity\": \"1\", \"condition\": { \"script\": { \"source\": \"return true\", \"lang\": \"painless\" } }, \"actions\": [{ \"name\": \"test-action\", \"destination_id\": \"RtaaOmkBC25HCRGm0fxi\", \"subject_template\": { \"source\": \"My Message Subject\", \"lang\": \"mustache\" }, \"message_template\": { \"source\": \"This is my message body.\", \"lang\": \"mustache\" } }] }], \"last_update_time\": 1551466761596 } } . ",
    "url": "https://vagimeli.github.io/observing-your-data/alerting/api/#update-monitor",
    "relUrl": "/observing-your-data/alerting/api/#update-monitor"
  },"1187": {
    "doc": "API",
    "title": "Get monitor",
    "content": "Introduced 1.0 . Request . GET _plugins/_alerting/monitors/&lt;monitor_id&gt; . Example response . { \"_id\": \"Q9aXOmkBC25HCRGmzfw-\", \"_version\": 3, \"_seq_no\": 3, \"_primary_term\": 1, \"monitor\": { \"type\": \"monitor\", \"name\": \"test-monitor\", \"enabled\": true, \"enabled_time\": 1551466220455, \"schedule\": { \"period\": { \"interval\": 1, \"unit\": \"MINUTES\" } }, \"inputs\": [{ \"search\": { \"indices\": [ \"*\" ], \"query\": { \"query\": { \"match_all\": { \"boost\": 1 } } } } }], \"triggers\": [{ \"id\": \"StaeOmkBC25HCRGmL_y-\", \"name\": \"test-trigger\", \"severity\": \"1\", \"condition\": { \"script\": { \"source\": \"return true\", \"lang\": \"painless\" } }, \"actions\": [{ \"name\": \"test-action\", \"destination_id\": \"RtaaOmkBC25HCRGm0fxi\", \"subject_template\": { \"source\": \"My Message Subject\", \"lang\": \"mustache\" }, \"message_template\": { \"source\": \"This is my message body.\", \"lang\": \"mustache\" } }] }], \"last_update_time\": 1551466639295 } } . ",
    "url": "https://vagimeli.github.io/observing-your-data/alerting/api/#get-monitor",
    "relUrl": "/observing-your-data/alerting/api/#get-monitor"
  },"1188": {
    "doc": "API",
    "title": "Monitor stats",
    "content": "Introduced 1.0 . Returns statistics about the alerting feature. Use _plugins/_alerting/stats to find node IDs and metrics. Then you can drill down using those values. Request . GET _plugins/_alerting/stats GET _plugins/_alerting/stats/&lt;metric&gt; GET _plugins/_alerting/&lt;node-id&gt;/stats GET _plugins/_alerting/&lt;node-id&gt;/stats/&lt;metric&gt; . Example response . { \"_nodes\": { \"total\": 9, \"successful\": 9, \"failed\": 0 }, \"cluster_name\": \"475300751431:alerting65-dont-delete\", \"plugins.scheduled_jobs.enabled\": true, \"scheduled_job_index_exists\": true, \"scheduled_job_index_status\": \"green\", \"nodes_on_schedule\": 9, \"nodes_not_on_schedule\": 0, \"nodes\": { \"qWcbKbb-TVyyI-Q7VSeOqA\": { \"name\": \"qWcbKbb\", \"schedule_status\": \"green\", \"roles\": [ \"MASTER\" ], \"job_scheduling_metrics\": { \"last_full_sweep_time_millis\": 207017, \"full_sweep_on_time\": true }, \"jobs_info\": {} }, \"Do-DX9ZcS06Y9w1XbSJo1A\": { \"name\": \"Do-DX9Z\", \"schedule_status\": \"green\", \"roles\": [ \"DATA\", \"INGEST\" ], \"job_scheduling_metrics\": { \"last_full_sweep_time_millis\": 230516, \"full_sweep_on_time\": true }, \"jobs_info\": {} }, \"n5phkBiYQfS5I0FDzcqjZQ\": { \"name\": \"n5phkBi\", \"schedule_status\": \"green\", \"roles\": [ \"MASTER\" ], \"job_scheduling_metrics\": { \"last_full_sweep_time_millis\": 228406, \"full_sweep_on_time\": true }, \"jobs_info\": {} }, \"Tazzo8cQSY-g3vOjgYYLzA\": { \"name\": \"Tazzo8c\", \"schedule_status\": \"green\", \"roles\": [ \"DATA\", \"INGEST\" ], \"job_scheduling_metrics\": { \"last_full_sweep_time_millis\": 211722, \"full_sweep_on_time\": true }, \"jobs_info\": { \"i-wsFmkB8NzS6aXjQSk0\": { \"last_execution_time\": 1550864912882, \"running_on_time\": true } } }, \"Nyf7F8brTOSJuFPXw6CnpA\": { \"name\": \"Nyf7F8b\", \"schedule_status\": \"green\", \"roles\": [ \"DATA\", \"INGEST\" ], \"job_scheduling_metrics\": { \"last_full_sweep_time_millis\": 223300, \"full_sweep_on_time\": true }, \"jobs_info\": { \"NbpoFmkBeSe-hD59AKgE\": { \"last_execution_time\": 1550864928354, \"running_on_time\": true }, \"-LlLFmkBeSe-hD59Ydtb\": { \"last_execution_time\": 1550864732727, \"running_on_time\": true }, \"pBFxFmkBNXkgNmTBaFj1\": { \"last_execution_time\": 1550863325024, \"running_on_time\": true }, \"hfasEmkBNXkgNmTBrvIW\": { \"last_execution_time\": 1550862000001, \"running_on_time\": true } } }, \"oOdJDIBVT5qbbO3d8VLeEw\": { \"name\": \"oOdJDIB\", \"schedule_status\": \"green\", \"roles\": [ \"DATA\", \"INGEST\" ], \"job_scheduling_metrics\": { \"last_full_sweep_time_millis\": 227570, \"full_sweep_on_time\": true }, \"jobs_info\": { \"4hKRFmkBNXkgNmTBKjYX\": { \"last_execution_time\": 1550864806101, \"running_on_time\": true } } }, \"NRDG6JYgR8m0GOZYQ9QGjQ\": { \"name\": \"NRDG6JY\", \"schedule_status\": \"green\", \"roles\": [ \"MASTER\" ], \"job_scheduling_metrics\": { \"last_full_sweep_time_millis\": 227652, \"full_sweep_on_time\": true }, \"jobs_info\": {} }, \"URMrXRz3Tm-CB72hlsl93Q\": { \"name\": \"URMrXRz\", \"schedule_status\": \"green\", \"roles\": [ \"DATA\", \"INGEST\" ], \"job_scheduling_metrics\": { \"last_full_sweep_time_millis\": 231048, \"full_sweep_on_time\": true }, \"jobs_info\": { \"m7uKFmkBeSe-hD59jplP\": { \"running_on_time\": true } } }, \"eXgt1k9oTRCLmx2HBGElUw\": { \"name\": \"eXgt1k9\", \"schedule_status\": \"green\", \"roles\": [ \"DATA\", \"INGEST\" ], \"job_scheduling_metrics\": { \"last_full_sweep_time_millis\": 229234, \"full_sweep_on_time\": true }, \"jobs_info\": { \"wWkFFmkBc2NG-PeLntxk\": { \"running_on_time\": true }, \"3usNFmkB8NzS6aXjO1Gs\": { \"last_execution_time\": 1550863959848, \"running_on_time\": true } } } } } . ",
    "url": "https://vagimeli.github.io/observing-your-data/alerting/api/#monitor-stats",
    "relUrl": "/observing-your-data/alerting/api/#monitor-stats"
  },"1189": {
    "doc": "API",
    "title": "Delete monitor",
    "content": "Introduced 1.0 . Request . DELETE _plugins/_alerting/monitors/&lt;monitor_id&gt; . Example response . { \"_index\": \".opensearch-scheduled-jobs\", \"_id\": \"OYAHOmgBl3cmwnqZl_yH\", \"_version\": 2, \"result\": \"deleted\", \"forced_refresh\": true, \"_shards\": { \"total\": 2, \"successful\": 2, \"failed\": 0 }, \"_seq_no\": 11, \"_primary_term\": 1 } . ",
    "url": "https://vagimeli.github.io/observing-your-data/alerting/api/#delete-monitor",
    "relUrl": "/observing-your-data/alerting/api/#delete-monitor"
  },"1190": {
    "doc": "API",
    "title": "Search monitors",
    "content": "Introduced 1.0 . Request . GET _plugins/_alerting/monitors/_search { \"query\": { \"match\" : { \"monitor.name\": \"my-monitor-name\" } } } . Example response . { \"took\": 17, \"timed_out\": false, \"_shards\": { \"total\": 5, \"successful\": 5, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": 1, \"max_score\": 0.6931472, \"hits\": [{ \"_index\": \".opensearch-scheduled-jobs\", \"_type\": \"_doc\", \"_id\": \"eGQi7GcBRS7-AJEqfAnr\", \"_score\": 0.6931472, \"_source\": { \"type\": \"monitor\", \"name\": \"my-monitor-name\", \"enabled\": true, \"enabled_time\": 1545854942426, \"schedule\": { \"period\": { \"interval\": 1, \"unit\": \"MINUTES\" } }, \"inputs\": [{ \"search\": { \"indices\": [ \"*\" ], \"query\": { \"size\": 0, \"query\": { \"bool\": { \"filter\": [{ \"range\": { \"@timestamp\": { \"from\": \"{{period_end}}||-1h\", \"to\": \"{{period_end}}\", \"include_lower\": true, \"include_upper\": true, \"format\": \"epoch_millis\", \"boost\": 1 } } }], \"adjust_pure_negative\": true, \"boost\": 1 } }, \"aggregations\": {} } } }], \"triggers\": [{ \"id\": \"Sooi7GcB53a0ewuj_6MH\", \"name\": \"Over\", \"severity\": \"1\", \"condition\": { \"script\": { \"source\": \"_ctx.results[0].hits.total &gt; 400000\", \"lang\": \"painless\" } }, \"actions\": [] }], \"last_update_time\": 1545854975758 } }] } } . ",
    "url": "https://vagimeli.github.io/observing-your-data/alerting/api/#search-monitors",
    "relUrl": "/observing-your-data/alerting/api/#search-monitors"
  },"1191": {
    "doc": "API",
    "title": "Run monitor",
    "content": "Introduced 1.0 . You can add the optional ?dryrun=true parameter to the URL to show the results of a run without actions sending any message. Request . POST _plugins/_alerting/monitors/&lt;monitor_id&gt;/_execute . Example response . { \"monitor_name\": \"logs\", \"period_start\": 1547161872322, \"period_end\": 1547161932322, \"error\": null, \"trigger_results\": { \"Sooi7GcB53a0ewuj_6MH\": { \"name\": \"Over\", \"triggered\": true, \"error\": null, \"action_results\": {} } } } . ",
    "url": "https://vagimeli.github.io/observing-your-data/alerting/api/#run-monitor",
    "relUrl": "/observing-your-data/alerting/api/#run-monitor"
  },"1192": {
    "doc": "API",
    "title": "Get alerts",
    "content": "Introduced 1.0 . Returns an array of all alerts. Request . GET _plugins/_alerting/monitors/alerts . Response . { \"alerts\": [ { \"id\": \"eQURa3gBKo1jAh6qUo49\", \"version\": 300, \"monitor_id\": \"awUMa3gBKo1jAh6qu47E\", \"schema_version\": 2, \"monitor_version\": 2, \"monitor_name\": \"Example_monitor_name\", \"monitor_user\": { \"name\": \"admin\", \"backend_roles\": [ \"admin\" ], \"roles\": [ \"all_access\", \"own_index\" ], \"custom_attribute_names\": [], \"user_requested_tenant\": null }, \"trigger_id\": \"bQUQa3gBKo1jAh6qnY6G\", \"trigger_name\": \"Example_trigger_name\", \"state\": \"ACTIVE\", \"error_message\": null, \"alert_history\": [ { \"timestamp\": 1617314504873, \"message\": \"Example error message\" }, { \"timestamp\": 1617312543925, \"message\": \"Example error message\" } ], \"severity\": \"1\", \"action_execution_results\": [ { \"action_id\": \"bgUQa3gBKo1jAh6qnY6G\", \"last_execution_time\": 1617317979908, \"throttled_count\": 0 } ], \"start_time\": 1616704000492, \"last_notification_time\": 1617317979908, \"end_time\": null, \"acknowledged_time\": null } ], \"totalAlerts\": 1 } . ",
    "url": "https://vagimeli.github.io/observing-your-data/alerting/api/#get-alerts",
    "relUrl": "/observing-your-data/alerting/api/#get-alerts"
  },"1193": {
    "doc": "API",
    "title": "Acknowledge alert",
    "content": "Introduced 1.0 . After getting your alerts, you can acknowledge any number of active alerts in one call. If the alert is already in an ERROR, COMPLETED, or ACKNOWLEDGED state, it appears in the failed array. Request . POST _plugins/_alerting/monitors/&lt;monitor-id&gt;/_acknowledge/alerts { \"alerts\": [\"eQURa3gBKo1jAh6qUo49\"] } . Example response . { \"success\": [ \"eQURa3gBKo1jAh6qUo49\" ], \"failed\": [] } . ",
    "url": "https://vagimeli.github.io/observing-your-data/alerting/api/#acknowledge-alert",
    "relUrl": "/observing-your-data/alerting/api/#acknowledge-alert"
  },"1194": {
    "doc": "API",
    "title": "Create destination",
    "content": "Introduced 1.0 . Requests . POST _plugins/_alerting/destinations { \"name\": \"my-destination\", \"type\": \"slack\", \"slack\": { \"url\": \"http://www.example.com\" } } POST _plugins/_alerting/destinations { \"type\": \"custom_webhook\", \"name\": \"my-custom-destination\", \"custom_webhook\": { \"path\": \"incomingwebhooks/123456-123456-XXXXXX\", \"header_params\": { \"Content-Type\": \"application/json\" }, \"scheme\": \"HTTPS\", \"port\": 443, \"query_params\": { \"token\": \"R2x1UlN4ZHF8MXxxVFJpelJNVDgzdGNwXXXXXXXXX\" }, \"host\": \"hooks.chime.aws\" } } POST _plugins/_alerting/destinations { \"type\": \"email\", \"name\": \"my-email-destination\", \"email\": { \"email_account_id\": \"YjY7mXMBx015759_IcfW\", \"recipients\": [ { \"type\": \"email_group\", \"email_group_id\": \"YzY-mXMBx015759_dscs\" }, { \"type\": \"email\", \"email\": \"example@email.com\" } ] } } // The email_account_id and email_group_id will be the document IDs of the email_account and email_group you have created. Example response . { \"_id\": \"nO-yFmkB8NzS6aXjJdiI\", \"_version\" : 1, \"_seq_no\" : 3, \"_primary_term\" : 1, \"destination\": { \"type\": \"slack\", \"name\": \"my-destination\", \"last_update_time\": 1550863967624, \"slack\": { \"url\": \"http://www.example.com\" } } } . ",
    "url": "https://vagimeli.github.io/observing-your-data/alerting/api/#create-destination",
    "relUrl": "/observing-your-data/alerting/api/#create-destination"
  },"1195": {
    "doc": "API",
    "title": "Update destination",
    "content": "Introduced 1.0 . When updating a destination, you can optionally include seq_no and primary_term as URL parameters. If these numbers don’t match the existing destination or the destination doesn’t exist, the alerting plugin throws an error. OpenSearch increments the version number and the sequence number automatically (see the example response). Request . PUT _plugins/_alerting/destinations/&lt;destination-id&gt; { \"name\": \"my-updated-destination\", \"type\": \"slack\", \"slack\": { \"url\": \"http://www.example.com\" } } PUT _plugins/_alerting/destinations/&lt;destination-id&gt;?if_seq_no=3&amp;if_primary_term=1 { \"name\": \"my-updated-destination\", \"type\": \"slack\", \"slack\": { \"url\": \"http://www.example.com\" } } . Example response . { \"_id\": \"pe-1FmkB8NzS6aXjqvVY\", \"_version\" : 2, \"_seq_no\" : 4, \"_primary_term\" : 1, \"destination\": { \"type\": \"slack\", \"name\": \"my-updated-destination\", \"last_update_time\": 1550864289375, \"slack\": { \"url\": \"http://www.example.com\" } } } . ",
    "url": "https://vagimeli.github.io/observing-your-data/alerting/api/#update-destination",
    "relUrl": "/observing-your-data/alerting/api/#update-destination"
  },"1196": {
    "doc": "API",
    "title": "Get destination",
    "content": "Introduced 1.0 . Retrieve one destination. Requests . GET _plugins/_alerting/destinations/&lt;destination-id&gt; . Example response . { \"totalDestinations\": 1, \"destinations\": [{ \"id\": \"1a2a3a4a5a6a7a\", \"type\": \"slack\", \"name\": \"sample-destination\", \"user\": { \"name\": \"psantos\", \"backend_roles\": [ \"human-resources\" ], \"roles\": [ \"alerting_full_access\", \"hr-role\" ], \"custom_attribute_names\": [] }, \"schema_version\": 3, \"seq_no\": 0, \"primary_term\": 6, \"last_update_time\": 1603943261722, \"slack\": { \"url\": \"https://example.com\" } } ] } . ",
    "url": "https://vagimeli.github.io/observing-your-data/alerting/api/#get-destination",
    "relUrl": "/observing-your-data/alerting/api/#get-destination"
  },"1197": {
    "doc": "API",
    "title": "Get destinations",
    "content": "Introduced 1.0 . Retrieve all destinations. Requests . GET _plugins/_alerting/destinations . Example response . { \"totalDestinations\": 1, \"destinations\": [{ \"id\": \"1a2a3a4a5a6a7a\", \"type\": \"slack\", \"name\": \"sample-destination\", \"user\": { \"name\": \"psantos\", \"backend_roles\": [ \"human-resources\" ], \"roles\": [ \"alerting_full_access\", \"hr-role\" ], \"custom_attribute_names\": [] }, \"schema_version\": 3, \"seq_no\": 0, \"primary_term\": 6, \"last_update_time\": 1603943261722, \"slack\": { \"url\": \"https://example.com\" } } ] } . ",
    "url": "https://vagimeli.github.io/observing-your-data/alerting/api/#get-destinations",
    "relUrl": "/observing-your-data/alerting/api/#get-destinations"
  },"1198": {
    "doc": "API",
    "title": "Delete destination",
    "content": "Introduced 1.0 . Request . DELETE _plugins/_alerting/destinations/&lt;destination-id&gt; . Example response . { \"_index\": \".opendistro-alerting-config\", \"_type\": \"_doc\", \"_id\": \"Zu-zFmkB8NzS6aXjLeBI\", \"_version\": 2, \"result\": \"deleted\", \"forced_refresh\": true, \"_shards\": { \"total\": 2, \"successful\": 2, \"failed\": 0 }, \"_seq_no\": 8, \"_primary_term\": 1 } . ",
    "url": "https://vagimeli.github.io/observing-your-data/alerting/api/#delete-destination",
    "relUrl": "/observing-your-data/alerting/api/#delete-destination"
  },"1199": {
    "doc": "API",
    "title": "Create email account",
    "content": "Introduced 1.0 . Request . POST _plugins/_alerting/destinations/email_accounts { \"name\": \"example_account\", \"email\": \"example@email.com\", \"host\": \"smtp.email.com\", \"port\": 465, \"method\": \"ssl\" } . Example response . { \"_id\" : \"email_account_id\", \"_version\" : 1, \"_seq_no\" : 7, \"_primary_term\" : 2, \"email_account\" : { \"schema_version\" : 2, \"name\" : \"example_account\", \"email\" : \"example@email.com\", \"host\" : \"smtp.email.com\", \"port\" : 465, \"method\" : \"ssl\" } } . ",
    "url": "https://vagimeli.github.io/observing-your-data/alerting/api/#create-email-account",
    "relUrl": "/observing-your-data/alerting/api/#create-email-account"
  },"1200": {
    "doc": "API",
    "title": "Update email account",
    "content": "Introduced 1.0 . When updating an email account, you can optionally include seq_no and primary_term as URL parameters. If these numbers don’t match the existing email account or the email account doesn’t exist, the alerting plugin throws an error. OpenSearch increments the version number and the sequence number automatically (see the example response). Request . PUT _plugins/_alerting/destinations/email_accounts/&lt;email_account_id&gt; { \"name\": \"example_account\", \"email\": \"example@email.com\", \"host\": \"smtp.email.com\", \"port\": 465, \"method\": \"ssl\" } PUT _plugins/_alerting/destinations/email_accounts/&lt;email_account_id&gt;?if_seq_no=18&amp;if_primary_term=2 { \"name\": \"example_account\", \"email\": \"example@email.com\", \"host\": \"smtp.email.com\", \"port\": 465, \"method\": \"ssl\" } . Example response . { \"_id\" : \"email_account_id\", \"_version\" : 3, \"_seq_no\" : 19, \"_primary_term\" : 2, \"email_account\" : { \"schema_version\" : 2, \"name\" : \"example_account\", \"email\" : \"example@email.com\", \"host\" : \"smtp.email.com\", \"port\" : 465, \"method\" : \"ssl\" } } . ",
    "url": "https://vagimeli.github.io/observing-your-data/alerting/api/#update-email-account",
    "relUrl": "/observing-your-data/alerting/api/#update-email-account"
  },"1201": {
    "doc": "API",
    "title": "Get email account",
    "content": "Introduced 1.0 . Request . GET _plugins/_alerting/destinations/email_accounts/&lt;email_account_id&gt; { \"name\": \"example_account\", \"email\": \"example@email.com\", \"host\": \"smtp.email.com\", \"port\": 465, \"method\": \"ssl\" } . Example response . { \"_id\" : \"email_account_id\", \"_version\" : 2, \"_seq_no\" : 8, \"_primary_term\" : 2, \"email_account\" : { \"schema_version\" : 2, \"name\" : \"test_account\", \"email\" : \"test@email.com\", \"host\" : \"smtp.test.com\", \"port\" : 465, \"method\" : \"ssl\" } } . ",
    "url": "https://vagimeli.github.io/observing-your-data/alerting/api/#get-email-account",
    "relUrl": "/observing-your-data/alerting/api/#get-email-account"
  },"1202": {
    "doc": "API",
    "title": "Delete email account",
    "content": "Introduced 1.0 . Request . DELETE _plugins/_alerting/destinations/email_accounts/&lt;email_account_id&gt; . Example response . { \"_index\" : \".opendistro-alerting-config\", \"_type\" : \"_doc\", \"_id\" : \"email_account_id\", \"_version\" : 1, \"result\" : \"deleted\", \"forced_refresh\" : true, \"_shards\" : { \"total\" : 2, \"successful\" : 2, \"failed\" : 0 }, \"_seq_no\" : 12, \"_primary_term\" : 2 } . ",
    "url": "https://vagimeli.github.io/observing-your-data/alerting/api/#delete-email-account",
    "relUrl": "/observing-your-data/alerting/api/#delete-email-account"
  },"1203": {
    "doc": "API",
    "title": "Search email account",
    "content": "Introduced 1.0 . Request . POST _plugins/_alerting/destinations/email_accounts/_search { \"from\": 0, \"size\": 20, \"sort\": { \"email_account.name.keyword\": \"desc\" }, \"query\": { \"bool\": { \"must\": { \"match_all\": {} } } } } . Example response . { \"took\" : 8, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 2, \"relation\" : \"eq\" }, \"max_score\" : null, \"hits\" : [ { \"_index\" : \".opendistro-alerting-config\", \"_type\" : \"_doc\", \"_id\" : \"email_account_id\", \"_seq_no\" : 8, \"_primary_term\" : 2, \"_score\" : null, \"_source\" : { \"schema_version\" : 2, \"name\" : \"example_account\", \"email\" : \"example@email.com\", \"host\" : \"smtp.email.com\", \"port\" : 465, \"method\" : \"ssl\" }, \"sort\" : [ \"example_account\" ] }, ... ] } } . ",
    "url": "https://vagimeli.github.io/observing-your-data/alerting/api/#search-email-account",
    "relUrl": "/observing-your-data/alerting/api/#search-email-account"
  },"1204": {
    "doc": "API",
    "title": "Create email group",
    "content": "Introduced 1.0 . Request . POST _plugins/_alerting/destinations/email_groups { \"name\": \"example_email_group\", \"emails\": [{ \"email\": \"example@email.com\" }] } . Example response . { \"_id\" : \"email_group_id\", \"_version\" : 1, \"_seq_no\" : 9, \"_primary_term\" : 2, \"email_group\" : { \"schema_version\" : 2, \"name\" : \"example_email_group\", \"emails\" : [ { \"email\" : \"example@email.com\" } ] } } . ",
    "url": "https://vagimeli.github.io/observing-your-data/alerting/api/#create-email-group",
    "relUrl": "/observing-your-data/alerting/api/#create-email-group"
  },"1205": {
    "doc": "API",
    "title": "Update email group",
    "content": "Introduced 1.0 . When updating an email group, you can optionally include seq_no and primary_term as URL parameters. If these numbers don’t match the existing email group or the email group doesn’t exist, the alerting plugin throws an error. OpenSearch increments the version number and the sequence number automatically (see the example response). Request . PUT _plugins/_alerting/destinations/email_groups/&lt;email_group_id&gt; { \"name\": \"example_email_group\", \"emails\": [{ \"email\": \"example@email.com\" }] } PUT _plugins/_alerting/destinations/email_groups/&lt;email_group_id&gt;?if_seq_no=16&amp;if_primary_term=2 { \"name\": \"example_email_group\", \"emails\": [{ \"email\": \"example@email.com\" }] } . Example response . { \"_id\" : \"email_group_id\", \"_version\" : 4, \"_seq_no\" : 17, \"_primary_term\" : 2, \"email_group\" : { \"schema_version\" : 2, \"name\" : \"example_email_group\", \"emails\" : [ { \"email\" : \"example@email.com\" } ] } } . ",
    "url": "https://vagimeli.github.io/observing-your-data/alerting/api/#update-email-group",
    "relUrl": "/observing-your-data/alerting/api/#update-email-group"
  },"1206": {
    "doc": "API",
    "title": "Get email group",
    "content": "Introduced 1.0 . Request . GET _plugins/_alerting/destinations/email_groups/&lt;email_group_id&gt; { \"name\": \"example_email_group\", \"emails\": [{ \"email\": \"example@email.com\" }] } . Example response . { \"_id\" : \"email_group_id\", \"_version\" : 4, \"_seq_no\" : 17, \"_primary_term\" : 2, \"email_group\" : { \"schema_version\" : 2, \"name\" : \"example_email_group\", \"emails\" : [ { \"email\" : \"example@email.com\" } ] } } . ",
    "url": "https://vagimeli.github.io/observing-your-data/alerting/api/#get-email-group",
    "relUrl": "/observing-your-data/alerting/api/#get-email-group"
  },"1207": {
    "doc": "API",
    "title": "Delete email group",
    "content": "Introduced 1.0 . Request . DELETE _plugins/_alerting/destinations/email_groups/&lt;email_group_id&gt; . Example response . { \"_index\" : \".opendistro-alerting-config\", \"_type\" : \"_doc\", \"_id\" : \"email_group_id\", \"_version\" : 1, \"result\" : \"deleted\", \"forced_refresh\" : true, \"_shards\" : { \"total\" : 2, \"successful\" : 2, \"failed\" : 0 }, \"_seq_no\" : 11, \"_primary_term\" : 2 } . ",
    "url": "https://vagimeli.github.io/observing-your-data/alerting/api/#delete-email-group",
    "relUrl": "/observing-your-data/alerting/api/#delete-email-group"
  },"1208": {
    "doc": "API",
    "title": "Search email group",
    "content": "Introduced 1.0 . Request . POST _plugins/_alerting/destinations/email_groups/_search { \"from\": 0, \"size\": 20, \"sort\": { \"email_group.name.keyword\": \"desc\" }, \"query\": { \"bool\": { \"must\": { \"match_all\": {} } } } } . Example response . { \"took\" : 7, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 5, \"relation\" : \"eq\" }, \"max_score\" : null, \"hits\" : [ { \"_index\" : \".opendistro-alerting-config\", \"_type\" : \"_doc\", \"_id\" : \"email_group_id\", \"_seq_no\" : 10, \"_primary_term\" : 2, \"_score\" : null, \"_source\" : { \"schema_version\" : 2, \"name\" : \"example_email_group\", \"emails\" : [ { \"email\" : \"example@email.com\" } ] }, \"sort\" : [ \"example_email_group\" ] }, ... ] } } . ",
    "url": "https://vagimeli.github.io/observing-your-data/alerting/api/#search-email-group",
    "relUrl": "/observing-your-data/alerting/api/#search-email-group"
  },"1209": {
    "doc": "API",
    "title": "API",
    "content": " ",
    "url": "https://vagimeli.github.io/observing-your-data/alerting/api/",
    "relUrl": "/observing-your-data/alerting/api/"
  },"1210": {
    "doc": "Cron",
    "title": "Cron expression reference",
    "content": "Monitors can run at a variety of fixed intervals (e.g. hourly, daily, etc.), but you can also define custom cron expressions for when they should run. Monitors use the Unix cron syntax and support five fields: . | Field | Valid values | . | Minute | 0-59 | . | Hour | 0-23 | . | Day of month | 1-31 | . | Month | 1-12 | . | Day of week | 0-7 (0 and 7 are both Sunday) or SUN, MON, TUE, WED, THU, FRI, SAT | . For example, the following expression translates to “every Monday through Friday at 11:30 AM”: . 30 11 * * 1-5 . ",
    "url": "https://vagimeli.github.io/observing-your-data/alerting/cron/#cron-expression-reference",
    "relUrl": "/observing-your-data/alerting/cron/#cron-expression-reference"
  },"1211": {
    "doc": "Cron",
    "title": "Features",
    "content": "| Feature | Description | . | * | Wildcard. Specifies all valid values. | . | , | List. Use to specify several values (e.g. 1,15,30). | . | - | Range. Use to specify a range of values (e.g. 1-15). | . | / | Step. Use after a wildcard or range to specify the “step” between values. For example, 0-11/2 is equivalent to 0,2,4,6,8,10. | . Note that you can specify the day using two fields: day of month and day of week. For most situations, we recommend that you use just one of these fields and leave the other as *. If you use a non-wildcard value in both fields, the monitor runs when either field matches the time. For example, 15 2 1,15 * 1 causes the monitor to run at 2:15 AM on the 1st of the month, the 15th of the month, and every Monday. ",
    "url": "https://vagimeli.github.io/observing-your-data/alerting/cron/#features",
    "relUrl": "/observing-your-data/alerting/cron/#features"
  },"1212": {
    "doc": "Cron",
    "title": "Sample expressions",
    "content": "Every other day at 1:45 PM: . 45 13 1-31/2 * * . Every 10 minutes on Saturday and Sunday: . 0/10 * * * 6-7 . Every three hours on the first day of every other month: . 0 0-23/3 1 1-12/2 * . ",
    "url": "https://vagimeli.github.io/observing-your-data/alerting/cron/#sample-expressions",
    "relUrl": "/observing-your-data/alerting/cron/#sample-expressions"
  },"1213": {
    "doc": "Cron",
    "title": "API",
    "content": "For an example of how to use a custom cron expression in an API call, see the create monitor API operation. ",
    "url": "https://vagimeli.github.io/observing-your-data/alerting/cron/#api",
    "relUrl": "/observing-your-data/alerting/cron/#api"
  },"1214": {
    "doc": "Cron",
    "title": "Cron",
    "content": " ",
    "url": "https://vagimeli.github.io/observing-your-data/alerting/cron/",
    "relUrl": "/observing-your-data/alerting/cron/"
  },"1215": {
    "doc": "Alerting",
    "title": "Alerting",
    "content": "OpenSearch Dashboards . You can use the Alerting plugin in OpenSearch Dashboards to monitor your data and create alert notifications that trigger when conditions occur in one or more indexes. You create a monitor with trigger conditions that generate various alert notifications through the message channel you select as a destination. Notifications can be sent to email, Slack, or Amazon Chime. The monitor you create notifies you when data from one or more OpenSearch indexes meets certain conditions. For example, you might want to notify a Slack channel if your application logs more than five HTTP 503 errors in one hour, or you might want to page a developer if no new documents have been indexed in the past 20 minutes. To get started, choose Alerting in OpenSearch Dashboards. Figure 1: Alerting plugin in OpenSearch Dashboards . ",
    "url": "https://vagimeli.github.io/observing-your-data/alerting/index/",
    "relUrl": "/observing-your-data/alerting/index/"
  },"1216": {
    "doc": "Monitors",
    "title": "Monitors",
    "content": ". | Monitor types | Key terms | Per document monitors . | Document findings | . | Create destinations . | Email as a destination | . | Create a monitor | Create triggers . | Visual editor | Extraction query | Anomaly detector | Available variables | . | Add actions . | Questions about destinations | . | Work with alerts | Create cluster metrics monitor . | Supported APIs | Restrict API fields | Painless triggers | Limitations | . | . ",
    "url": "https://vagimeli.github.io/observing-your-data/alerting/monitors/",
    "relUrl": "/observing-your-data/alerting/monitors/"
  },"1217": {
    "doc": "Monitors",
    "title": "Monitor types",
    "content": "The OpenSearch Dashboard Alerting plugin provides four monitor types: . | per query – This monitor runs a query and generates alert notifications based on criteria that matches. | per bucket – This monitor runs a query that evaluates trigger criteria based on aggregated values in the dataset. | per cluster metrics – This monitor runs API requests on the cluster to monitor its health. | per document – This monitor runs a query (or multiple queries combined by a tag) that returns individual documents that match the alert notification trigger condition. | . ",
    "url": "https://vagimeli.github.io/observing-your-data/alerting/monitors/#monitor-types",
    "relUrl": "/observing-your-data/alerting/monitors/#monitor-types"
  },"1218": {
    "doc": "Monitors",
    "title": "Key terms",
    "content": "| Term | Definition | . | Monitor | A job that runs on a defined schedule and queries OpenSearch indexes. The results of these queries are then used as input for one or more triggers. | . | Trigger | Conditions that, if met, generate alerts. | . | Tag | A label that can be applied to multiple queries to combine them with the logical OR operation in a per document monitor. You cannot use tags with other monitor types. | . | Alert | An event associated with a trigger. When an alert is created, the trigger performs actions, which can include sending a notification. | . | Action | The information that you want the monitor to send out after being triggered. Actions have a destination, a message subject, and a message body. | . | Destination | A reusable location for an action. Supported locations are Amazon Chime, Email, Slack, or custom webhook. | . | Finding | An entry for an individual document found by a per document monitor query that contains the document ID, index name, and timestamp. Findings are stored in the Findings index: .opensearch-alerting-finding*. | . | Channel | A notification channel to use in an action. See notifications for more information. | . ",
    "url": "https://vagimeli.github.io/observing-your-data/alerting/monitors/#key-terms",
    "relUrl": "/observing-your-data/alerting/monitors/#key-terms"
  },"1219": {
    "doc": "Monitors",
    "title": "Per document monitors",
    "content": "Introduced 2.0 . Per document monitors allow you to define up to 10 queries that compare the selected field with your desired value. You can define supported field data types using the following operators: . | is | is not | is greater than | is greater than equal | is less than | is less than equal | . You query each trigger using up to 10 tags, adding the tag as a single trigger condition instead of specifying a single query. The Alerting plugin processes the trigger conditions from all queries as a logical OR operation, so if any of the query conditions are met, it triggers an alert. Next, the Alerting plugin tells the Notifications plugin to send the notification to a channel. The Alerting plugin also creates a list of document findings that contains metadata about which document matches each query. Security analytics can use the document findings data to keep track of and analyze the query data separately from the alert processes. The Alerting API provides a document-level monitor that programmatically accomplishes the same function as the per document monitor in the OpenSearch Dashboards. To learn more, see Document-level monitors. Document findings . When a per document monitor executes a query that matches a document in an index, a finding is created. OpenSearch provides a Findings index: .opensearch-alerting-finding* that contains findings data for all per document monitor queries. You can search the findings index with the Alerting API search operation. To learn more, see Search for monitor findings. The following metadata is provided for each document finding entry: . | Document – The document ID and index name. For example: Re5akdirhj3fl | test-logs-index. | Query – The query name that matched the document. | Time found – The timestamp that indicates when the document was found during the runtime. | . It is possible to configure an alert notification for each finding, however we don’t recommend this unless rules are well defined to prevent a huge volume of findings in a high ingestion cluster. ",
    "url": "https://vagimeli.github.io/observing-your-data/alerting/monitors/#per-document-monitors",
    "relUrl": "/observing-your-data/alerting/monitors/#per-document-monitors"
  },"1220": {
    "doc": "Monitors",
    "title": "Create destinations",
    "content": ". | Choose Alerting, Destinations, Add destination. | Specify a name for the destination so that you can identify it later. | For Type, choose Slack, Amazon Chime, custom webhook, or email. | . For Email, refer to the Email as a destination section below. For all other types, specify the webhook URL. See the documentation for Slack and Amazon Chime to learn more about webhooks. If you’re using custom webhooks, you must specify more information: parameters and headers. For example, if your endpoint requires basic authentication, you might need to add a header with a key of Authorization and a value of Basic &lt;Base64-encoded-credential-string&gt;. You might also need to change Content-Type to whatever your webhook requires. Popular values are application/json, application/xml, and text/plain. This information is stored in plain text in the OpenSearch cluster. We will improve this design in the future, but for now, the encoded credentials (which are neither encrypted nor hashed) might be visible to other OpenSearch users. Email as a destination . To send or receive an alert notification as an email, choose Email as the destination type. Next, add at least one sender and recipient. We recommend adding email groups if you want to notify more than a few people of an alert. You can configure senders and recipients using Manage senders and Manage email groups. Manage senders . You need to specify an email account from which the Alerting plugin can send notifications. To configure a sender email, do the following: . | After you choose Email as the destination type, choose Manage senders. | Choose Add sender, New sender and enter a unique name. | Enter the email address, SMTP host (e.g. smtp.gmail.com for a Gmail account), and the port. | Choose an encryption method, or use the default value of None. However, most email providers require SSL or TLS, which require a username and password in OpenSearch keystore. Refer to Authenticate sender account to learn more. | Choose Save to save the configuration and create the sender. You can create a sender even before you add your credentials to the OpenSearch keystore. However, you must authenticate each sender account before you use the destination to send your alert. | . You can reuse senders across many different destinations, but each destination only supports one sender. Manage email groups or recipients . Use email groups to create and manage reusable lists of email addresses. For example, one alert might email the DevOps team, whereas another might email the executive team and the engineering team. You can enter individual email addresses or an email group in the Recipients field. | After you choose Email as the destination type, choose Manage email groups. Then choose Add email group, New email group. | Enter a unique name. | For recipient emails, enter any number of email addresses. | Choose Save. | . Authenticate sender account . If your email provider requires SSL or TLS, you must authenticate each sender account before you can send an email. Enter these credentials in the OpenSearch keystore using the CLI. Run the following commands (in your OpenSearch directory) to enter your username and password. The &lt;sender_name&gt; is the name you entered for Sender earlier./bin/opensearch-keystore add plugins.alerting.destination.email.&lt;sender_name&gt;.username ./bin/opensearch-keystore add plugins.alerting.destination.email.&lt;sender_name&gt;.password . Note: Keystore settings are node-specific. You must run these commands on each node. To change or update your credentials (after you’ve added them to the keystore on every node), call the reload API to automatically update those credentials without restarting OpenSearch: . POST _nodes/reload_secure_settings { \"secure_settings_password\": \"1234\" } . ",
    "url": "https://vagimeli.github.io/observing-your-data/alerting/monitors/#create-destinations",
    "relUrl": "/observing-your-data/alerting/monitors/#create-destinations"
  },"1221": {
    "doc": "Monitors",
    "title": "Create a monitor",
    "content": ". | Choose Alerting, Monitors, Create monitor. | Specify a name for the monitor. | Choose either Per query monitor, Per bucket monitor, Per cluster metrics monitor, or Per document monitor. | . OpenSearch supports the following types of monitors: . | Per query monitors run your specified query and then check whether the query’s results trigger any alerts. Per query monitors can only trigger one alert at a time. | Per bucket monitors let you create buckets based on selected fields and then categorize your results into those buckets. The Alerting plugin runs each bucket’s unique results against a script you define later, so you have finer control over which results should trigger alerts. Furthermore, each bucket can trigger an alert. | . The maximum number of monitors you can create is 1,000. You can change the default maximum number of alerts for your cluster by calling the cluster settings API plugins.alerting.monitor.max_monitors. | Decide how you want to define your query and triggers. You can use any of the following methods: visual editor, query editor, or anomaly detector. | Visual definition works well for monitors that you can define as “some value is above or below some threshold for some amount of time.” . | Query definition gives you flexibility in terms of what you query for (using OpenSearch query DSL) and how you evaluate the results of that query (Painless scripting). This example averages the cpu_usage field: . { \"size\": 0, \"query\": { \"match_all\": {} }, \"aggs\": { \"avg_cpu\": { \"avg\": { \"field\": \"cpu_usage\" } } } } . You can even filter query results using {{period_start}} and {{period_end}}: . { \"size\": 0, \"query\": { \"bool\": { \"filter\": [{ \"range\": { \"timestamp\": { \"from\": \"{{period_end}}||-1h\", \"to\": \"{{period_end}}\", \"include_lower\": true, \"include_upper\": true, \"format\": \"epoch_millis\", \"boost\": 1 } } }], \"adjust_pure_negative\": true, \"boost\": 1 } }, \"aggregations\": {} } . | . “Start” and “end” refer to the interval at which the monitor runs. See Available variables. To define a monitor visually, choose Visual editor. Then choose a source index, a timeframe, an aggregation (for example, count() or average()), a data filter if you want to monitor a subset of your source index, and a group-by field if you want to include an aggregation field in your query. At least one group-by field is required if you’re defining a bucket-level monitor. Visual definition works well for most monitors. If you use the Security plugin, you can only choose indexes that you have permission to access. For details, see Alerting security. To use a query, choose Extraction query editor, add your query (using OpenSearch query DSL), and test it using the Run button. The monitor makes this query to OpenSearch as often as the schedule dictates; check the Query Performance section and make sure you’re comfortable with the performance implications. To use an anomaly detector, choose Anomaly detector and select your Detector. The anomaly detection option is for pairing with the anomaly detection plugin. See Anomaly Detection. For anomaly detector, choose an appropriate schedule for the monitor based on the detector interval. Otherwise, the alerting monitor might miss reading the results. For example, assume you set the monitor interval and the detector interval as 5 minutes, and you start the detector at 12:00. If an anomaly is detected at 12:05, it might be available at 12:06 because of the delay between writing the anomaly and it being available for queries. The monitor reads the anomaly results between 12:00 and 12:05, so it does not get the anomaly results available at 12:06. To avoid this issue, make sure the alerting monitor is at least twice the detector interval. When you create a monitor using OpenSearch Dashboards, the anomaly detector plugin generates a default monitor schedule that’s twice the detector interval. Whenever you update a detector’s interval, make sure to update the associated monitor interval as well, as the anomaly detection plugin does not do this automatically. Note: Anomaly detection is available only if you are defining a per query monitor. | Choose how frequently to run your monitor. You can run it either by time intervals (minutes, hours, or days) or on a schedule. If you run it on a daily, weekly or monthly schedule or according to a custom custom cron expression, then you need to also provide the time zone. | Add a trigger to your monitor. | . ",
    "url": "https://vagimeli.github.io/observing-your-data/alerting/monitors/#create-a-monitor",
    "relUrl": "/observing-your-data/alerting/monitors/#create-a-monitor"
  },"1222": {
    "doc": "Monitors",
    "title": "Create triggers",
    "content": "Steps to create a trigger differ depending on whether you chose Visual editor, Extraction query editor, or Anomaly detector when you created the monitor. You begin by specifying a name and severity level for the trigger. Severity levels help you manage alerts. A trigger with a high severity level (e.g. 1) might page a specific individual, whereas a trigger with a low severity level might message a chat room. Remember that query-level monitors run your trigger’s script just once against the query’s results, but bucket-level monitors execute your trigger’s script on each bucket, so you should create a trigger that best fits the monitor you chose. If you want to execute multiple scripts, you must create multiple triggers. Visual editor . For a query-level monitor’s Trigger condition, specify a threshold for the aggregation and timeframe you chose earlier, such as “is below 1,000” or “is exactly 10.” . The line moves up and down as you increase and decrease the threshold. Once this line is crossed, the trigger evaluates to true. Bucket-level monitors also require you to specify a threshold and value for your aggregation and timeframe, but you can use a maximum of five conditions to better refine your trigger. Optionally, you can also use a keyword filter to filter for a specific field in your index. Document-level monitors provide the added option to use tags that represent multiple queries connected by the logical OR operator. To create a multiple query combination trigger, do the following steps: . | Create a per document monitor with more than one query. | Create the first query with a field, an operator, and a value. For example, set the query to search for the region field with either operator: “is” or “is not”, and set the value “us-west-2”. | Select Add Tag and give the tag a name. | Create the second query and add the same tag to it. | Now you can create the trigger condition and specify the tag name. This creates a combination trigger that checks two queries that both contain the same tag. The monitor checks both queries with a logical OR operation and if either query’s conditions are met, then it will generate the alert notification. | . Extraction query . If you’re using a query-level monitor, specify a Painless script that returns true or false. Painless is the default OpenSearch scripting language and has a syntax similar to Groovy. Trigger condition scripts revolve around the ctx.results[0] variable, which corresponds to the extraction query response. For example, your script might reference ctx.results[0].hits.total.value or ctx.results[0].hits.hits[i]._source.error_code. A return value of true means the trigger condition has been met, and the trigger should execute its actions. Test your script using the Run button. The Info link next to Trigger condition contains a useful summary of the variables and results available to your query. Bucket-level monitors require you to specify more information in your trigger condition. At a minimum, you must have the following fields: . | buckets_path, which maps variable names to metrics to use in your script. | parent_bucket_path, which is a path to a multi-bucket aggregation. The path can include single-bucket aggregations, but the last aggregation must be multi-bucket. For example, if you have a pipeline such as agg1&gt;agg2&gt;agg3, agg1 and agg2 are single-bucket aggregations, but agg3 must be a multi-bucket aggregation. | script, which is the script that OpenSearch runs to evaluate whether to trigger any alerts. | . For example, you might have a script that looks like the following: . { \"buckets_path\": { \"count_var\": \"_count\" }, \"parent_bucket_path\": \"composite_agg\", \"script\": { \"source\": \"params.count_var &gt; 5\" } } . After mapping the count_var variable to the _count metric, you can use count_var in your script and reference _count data. Finally, composite_agg is a path to a multi-bucket aggregation. Anomaly detector . For Trigger type, choose Anomaly detector grade and confidence. Specify the Anomaly grade condition for the aggregation and timeframe you chose earlier, “IS ABOVE 0.7” or “IS EXACTLY 0.5.” The anomaly grade is a number between 0 and 1 that indicates the level of severity of how anomalous a data point is. Specify the Anomaly confidence condition for the aggregation and timeframe you chose earlier, “IS ABOVE 0.7” or “IS EXACTLY 0.5.” The anomaly confidence is an estimate of the probability that the reported anomaly grade matches the expected anomaly grade. The line moves up and down as you increase and decrease the threshold. Once this line is crossed, the trigger evaluates to true. Sample scripts . // Evaluates to true if the query returned any documents ctx.results[0].hits.total.value &gt; 0 . // Returns true if the avg_cpu aggregation exceeds 90 if (ctx.results[0].aggregations.avg_cpu.value &gt; 90) { return true; } . // Performs some crude custom scoring and returns true if that score exceeds a certain value int score = 0; for (int i = 0; i &lt; ctx.results[0].hits.hits.length; i++) { // Weighs 500 errors 10 times as heavily as 503 errors if (ctx.results[0].hits.hits[i]._source.http_status_code == \"500\") { score += 10; } else if (ctx.results[0].hits.hits[i]._source.http_status_code == \"503\") { score += 1; } } if (score &gt; 99) { return true; } else { return false; } . Below are some variables you can include in your message using Mustache templates to see more information about your monitors. Available variables . Monitor variables . | Variable | Data type | Description | . | ctx.monitor | Object | Includes ctx.monitor.name, ctx.monitor.type, ctx.monitor.enabled, ctx.monitor.enabled_time, ctx.monitor.schedule, ctx.monitor.inputs, triggers and ctx.monitor.last_update_time. | . | ctx.monitor.user | Object | Includes information about the user who created the monitor. Includes ctx.monitor.user.backend_roles and ctx.monitor.user.roles, which are arrays that contain the backend roles and roles assigned to the user. See alerting security for more information. | . | ctx.monitor.enabled | Boolean | Whether the monitor is enabled. | . | ctx.monitor.enabled_time | Milliseconds | Unix epoch time of when the monitor was last enabled. | . | ctx.monitor.schedule | Object | Contains a schedule of how often or when the monitor should run. | . | ctx.monitor.schedule.period.interval | Integer | The interval at which the monitor runs. | . | ctx.monitor.schedule.period.unit | String | The interval’s unit of time. | . | ctx.monitor.inputs | Array | An array that contains the indexes and definition used to create the monitor. | . | ctx.monitor.inputs.search.indices | Array | An array that contains the indexes the monitor observes. | . | ctx.monitor.inputs.search.query | N/A | The definition used to define the monitor. | . Trigger variables . | Variable | Data type | Description | . | ctx.trigger.id | String | The trigger’s ID. | . | ctx.trigger.name | String | The trigger’s name. | . | ctx.trigger.severity | String | The trigger’s severity. | . | ctx.trigger.condition | Object | Contains the Painless script used when creating the monitor. | . | ctx.trigger.condition.script.source | String | The language used to define the script. Must be painless. | . | ctx.trigger.condition.script.lang | String | The script used to define the trigger. | . | ctx.trigger.actions | Array | An array with one element that contains information about the action the monitor needs to trigger. | . Action variables . | Variable | Data type | Description | . | ctx.trigger.actions.id | String | The action’s ID. | . | ctx.trigger.actions.name | String | The action’s name. | . | ctx.trigger.actions.message_template.source | String | The message to send in the alert. | . | ctx.trigger.actions.message_template.lang | String | The scripting language used to define the message. Must be Mustache. | . | ctx.trigger.actions.throttle_enabled | Boolean | Whether throttling is enabled for this trigger. See adding actions for more information about throttling. | . | ctx.trigger.actions.subject_template.source | String | The message’s subject in the alert. | . | ctx.trigger.actions.subject_template.lang | String | The scripting language used to define the subject. Must be mustache. | . Other variables . | Variable | Data type | Description | . | ctx.results | Array | An array with one element (i.e. ctx.results[0]). Contains the query results. This variable is empty if the trigger was unable to retrieve results. See ctx.error. | . | ctx.last_update_time | Milliseconds | Unix epoch time of when the monitor was last updated. | . | ctx.periodStart | String | Unix timestamp for the beginning of the period during which the alert triggered. For example, if a monitor runs every ten minutes, a period might begin at 10:40 and end at 10:50. | . | ctx.periodEnd | String | The end of the period during which the alert triggered. | . | ctx.error | String | The error message if the trigger was unable to retrieve results or unable to evaluate the trigger, typically due to a compile error or null pointer exception. Null otherwise. | . | ctx.alert | Object | The current, active alert (if it exists). Includes ctx.alert.id, ctx.alert.version, and ctx.alert.isAcknowledged. Null if no alert is active. Only available with query-level monitors. | . | ctx.dedupedAlerts | Object | Alerts that have already been triggered. OpenSearch keeps the existing alert to prevent the plugin from creating endless amounts of the same alerts. Only available with bucket-level monitors. | . | ctx.newAlerts | Object | Newly created alerts. Only available with bucket-level monitors. | . | ctx.completedAlerts | Object | Alerts that are no longer ongoing. Only available with bucket-level monitors. | . | bucket_keys | String | Comma-separated list of the monitor’s bucket key values. Available only for ctx.dedupedAlerts, ctx.newAlerts, and ctx.completedAlerts. Accessed through ctx.dedupedAlerts[0].bucket_keys. | . | parent_bucket_path | String | The parent bucket path of the bucket that triggered the alert. Accessed through ctx.dedupedAlerts[0].parent_bucket_path. | . ",
    "url": "https://vagimeli.github.io/observing-your-data/alerting/monitors/#create-triggers",
    "relUrl": "/observing-your-data/alerting/monitors/#create-triggers"
  },"1223": {
    "doc": "Monitors",
    "title": "Add actions",
    "content": "The final step in creating a monitor is to add one or more actions. Actions send notifications when trigger conditions are met. See the Notifications plugin to see what communication channels are supported. If you don’t want to receive notifications for alerts, you don’t have to add actions to your triggers. Instead, you can periodically check OpenSearch Dashboards. | Specify a name for the action. | Choose a notification channel. | Add a subject and body for the message. You can add variables to your messages using Mustache templates. You have access to ctx.action.name, the name of the current action, as well as all trigger variables. If your destination is a custom webhook that expects a particular data format, you might need to include JSON (or even XML) directly in the message body: . { \"text\": \"Monitor {{ctx.monitor.name}} just entered alert status. Please investigate the issue. - Trigger: {{ctx.trigger.name}} - Severity: {{ctx.trigger.severity}} - Period start: {{ctx.periodStart}} - Period end: {{ctx.periodEnd}}\" } . In this case, the message content must conform to the Content-Type header in the custom webhook. | If you’re using a bucket-level monitor, you can choose whether the monitor should perform an action for each execution or for each alert. | (Optional) Use action throttling to limit the number of notifications you receive within a given span of time. For example, if a monitor checks a trigger condition every minute, you could receive one notification per minute. If you set action throttling to 60 minutes, you receive no more than one notification per hour, even if the trigger condition is met dozens of times in that hour. | Choose Create. | . After an action sends a message, the content of that message has left the purview of the Security plugin. Securing access to the message (e.g. access to the Slack channel) is your responsibility. Sample message . Monitor {{ctx.monitor.name}} just entered an alert state. Please investigate the issue. - Trigger: {{ctx.trigger.name}} - Severity: {{ctx.trigger.severity}} - Period start: {{ctx.periodStart}} - Period end: {{ctx.periodEnd}} . If you want to use the ctx.results variable in a message, use {{ctx.results.0}} rather than {{ctx.results[0]}}. This difference is due to how Mustache handles bracket notation. Questions about destinations . Q: What plugins do I need installed besides Alerting? . A: To continue using the notification action in the Alerting plugin, you need to install the backend plugins notifications-core and notifications. You can also install the Notifications Dashboards plugin to manage Notification channels via OpenSearch Dashboards. Q: Can I still create destinations? A: No, destinations have been deprecated and can no longer be created/edited. Q: Will I need to move my destinations to the Notifications plugin? A: No. To upgrade users, a background process will automatically move destinations to notification channels. These channels will have the same ID as the destinations, and monitor execution will choose the correct ID, so you don’t have to make any changes to the monitor’s definition. The migrated destinations will be deleted. Q: What happens if any destinations fail to migrate? A: If a destination failed to migrate, the monitor will continue using it until the monitor is migrated to a notification channel. You don’t need to do anything in this case. Q: Do I need to install the Notifications plugins if monitors can still use destinations? A: Yes. The fallback on destination is to prevent failures in sending messages if migration fails; however, the Notification plugin is what actually sends the message. Not having the Notification plugin installed will lead to the action failing. ",
    "url": "https://vagimeli.github.io/observing-your-data/alerting/monitors/#add-actions",
    "relUrl": "/observing-your-data/alerting/monitors/#add-actions"
  },"1224": {
    "doc": "Monitors",
    "title": "Work with alerts",
    "content": "Alerts persist until you resolve the root cause and have the following states: . | State | Description | . | Active | The alert is ongoing and unacknowledged. Alerts remain in this state until you acknowledge them, delete the trigger associated with the alert, or delete the monitor entirely. | . | Acknowledged | Someone has acknowledged the alert, but not fixed the root cause. | . | Completed | The alert is no longer ongoing. Alerts enter this state after the corresponding trigger evaluates to false. | . | Error | An error occurred while executing the trigger—usually the result of a a bad trigger or destination. | . | Deleted | Someone deleted the monitor or trigger associated with this alert while the alert was ongoing. | . ",
    "url": "https://vagimeli.github.io/observing-your-data/alerting/monitors/#work-with-alerts",
    "relUrl": "/observing-your-data/alerting/monitors/#work-with-alerts"
  },"1225": {
    "doc": "Monitors",
    "title": "Create cluster metrics monitor",
    "content": "In addition to monitoring conditions for indexes, the Alerting plugin allows monitoring conditions for clusters. Alerts can be set by cluster metrics to watch for the following conditions: . | The health of your cluster reaches a status of yellow or red | Cluster-level metrics, such as CPU usage and JVM memory usage, reach specified thresholds | Node-level metrics, such as available disk space, JVM memory usage, and CPU usage, reach a specified threshold | The total number of documents stores reaches a specified amount | . To create a cluster metrics monitor: . | Select Alerting &gt; Monitors &gt; Create monitor. | Select the Per cluster metrics monitor option. | In the Query section, pick the Request type from the dropdown. | (Optional) If you want to filter the API response to use only certain path parameters, enter those parameters under Query parameters. Most APIs that can be used to monitor cluster status support path parameters as described in their documentation (e.g., comma-separated lists of index names). | In the Triggers section, indicate what conditions trigger an alert. The trigger condition autopopulates a painless ctx variable. For example, a cluster monitor watching for Cluster Stats uses the trigger condition ctx.results[0].indices.count &lt;= 0, which triggers an alert based on the number of indexes returned by the query. For more specificity, add any additional painless conditions supported by the API. To see an example of the condition response, select Preview condition response. | In the Actions section, indicate how you want your users to be notified when a trigger condition is met. | Select Create. Your new monitor appears in the Monitors list. | . Supported APIs . Trigger conditions use responses from the following cat API endpoints. Most APIs that can be used to monitor cluster status support path parameters as described in their documentation (e.g., comma-separated lists of index names). However, they do not support query parameters. | _cluster/health | _cluster/stats | _cluster/settings | _nodes/stats | _cat/pending_tasks | _cat/recovery | _cat/snapshots | _cat/tasks | . Restrict API fields . If you want to hide fields from the API response that you do not want exposed for alerting, reconfigure the supported_json_payloads.json file inside the Alerting plugin. The file functions as an allow list for the API fields you want to use in an alert. By default, all APIs and their parameters can be used for monitors and trigger conditions. However, you can modify the file so that cluster metric monitors can only be created for APIs referenced. Furthermore, only fields referenced in the supported files can create trigger conditions. This supported_json_payloads.json allows for a cluster metrics monitor to be created for the _cluster/stats API, and triggers conditions for the indices.shards.total and indices.shards.index.shards.min fields. \"/_cluster/stats\": { \"indices\": [ \"shards.total\", \"shards.index.shards.min\" ] } . Painless triggers . Painless scripts define triggers for cluster metrics monitors, similar to query or bucket-level monitors that are defined using the extraction query definition option. Painless scripts are comprised of at least one statement and any additional functions you wish to execute. The cluster metrics monitor supports up to ten triggers. In this example, a JSON object creates a trigger that sends an alert when the Cluster Health is yellow. script points the source to the painless script ctx.results[0].status == \\\"yellow\\. { \"name\": \"Cluster Health Monitor\", \"type\": \"monitor\", \"monitor_type\": \"query_level_monitor\", \"enabled\": true, \"schedule\": { \"period\": { \"unit\": \"MINUTES\", \"interval\": 1 } }, \"inputs\": [ { \"uri\": { \"api_type\": \"CLUSTER_HEALTH\", \"path\": \"_cluster/health/\", \"path_params\": \"\", \"url\": \"http://localhost:9200/_cluster/health/\" } } ], \"triggers\": [ { \"query_level_trigger\": { \"id\": \"Tf_L_nwBti6R6Bm-18qC\", \"name\": \"Yellow status trigger\", \"severity\": \"1\", \"condition\": { \"script\": { \"source\": \"ctx.results[0].status == \\\"yellow\\\"\", \"lang\": \"painless\" } }, \"actions\": [] } } ] } . See trigger variables for more painless ctx options. Limitations . Currently, the cluster metrics monitor has the following limitations: . | You cannot create monitors for remote clusters. | The OpenSearch cluster must be in a state where an index’s conditions can be monitored and actions can be executed against the index. | Removing resource permissions from a user will not prevent that user’s preexisting monitors for that resource from executing. | Users with permissions to create monitors are not blocked from creating monitors for resources for which they do not have permissions; however, those monitors will not execute. | . ",
    "url": "https://vagimeli.github.io/observing-your-data/alerting/monitors/#create-cluster-metrics-monitor",
    "relUrl": "/observing-your-data/alerting/monitors/#create-cluster-metrics-monitor"
  },"1226": {
    "doc": "Alerting security",
    "title": "Alerting security",
    "content": "If you use the Security plugin alongside alerting, you might want to limit certain users to certain actions. For example, you might want some users to only be able to view and acknowledge alerts, while others can modify monitors and destinations. ",
    "url": "https://vagimeli.github.io/observing-your-data/alerting/security/",
    "relUrl": "/observing-your-data/alerting/security/"
  },"1227": {
    "doc": "Alerting security",
    "title": "Basic permissions",
    "content": "The Security plugin has three built-in roles that cover most alerting use cases: alerting_read_access, alerting_ack_alerts, and alerting_full_access. For descriptions of each, see Predefined roles. If these roles don’t meet your needs, mix and match individual alerting permissions to suit your use case. Each action corresponds to an operation in the REST API. For example, the cluster:admin/opensearch/alerting/destination/delete permission lets you delete destinations. ",
    "url": "https://vagimeli.github.io/observing-your-data/alerting/security/#basic-permissions",
    "relUrl": "/observing-your-data/alerting/security/#basic-permissions"
  },"1228": {
    "doc": "Alerting security",
    "title": "How monitors access data",
    "content": "Monitors run with the permissions of the user who created or last modified them. For example, consider the user jdoe, who works at a chain of retail stores. jdoe has two roles. Together, these two roles allow read access to three indices: store1-returns, store2-returns, and store3-returns. jdoe creates a monitor that sends an email to management whenever the number of returns across all three indices exceeds 40 per hour. Later, the user psantos wants to edit the monitor to run every two hours, but psantos only has access to store1-returns. To make the change, psantos has two options: . | Update the monitor so that it only checks store1-returns. | Ask an administrator for read access to the other two indices. | . After making the change, the monitor now runs with the same permissions as psantos, including any document-level security queries, excluded fields, and masked fields. If you use an extraction query to define your monitor, use the Run button to ensure that the response includes the fields you need. Once a monitor is created, the Alerting plugin will continue executing the monitor, even if the user who created the monitor has their permissions removed. Only a user with the correct cluster permissions can manually disable or delete a monitor to stop it from executing: . | Disable a monitor: cluster:admin/opendistro/alerting/monitor/write | Delete a monitor: cluster:admin/opendistro/alerting/monitor/delete | . If your monitor’s trigger has notifications configured, the Alerting plugin continues to send out notifications regardless of destination type. To stop notifications, a user must manually delete them in the trigger’s actions. A note on alerts and fine-grained access control . When a trigger generates an alert, the monitor configuration, the alert itself, and any notification that is sent to a channel may include metadata describing the index being queried. By design, the plugin must extract the data and store it as metadata outside of the index. Document-level security (DLS) and field-level security (FLS) access controls are designed to protect the data in the index. But once the data is stored outside the index as metadata, users with access to the monitor configurations, alerts, and their notifications will be able to view this metadata and possibly infer the contents and quality of data in the index, which would otherwise be concealed by DLS and FLS access control. To reduce the chances of unintended users viewing metadata that could describe an index, we recommend that administrators enable role-based access control and keep these kinds of design elements in mind when assigning permissions to the intended group of users. See Limit access by backend role for details. ",
    "url": "https://vagimeli.github.io/observing-your-data/alerting/security/#how-monitors-access-data",
    "relUrl": "/observing-your-data/alerting/security/#how-monitors-access-data"
  },"1229": {
    "doc": "Alerting security",
    "title": "(Advanced) Limit access by backend role",
    "content": "Out of the box, the alerting plugin has no concept of ownership. For example, if you have the cluster:admin/opensearch/alerting/monitor/write permission, you can edit all monitors, regardless of whether you created them. If a small number of trusted users manage your monitors and destinations, this lack of ownership generally isn’t a problem. A larger organization might need to segment access by backend role. First, make sure that your users have the appropriate backend roles. Backend roles usually come from an LDAP server or SAML provider. However, if you use the internal user database, you can use the REST API to add them manually with a create user operation. To add a backend role to a create user request, follow the Create user instructions in the Security plugin API documentation. Next, enable the following setting: . PUT _cluster/settings { \"transient\": { \"plugins.alerting.filter_by_backend_roles\": \"true\" } } . Now when users view alerting resources in OpenSearch Dashboards (or make REST API calls), they only see monitors and destinations that are created by users who share at least one backend role. For example, consider three users who all have full access to alerting: jdoe, jroe, and psantos. jdoe and jroe are on the same team at work and both have the analyst backend role. psantos has the human-resources backend role. If jdoe creates a monitor, jroe can see and modify it, but psantos can’t. If that monitor generates an alert, the situation is the same: jroe can see and acknowledge it, but psantos can’t. If psantos creates a destination, jdoe and jroe can’t see or modify it. Specify RBAC backend roles . You can specify role-based access control (RBAC) backend roles when you create or update a monitor with the Alerting API. In a create monitor scenario, follow these guidelines to specify roles: . | User type | Role is specified by user or not (Y/N) | How to use the RBAC roles | . | Admin user | Yes | Use all the specified backend roles to associate to the monitor. | . | Regular user | Yes | Use all the specified backend roles from the list of backend roles that the user has permission to use to associate with the monitor. | . | Regular user | No | Copy user’s backend roles and associate them to the monitor. | . In an update monitor scenario, follow these guidelines to specify roles: . | User type | Role is specified by user or not (Y/N) | How to use the RBAC roles | . | Admin user | Yes | Remove all the backend roles associate to the monitor and then use all the specified backend roles associated to the monitor. | . | Regular user | Yes | Remove backend roles associated to the monitor that the user has access to, but didn’t specify. Then add all the other specified backend roles from the list of backend roles that the user has permission to use to the monitor. | . | Regular user | No | Don’t update the backend roles on the monitor. | . | For admin users, an empty list is considered the same as removing all permissions that the user possesses. If a non-admin user passes in an empty list, that will throw an exception, because that is not allowed by non-admin users. | If the user tries to associate roles that they don’t have permission to use, it will throw an exception. | . To create an RBAC role, follow instructions in the Security plugin API documentation to Create role. Create a monitor with an RBAC role . When you create a monitor with the Alerting API, you can specify the RBAC roles at the bottom of the request body. Use the rbac_roles parameter. The following sample shows the RBAC roles specified by the RBAC parameter: ... \"rbac_roles\": [\"role1\", \"role2\"] } . To see a full request sample, see Create a monitor. ",
    "url": "https://vagimeli.github.io/observing-your-data/alerting/security/#advanced-limit-access-by-backend-role",
    "relUrl": "/observing-your-data/alerting/security/#advanced-limit-access-by-backend-role"
  },"1230": {
    "doc": "Management",
    "title": "Management",
    "content": " ",
    "url": "https://vagimeli.github.io/observing-your-data/alerting/settings/",
    "relUrl": "/observing-your-data/alerting/settings/"
  },"1231": {
    "doc": "Management",
    "title": "Alerting indices",
    "content": "The alerting feature creates several indices and one alias. The Security plugin demo script configures them as system indices for an extra layer of protection. Don’t delete these indices or modify their contents without using the alerting APIs. | Index | Purpose | . | .opendistro-alerting-alerts | Stores ongoing alerts. | . | .opendistro-alerting-alert-history-&lt;date&gt; | Stores a history of completed alerts. | . | .opendistro-alerting-config | Stores monitors, triggers, and destinations. Take a snapshot of this index to back up your alerting configuration. | . | .opendistro-alerting-alert-history-write (alias) | Provides a consistent URI for the .opendistro-alerting-alert-history-&lt;date&gt; index. | . All alerting indices are hidden by default. For a summary, make the following request: . GET _cat/indices?expand_wildcards=open,hidden . ",
    "url": "https://vagimeli.github.io/observing-your-data/alerting/settings/#alerting-indices",
    "relUrl": "/observing-your-data/alerting/settings/#alerting-indices"
  },"1232": {
    "doc": "Management",
    "title": "Alerting settings",
    "content": "We don’t recommend changing these settings; the defaults should work well for most use cases. All settings are available using the OpenSearch _cluster/settings API. None require a restart, and all can be marked persistent or transient. | Setting | Default | Description | . | plugins.scheduled_jobs.enabled | true | Whether the alerting plugin is enabled or not. If disabled, all monitors immediately stop running. | . | plugins.alerting.index_timeout | 60s | The timeout for creating monitors and destinations using the REST APIs. | . | plugins.alerting.request_timeout | 10s | The timeout for miscellaneous requests from the plugin. | . | plugins.alerting.action_throttle_max_value | 24h | The maximum amount of time you can set for action throttling. By default, this value displays as 1440 minutes in OpenSearch Dashboards. | . | plugins.alerting.input_timeout | 30s | How long the monitor can take to issue the search request. | . | plugins.alerting.bulk_timeout | 120s | How long the monitor can write alerts to the alert index. | . | plugins.alerting.alert_backoff_count | 3 | The number of retries for writing alerts before the operation fails. | . | plugins.alerting.alert_backoff_millis | 50ms | The amount of time to wait between retries—increases exponentially after each failed retry. | . | plugins.alerting.alert_history_rollover_period | 12h | How frequently to check whether the .opendistro-alerting-alert-history-write alias should roll over to a new history index and whether the Alerting plugin should delete any history indices. | . | plugins.alerting.move_alerts_backoff_millis | 250 | The amount of time to wait between retries—increases exponentially after each failed retry. | . | plugins.alerting.move_alerts_backoff_count | 3 | The number of retries for moving alerts to a deleted state after their monitor or trigger has been deleted. | . | plugins.alerting.monitor.max_monitors | 1000 | The maximum number of monitors users can create. | . | plugins.alerting.alert_history_max_age | 30d | The oldest document to store in the .opendistro-alert-history-&lt;date&gt; index before creating a new index. If the number of alerts in this time period does not exceed alert_history_max_docs, alerting creates one history index per period (e.g. one index every 30 days). | . | plugins.alerting.alert_history_max_docs | 1000 | The maximum number of alerts to store in the .opendistro-alert-history-&lt;date&gt; index before creating a new index. | . | plugins.alerting.alert_history_enabled | true | Whether to create .opendistro-alerting-alert-history-&lt;date&gt; indices. | . | plugins.alerting.alert_history_retention_period | 60d | The amount of time to keep history indices before automatically deleting them. | . | plugins.alerting.destination.allow_list | [“chime”, “slack”, “custom_webhook”, “email”, “test_action”] | The list of allowed destinations. If you don’t want to allow users to a certain type of destination, you can remove it from this list, but we recommend leaving this setting as-is. | . | plugins.alerting.filter_by_backend_roles | “false” | Restricts access to monitors by backend role. See Alerting security. | . | plugins.scheduled_jobs.sweeper.period | 5m | The alerting feature uses its “job sweeper” component to periodically check for new or updated jobs. This setting is the rate at which the sweeper checks to see if any jobs (monitors) have changed and need to be rescheduled. | . | plugins.scheduled_jobs.sweeper.page_size | 100 | The page size for the sweeper. You shouldn’t need to change this value. | . | plugins.scheduled_jobs.sweeper.backoff_millis | 50ms | The amount of time the sweeper waits between retries—increases exponentially after each failed retry. | . | plugins.scheduled_jobs.sweeper.retry_count | 3 | The total number of times the sweeper should retry before throwing an error. | . | plugins.scheduled_jobs.request_timeout | 10s | The timeout for the request that sweeps shards for jobs. | . ",
    "url": "https://vagimeli.github.io/observing-your-data/alerting/settings/#alerting-settings",
    "relUrl": "/observing-your-data/alerting/settings/#alerting-settings"
  },"1233": {
    "doc": "Application analytics",
    "title": "Application analytics",
    "content": "You can use application analytics to create custom observability applications to view the availability status of your systems, where you can combine log events with trace and metric data into a single view of overall system health. This lets you quickly pivot between logs, traces, and metrics to dig into the source of any issues. ",
    "url": "https://vagimeli.github.io/observing-your-data/app-analytics/",
    "relUrl": "/observing-your-data/app-analytics/"
  },"1234": {
    "doc": "Application analytics",
    "title": "Get started with application analytics",
    "content": "To get started, select the Menu button on the upper left corner of the OpenSearch Dashboards interface. Next, select Observability, and then choose Application analytics. Create an application . | Choose Create application. | Enter a name for your application and optionally add a description. | Do at least one of the following: | . | Use PPL to specify the base query. | . You can’t change the base query after the application is created. | Select services &amp; entities from the dropdown or the service map. | Select trace groups from the dropdown or the table. | . 4. Choose Create. Create a visualization . | Choose the Log Events tab. | Use PPL to build upon your base query. | Choose the Visualizations tab to see your visualizations. | Expand the Save dropdown menu, enter a name for your visualization, then choose Save. | . To see your visualizations, choose the Panel tab. Configure availability . Availability is the status of your application determined by availability levels set on a time series metric. To create an availability level, you must configure the following: . | color: The color of the availability badge on the home page. | name: The text in the availability badge on the home page. | expression: Comparison operator to determine the availability. | value: Value to use when calculating availability. | . By default, application analytics shows results from the last 24 hours of your data. To see data from a different time frame, use the date and time selector. Time series metric . A time series metric is any visualization that has a query that spans over a timestamp and is a line chart. You can then use PPL to define arbitrary conditions on your logs to create a visualization over time. Example . source = &lt;index_name&gt; | ... | ... | stats ... by span(&lt;timestamp_field&gt;, 1h) . Choose Line in visualization configurations to create a time series metric. ",
    "url": "https://vagimeli.github.io/observing-your-data/app-analytics/#get-started-with-application-analytics",
    "relUrl": "/observing-your-data/app-analytics/#get-started-with-application-analytics"
  },"1235": {
    "doc": "Event analytics",
    "title": "Event analytics",
    "content": "Event analytics in Observability is where you can use Piped Processing Language (PPL) queries to build and view different visualizations of your data. ",
    "url": "https://vagimeli.github.io/observing-your-data/event-analytics/",
    "relUrl": "/observing-your-data/event-analytics/"
  },"1236": {
    "doc": "Event analytics",
    "title": "Getting started with event analytics",
    "content": "To get started, choose Observability in OpenSearch Dashboards and then choose Event analytics. If you want to start exploring without adding any of your own data, choose Add samples, and Dashboards adds sample visualizations you can interact with. ",
    "url": "https://vagimeli.github.io/observing-your-data/event-analytics/#getting-started-with-event-analytics",
    "relUrl": "/observing-your-data/event-analytics/#getting-started-with-event-analytics"
  },"1237": {
    "doc": "Event analytics",
    "title": "Building a query",
    "content": "To generate custom visualizations, you must first specify a PPL query. OpenSearch Dashboards then automatically creates a visualization based on the results of your query. For example, the following PPL query returns a count of how many host addresses are currently in your data. source = opensearch_dashboards_sample_data_logs | fields host | stats count() . By default, Dashboards shows results from the last 15 minutes of your data. To see data from a different time frame, use the date and time selector. For more information about building PPL queries, see Piped Processing Language. ",
    "url": "https://vagimeli.github.io/observing-your-data/event-analytics/#building-a-query",
    "relUrl": "/observing-your-data/event-analytics/#building-a-query"
  },"1238": {
    "doc": "Event analytics",
    "title": "Saving a visualization",
    "content": "After Dashboards generates a visualization, you must save it if you want to return to it at a later time or if you want to add it to an operational panel. To save a visualization, expand the save dropdown menu next to Refresh, enter a name for your visualization, then choose Save. You can reopen any saved visualizations on the event analytics page. ",
    "url": "https://vagimeli.github.io/observing-your-data/event-analytics/#saving-a-visualization",
    "relUrl": "/observing-your-data/event-analytics/#saving-a-visualization"
  },"1239": {
    "doc": "Event analytics",
    "title": "Creating event analytics visualizations and adding them to dashboards",
    "content": "This feature is available in OpenSearch Dashboards version 2.7 and later. It works with new visualizations created in version 2.7 or later that use PPL to query data from OpenSearch or federated data sources such as Prometheus. Presenting your visualizations on a dashboard, instead of the event analytics page, makes it easier for users to understand and interpret the data at a glance. To create a PPL visualization, follow these steps: . | On the main menu, choose Visualize &gt; PPL. | In the Observability &gt; Logs &gt; Explorer window, enter the index source in the PPL query field, for example, source = opensearch_dashboards_sample_data_flights | stats count() by DestCountry. You must enter the query using PPL syntax. | Set the time filter, for example, This week, and then select Refresh. | Choose the visualization type, for example, Pie, from the right sidebar dropdown menu. | Select Save and enter a name for the visualization. | . You’ve created a new visualization that can be added to a new or existing dashboard. To add a PPL query to a dashboard, follow these steps: . | Select Dashboard from the main menu. | In the Dashboards window, select Create &gt; Dashboard. | In the Editing New Dashboard window, choose Add an existing. | In the Add panels window, choose PPL and select the visualization. It is now displayed on your dashboard. | Select Save and enter a name for the dashboard. | To add more visualizations to the dashboard, choose Select existing visualization and follow the steps above. Alternatively, choose Create new and then select PPL in the New Visualization window. You’ll return to the event analytics page and follow steps 1–6 in the preceding instructions. | . Limitations of event analytics visualizations . Event analytics visualizations currently do not support Dashboards Query Language (DQL) or query domain-specific language (DSL), and they do not use index patterns. Note the following limitations: . | Event analytics visualizations only use filters created using the dropdown interface. If you have DQL query or DSL filters in a dashboard, the visualizations do not use them. | The Dashboard filter dropdown interface only shows fields from the default index pattern or index patterns used by other visualizations in the same dashboard. | . ",
    "url": "https://vagimeli.github.io/observing-your-data/event-analytics/#creating-event-analytics-visualizations-and-adding-them-to-dashboards",
    "relUrl": "/observing-your-data/event-analytics/#creating-event-analytics-visualizations-and-adding-them-to-dashboards"
  },"1240": {
    "doc": "Event analytics",
    "title": "Viewing logs",
    "content": "The following are methods you can use to view logs. Correlating logs and traces . If you regularly track events across applications, you can correlate logs and traces. To view the correlation, you have to index the traces according to Open Telemetry standards (similar to trace analytics). Once you add a TraceId field to your logs, you can view the correlated trace information in the event explorer log details. This method lets you correlate logs and traces that correspond to the same execution context. Viewing surrounding events . If you want to know more about a log event you’re looking at, you can select View surrounding events to get a bigger picture of what was happening around the time of interest. Livestreaming logs . If you prefer watching events happen live, you can configure an interval so event analytics automatically refreshes the content. Live tail lets you stream logs live to OpenSearch observability event analytics based on the provided PPL query, as well as provide rich functionality such as filters. Doing so improves your debugging experience and lets you monitor your logs in real-time without having to manually refresh. You can also choose intervals and switch between them to dictate how often live tail should stream live logs. This feature is similar to the CLI’s tail -f command in that it only retrieves the most recent live logs by possibly eliminating a large portion of live logs. Live tail also provides you with the total count of live logs received by OpenSearch during the live stream, which you can use to better understand the incoming traffic. ",
    "url": "https://vagimeli.github.io/observing-your-data/event-analytics/#viewing-logs",
    "relUrl": "/observing-your-data/event-analytics/#viewing-logs"
  },"1241": {
    "doc": "Observability",
    "title": "Observability",
    "content": "OpenSearch Dashboards . Observability is collection of plugins and applications that let you visualize data-driven events by using Piped Processing Language to explore, discover, and query data stored in OpenSearch. Your experience of exploring data might differ, but if you’re new to exploring data to create visualizations, we recommend trying a workflow like the following: . | Explore data within a certain timeframe using Piped Processing Language. | Use event analytics to turn data-driven events into visualizations. | Create operational panels and add visualizations to compare data the way you like. | Use log analytics to transform unstructured log data. | Use trace analytics to create traces and dive deep into your data. | Leverage notebooks to combine different visualizations and code blocks that you can share with team members. | . ",
    "url": "https://vagimeli.github.io/observing-your-data/index/",
    "relUrl": "/observing-your-data/index/"
  },"1242": {
    "doc": "Log ingestion",
    "title": "Log Ingestion",
    "content": "Log ingestion provides a way to transform unstructured log data into structured data and ingest into OpenSearch. Structured log data allows for improved queries and filtering based on the data format when searching logs for an event. ",
    "url": "https://vagimeli.github.io/observing-your-data/log-ingestion/#log-ingestion",
    "relUrl": "/observing-your-data/log-ingestion/#log-ingestion"
  },"1243": {
    "doc": "Log ingestion",
    "title": "Get started with log ingestion",
    "content": "OpenSearch Log Ingestion consists of three components—Data Prepper, OpenSearch and OpenSearch Dashboards—that fit into the OpenSearch ecosystem. The Data Prepper repository has several sample applications to help you get started. Basic flow of data . | Log Ingestion relies on you adding log collection to your application’s environment to gather and send log data. (In the example below, FluentBit is used as a log collector that collects log data from a file and sends the log data to Data Prepper). | Data Prepper receives the log data, transforms the data into a structure format, and indexes it on an OpenSearch cluster. | The data can then be explored through OpenSearch search queries or the Discover page in OpenSearch Dashboards. | . Example . This example mimics the writing of log entries to a log file that are then processed by Data Prepper and stored in OpenSearch. Download or clone the Data Prepper repository. Then navigate to examples/log-ingestion/ and open docker-compose.yml in a text editor. This file contains a container for: . | Fluent Bit (fluent-bit) | Data Prepper (data-prepper) | A single-node OpenSearch cluster (opensearch) | OpenSearch Dashboards (opensearch-dashboards). | . Close the file and run docker-compose up --build to start the containers. After the containers start, your ingestion pipeline is set up and ready to ingest log data. The fluent-bit container is configured to read log data from test.log. Run the following command to generate log data to send to the log ingestion pipeline. echo '63.173.168.120 - - [04/Nov/2021:15:07:25 -0500] \"GET /search/tag/list HTTP/1.0\" 200 5003' &gt;&gt; test.log . Fluent-Bit will collect the log data and send it to Data Prepper: . [2021/12/02 15:35:41] [ info] [output:http:http.0] data-prepper:2021, HTTP status=200 200 OK . Data Prepper will process the log and index it: . 2021-12-02T15:35:44,499 [log-pipeline-processor-worker-1-thread-1] INFO com.amazon.dataprepper.pipeline.ProcessWorker - log-pipeline Worker: Processing 1 records from buffer . This should result in a single document being written to the OpenSearch cluster in the apache-logs index as defined in the log_pipeline.yaml file. Run the following command to see one of the raw documents in the OpenSearch cluster: . curl -X GET -u 'admin:admin' -k 'https://localhost:9200/apache_logs/_search?pretty&amp;size=1' . The response should show the parsed log data: . \"hits\" : [ { \"_index\" : \"apache_logs\", \"_type\" : \"_doc\", \"_id\" : \"yGrJe30BgI2EWNKtDZ1g\", \"_score\" : 1.0, \"_source\" : { \"date\" : 1.638459307042312E9, \"log\" : \"63.173.168.120 - - [04/Nov/2021:15:07:25 -0500] \\\"GET /search/tag/list HTTP/1.0\\\" 200 5003\", \"request\" : \"/search/tag/list\", \"auth\" : \"-\", \"ident\" : \"-\", \"response\" : \"200\", \"bytes\" : \"5003\", \"clientip\" : \"63.173.168.120\", \"verb\" : \"GET\", \"httpversion\" : \"1.0\", \"timestamp\" : \"04/Nov/2021:15:07:25 -0500\" } } ] . The same data can be viewed in OpenSearch Dashboards by visiting the Discover page and searching the apache_logs index. Remember, you must create the index in OpenSearch Dashboards if this is your first time searching for the index. ",
    "url": "https://vagimeli.github.io/observing-your-data/log-ingestion/#get-started-with-log-ingestion",
    "relUrl": "/observing-your-data/log-ingestion/#get-started-with-log-ingestion"
  },"1244": {
    "doc": "Log ingestion",
    "title": "Log ingestion",
    "content": " ",
    "url": "https://vagimeli.github.io/observing-your-data/log-ingestion/",
    "relUrl": "/observing-your-data/log-ingestion/"
  },"1245": {
    "doc": "Notebooks",
    "title": "Notebooks",
    "content": "An OpenSearch Dashboards notebook is an interface that lets you easily combine code snippets, live visualizations, and narrative text in a single notebook interface. Notebooks let you interactively explore data by running different visualizations that you can share with team members to collaborate on a project. A notebook is a document composed of two elements: code blocks (Markdown/SQL/PPL) and visualizations. Choose multiple timelines to compare and contrast visualizations. You can also generate reports directly from your notebooks. Common use cases include creating postmortem reports, designing runbooks, building live infrastructure reports, and writing documentation. Tenants in OpenSearch Dashboards are spaces for saving notebooks and other OpenSearch Dashboards objects. For more information, see OpenSearch Dashboards multi-tenancy. ",
    "url": "https://vagimeli.github.io/observing-your-data/notebooks/",
    "relUrl": "/observing-your-data/notebooks/"
  },"1246": {
    "doc": "Notebooks",
    "title": "Get started with notebooks",
    "content": "To get started, choose Notebooks within OpenSearch Dashboards. Step 1: Create a notebook . A notebook is an interface for creating reports. | Choose Create notebook and enter a descriptive name. | Choose Create. | . Choose Actions to rename, duplicate, or delete a notebook. Step 2: Add a paragraph . Paragraphs combine code blocks and visualizations for describing data. Add a code block . Code blocks support markdown, SQL, and PPL languages. Specify the input language on the first line using %[language type] syntax. For example, type %md for markdown, %sql for SQL, and %ppl for PPL. Sample markdown block . %md Add in text formatted in markdown. Sample SQL block . %sql Select * from opensearch_dashboards_sample_data_flights limit 20; . Sample PPL block . %ppl source=opensearch_dashboards_sample_data_logs | head 20 . Add a visualization . | To add a visualization, choose Add paragraph and select Visualization. | In Title, select your visualization and choose a date range. You can choose multiple timelines to compare and contrast visualizations. | To run and save a paragraph, choose Run. | . ",
    "url": "https://vagimeli.github.io/observing-your-data/notebooks/#get-started-with-notebooks",
    "relUrl": "/observing-your-data/notebooks/#get-started-with-notebooks"
  },"1247": {
    "doc": "Notebooks",
    "title": "Paragraph actions",
    "content": "You can perform the following actions on paragraphs: . | Add a new paragraph to the top of a report. | Add a new paragraph to the bottom of a report. | Run all the paragraphs at the same time. | Clear the outputs of all paragraphs. | Delete all the paragraphs. | . ",
    "url": "https://vagimeli.github.io/observing-your-data/notebooks/#paragraph-actions",
    "relUrl": "/observing-your-data/notebooks/#paragraph-actions"
  },"1248": {
    "doc": "Notebooks",
    "title": "Sample notebooks",
    "content": "We prepared the following sample notebooks that showcase a variety of use cases: . | Using SQL to query the OpenSearch Dashboards sample flight data. | Using PPL to query the OpenSearch Dashboards sample web logs data. | Using PPL and visualizations to perform sample root cause event analysis on the OpenSearch Dashboards sample web logs data. | . To add a sample notebook, choose Actions and select Add sample notebooks. ",
    "url": "https://vagimeli.github.io/observing-your-data/notebooks/#sample-notebooks",
    "relUrl": "/observing-your-data/notebooks/#sample-notebooks"
  },"1249": {
    "doc": "Notebooks",
    "title": "Create a report",
    "content": "You can use notebooks to create PNG and PDF reports: . | From the top menu bar, choose Reporting actions. | You can choose to Download PDF or Download PNG. Reports generate asynchronously in the background and might take a few minutes, depending on the size of the report. A notification appears when your report is ready to download. | To create a schedule-based report, choose Create report definition. For steps to create a report definition, see Create reports using a definition. | To see all your reports, choose View all reports. | . ",
    "url": "https://vagimeli.github.io/observing-your-data/notebooks/#create-a-report",
    "relUrl": "/observing-your-data/notebooks/#create-a-report"
  },"1250": {
    "doc": "API",
    "title": "Notifications API",
    "content": "If you want to programmatically define your notification channels and sources for versioning and reuse, you can use the Notifications REST API to define, configure, and delete notification channels and send test messages. . | List supported channel configurations | List all notification configurations | Create channel configuration | Get channel configuration | Update channel configuration | Delete channel configuration | Send test notification | . ",
    "url": "https://vagimeli.github.io/observing-your-data/notifications/api/#notifications-api",
    "relUrl": "/observing-your-data/notifications/api/#notifications-api"
  },"1251": {
    "doc": "API",
    "title": "List supported channel configurations",
    "content": "To retrieve a list of all supported notification configuration types, send a GET request to the features resource. Sample Request . GET /_plugins/_notifications/features . Sample Response . { \"allowed_config_type_list\" : [ \"slack\", \"chime\", \"webhook\", \"email\", \"sns\", \"ses_account\", \"smtp_account\", \"email_group\" ], \"plugin_features\" : { \"tooltip_support\" : \"true\" } } . ",
    "url": "https://vagimeli.github.io/observing-your-data/notifications/api/#list-supported-channel-configurations",
    "relUrl": "/observing-your-data/notifications/api/#list-supported-channel-configurations"
  },"1252": {
    "doc": "API",
    "title": "List all notification configurations",
    "content": "To retrieve a list of all notification configurations, send a GET request to the configs resource. Sample Request . GET _plugins/_notifications/configs . Sample Response . { \"start_index\" : 0, \"total_hits\" : 2, \"total_hit_relation\" : \"eq\", \"config_list\" : [ { \"config_id\" : \"sample-id\", \"last_updated_time_ms\" : 1652760532774, \"created_time_ms\" : 1652760532774, \"config\" : { \"name\" : \"Sample Slack Channel\", \"description\" : \"This is a Slack channel\", \"config_type\" : \"slack\", \"is_enabled\" : true, \"slack\" : { \"url\" : \"https://sample-slack-webhook\" } } }, { \"config_id\" : \"sample-id2\", \"last_updated_time_ms\" : 1652760735380, \"created_time_ms\" : 1652760735380, \"config\" : { \"name\" : \"Test chime channel\", \"description\" : \"A test chime channel\", \"config_type\" : \"chime\", \"is_enabled\" : true, \"chime\" : { \"url\" : \"https://sample-chime-webhook\" } } } ] } . To filter the notification configuration types this request returns, you can refine your query with the following optional path parameters. | Parameter | Description | . | config_id | Specifies the channel identifier. | . | config_id_list | Specifies a comma-separated list of channel IDs. | . | from_index | The starting index to search from. | . | max_items | The maximum amount of items to return in your request. | . | sort_order | Specifies the direction to sort results in. Valid options are asc and desc. | . | sort_field | Field to sort results with. | . | last_updated_time_ms | The Unix time in milliseconds of when the channel was last updated. | . | created_time_ms | The Unix time in milliseconds of when the channel was created. | . | is_enabled | Indicates whether the channel is enabled. | . | config_type | The channel type. Valid options are sns, slack, chime, webhook, smtp_account, ses_account, email_group, and email. | . | name | The channel’s name. | . | description | The channel’s description. | . | email.email_account_id | The sender email addresses the channel uses. | . | email.email_group_id_list | The email groups the channel uses. | . | email.recipient_list | The channel’s recipient list. | . | email_group.recipient_list | The channel’s list of email recipient groups. | . | smtp_account.method | The email encryption method. | . | slack.url | The Slack channel’s URL. | . | chime.url | The Amazon Chime connection’s URL. | . | webhook.url | The webhook’s URL. | . | smtp_account.host | The domain of the SMTP account. | . | smtp_account.from_address | The email account’s sender address. | . | smtp_account.method | The SMTP account’s encryption method. | . | sns.topic_arn | The Amazon Simple Notification Service (SNS) topic’s ARN. | . | sns.role_arn | The Amazon SNS topic’s role ARN. | . | ses_account.region | The Amazon Simple Email Service (SES) account’s AWS Region. | . | ses_account.role_arn | The Amazon SES account’s role ARN. | . | ses_account.from_address | The Amazon SES account’s sender email address. | . ",
    "url": "https://vagimeli.github.io/observing-your-data/notifications/api/#list-all-notification-configurations",
    "relUrl": "/observing-your-data/notifications/api/#list-all-notification-configurations"
  },"1253": {
    "doc": "API",
    "title": "Create channel configuration",
    "content": "To create a notification channel configuration, send a POST request to the configs resource. Sample Request . POST /_plugins/_notifications/configs/ { \"config_id\": \"sample-id\", \"name\": \"sample-name\", \"config\": { \"name\": \"Sample Slack Channel\", \"description\": \"This is a Slack channel\", \"config_type\": \"slack\", \"is_enabled\": true, \"slack\": { \"url\": \"https://sample-slack-webhook\" } } } . The create channel API operation accepts the following fields in its request body: . | Field | Data type | Description | Required | . | config_id | String | The configuration’s custom ID. | No | . | config | Object | Contains all relevant information, such as channel name, configuration type, and plugin source. | Yes | . | name | String | Name of the channel. | Yes | . | description | String | The channel’s description. | No | . | config_type | String | The destination of your notification. Valid options are sns, slack, chime, webhook, smtp_account, ses_account, email_group, and email. | Yes | . | is_enabled | Boolean | Indicates whether the channel is enabled for sending and receiving notifications. Default is true. | No | . The create channel operation accepts multiple config_types as possible notification destinations, so follow the format for your preferred config_type. \"sns\": { \"topic_arn\": \"&lt;arn&gt;\", \"role_arn\": \"&lt;arn&gt;\" //optional } \"slack\": { \"url\": \"https://sample-chime-webhoook\" } \"chime\": { \"url\": \"https://sample-amazon-chime-webhoook\" } \"webhook\": { \"url\": \"https://custom-webhook-test-url.com:8888/test-path?params1=value1&amp;params2=value2\" } \"smtp_account\": { \"host\": \"test-host.com\", \"port\": 123, \"method\": \"start_tls\", \"from_address\": \"test@email.com\" } \"ses_account\": { \"region\": \"us-east-1\", \"role_arn\": \"arn:aws:iam::012345678912:role/NotificationsSESRole\", \"from_address\": \"test@email.com\" } \"email_group\": { //Email recipient group \"recipient_list\": [ { \"recipient\": \"test-email1@test.com\" }, { \"recipient\": \"test-email2@test.com\" } ] } \"email\": { //The channel that sends emails \"email_account_id\": \"&lt;smtp or ses account config id&gt;\", \"recipient_list\": [ { \"recipient\": \"custom.email@test.com\" } ], \"email_group_id_list\": [] } . The following example demonstrates how to create a channel using email as a config_type: . POST /_plugins/_notifications/configs/ { \"id\": \"sample-email-id\", \"name\": \"sample-name\", \"config\": { \"name\": \"Sample Email Channel\", \"description\": \"Sample email description\", \"config_type\": \"email\", \"is_enabled\": true, \"email\": { \"email_account_id\": \"&lt;email_account_id&gt;\", \"recipient_list\": [ \"sample@email.com\" ] } } } . Sample Response . { \"config_id\" : \"&lt;config_id&gt;\" } . ",
    "url": "https://vagimeli.github.io/observing-your-data/notifications/api/#create-channel-configuration",
    "relUrl": "/observing-your-data/notifications/api/#create-channel-configuration"
  },"1254": {
    "doc": "API",
    "title": "Get channel configuration",
    "content": "To get a channel configuration by config_id, send a GET request and specify the config_id as a path parameter. Sample Request . GET _plugins/_notifications/configs/&lt;config_id&gt; . Sample Response . { \"start_index\" : 0, \"total_hits\" : 1, \"total_hit_relation\" : \"eq\", \"config_list\" : [ { \"config_id\" : \"sample-id\", \"last_updated_time_ms\" : 1652760532774, \"created_time_ms\" : 1652760532774, \"config\" : { \"name\" : \"Sample Slack Channel\", \"description\" : \"This is a Slack channel\", \"config_type\" : \"slack\", \"is_enabled\" : true, \"slack\" : { \"url\" : \"https://sample-slack-webhook\" } } } ] } . ",
    "url": "https://vagimeli.github.io/observing-your-data/notifications/api/#get-channel-configuration",
    "relUrl": "/observing-your-data/notifications/api/#get-channel-configuration"
  },"1255": {
    "doc": "API",
    "title": "Update channel configuration",
    "content": "To update a channel configuration, send a POST request to the configs resource and specify the channel’s config_id as a path parameter. Specify the new configuration details in the request body. Sample Request . PUT _plugins/_notifications/configs/&lt;config_id&gt; { \"config\": { \"name\": \"Slack Channel\", \"description\": \"This is an updated channel configuration\", \"config_type\": \"slack\", \"is_enabled\": true, \"slack\": { \"url\": \"https://hooks.slack.com/sample-url\" } } } . Sample Response . { \"config_id\" : \"&lt;config_id&gt;\" } . ",
    "url": "https://vagimeli.github.io/observing-your-data/notifications/api/#update-channel-configuration",
    "relUrl": "/observing-your-data/notifications/api/#update-channel-configuration"
  },"1256": {
    "doc": "API",
    "title": "Delete channel configuration",
    "content": "To delete a channel configuration, send a DELETE request to the configs resource and specify the config_id as a path parameter. Sample Request . DELETE /_plugins/_notifications/configs/&lt;config_id&gt; . Sample Response . { \"delete_response_list\" : { \"&lt;config_id&gt;\" : \"OK\" } } . You can also submit a comma-separated list of channel IDs you want to delete, and OpenSearch deletes all of the specified notification channels. Sample Request . DELETE /_plugins/_notifications/configs/?config_id_list=&lt;config_id1&gt;,&lt;config_id2&gt;,&lt;config_id3&gt;... Sample Response . { \"delete_response_list\" : { \"&lt;config_id1&gt;\" : \"OK\", \"&lt;config_id2&gt;\" : \"OK\", \"&lt;config_id3&gt;\" : \"OK\" } } . ",
    "url": "https://vagimeli.github.io/observing-your-data/notifications/api/#delete-channel-configuration",
    "relUrl": "/observing-your-data/notifications/api/#delete-channel-configuration"
  },"1257": {
    "doc": "API",
    "title": "Send test notification",
    "content": "To send a test notification, send a GET request to /feature/test/ and specify the channel configuration’s config_id as a path parameter. Sample Request . GET _plugins/_notifications/feature/test/&lt;config_id&gt; . Sample Response . { \"event_source\" : { \"title\" : \"Test Message Title-0Jnlh4ABa4TCWn5C5H2G\", \"reference_id\" : \"0Jnlh4ABa4TCWn5C5H2G\", \"severity\" : \"info\", \"tags\" : [ ] }, \"status_list\" : [ { \"config_id\" : \"0Jnlh4ABa4TCWn5C5H2G\", \"config_type\" : \"slack\", \"config_name\" : \"sample-id\", \"email_recipient_status\" : [ ], \"delivery_status\" : { \"status_code\" : \"200\", \"status_text\" : \"\"\"&lt;!doctype html&gt; &lt;html&gt; &lt;head&gt; &lt;/head&gt; &lt;body&gt; &lt;div&gt; &lt;h1&gt;Example Domain&lt;/h1&gt; &lt;p&gt;Sample paragraph.&lt;/p&gt; &lt;p&gt;&lt;a href=\"sample.example.com\"&gt;TO BE OR NOT TO BE, THAT IS THE QUESTION&lt;/a&gt;&lt;/p&gt; &lt;/div&gt; &lt;/body&gt; &lt;/html&gt; \"\"\" } } ] } . ",
    "url": "https://vagimeli.github.io/observing-your-data/notifications/api/#send-test-notification",
    "relUrl": "/observing-your-data/notifications/api/#send-test-notification"
  },"1258": {
    "doc": "API",
    "title": "API",
    "content": " ",
    "url": "https://vagimeli.github.io/observing-your-data/notifications/api/",
    "relUrl": "/observing-your-data/notifications/api/"
  },"1259": {
    "doc": "Notifications",
    "title": "Notifications",
    "content": "The Notifications plugin provides a central location for all of your notifications from OpenSearch plugins. Using the plugin, you can configure which communication service you want to use and see relevant statistics and troubleshooting information. Currently, the Alerting and ISM plugins have integrated with the Notifications plugin. You can use either OpenSearch Dashboards or the REST API to configure notifications. Dashboards offers a more organized way of selecting a channel type and selecting which OpenSearch plugin sources you want to use, whereas the REST API lets you programmatically define your notification channels for better versioning and reuse later on. | Use the Dashboards UI to first create a channel that receives notifications from other plugins. Supported communication channels include Amazon Chime, Amazon Simple Notification Service (Amazon SNS), Amazon Simple Email Service (Amazon SES), email through SMTP, Slack, and custom webhooks. After you’ve configured your channel and plugin sources, send messages and start tracking your notifications from the Notifications plugin’s dashboard. | Use the Notifications REST API to configure all of your channel’s settings. To use the API, you must have your notification’s name, description, channel type, which OpenSearch plugins to use as sources, and other associated URLs or groups. | . ",
    "url": "https://vagimeli.github.io/observing-your-data/notifications/index/",
    "relUrl": "/observing-your-data/notifications/index/"
  },"1260": {
    "doc": "Notifications",
    "title": "Create a channel",
    "content": "In OpenSearch Dashboards, choose Notifications, Channels, and Create channel. | In the Name and description section, specify a name and optional description for your channel. | In the Configurations section, select the channel type and enter the necessary information for each type. For more information about configuring a channel that uses Amazon SNS or email, refer to the sections below. If you want to use Amazon Chime or Slack, you need to specify the webhook URL. For more information about using webhooks, see the documentation for Slack and Amazon Chime. | . If you want to use custom webhooks, you must specify more information: parameters and headers. For example, if your endpoint requires basic authentication, you might need to add a header with an authorization key and a value of Basic &lt;Base64-encoded-credential-string&gt;. You might also need to change Content-Type to whatever your webhook requires. Popular values are application/json, application/xml, and text/plain. This information is stored in plain text in the OpenSearch cluster. We will improve this design in the future, but for now, the encoded credentials (which are neither encrypted nor hashed) might be visible to other OpenSearch users. | In the Availability section, select the OpenSearch plugins you want to use with the notification channel. | Choose Create. | . Amazon SNS as a channel type . OpenSearch supports Amazon SNS for notifications. This integration with Amazon SNS means that, in addition to the other channel types, the Notifications plugin can send email messages, text messages, and even run AWS Lambda functions using SNS topics. For more information about Amazon SNS, see the Amazon Simple Notification Service Developer Guide. The Notifications plugin currently supports two ways to authenticate users: . | Provide the user with full access to Amazon SNS. | Let the user assume an AWS Identity and Access Management (IAM) role that has permissions to access Amazon SNS. Once you configure the notification channel to use the right Amazon SNS permissions, select the OpenSearch plugins that can trigger notifications. | . Provide full Amazon SNS access permissions . If you want to provide full Amazon SNS access to the IAM user, ensure that the user has the following permissions: . { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": [ \"sns:*\" ], \"Effect\": \"Allow\", \"Resource\": \"*\" } ] } . Assuming an IAM role with Amazon SNS permissions . If you want to let the user send notifications without directly having full permissions to Amazon SNS, let the user assume a role that does have the necessary permissions. The IAM user must have the following permissions to assume a role: . { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"ec2:Describe*\", \"iam:ListRoles\", \"sts:AssumeRole\" ], \"Resource\": \"*\" } ] } . Then add this policy into the IAM user’s trust relationship to actually assume the role: . { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::&lt;arn_number&gt;:user/&lt;iam_username&gt;\", }, \"Action\": \"sts:AssumeRole\" } ] } . ",
    "url": "https://vagimeli.github.io/observing-your-data/notifications/index/#create-a-channel",
    "relUrl": "/observing-your-data/notifications/index/#create-a-channel"
  },"1261": {
    "doc": "Notifications",
    "title": "Email as a channel type",
    "content": "To send or receive notifications with email, choose Email as the channel type. Next, select at least one sender and default recipient. To send notifications to more than a few people at a time, specify multiple email addresses or select a recipient group. If the Notifications plugin doesn’t currently have the necessary senders or groups, you can add them by first selecting SMTP sender and then choosing Create SMTP sender or Create recipient group. Choose SES sender to use Amazon Simple Email Service (Amazon SES). Create email sender . | Specify a unique name to associate with the sender. | Enter an email address and, if applicable, its host (for example, smtp.gmail.com) and the port. If you’re using Amazon SES, enter the IAM role Amazon Resource Name (ARN) of the AWS account to send notifications from, along with the AWS Region. | Choose an encryption method. Most email providers require Secure Sockets Layer (SSL) or Transport Layer Security (TLS), which require a user name and password in the OpenSearch keystore. See Authenticate sender account to learn more. Selecting an encryption method is only applicable if you’re creating an SMTP sender. | Choose Create to save the configuration and create the sender. You can create a sender before you add your credentials to the OpenSearch keystore; however, you must authenticate each sender account before you use the sender in your channel configuration. | . Create email recipient group . | After choosing Create recipient group, enter a unique name to associate with the email group and an optional description. | Select or enter the email addresses you want to add to the recipient group. | Choose Create. | . Authenticate sender account . If your email provider requires SSL or TLS, you must authenticate each sender account before you can send an email. Enter the sender account credentials in the OpenSearch keystore using the command line interface (CLI). Run the following commands (in your OpenSearch directory) to enter your user name and password. The &lt;sender_name&gt; is the name you entered for Sender earlier. opensearch.notifications.core.email.&lt;sender_name&gt;.username opensearch.notifications.core.email.&lt;sender_name&gt;.password . To change or update your credentials (after you’ve added them to the keystore on every node), call the reload API to automatically update those credentials without restarting OpenSearch. POST _nodes/reload_secure_settings { \"secure_settings_password\": \"1234\" } . ",
    "url": "https://vagimeli.github.io/observing-your-data/notifications/index/#email-as-a-channel-type",
    "relUrl": "/observing-your-data/notifications/index/#email-as-a-channel-type"
  },"1262": {
    "doc": "Observability security",
    "title": "Observability security",
    "content": "You can use the Security plugin with Observability in OpenSearch to limit non-admin users to specific actions. For example, you might want some users to only view visualizations, notebooks, and other Observability objects, while others can create and modify them. ",
    "url": "https://vagimeli.github.io/observing-your-data/observability-security/",
    "relUrl": "/observing-your-data/observability-security/"
  },"1263": {
    "doc": "Observability security",
    "title": "Basic permissions",
    "content": "The Security plugin has two built-in roles that cover most Observability use cases: observability_full_access and observability_read_access. For descriptions of each, see Predefined roles. If you don’t see these predefined roles in OpenSearch Dashboards, you can create them with the following commands: . PUT _plugins/_security/api/roles/observability_read_access { \"cluster_permissions\": [ \"cluster:admin/opensearch/observability/get\" ] } . PUT _plugins/_security/api/roles/observability_full_access { \"cluster_permissions\": [ \"cluster:admin/opensearch/observability/*\" ] } . If these roles don’t meet your needs, mix and match individual Observability permissions to suit your use case. For example, the cluster:admin/opensearch/observability/create permission lets you create Observability objects (visualizations, operational panels, notebooks, etc.). The following is an example role that provides access to Observability: . PUT _plugins/_security/api/roles/observability_permissions { \"cluster_permissions\": [ \"cluster:admin/opensearch/observability/create\", \"cluster:admin/opensearch/observability/update\", \"cluster:admin/opensearch/observability/delete\", \"cluster:admin/opensearch/observability/get\" ], \"index_permissions\": [{ \"index_patterns\": [\".opensearch-observability\"], \"allowed_actions\": [\"write\", \"read\", \"search\"] }], \"tenant_permissions\": [{ \"tenant_patterns\": [\"global_tenant\"], \"allowed_actions\": [\"opensearch_dashboards_all_write\"] }] } . ",
    "url": "https://vagimeli.github.io/observing-your-data/observability-security/#basic-permissions",
    "relUrl": "/observing-your-data/observability-security/#basic-permissions"
  },"1264": {
    "doc": "Operational panels",
    "title": "Operational panels",
    "content": "Operational panels in OpenSearch Dashboards are collections of visualizations generated using Piped Processing Language (PPL) queries. ",
    "url": "https://vagimeli.github.io/observing-your-data/operational-panels/",
    "relUrl": "/observing-your-data/operational-panels/"
  },"1265": {
    "doc": "Operational panels",
    "title": "Get started with operational panels",
    "content": "If you want to start using operational panels without adding any data, expand the Action menu, choose Add samples, and Dashboards adds a set of operational panels with saved visualizations for you to explore. ",
    "url": "https://vagimeli.github.io/observing-your-data/operational-panels/#get-started-with-operational-panels",
    "relUrl": "/observing-your-data/operational-panels/#get-started-with-operational-panels"
  },"1266": {
    "doc": "Operational panels",
    "title": "Create an operational panel",
    "content": "To create an operational panel and add visualizations: . | From the Add Visualization dropdown menu, choose Select Existing Visualization or Create New Visualization, which takes you to the event analytics explorer, where you can use PPL to create visualizations. | If you’re adding already existing visualizations, choose a visualization from the dropdown menu. | Choose Add. | . To search for a particular visualization in your operation panels, use PPL queries to search for data you’ve already added to your panel. ",
    "url": "https://vagimeli.github.io/observing-your-data/operational-panels/#create-an-operational-panel",
    "relUrl": "/observing-your-data/operational-panels/#create-an-operational-panel"
  },"1267": {
    "doc": "Metrics analytics",
    "title": "Metrics analytics",
    "content": "Introduced 2.4 . Starting with OpenSearch 2.4, you can ingest and visualize metric data from log data aggregated within OpenSearch, allowing you to analyze and correlate data across logs, traces, and metrics. Previously, you could ingest and visualize only logs and traces from your monitored environments. With this feature, you can observe your digital assets with more granularity, gain deeper insight into the health of your infrastructure, and better inform your root cause analysis. The following image shows the process of ingesting metrics from Prometheus and visualizing them in a dashboard. ",
    "url": "https://vagimeli.github.io/observing-your-data/prometheusmetrics/",
    "relUrl": "/observing-your-data/prometheusmetrics/"
  },"1268": {
    "doc": "Metrics analytics",
    "title": "Configuring a data source connection from Prometheus to OpenSearch",
    "content": "You can view metrics collected from Prometheus in OpenSearch Dashboards by first creating a connection from Prometheus to OpenSearch using the SQL plugin. To configure a connection to Prometheus, create a file on your OpenSearch nodes named datasources.json containing the Prometheus data source settings. The following examples demonstrate the various Prometheus data source configurations using different authentication methods. No authentication: . [{ \"name\" : \"my_prometheus\", \"connector\": \"prometheus\", \"properties\" : { \"prometheus.uri\" : \"http://localhost:9090\" } }] . Basic authentication: . [{ \"name\" : \"my_prometheus\", \"connector\": \"prometheus\", \"properties\" : { \"prometheus.uri\" : \"http://localhost:9090\", \"prometheus.auth.type\" : \"basicauth\", \"prometheus.auth.username\" : \"admin\", \"prometheus.auth.password\" : \"admin\" } }] . AWS SigV4 authentication: . [{ \"name\" : \"my_prometheus\", \"connector\": \"prometheus\", \"properties\" : { \"prometheus.uri\" : \"http://localhost:8080\", \"prometheus.auth.type\" : \"awssigv4\", \"prometheus.auth.region\" : \"us-east-1\", \"prometheus.auth.access_key\" : \"\" \"prometheus.auth.secret_key\" : \"\" } }] . After configuring Prometheus in the datasources.json file, run the following command to load the configuration into the OpenSearch keystore. The configuration is securely stored in the keystore because it contains sensitive credential information. bin/opensearch-keystore add-file plugins.query.federation.datasources.config datasources.json . If you are updating the keystore during runtime, refresh the keystore using following API command: . POST /_nodes/reload_secure_settings { \"secure_settings_password\":\"\" } . copy . After configuring the connection from Prometheus to OpenSearch, Prometheus metrics are displayed in Dashboards in the Observability &gt; Metrics analytics window, as shown in the following image. For more information, see the Prometheus Connector GitHub page. ",
    "url": "https://vagimeli.github.io/observing-your-data/prometheusmetrics/#configuring-a-data-source-connection-from-prometheus-to-opensearch",
    "relUrl": "/observing-your-data/prometheusmetrics/#configuring-a-data-source-connection-from-prometheus-to-opensearch"
  },"1269": {
    "doc": "Metrics analytics",
    "title": "Creating visualizations based on metrics",
    "content": "You can create visualizations based on Prometheus metrics and other metrics collected by your OpenSearch cluster. To create a visualization, do the following: . | In Observability &gt; Metrics analytics &gt; Available Metrics, select the metrics you would like to include in your visualization. | These visualizations can now be saved. | From the Metrics analytics window, select Save. | When prompted for a Custom operational dashboards/application, choose one of the available options. | Optionally, you can edit the predefined name values under the Metric Name fields to suit your needs. | Select Save. | . The following image shows an example of the visualizations that are displayed in the Observability &gt; Metrics analytics window. ",
    "url": "https://vagimeli.github.io/observing-your-data/prometheusmetrics/#creating-visualizations-based-on-metrics",
    "relUrl": "/observing-your-data/prometheusmetrics/#creating-visualizations-based-on-metrics"
  },"1270": {
    "doc": "Metrics analytics",
    "title": "Defining PPL queries for use with Prometheus",
    "content": "You can define Piped Processing Language (PPL) queries against metrics collected by Prometheus. The following example shows a metric-selecting query with specific dimensions: . source = my_prometheus.prometheus_http_requests_total | stats avg(@value) by span(@timestamp,15s), handler, code . Additionally, you can create a custom visualization by performing the following steps: . | From the Events Analytics window, enter your PPL query and select Refresh. The Explorer page is now displayed. | From the Explorer page, select Save. | When prompted for a Custom operational dashboards/application, choose one of the available options. | Optionally, you can edit the predefined name values under the Metric Name fields to suit your needs. | Optionally, you can choose to save the visualization as a metric. | Select Save. | . Note: Only queries that include a time-series visualization and stats/span can be saved as a metric, as shown in the following image. ",
    "url": "https://vagimeli.github.io/observing-your-data/prometheusmetrics/#defining-ppl-queries-for-use-with-prometheus",
    "relUrl": "/observing-your-data/prometheusmetrics/#defining-ppl-queries-for-use-with-prometheus"
  },"1271": {
    "doc": "Simple Schema for Observability",
    "title": "Simple Schema for Observability",
    "content": "Introduced 2.6 . OpenSearch 2.6 introduced a standardization for conforming to a common and unified observability schema: Simple Schema for Observability (SSFO). Observability is a collection of plugins and applications that let you visualize data-driven events by using Piped Processing Language (PPL) to explore and query data stored in OpenSearch. With the schema in place, Observability tools can ingest, automatically extract, and aggregate data and create custom dashboards, making it easier to understand the system at a higher level. SSFO is inspired by both OpenTelemetry and the Elastic Common Schema (ECS) and uses Amazon Elastic Container Service (Amazon ECS) event logs and OpenTelemetry metadata. Alerts will be supported in a future release. ",
    "url": "https://vagimeli.github.io/observing-your-data/ssfo/",
    "relUrl": "/observing-your-data/ssfo/"
  },"1272": {
    "doc": "Simple Schema for Observability",
    "title": "Use cases",
    "content": "Use cases for SSFO include: . | Ingesting observability data from different data types. | Moving from proprietary configurations that are non-transferable to a consolidated, sharable observability solution that allows users to ingest and display an analysis of any type of telemetry data from any type of provider. | . Data Prepper conforms to the SSFO schema for metrics and will gradually support traces and logs. Data Prepper’s trace mapping currently provides service-map data in a different way than SSFO traces. To make the trace mapping compatible with Observability, it will be integrated with the SSFO traces schema and will introduce service-map as an enriched field. ",
    "url": "https://vagimeli.github.io/observing-your-data/ssfo/#use-cases",
    "relUrl": "/observing-your-data/ssfo/#use-cases"
  },"1273": {
    "doc": "Simple Schema for Observability",
    "title": "Traces and metrics",
    "content": "Schema definitions for traces and metrics are defined and supported by the Observability plugin. These schema definitions include: . | The index structure (mapping). | The index naming conventions. | A JSON schema for enforcement and validation of the structure. | The integration feature for adding preconfigured dashboards and assets. | . ",
    "url": "https://vagimeli.github.io/observing-your-data/ssfo/#traces-and-metrics",
    "relUrl": "/observing-your-data/ssfo/#traces-and-metrics"
  },"1274": {
    "doc": "Getting Started",
    "title": "Getting started with Trace Analytics",
    "content": "OpenSearch Trace Analytics consists of two components—Data Prepper and the Trace Analytics OpenSearch Dashboards plugin—that fit into the OpenTelemetry and OpenSearch ecosystems. The Data Prepper repository has several sample applications to help you get started. ",
    "url": "https://vagimeli.github.io/observing-your-data/trace/getting-started/#getting-started-with-trace-analytics",
    "relUrl": "/observing-your-data/trace/getting-started/#getting-started-with-trace-analytics"
  },"1275": {
    "doc": "Getting Started",
    "title": "Basic flow of data",
    "content": ". | Trace Analytics relies on you adding instrumentation to your application and generating trace data. The OpenTelemetry documentation contains example applications for many programming languages that can help you get started, including Java, Python, Go, and JavaScript. (In the Jaeger HotROD example below, an extra component, the Jaeger agent, runs alongside the application and sends the data to the OpenTelemetry Collector, but the concept is similar.) . | The OpenTelemetry Collector receives data from the application and formats it into OpenTelemetry data. | Data Prepper processes the OpenTelemetry data, transforms it for use in OpenSearch, and indexes it on an OpenSearch cluster. | The Trace Analytics OpenSearch Dashboards plugin displays the data in near real-time as a series of charts and tables, with an emphasis on service architecture, latency, error rate, and throughput. | . ",
    "url": "https://vagimeli.github.io/observing-your-data/trace/getting-started/#basic-flow-of-data",
    "relUrl": "/observing-your-data/trace/getting-started/#basic-flow-of-data"
  },"1276": {
    "doc": "Getting Started",
    "title": "Jaeger HotROD",
    "content": "One Trace Analytics sample application is the Jaeger HotROD demo, which mimics the flow of data through a distributed application. Download or clone the Data Prepper repository. Then navigate to examples/jaeger-hotrod/ and open docker-compose.yml in a text editor. This file contains a container for each element from Basic flow of data: . | A distributed application (jaeger-hot-rod) with the Jaeger agent (jaeger-agent) | The OpenTelemetry Collector (otel-collector) | Data Prepper (data-prepper) | A single-node OpenSearch cluster (opensearch) | OpenSearch Dashboards (opensearch-dashboards). | . Close the file and run docker-compose up --build. After the containers start, navigate to http://localhost:8080 in a web browser. Click one of the buttons in the web interface to send a request to the application. Each request starts a series of operations across the services that make up the application. From the console logs, you can see that these operations share the same trace-id, which lets you track all of the operations in the request as a single trace: . jaeger-hot-rod | http://0.0.0.0:8081/customer?customer=392 jaeger-hot-rod | 2020-11-19T16:29:53.425Z INFO frontend/server.go:92 HTTP request received {\"service\": \"frontend\", \"trace_id\": \"12091bd60f45ea2c\", \"span_id\": \"12091bd60f45ea2c\", \"method\": \"GET\", \"url\": \"/dispatch?customer=392&amp;nonse=0.6509021735471818\"} jaeger-hot-rod | 2020-11-19T16:29:53.426Z INFO customer/client.go:54 Getting customer{\"service\": \"frontend\", \"component\": \"customer_client\", \"trace_id\": \"12091bd60f45ea2c\", \"span_id\": \"12091bd60f45ea2c\", \"customer_id\": \"392\"} jaeger-hot-rod | 2020-11-19T16:29:53.430Z INFO customer/server.go:67 HTTP request received {\"service\": \"customer\", \"trace_id\": \"12091bd60f45ea2c\", \"span_id\": \"252ff7d0e1ac533b\", \"method\": \"GET\", \"url\": \"/customer?customer=392\"} jaeger-hot-rod | 2020-11-19T16:29:53.430Z INFO customer/database.go:73 Loading customer{\"service\": \"customer\", \"component\": \"mysql\", \"trace_id\": \"12091bd60f45ea2c\", \"span_id\": \"252ff7d0e1ac533b\", \"customer_id\": \"392\"} . These operations also have a span_id. Spans are units of work from a single service. Each trace contains some number of spans. Shortly after the application starts processing the request, you can see the OpenTelemetry Collector starts exporting the spans: . otel-collector | 2020-11-19T16:29:53.781Z INFO loggingexporter/logging_exporter.go:296 TraceExporter {\"#spans\": 1} otel-collector | 2020-11-19T16:29:53.787Z INFO loggingexporter/logging_exporter.go:296 TraceExporter {\"#spans\": 3} . Then Data Prepper processes the data from the OpenTelemetry Collector and indexes it: . data-prepper | 1031918 [service-map-pipeline-process-worker-2-thread-1] INFO com.amazon.dataprepper.pipeline.ProcessWorker – service-map-pipeline Worker: Processing 3 records from buffer data-prepper | 1031923 [entry-pipeline-process-worker-1-thread-1] INFO com.amazon.dataprepper.pipeline.ProcessWorker – entry-pipeline Worker: Processing 1 records from buffer . Finally, you can see the OpenSearch node responding to the indexing request. node-0.example.com | [2020-11-19T16:29:55,064][INFO ][o.e.c.m.MetadataMappingService] [9fb4fb37a516] [otel-v1-apm-span-000001/NGYbmVD9RmmqnxjfTzBQsQ] update_mapping [_doc] node-0.example.com | [2020-11-19T16:29:55,267][INFO ][o.e.c.m.MetadataMappingService] [9fb4fb37a516] [otel-v1-apm-span-000001/NGYbmVD9RmmqnxjfTzBQsQ] update_mapping [_doc] . In a new terminal window, run the following command to see one of the raw documents in the OpenSearch cluster: . curl -X GET -u 'admin:admin' -k 'https://localhost:9200/otel-v1-apm-span-000001/_search?pretty&amp;size=1' . Navigate to http://localhost:5601 in a web browser and choose Trace Analytics. You can see the results of your single click in the Jaeger HotROD web interface: the number of traces per API and HTTP method, latency trends, a color-coded map of the service architecture, and a list of trace IDs that you can use to drill down on individual operations. If you don’t see your trace, adjust the timeframe in OpenSearch Dashboards. For more information on using the plugin, see OpenSearch Dashboards plugin. ",
    "url": "https://vagimeli.github.io/observing-your-data/trace/getting-started/#jaeger-hotrod",
    "relUrl": "/observing-your-data/trace/getting-started/#jaeger-hotrod"
  },"1277": {
    "doc": "Getting Started",
    "title": "Getting Started",
    "content": " ",
    "url": "https://vagimeli.github.io/observing-your-data/trace/getting-started/",
    "relUrl": "/observing-your-data/trace/getting-started/"
  },"1278": {
    "doc": "Trace Analytics",
    "title": "Trace Analytics",
    "content": "Trace Analytics provides a way to ingest and visualize OpenTelemetry data in OpenSearch. This data can help you find and fix performance problems in distributed applications. A single operation, such as a user choosing a button, can trigger an extended series of events. The frontend might call a backend service, which calls another service, which queries a database, processes the data, and sends it to the original service, which sends a confirmation to the frontend. Trace Analytics can help you visualize this flow of events and identify performance problems, as shown in the following image. ",
    "url": "https://vagimeli.github.io/observing-your-data/trace/index/",
    "relUrl": "/observing-your-data/trace/index/"
  },"1279": {
    "doc": "Trace Analytics",
    "title": "Trace Analytics with Jaeger data",
    "content": "Trace Analytics supports Jaeger trace data in the OpenSearch Observability plugin. If you use OpenSearch as the backend for Jaeger trace data, you can use the built-in Trace Analytics capabilities. To set up your environment to use Trace Analytics, see Analyze Jaeger trace data. ",
    "url": "https://vagimeli.github.io/observing-your-data/trace/index/#trace-analytics-with-jaeger-data",
    "relUrl": "/observing-your-data/trace/index/#trace-analytics-with-jaeger-data"
  },"1280": {
    "doc": "OpenSearch Dashboards plugin",
    "title": "Trace Analytics OpenSearch Dashboards plugin",
    "content": "The Trace Analytics plugin for OpenSearch Dashboards provides at-a-glance visibility into your application performance, along with the ability to drill down on individual traces. For installation instructions, see Standalone OpenSearch Dashboards plugin install. The Dashboard view groups traces together by HTTP method and path so that you can see the average latency, error rate, and trends associated with a particular operation. For a more focused view, try filtering by trace group name. To drill down on the traces that make up a trace group, choose the number of traces in righthand column. Then choose an individual trace for a detailed summary. The Services view lists all services in the application, plus an interactive map that shows how the various services connect to each other. In contrast to the dashboard, which helps identify problems by operation, the service map helps identify problems by service. Try sorting by error rate or latency to get a sense of potential problem areas of your application. ",
    "url": "https://vagimeli.github.io/observing-your-data/trace/ta-dashboards/#trace-analytics-opensearch-dashboards-plugin",
    "relUrl": "/observing-your-data/trace/ta-dashboards/#trace-analytics-opensearch-dashboards-plugin"
  },"1281": {
    "doc": "OpenSearch Dashboards plugin",
    "title": "OpenSearch Dashboards plugin",
    "content": " ",
    "url": "https://vagimeli.github.io/observing-your-data/trace/ta-dashboards/",
    "relUrl": "/observing-your-data/trace/ta-dashboards/"
  },"1282": {
    "doc": "Analyzing Jaeger trace data",
    "title": "Analyzing Jaeger trace data",
    "content": "Introduced 2.5 . The trace analytics functionality in the OpenSearch Observability plugin now supports Jaeger trace data. If you use OpenSearch as the backend for Jaeger trace data, you can use the built-in trace analytics capabilities. This provides support for OpenTelemetry (OTel) trace data. When you perform trace analytics, you can select from two data sources: . | Data Prepper – Data ingested into OpenSearch through Data Prepper | Jaeger – Trace data stored within OpenSearch as its backend | . If you store your Jaeger trace data in OpenSearch, you can now use the built-in trace analytics capabilities to analyze the error rates and latency. You can also filter the traces and analyze the span details of a trace to pinpoint any service issues. When you ingest Jaeger data into OpenSearch, it gets stored in a different index than the OTel-generated index that gets created when you run data through Data Prepper. Use the data source selector in OpenSearch Dashboards to indicate the data source on which you want to perform trace analytics. Jaeger trace data that you can analyze includes span data as well as service and operation endpoint data. By default, each time you ingest data for Jaeger, it creates a separate index for that day. To learn more about Jaeger data tracing, see the Jaeger documentation. ",
    "url": "https://vagimeli.github.io/observing-your-data/trace/trace-analytics-jaeger/",
    "relUrl": "/observing-your-data/trace/trace-analytics-jaeger/"
  },"1283": {
    "doc": "Analyzing Jaeger trace data",
    "title": "Data ingestion requirements",
    "content": "To perform trace analytics on Jaeger data, you need to configure error capability. Jaeger data that is ingested into OpenSearch must have the environment variable ES_TAGS_AS_FIELDS_ALL set to true for errors. If data is not ingested in this format, it will not work for errors, and error data will not be available for traces in trace analytics with OpenSearch. About data ingestion with Jaeger indexes . Trace analytics for non-Jaeger data uses OTel indexes with the naming conventions otel-v1-apm-span-* or otel-v1-apm-service-map*. Jaeger indexes follow the naming conventions jaeger-span-* or jaeger-service-*. ",
    "url": "https://vagimeli.github.io/observing-your-data/trace/trace-analytics-jaeger/#data-ingestion-requirements",
    "relUrl": "/observing-your-data/trace/trace-analytics-jaeger/#data-ingestion-requirements"
  },"1284": {
    "doc": "Analyzing Jaeger trace data",
    "title": "Setting up OpenSearch to use Jaeger data",
    "content": "The following section provides a sample Docker Compose file that contains the configuration required to enable errors for trace analytics. Step 1: Run the Docker Compose file . Use the following Docker Compose file to enable Jaeger data for trace analytics. Set the ES_TAGS_AS_FIELDS_ALL environment variable set to true to enable errors to be added to trace data. Copy the following Docker Compose file and save it as docker-compose.yml: . version: '3' services: opensearch-node1: # This is also the hostname of the container within the Docker network (i.e. https://opensearch-node1/) image: opensearchproject/opensearch:latest # Specifying the latest available image - modify if you want a specific version container_name: opensearch-node1 environment: - cluster.name=opensearch-cluster # Name the cluster - node.name=opensearch-node1 # Name the node that will run in this container - discovery.seed_hosts=opensearch-node1,opensearch-node2 # Nodes to look for when discovering the cluster - cluster.initial_cluster_manager_nodes=opensearch-node1,opensearch-node2 # Nodes eligible to serve as cluster manager - bootstrap.memory_lock=true # Disable JVM heap memory swapping - \"OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m\" # Set min and max JVM heap sizes to at least 50% of system RAM ulimits: memlock: soft: -1 # Set memlock to unlimited (no soft or hard limit) hard: -1 nofile: soft: 65536 # Maximum number of open files for the opensearch user - set to at least 65536 hard: 65536 volumes: - opensearch-data1:/usr/share/opensearch/data # Creates volume called opensearch-data1 and mounts it to the container ports: - \"9200:9200\" - \"9600:9600\" networks: - opensearch-net # All of the containers will join the same Docker bridge network opensearch-node2: image: opensearchproject/opensearch:latest # This should be the same image used for opensearch-node1 to avoid issues container_name: opensearch-node2 environment: - cluster.name=opensearch-cluster - node.name=opensearch-node2 - discovery.seed_hosts=opensearch-node1,opensearch-node2 - cluster.initial_cluster_manager_nodes=opensearch-node1,opensearch-node2 - bootstrap.memory_lock=true - \"OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m\" ulimits: memlock: soft: -1 hard: -1 nofile: soft: 65536 hard: 65536 volumes: - opensearch-data2:/usr/share/opensearch/data networks: - opensearch-net opensearch-dashboards: image: opensearchproject/opensearch-dashboards:latest # Make sure the version of opensearch-dashboards matches the version of opensearch installed on other nodes container_name: opensearch-dashboards ports: - 5601:5601 # Map host port 5601 to container port 5601 expose: - \"5601\" # Expose port 5601 for web access to OpenSearch Dashboards environment: OPENSEARCH_HOSTS: '[\"https://opensearch-node1:9200\",\"https://opensearch-node2:9200\"]' # Define the OpenSearch nodes that OpenSearch Dashboards will query networks: - opensearch-net jaeger-collector: image: jaegertracing/jaeger-collector:latest ports: - \"14269:14269\" - \"14268:14268\" - \"14267:14267\" - \"14250:14250\" - \"9411:9411\" networks: - opensearch-net restart: on-failure environment: - SPAN_STORAGE_TYPE=opensearch - ES_TAGS_AS_FIELDS_ALL=true - ES_USERNAME=admin - ES_PASSWORD=admin - ES_TLS_SKIP_HOST_VERIFY=true command: [ \"--es.server-urls=https://opensearch-node1:9200\", \"--es.tls.enabled=true\", ] depends_on: - opensearch-node1 jaeger-agent: image: jaegertracing/jaeger-agent:latest hostname: jaeger-agent command: [\"--reporter.grpc.host-port=jaeger-collector:14250\"] ports: - \"5775:5775/udp\" - \"6831:6831/udp\" - \"6832:6832/udp\" - \"5778:5778\" networks: - opensearch-net restart: on-failure environment: - SPAN_STORAGE_TYPE=opensearch depends_on: - jaeger-collector hotrod: image: jaegertracing/example-hotrod:latest ports: - \"8080:8080\" command: [\"all\"] environment: - JAEGER_AGENT_HOST=jaeger-agent - JAEGER_AGENT_PORT=6831 networks: - opensearch-net depends_on: - jaeger-agent volumes: opensearch-data1: opensearch-data2: networks: opensearch-net: . Step 2: Start the cluster . Run the following command to deploy the Docker compose YAML file: . docker compose up -d . To stop the cluster, run the following command: . docker compose down . Step 3: Generate sample data . Use the sample app provided with the Docker file to generate data. After you run the Docker Compose file, it runs the sample app on local host port 8080. To open the app, go to http://localhost:8080. In the sample app, Hot R.O.D., select any button to generate data. Now you can view trace data in Dashboards. Step 4: View trace data in OpenSearch Dashboards . After you generate Jaeger trace data, you can view it in Dashboards. Go to Trace analytics at http://localhost:5601/app/observability-dashboards#/trace_analytics/home. ",
    "url": "https://vagimeli.github.io/observing-your-data/trace/trace-analytics-jaeger/#setting-up-opensearch-to-use-jaeger-data",
    "relUrl": "/observing-your-data/trace/trace-analytics-jaeger/#setting-up-opensearch-to-use-jaeger-data"
  },"1285": {
    "doc": "Analyzing Jaeger trace data",
    "title": "Using trace analytics in OpenSearch Dashboards",
    "content": "To analyze the Jaeger trace data in Dashboards, first set up the trace analytics functionality. To get started, see Get started with trace analytics. Data sources . You can specify either Data Prepper or Jaeger as the data source when you perform trace analytics. From Dashboards, go to Observability &gt; Trace analytics and select Jaeger. ",
    "url": "https://vagimeli.github.io/observing-your-data/trace/trace-analytics-jaeger/#using-trace-analytics-in-opensearch-dashboards",
    "relUrl": "/observing-your-data/trace/trace-analytics-jaeger/#using-trace-analytics-in-opensearch-dashboards"
  },"1286": {
    "doc": "Analyzing Jaeger trace data",
    "title": "Dashboard view",
    "content": "After you select Jaeger as the data source, you can view all of the indexed data in Dashboard view, including Error rate and Throughput. Error rate . You can view the trace error count over time in Dashboard view and also see the top five combinations of services and operations that have a non-zero error rate. Throughput . With Throughput selected, you can see the throughput of Jaeger index traces over time. You can select an individual trace from the Top 5 Service and Operation Latency list and view the detailed trace data. You can also see the combinations of services and operations that have the highest latency. If you select one of the entries for Service and Operation Name and go to the Traces column to select a trace, it will automatically add the service and operation as filters. ",
    "url": "https://vagimeli.github.io/observing-your-data/trace/trace-analytics-jaeger/#dashboard-view",
    "relUrl": "/observing-your-data/trace/trace-analytics-jaeger/#dashboard-view"
  },"1287": {
    "doc": "Analyzing Jaeger trace data",
    "title": "Traces",
    "content": "In Traces, you can see the latency and errors for the filtered service and operation for each individual trace ID in the list. If you select an individual trace ID, you can see more detailed information about the trace, such as Time spent by the service and Spans. You can also view the index payload in JSON format. ",
    "url": "https://vagimeli.github.io/observing-your-data/trace/trace-analytics-jaeger/#traces",
    "relUrl": "/observing-your-data/trace/trace-analytics-jaeger/#traces"
  },"1288": {
    "doc": "Analyzing Jaeger trace data",
    "title": "Services",
    "content": "You can also view the individual error rates and latency for each individual service. Go to Observability &gt; Trace analytics &gt; Services. In Services, you can see the average latency, error rate, throughput and trace for each service in the list. ",
    "url": "https://vagimeli.github.io/observing-your-data/trace/trace-analytics-jaeger/#services",
    "relUrl": "/observing-your-data/trace/trace-analytics-jaeger/#services"
  },"1289": {
    "doc": "Aggregations",
    "title": "Aggregations",
    "content": "OpenSearch isn’t just for search. Aggregations let you tap into OpenSearch’s powerful analytics engine to analyze your data and extract statistics from it. The use cases of aggregations vary from analyzing data in real time to take some action to using OpenSearch Dashboards to create a visualization dashboard. OpenSearch can perform aggregations on massive datasets in milliseconds. Compared to queries, aggregations consume more CPU cycles and memory. ",
    "url": "https://vagimeli.github.io/aggregations/",
    "relUrl": "/aggregations/"
  },"1290": {
    "doc": "Aggregations",
    "title": "Aggregations on text fields",
    "content": "By default, OpenSearch doesn’t support aggregations on a text field. Because text fields are tokenized, an aggregation on a text field has to reverse the tokenization process back to its original string and then formulate an aggregation based on that. This kind of an operation consumes significant memory and degrades cluster performance. While you can enable aggregations on text fields by setting the fielddata parameter to true in the mapping, the aggregations are still based on the tokenized words and not on the raw text. We recommend keeping a raw version of the text field as a keyword field that you can aggregate on. In this case, you can perform aggregations on the title.raw field, instead of on the title field: . PUT movies { \"mappings\": { \"properties\": { \"title\": { \"type\": \"text\", \"fielddata\": true, \"fields\": { \"raw\": { \"type\": \"keyword\" } } } } } } . ",
    "url": "https://vagimeli.github.io/aggregations/#aggregations-on-text-fields",
    "relUrl": "/aggregations/#aggregations-on-text-fields"
  },"1291": {
    "doc": "Aggregations",
    "title": "General aggregation structure",
    "content": "The structure of an aggregation query is as follows: . GET _search { \"size\": 0, \"aggs\": { \"NAME\": { \"AGG_TYPE\": {} } } } . If you’re only interested in the aggregation result and not in the results of the query, set size to 0. In the aggs property (you can use aggregations if you want), you can define any number of aggregations. Each aggregation is defined by its name and one of the types of aggregations that OpenSearch supports. The name of the aggregation helps you to distinguish between different aggregations in the response. The AGG_TYPE property is where you specify the type of aggregation. ",
    "url": "https://vagimeli.github.io/aggregations/#general-aggregation-structure",
    "relUrl": "/aggregations/#general-aggregation-structure"
  },"1292": {
    "doc": "Aggregations",
    "title": "Sample aggregation",
    "content": "This section uses the OpenSearch Dashboards sample ecommerce data and web log data. To add the sample data, log in to OpenSearch Dashboards, choose Home, and then choose Try our sample data. For Sample eCommerce orders and Sample web logs, choose Add data. avg . To find the average value of the taxful_total_price field: . GET opensearch_dashboards_sample_data_ecommerce/_search { \"size\": 0, \"aggs\": { \"avg_taxful_total_price\": { \"avg\": { \"field\": \"taxful_total_price\" } } } } . Example response . { \"took\" : 1, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 4675, \"relation\" : \"eq\" }, \"max_score\" : null, \"hits\" : [ ] }, \"aggregations\" : { \"avg_taxful_total_price\" : { \"value\" : 75.05542864304813 } } } . The aggregation block in the response shows the average value for the taxful_total_price field. ",
    "url": "https://vagimeli.github.io/aggregations/#sample-aggregation",
    "relUrl": "/aggregations/#sample-aggregation"
  },"1293": {
    "doc": "Aggregations",
    "title": "Types of aggregations",
    "content": "There are three main types of aggregations: . | Metric aggregations - Calculate metrics such as sum, min, max, and avg on numeric fields. | Bucket aggregations - Sort query results into groups based on some criteria. | Pipeline aggregations - Pipe the output of one aggregation as an input to another. | . ",
    "url": "https://vagimeli.github.io/aggregations/#types-of-aggregations",
    "relUrl": "/aggregations/#types-of-aggregations"
  },"1294": {
    "doc": "Aggregations",
    "title": "Nested aggregations",
    "content": "Aggregations within aggregations are called nested or subaggregations. Metric aggregations produce simple results and can’t contain nested aggregations. Bucket aggregations produce buckets of documents that you can nest in other aggregations. You can perform complex analysis on your data by nesting metric and bucket aggregations within bucket aggregations. General nested aggregation syntax . { \"aggs\": { \"name\": { \"type\": { \"data\" }, \"aggs\": { \"nested\": { \"type\": { \"data\" } } } } } } . The inner aggs keyword begins a new nested aggregation. The syntax of the parent aggregation and the nested aggregation is the same. Nested aggregations run in the context of the preceding parent aggregations. You can also pair your aggregations with search queries to narrow down things you’re trying to analyze before aggregating. If you don’t add a query, OpenSearch implicitly uses the match_all query. ",
    "url": "https://vagimeli.github.io/aggregations/#nested-aggregations",
    "relUrl": "/aggregations/#nested-aggregations"
  },"1295": {
    "doc": "Aggregations",
    "title": "Limitations",
    "content": "Because aggregators are processed using the double data type for all values, long values of 253 and greater are approximate. ",
    "url": "https://vagimeli.github.io/aggregations/#limitations",
    "relUrl": "/aggregations/#limitations"
  },"1296": {
    "doc": "Bucket aggregations",
    "title": "Bucket aggregations",
    "content": "Bucket aggregations categorize sets of documents as buckets. The type of bucket aggregation determines whether a given document falls into a bucket or not. You can use bucket aggregations to implement faceted navigation (usually placed as a sidebar on a search result landing page) to help your users narrow down the results. ",
    "url": "https://vagimeli.github.io/aggregations/bucket-agg/",
    "relUrl": "/aggregations/bucket-agg/"
  },"1297": {
    "doc": "Bucket aggregations",
    "title": "Terms",
    "content": "The terms aggregation dynamically creates a bucket for each unique term of a field. The following example uses the terms aggregation to find the number of documents per response code in web log data: . GET opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"aggs\": { \"response_codes\": { \"terms\": { \"field\": \"response.keyword\", \"size\": 10 } } } } . Sample Response ... \"aggregations\" : { \"response_codes\" : { \"doc_count_error_upper_bound\" : 0, \"sum_other_doc_count\" : 0, \"buckets\" : [ { \"key\" : \"200\", \"doc_count\" : 12832 }, { \"key\" : \"404\", \"doc_count\" : 801 }, { \"key\" : \"503\", \"doc_count\" : 441 } ] } } } . The values are returned with the key key. doc_count specifies the number of documents in each bucket. By default, the buckets are sorted in descending order of doc-count. The response also includes two keys named doc_count_error_upper_bound and sum_other_doc_count. The terms aggregation returns the top unique terms. So, if the data has many unique terms, then some of them might not appear in the results. The sum_other_doc_count field is the sum of the documents that are left out of the response. In this case, the number is 0 because all the unique values appear in the response. The doc_count_error_upper_bound field represents the maximum possible count for a unique value that’s left out of the final results. Use this field to estimate the error margin for the count. The count might not be accurate. A coordinating node that’s responsible for the aggregation prompts each shard for its top unique terms. Imagine a scenario where the size parameter is 3. The terms aggregation requests each shard for its top 3 unique terms. The coordinating node takes each of the results and aggregates them to compute the final result. If a shard has an object that’s not part of the top 3, then it won’t show up in the response. This is especially true if size is set to a low number. Because the default size is 10, an error is unlikely to happen. If you don’t need high accuracy and want to increase the performance, you can reduce the size. Account for pre-aggregated data . While the doc_count field provides a representation of the number of individual documents aggregated in a bucket, doc_count by itself does not have a way to correctly increment documents that store pre-aggregated data. To account for pre-aggregated data and accurately calculate the number of documents in a bucket, you can use the _doc_count field to add the number of documents in a single summary field. When a document includes the _doc_count field, all bucket aggregations recognize its value and increase the bucket doc_count cumulatively. Keep these considerations in mind when using the _doc_count field: . | The field does not support nested arrays; only positive integers can be used. | If a document does not contain the _doc_count field, aggregation uses the document to increase the count by 1. | . OpenSearch features that rely on an accurate document count illustrate the importance of using the _doc_count field. To see how this field can be used to support other search tools, refer to Index rollups, an OpenSearch feature for the Index Management (IM) plugin that stores documents with pre-aggregated data in rollup indexes. Example usage . PUT /my_index/_doc/1 { \"response_code\": 404, \"date\":\"2022-08-05\", \"_doc_count\": 20 } PUT /my_index/_doc/2 { \"response_code\": 404, \"date\":\"2022-08-06\", \"_doc_count\": 10 } PUT /my_index/_doc/3 { \"response_code\": 200, \"date\":\"2022-08-06\", \"_doc_count\": 300 } GET /my_index/_search { \"size\": 0, \"aggs\": { \"response_codes\": { \"terms\": { \"field\" : \"response_code\" } } } } . Example response . { \"took\" : 20, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 3, \"relation\" : \"eq\" }, \"max_score\" : null, \"hits\" : [ ] }, \"aggregations\" : { \"response_codes\" : { \"doc_count_error_upper_bound\" : 0, \"sum_other_doc_count\" : 0, \"buckets\" : [ { \"key\" : 200, \"doc_count\" : 300 }, { \"key\" : 404, \"doc_count\" : 30 } ] } } } . ",
    "url": "https://vagimeli.github.io/aggregations/bucket-agg/#terms",
    "relUrl": "/aggregations/bucket-agg/#terms"
  },"1298": {
    "doc": "Bucket aggregations",
    "title": "Multi-terms",
    "content": "Similar to the terms bucket aggregation, you can also search for multiple terms using the multi_terms aggregation. Multi-terms aggregations are useful when you need to sort by document count, or when you need to sort by a metric aggregation on a composite key and get the top n results. For example, you could search for a specific number of documents (e.g., 1000) and the number of servers per location that show CPU usage greater than 90%. The top number of results would be returned for this multi-term query. The multi_terms aggregation does consume more memory than a terms aggregation, so its performance might be slower. Multi-terms aggregation parameters . | Parameter | Description | . | multi_terms | Indicates a multi-terms aggregation that gathers buckets of documents together based on criteria specified by multiple terms. | . | size | Specifies the number of buckets to return. Default is 10. | . | order | Indicates the order to sort the buckets. By default, buckets are ordered according to document count per bucket. If the buckets contain the same document count, then order can be explicitly set to the term value instead of document count. (e.g., set order to “max-cpu”). | . | doc_count | Specifies the number of documents to be returned in each bucket. By default, the top 10 terms are returned. | . Sample Request . GET sample-index100/_search { \"size\": 0, \"aggs\": { \"hot\": { \"multi_terms\": { \"terms\": [{ \"field\": \"region\" },{ \"field\": \"host\" }], \"order\": {\"max-cpu\": \"desc\"} }, \"aggs\": { \"max-cpu\": { \"max\": { \"field\": \"cpu\" } } } } } } . Sample Response . { \"took\": 118, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 8, \"relation\": \"eq\" }, \"max_score\": null, \"hits\": [] }, \"aggregations\": { \"multi-terms\": { \"doc_count_error_upper_bound\": 0, \"sum_other_doc_count\": 0, \"buckets\": [ { \"key\": [ \"dub\", \"h1\" ], \"key_as_string\": \"dub|h1\", \"doc_count\": 2, \"max-cpu\": { \"value\": 90.0 } }, { \"key\": [ \"dub\", \"h2\" ], \"key_as_string\": \"dub|h2\", \"doc_count\": 2, \"max-cpu\": { \"value\": 70.0 } }, { \"key\": [ \"iad\", \"h2\" ], \"key_as_string\": \"iad|h2\", \"doc_count\": 2, \"max-cpu\": { \"value\": 50.0 } }, { \"key\": [ \"iad\", \"h1\" ], \"key_as_string\": \"iad|h1\", \"doc_count\": 2, \"max-cpu\": { \"value\": 15.0 } } ] } } } . ",
    "url": "https://vagimeli.github.io/aggregations/bucket-agg/#multi-terms",
    "relUrl": "/aggregations/bucket-agg/#multi-terms"
  },"1299": {
    "doc": "Bucket aggregations",
    "title": "sampler, diversified_sampler",
    "content": "If you’re aggregating over millions of documents, you can use a sampler aggregation to reduce its scope to a small sample of documents for a faster response. The sampler aggregation selects the samples by top-scoring documents. The results are approximate but closely represent the distribution of the real data. The sampler aggregation significantly improves query performance, but the estimated responses are not entirely reliable. The basic syntax is: . “aggs”: { \"SAMPLE\": { \"sampler\": { \"shard_size\": 100 }, \"aggs\": {...} } } . The shard_size property tells OpenSearch how many documents (at most) to collect from each shard. The following example limits the number of documents collected on each shard to 1,000 and then buckets the documents by a terms aggregation: . GET opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"aggs\": { \"sample\": { \"sampler\": { \"shard_size\": 1000 }, \"aggs\": { \"terms\": { \"terms\": { \"field\": \"agent.keyword\" } } } } } } . Example response ... \"aggregations\" : { \"sample\" : { \"doc_count\" : 1000, \"terms\" : { \"doc_count_error_upper_bound\" : 0, \"sum_other_doc_count\" : 0, \"buckets\" : [ { \"key\" : \"Mozilla/5.0 (X11; Linux x86_64; rv:6.0a1) Gecko/20110421 Firefox/6.0a1\", \"doc_count\" : 368 }, { \"key\" : \"Mozilla/5.0 (X11; Linux i686) AppleWebKit/534.24 (KHTML, like Gecko) Chrome/11.0.696.50 Safari/534.24\", \"doc_count\" : 329 }, { \"key\" : \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; .NET CLR 1.1.4322)\", \"doc_count\" : 303 } ] } } } } . The diversified_sampler aggregation lets you reduce the bias in the distribution of the sample pool. You can use the field setting to control the maximum number of documents collected on any one shard which shares a common value: . GET opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"aggs\": { \"sample\": { \"diversified_sampler\": { \"shard_size\": 1000, \"field\": \"response.keyword\" }, \"aggs\": { \"terms\": { \"terms\": { \"field\": \"agent.keyword\" } } } } } } . Example response ... \"aggregations\" : { \"sample\" : { \"doc_count\" : 3, \"terms\" : { \"doc_count_error_upper_bound\" : 0, \"sum_other_doc_count\" : 0, \"buckets\" : [ { \"key\" : \"Mozilla/5.0 (X11; Linux x86_64; rv:6.0a1) Gecko/20110421 Firefox/6.0a1\", \"doc_count\" : 2 }, { \"key\" : \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; .NET CLR 1.1.4322)\", \"doc_count\" : 1 } ] } } } } . ",
    "url": "https://vagimeli.github.io/aggregations/bucket-agg/#sampler-diversified_sampler",
    "relUrl": "/aggregations/bucket-agg/#sampler-diversified_sampler"
  },"1300": {
    "doc": "Bucket aggregations",
    "title": "significant_terms, significant_text",
    "content": "The significant_terms aggregation lets you spot unusual or interesting term occurrences in a filtered subset relative to the rest of the data in an index. A foreground set is the set of documents that you filter. A background set is a set of all documents in an index. The significant_terms aggregation examines all documents in the foreground set and finds a score for significant occurrences in contrast to the documents in the background set. In the sample web log data, each document has a field containing the user-agent of the visitor. This example searches for all requests from an iOS operating system. A regular terms aggregation on this foreground set returns Firefox because it has the most number of documents within this bucket. On the other hand, a significant_terms aggregation returns Internet Explorer (IE) because IE has a significantly higher appearance in the foreground set as compared to the background set. GET opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"query\": { \"terms\": { \"machine.os.keyword\": [ \"ios\" ] } }, \"aggs\": { \"significant_response_codes\": { \"significant_terms\": { \"field\": \"agent.keyword\" } } } } . Example response ... \"aggregations\" : { \"significant_response_codes\" : { \"doc_count\" : 2737, \"bg_count\" : 14074, \"buckets\" : [ { \"key\" : \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; .NET CLR 1.1.4322)\", \"doc_count\" : 818, \"score\" : 0.01462731514608217, \"bg_count\" : 4010 }, { \"key\" : \"Mozilla/5.0 (X11; Linux x86_64; rv:6.0a1) Gecko/20110421 Firefox/6.0a1\", \"doc_count\" : 1067, \"score\" : 0.009062566630410223, \"bg_count\" : 5362 } ] } } } . If the significant_terms aggregation doesn’t return any result, you might have not filtered the results with a query. Alternatively, the distribution of terms in the foreground set might be the same as the background set, implying that there isn’t anything unusual in the foreground set. The significant_text aggregation is similar to the significant_terms aggregation but it’s for raw text fields. Significant text measures the change in popularity measured between the foreground and background sets using statistical analysis. For example, it might suggest Tesla when you look for its stock acronym TSLA. The significant_text aggregation re-analyzes the source text on the fly, filtering noisy data like duplicate paragraphs, boilerplate headers and footers, and so on, which might otherwise skew the results. Re-analyzing high-cardinality datasets can be a very CPU-intensive operation. We recommend using the significant_text aggregation inside a sampler aggregation to limit the analysis to a small selection of top-matching documents, for example 200. You can set the following parameters: . | min_doc_count - Return results that match more than a configured number of top hits. We recommend not setting min_doc_count to 1 because it tends to return terms that are typos or misspellings. Finding more than one instance of a term helps reinforce that the significance is not the result of a one-off accident. The default value of 3 is used to provide a minimum weight-of-evidence. | shard_size - Setting a high value increases stability (and accuracy) at the expense of computational performance. | shard_min_doc_count - If your text contains many low frequency words and you’re not interested in these (for example typos), then you can set the shard_min_doc_count parameter to filter out candidate terms at a shard level with a reasonable certainty to not reach the required min_doc_count even after merging the local significant text frequencies. The default value is 1, which has no impact until you explicitly set it. We recommend setting this value much lower than the min_doc_count value. | . Assume that you have the complete works of Shakespeare indexed in an OpenSearch cluster. You can find significant texts in relation to the word “breathe” in the text_entry field: . GET shakespeare/_search { \"query\": { \"match\": { \"text_entry\": \"breathe\" } }, \"aggregations\": { \"my_sample\": { \"sampler\": { \"shard_size\": 100 }, \"aggregations\": { \"keywords\": { \"significant_text\": { \"field\": \"text_entry\", \"min_doc_count\": 4 } } } } } } . Example response . \"aggregations\" : { \"my_sample\" : { \"doc_count\" : 59, \"keywords\" : { \"doc_count\" : 59, \"bg_count\" : 111396, \"buckets\" : [ { \"key\" : \"breathe\", \"doc_count\" : 59, \"score\" : 1887.0677966101694, \"bg_count\" : 59 }, { \"key\" : \"air\", \"doc_count\" : 4, \"score\" : 2.641295376716233, \"bg_count\" : 189 }, { \"key\" : \"dead\", \"doc_count\" : 4, \"score\" : 0.9665839666414213, \"bg_count\" : 495 }, { \"key\" : \"life\", \"doc_count\" : 5, \"score\" : 0.9090787433467572, \"bg_count\" : 805 } ] } } } } . The most significant texts in relation to breathe are air, dead, and life. The significant_text aggregation has the following limitations: . | Doesn’t support child aggregations because child aggregations come at a high memory cost. As a workaround, you can add a follow-up query using a terms aggregation with an include clause and a child aggregation. | Doesn’t support nested objects because it works with the document JSON source. | The counts of documents might have some (typically small) inaccuracies as it’s based on summing the samples returned from each shard. You can use the shard_size parameter to fine-tune the trade-off between accuracy and performance. By default, the shard_size is set to -1 to automatically estimate the number of shards and the size parameter. | . For both significant_terms and significant_text aggregations, the default source of statistical information for background term frequencies is the entire index. You can narrow this scope with a background filter for more focus: . GET shakespeare/_search { \"query\": { \"match\": { \"text_entry\": \"breathe\" } }, \"aggregations\": { \"my_sample\": { \"sampler\": { \"shard_size\": 100 }, \"aggregations\": { \"keywords\": { \"significant_text\": { \"field\": \"text_entry\", \"background_filter\": { \"term\": { \"speaker\": \"JOHN OF GAUNT\" } } } } } } } } . ",
    "url": "https://vagimeli.github.io/aggregations/bucket-agg/#significant_terms-significant_text",
    "relUrl": "/aggregations/bucket-agg/#significant_terms-significant_text"
  },"1301": {
    "doc": "Bucket aggregations",
    "title": "missing",
    "content": "If you have documents in your index that don’t contain the aggregating field at all or the aggregating field has a value of NULL, use the missing parameter to specify the name of the bucket such documents should be placed in. The following example adds any missing values to a bucket named “N/A”: . GET opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"aggs\": { \"response_codes\": { \"terms\": { \"field\": \"response.keyword\", \"size\": 10, \"missing\": \"N/A\" } } } } . Because the default value for the min_doc_count parameter is 1, the missing parameter doesn’t return any buckets in its response. Set min_doc_count parameter to 0 to see the “N/A” bucket in the response: . GET opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"aggs\": { \"response_codes\": { \"terms\": { \"field\": \"response.keyword\", \"size\": 10, \"missing\": \"N/A\", \"min_doc_count\": 0 } } } } . Example response ... \"aggregations\" : { \"response_codes\" : { \"doc_count_error_upper_bound\" : 0, \"sum_other_doc_count\" : 0, \"buckets\" : [ { \"key\" : \"200\", \"doc_count\" : 12832 }, { \"key\" : \"404\", \"doc_count\" : 801 }, { \"key\" : \"503\", \"doc_count\" : 441 }, { \"key\" : \"N/A\", \"doc_count\" : 0 } ] } } } . ",
    "url": "https://vagimeli.github.io/aggregations/bucket-agg/#missing",
    "relUrl": "/aggregations/bucket-agg/#missing"
  },"1302": {
    "doc": "Bucket aggregations",
    "title": "histogram, date_histogram",
    "content": "The histogram aggregation buckets documents based on a specified interval. With histogram aggregations, you can visualize the distributions of values in a given range of documents very easily. Now OpenSearch doesn’t give you back an actual graph of course, that’s what OpenSearch Dashboards is for. But it’ll give you the JSON response that you can use to construct your own graph. The following example buckets the number_of_bytes field by 10,000 intervals: . GET opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"aggs\": { \"number_of_bytes\": { \"histogram\": { \"field\": \"bytes\", \"interval\": 10000 } } } } . Sample Response ... \"aggregations\" : { \"number_of_bytes\" : { \"buckets\" : [ { \"key\" : 0.0, \"doc_count\" : 13372 }, { \"key\" : 10000.0, \"doc_count\" : 702 } ] } } } . The date_histogram aggregation uses date math to generate histograms for time-series data. For example, you can find how many hits your website gets per month: . GET opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"aggs\": { \"logs_per_month\": { \"date_histogram\": { \"field\": \"@timestamp\", \"interval\": \"month\" } } } } . Example response ... \"aggregations\" : { \"logs_per_month\" : { \"buckets\" : [ { \"key_as_string\" : \"2020-10-01T00:00:00.000Z\", \"key\" : 1601510400000, \"doc_count\" : 1635 }, { \"key_as_string\" : \"2020-11-01T00:00:00.000Z\", \"key\" : 1604188800000, \"doc_count\" : 6844 }, { \"key_as_string\" : \"2020-12-01T00:00:00.000Z\", \"key\" : 1606780800000, \"doc_count\" : 5595 } ] } } } . The response has three months worth of logs. If you graph these values, you can see the peak and valleys of the request traffic to your website month over month. ",
    "url": "https://vagimeli.github.io/aggregations/bucket-agg/#histogram-date_histogram",
    "relUrl": "/aggregations/bucket-agg/#histogram-date_histogram"
  },"1303": {
    "doc": "Bucket aggregations",
    "title": "range, date_range, ip_range",
    "content": "The range aggregation lets you define the range for each bucket. For example, you can find the number of bytes between 1000 and 2000, 2000 and 3000, and 3000 and 4000. Within the range parameter, you can define ranges as objects of an array. GET opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"aggs\": { \"number_of_bytes_distribution\": { \"range\": { \"field\": \"bytes\", \"ranges\": [ { \"from\": 1000, \"to\": 2000 }, { \"from\": 2000, \"to\": 3000 }, { \"from\": 3000, \"to\": 4000 } ] } } } } . The response includes the from key values and excludes the to key values: . Example response ... \"aggregations\" : { \"number_of_bytes_distribution\" : { \"buckets\" : [ { \"key\" : \"1000.0-2000.0\", \"from\" : 1000.0, \"to\" : 2000.0, \"doc_count\" : 805 }, { \"key\" : \"2000.0-3000.0\", \"from\" : 2000.0, \"to\" : 3000.0, \"doc_count\" : 1369 }, { \"key\" : \"3000.0-4000.0\", \"from\" : 3000.0, \"to\" : 4000.0, \"doc_count\" : 1422 } ] } } } . The date_range aggregation is conceptually the same as the range aggregation, except that it lets you perform date math. For example, you can get all documents from the last 10 days. To make the date more readable, include the format with a format parameter: . GET opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"aggs\": { \"number_of_bytes\": { \"date_range\": { \"field\": \"@timestamp\", \"format\": \"MM-yyyy\", \"ranges\": [ { \"from\": \"now-10d/d\", \"to\": \"now\" } ] } } } } . Example response ... \"aggregations\" : { \"number_of_bytes\" : { \"buckets\" : [ { \"key\" : \"03-2021-03-2021\", \"from\" : 1.6145568E12, \"from_as_string\" : \"03-2021\", \"to\" : 1.615451329043E12, \"to_as_string\" : \"03-2021\", \"doc_count\" : 0 } ] } } } . The ip_range aggregation is for IP addresses. It works on ip type fields. You can define the IP ranges and masks in the CIDR notation. GET opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"aggs\": { \"access\": { \"ip_range\": { \"field\": \"ip\", \"ranges\": [ { \"from\": \"1.0.0.0\", \"to\": \"126.158.155.183\" }, { \"mask\": \"1.0.0.0/8\" } ] } } } } . Example response ... \"aggregations\" : { \"access\" : { \"buckets\" : [ { \"key\" : \"1.0.0.0/8\", \"from\" : \"1.0.0.0\", \"to\" : \"2.0.0.0\", \"doc_count\" : 98 }, { \"key\" : \"1.0.0.0-126.158.155.183\", \"from\" : \"1.0.0.0\", \"to\" : \"126.158.155.183\", \"doc_count\" : 7184 } ] } } } . If you add a document with malformed fields to an index that has ip_range set to false in its mappings, OpenSearch rejects the entire document. You can set ignore_malformed to true to specify that OpenSearch should ignore malformed fields. The default is false... \"mappings\": { \"properties\": { \"ips\": { \"type\": \"ip_range\", \"ignore_malformed\": true } } } . ",
    "url": "https://vagimeli.github.io/aggregations/bucket-agg/#range-date_range-ip_range",
    "relUrl": "/aggregations/bucket-agg/#range-date_range-ip_range"
  },"1304": {
    "doc": "Bucket aggregations",
    "title": "filter, filters",
    "content": "A filter aggregation is a query clause, exactly like a search query — match or term or range. You can use the filter aggregation to narrow down the entire set of documents to a specific set before creating buckets. The following example shows the avg aggregation running within the context of a filter. The avg aggregation only aggregates the documents that match the range query: . GET opensearch_dashboards_sample_data_ecommerce/_search { \"size\": 0, \"aggs\": { \"low_value\": { \"filter\": { \"range\": { \"taxful_total_price\": { \"lte\": 50 } } }, \"aggs\": { \"avg_amount\": { \"avg\": { \"field\": \"taxful_total_price\" } } } } } } . Example response ... \"aggregations\" : { \"low_value\" : { \"doc_count\" : 1633, \"avg_amount\" : { \"value\" : 38.363175998928355 } } } } . A filters aggregation is the same as the filter aggregation, except that it lets you use multiple filter aggregations. While the filter aggregation results in a single bucket, the filters aggregation returns multiple buckets, one for each of the defined filters. To create a bucket for all the documents that didn’t match the any of the filter queries, set the other_bucket property to true: . GET opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"aggs\": { \"200_os\": { \"filters\": { \"other_bucket\": true, \"filters\": [ { \"term\": { \"response.keyword\": \"200\" } }, { \"term\": { \"machine.os.keyword\": \"osx\" } } ] }, \"aggs\": { \"avg_amount\": { \"avg\": { \"field\": \"bytes\" } } } } } } . Example response ... \"aggregations\" : { \"200_os\" : { \"buckets\" : [ { \"doc_count\" : 12832, \"avg_amount\" : { \"value\" : 5897.852711970075 } }, { \"doc_count\" : 2825, \"avg_amount\" : { \"value\" : 5620.347256637168 } }, { \"doc_count\" : 1017, \"avg_amount\" : { \"value\" : 3247.0963618485744 } } ] } } } . ",
    "url": "https://vagimeli.github.io/aggregations/bucket-agg/#filter-filters",
    "relUrl": "/aggregations/bucket-agg/#filter-filters"
  },"1305": {
    "doc": "Bucket aggregations",
    "title": "global",
    "content": "The global aggregations lets you break out of the aggregation context of a filter aggregation. Even if you have included a filter query that narrows down a set of documents, the global aggregation aggregates on all documents as if the filter query wasn’t there. It ignores the filter aggregation and implicitly assumes the match_all query. The following example returns the avg value of the taxful_total_price field from all documents in the index: . GET opensearch_dashboards_sample_data_ecommerce/_search { \"size\": 0, \"query\": { \"range\": { \"taxful_total_price\": { \"lte\": 50 } } }, \"aggs\": { \"total_avg_amount\": { \"global\": {}, \"aggs\": { \"avg_price\": { \"avg\": { \"field\": \"taxful_total_price\" } } } } } } . Example response ... \"aggregations\" : { \"total_avg_amount\" : { \"doc_count\" : 4675, \"avg_price\" : { \"value\" : 75.05542864304813 } } } } . You can see that the average value for the taxful_total_price field is 75.05 and not the 38.36 as seen in the filter example when the query matched. ",
    "url": "https://vagimeli.github.io/aggregations/bucket-agg/#global",
    "relUrl": "/aggregations/bucket-agg/#global"
  },"1306": {
    "doc": "Bucket aggregations",
    "title": "geo_distance, geohash_grid",
    "content": "The geo_distance aggregation groups documents into concentric circles based on distances from an origin geo_point field. It’s the same as the range aggregation, except that it works on geo locations. For example, you can use the geo_distance aggregation to find all pizza places within 1 km of you. The search results are limited to the 1 km radius specified by you, but you can add another result found within 2 km. You can only use the geo_distance aggregation on fields mapped as geo_point. A point is a single geographical coordinate, such as your current location shown by your smart-phone. A point in OpenSearch is represented as follows: . { \"location\": { \"type\": \"point\", \"coordinates\": { \"lat\": 83.76, \"lon\": -81.2 } } } . You can also specify the latitude and longitude as an array [-81.20, 83.76] or as a string \"83.76, -81.20\" . This table lists the relevant fields of a geo_distance aggregation: . | Field | Description | Required | . | field | Specify the geo point field that you want to work on. | Yes | . | origin | Specify the geo point that’s used to compute the distances from. | Yes | . | ranges | Specify a list of ranges to collect documents based on their distance from the target point. | Yes | . | unit | Define the units used in the ranges array. The unit defaults to m (meters), but you can switch to other units like km (kilometers), mi (miles), in (inches), yd (yards), cm (centimeters), and mm (millimeters). | No | . | distance_type | Specify how OpenSearch calculates the distance. The default is sloppy_arc (faster but less accurate), but can also be set to arc (slower but most accurate) or plane (fastest but least accurate). Because of high error margins, use plane only for small geographic areas. | No | . The syntax is as follows: . { \"aggs\": { \"aggregation_name\": { \"geo_distance\": { \"field\": \"field_1\", \"origin\": \"x, y\", \"ranges\": [ { \"to\": \"value_1\" }, { \"from\": \"value_2\", \"to\": \"value_3\" }, { \"from\": \"value_4\" } ] } } } } . This example forms buckets from the following distances from a geo-point field: . | Fewer than 10 km | From 10 to 20 km | From 20 to 50 km | From 50 to 100 km | Above 100 km | . GET opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"aggs\": { \"position\": { \"geo_distance\": { \"field\": \"geo.coordinates\", \"origin\": { \"lat\": 83.76, \"lon\": -81.2 }, \"ranges\": [ { \"to\": 10 }, { \"from\": 10, \"to\": 20 }, { \"from\": 20, \"to\": 50 }, { \"from\": 50, \"to\": 100 }, { \"from\": 100 } ] } } } } . Example response ... \"aggregations\" : { \"position\" : { \"buckets\" : [ { \"key\" : \"*-10.0\", \"from\" : 0.0, \"to\" : 10.0, \"doc_count\" : 0 }, { \"key\" : \"10.0-20.0\", \"from\" : 10.0, \"to\" : 20.0, \"doc_count\" : 0 }, { \"key\" : \"20.0-50.0\", \"from\" : 20.0, \"to\" : 50.0, \"doc_count\" : 0 }, { \"key\" : \"50.0-100.0\", \"from\" : 50.0, \"to\" : 100.0, \"doc_count\" : 0 }, { \"key\" : \"100.0-*\", \"from\" : 100.0, \"doc_count\" : 14074 } ] } } } . The geohash_grid aggregation buckets documents for geographical analysis. It organizes a geographical region into a grid of smaller regions of different sizes or precisions. Lower values of precision represent larger geographical areas and higher values represent smaller, more precise geographical areas. The number of results returned by a query might be far too many to display each geo point individually on a map. The geohash_grid aggregation buckets nearby geo points together by calculating the Geohash for each point, at the level of precision that you define (between 1 to 12; the default is 5). To learn more about Geohash, see Wikipedia. The web logs example data is spread over a large geographical area, so you can use a lower precision value. You can zoom in on this map by increasing the precision value: . GET opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"aggs\": { \"geo_hash\": { \"geohash_grid\": { \"field\": \"geo.coordinates\", \"precision\": 4 } } } } . Example response ... \"aggregations\" : { \"geo_hash\" : { \"buckets\" : [ { \"key\" : \"c1cg\", \"doc_count\" : 104 }, { \"key\" : \"dr5r\", \"doc_count\" : 26 }, { \"key\" : \"9q5b\", \"doc_count\" : 20 }, { \"key\" : \"c20g\", \"doc_count\" : 19 }, { \"key\" : \"dr70\", \"doc_count\" : 18 } ... ] } } } . You can visualize the aggregated response on a map using OpenSearch Dashboards. The more accurate you want the aggregation to be, the more resources OpenSearch consumes, because of the number of buckets that the aggregation has to calculate. By default, OpenSearch does not generate more than 10,000 buckets. You can change this behavior by using the size attribute, but keep in mind that the performance might suffer for very wide queries consisting of thousands of buckets. ",
    "url": "https://vagimeli.github.io/aggregations/bucket-agg/#geo_distance-geohash_grid",
    "relUrl": "/aggregations/bucket-agg/#geo_distance-geohash_grid"
  },"1307": {
    "doc": "Bucket aggregations",
    "title": "adjacency_matrix",
    "content": "The adjacency_matrix aggregation lets you define filter expressions and returns a matrix of the intersecting filters where each non-empty cell in the matrix represents a bucket. You can find how many documents fall within any combination of filters. Use the adjacency_matrix aggregation to discover how concepts are related by visualizing the data as graphs. For example, in the sample eCommerce dataset, to analyze how the different manufacturing companies are related: . GET opensearch_dashboards_sample_data_ecommerce/_search { \"size\": 0, \"aggs\": { \"interactions\": { \"adjacency_matrix\": { \"filters\": { \"grpA\": { \"match\": { \"manufacturer.keyword\": \"Low Tide Media\" } }, \"grpB\": { \"match\": { \"manufacturer.keyword\": \"Elitelligence\" } }, \"grpC\": { \"match\": { \"manufacturer.keyword\": \"Oceanavigations\" } } } } } } } . Example response . { ... \"aggregations\" : { \"interactions\" : { \"buckets\" : [ { \"key\" : \"grpA\", \"doc_count\" : 1553 }, { \"key\" : \"grpA&amp;grpB\", \"doc_count\" : 590 }, { \"key\" : \"grpA&amp;grpC\", \"doc_count\" : 329 }, { \"key\" : \"grpB\", \"doc_count\" : 1370 }, { \"key\" : \"grpB&amp;grpC\", \"doc_count\" : 299 }, { \"key\" : \"grpC\", \"doc_count\" : 1218 } ] } } } . Let’s take a closer look at the result: . { \"key\" : \"grpA&amp;grpB\", \"doc_count\" : 590 } . | grpA: Products manufactured by Low Tide Media. | grpB: Products manufactured by Elitelligence. | 590: Number of products that are manufactured by both. | . You can use OpenSearch Dashboards to represent this data with a network graph. ",
    "url": "https://vagimeli.github.io/aggregations/bucket-agg/#adjacency_matrix",
    "relUrl": "/aggregations/bucket-agg/#adjacency_matrix"
  },"1308": {
    "doc": "Bucket aggregations",
    "title": "nested, reverse_nested",
    "content": "The nested aggregation lets you aggregate on fields inside a nested object. The nested type is a specialized version of the object data type that allows arrays of objects to be indexed in a way that they can be queried independently of each other . With the object type, all the data is stored in the same document, so matches for a search can go across sub documents. For example, imagine a logs index with pages mapped as an object datatype: . PUT logs/_doc/0 { \"response\": \"200\", \"pages\": [ { \"page\": \"landing\", \"load_time\": 200 }, { \"page\": \"blog\", \"load_time\": 500 } ] } . OpenSearch merges all sub-properties of the entity relations that looks something like this: . { \"logs\": { \"pages\": [\"landing\", \"blog\"], \"load_time\": [\"200\", \"500\"] } } . So, if you wanted to search this index with pages=landing and load_time=500, this document matches the criteria even though the load_time value for landing is 200. If you want to make sure such cross-object matches don’t happen, map the field as a nested type: . PUT logs { \"mappings\": { \"properties\": { \"pages\": { \"type\": \"nested\", \"properties\": { \"page\": { \"type\": \"text\" }, \"load_time\": { \"type\": \"double\" } } } } } } . Nested documents allow you to index the same JSON document but will keep your pages in separate Lucene documents, making only searches like pages=landing and load_time=200 return the expected result. Internally, nested objects index each object in the array as a separate hidden document, meaning that each nested object can be queried independently of the others. You have to specify a nested path relative to parent that contains the nested documents: . GET logs/_search { \"query\": { \"match\": { \"response\": \"200\" } }, \"aggs\": { \"pages\": { \"nested\": { \"path\": \"pages\" }, \"aggs\": { \"min_load_time\": { \"min\": { \"field\": \"pages.load_time\" } } } } } } . Example response ... \"aggregations\" : { \"pages\" : { \"doc_count\" : 2, \"min_price\" : { \"value\" : 200.0 } } } } . You can also aggregate values from nested documents to their parent; this aggregation is called reverse_nested. You can use reverse_nested to aggregate a field from the parent document after grouping by the field from the nested object. The reverse_nested aggregation “joins back” the root page and gets the load_time for each for your variations. The reverse_nested aggregation is a sub-aggregation inside a nested aggregation. It accepts a single option named path. This option defines how many steps backwards in the document hierarchy OpenSearch takes to calculate the aggregations. GET logs/_search { \"query\": { \"match\": { \"response\": \"200\" } }, \"aggs\": { \"pages\": { \"nested\": { \"path\": \"pages\" }, \"aggs\": { \"top_pages_per_load_time\": { \"terms\": { \"field\": \"pages.load_time\" }, \"aggs\": { \"comment_to_logs\": { \"reverse_nested\": {}, \"aggs\": { \"min_load_time\": { \"min\": { \"field\": \"pages.load_time\" } } } } } } } } } } . Example response ... \"aggregations\" : { \"pages\" : { \"doc_count\" : 2, \"top_pages_per_load_time\" : { \"doc_count_error_upper_bound\" : 0, \"sum_other_doc_count\" : 0, \"buckets\" : [ { \"key\" : 200.0, \"doc_count\" : 1, \"comment_to_logs\" : { \"doc_count\" : 1, \"min_load_time\" : { \"value\" : null } } }, { \"key\" : 500.0, \"doc_count\" : 1, \"comment_to_logs\" : { \"doc_count\" : 1, \"min_load_time\" : { \"value\" : null } } } ] } } } } . The response shows the logs index has one page with a load_time of 200 and one with a load_time of 500. ",
    "url": "https://vagimeli.github.io/aggregations/bucket-agg/#nested-reverse_nested",
    "relUrl": "/aggregations/bucket-agg/#nested-reverse_nested"
  },"1309": {
    "doc": "GeoHex grid aggregations",
    "title": "GeoHex grid aggregations",
    "content": "The Hexagonal Hierarchical Geospatial Indexing System (H3) partitions the Earth’s areas into identifiable hexagon-shaped cells. The H3 grid system works well for proximity applications because it overcomes the limitations of Geohash’s non-uniform partitions. Geohash encodes latitude and longitude pairs, leading to significantly smaller partitions near the poles and a degree of longitude near the equator. However, the H3 grid system’s distortions are low and limited to 5 partitions of 122. These five partitions are placed in low-use areas (for example, in the middle of the ocean), leaving the essential areas error free. Thus, grouping documents based on the H3 grid system provides a better aggregation than the Geohash grid. The GeoHex grid aggregation groups geopoints into grid cells for geographical analysis. Each grid cell corresponds to an H3 cell and is identified using the H3Index representation. ",
    "url": "https://vagimeli.github.io/aggregations/geohexgrid/",
    "relUrl": "/aggregations/geohexgrid/"
  },"1310": {
    "doc": "GeoHex grid aggregations",
    "title": "Precision",
    "content": "The precision parameter controls the level of granularity that determines the grid cell size. The lower the precision, the larger the grid cells. The following example illustrates low-precision and high-precision aggregation requests. To start, create an index and map the location field as a geo_point: . PUT national_parks { \"mappings\": { \"properties\": { \"location\": { \"type\": \"geo_point\" } } } } . Index the following documents into the sample index: . PUT national_parks/_doc/1 { \"name\": \"Yellowstone National Park\", \"location\": \"44.42, -110.59\" } PUT national_parks/_doc/2 { \"name\": \"Yosemite National Park\", \"location\": \"37.87, -119.53\" } PUT national_parks/_doc/3 { \"name\": \"Death Valley National Park\", \"location\": \"36.53, -116.93\" } . You can index geopoints in several formats. For a list of all supported formats, see the geopoint documentation. ",
    "url": "https://vagimeli.github.io/aggregations/geohexgrid/#precision",
    "relUrl": "/aggregations/geohexgrid/#precision"
  },"1311": {
    "doc": "GeoHex grid aggregations",
    "title": "Low-precision requests",
    "content": "Run a low-precision request that buckets all three documents together: . GET national_parks/_search { \"aggregations\": { \"grouped\": { \"geohex_grid\": { \"field\": \"location\", \"precision\": 1 } } } } . You can use either the GET or POST HTTP method for GeoHex grid aggregation queries. The response groups documents 2 and 3 together because they are close enough to be bucketed in one grid cell: . { \"took\" : 4, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 3, \"relation\" : \"eq\" }, \"max_score\" : 1.0, \"hits\" : [ { \"_index\" : \"national_parks\", \"_id\" : \"1\", \"_score\" : 1.0, \"_source\" : { \"name\" : \"Yellowstone National Park\", \"location\" : \"44.42, -110.59\" } }, { \"_index\" : \"national_parks\", \"_id\" : \"2\", \"_score\" : 1.0, \"_source\" : { \"name\" : \"Yosemite National Park\", \"location\" : \"37.87, -119.53\" } }, { \"_index\" : \"national_parks\", \"_id\" : \"3\", \"_score\" : 1.0, \"_source\" : { \"name\" : \"Death Valley National Park\", \"location\" : \"36.53, -116.93\" } } ] }, \"aggregations\" : { \"grouped\" : { \"buckets\" : [ { \"key\" : \"8129bffffffffff\", \"doc_count\" : 2 }, { \"key\" : \"8128bffffffffff\", \"doc_count\" : 1 } ] } } } . ",
    "url": "https://vagimeli.github.io/aggregations/geohexgrid/#low-precision-requests",
    "relUrl": "/aggregations/geohexgrid/#low-precision-requests"
  },"1312": {
    "doc": "GeoHex grid aggregations",
    "title": "High-precision requests",
    "content": "Now run a high-precision request: . GET national_parks/_search { \"aggregations\": { \"grouped\": { \"geohex_grid\": { \"field\": \"location\", \"precision\": 6 } } } } . All three documents are bucketed separately because of higher granularity: . { \"took\" : 5, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 3, \"relation\" : \"eq\" }, \"max_score\" : 1.0, \"hits\" : [ { \"_index\" : \"national_parks\", \"_id\" : \"1\", \"_score\" : 1.0, \"_source\" : { \"name\" : \"Yellowstone National Park\", \"location\" : \"44.42, -110.59\" } }, { \"_index\" : \"national_parks\", \"_id\" : \"2\", \"_score\" : 1.0, \"_source\" : { \"name\" : \"Yosemite National Park\", \"location\" : \"37.87, -119.53\" } }, { \"_index\" : \"national_parks\", \"_id\" : \"3\", \"_score\" : 1.0, \"_source\" : { \"name\" : \"Death Valley National Park\", \"location\" : \"36.53, -116.93\" } } ] }, \"aggregations\" : { \"grouped\" : { \"buckets\" : [ { \"key\" : \"8629ab6dfffffff\", \"doc_count\" : 1 }, { \"key\" : \"8629857a7ffffff\", \"doc_count\" : 1 }, { \"key\" : \"862896017ffffff\", \"doc_count\" : 1 } ] } } } . ",
    "url": "https://vagimeli.github.io/aggregations/geohexgrid/#high-precision-requests",
    "relUrl": "/aggregations/geohexgrid/#high-precision-requests"
  },"1313": {
    "doc": "GeoHex grid aggregations",
    "title": "Filtering requests",
    "content": "High-precision requests are resource intensive, so we recommend using a filter like geo_bounding_box to limit the geographical area. For example, the following query applies a filter to limit the search area: . GET national_parks/_search { \"size\" : 0, \"aggregations\": { \"filtered\": { \"filter\": { \"geo_bounding_box\": { \"location\": { \"top_left\": \"38, -120\", \"bottom_right\": \"36, -116\" } } }, \"aggregations\": { \"grouped\": { \"geohex_grid\": { \"field\": \"location\", \"precision\": 6 } } } } } } . The response contains the two documents that are within the geo_bounding_box bounds: . { \"took\" : 4, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 3, \"relation\" : \"eq\" }, \"max_score\" : null, \"hits\" : [ ] }, \"aggregations\" : { \"filtered\" : { \"doc_count\" : 2, \"grouped\" : { \"buckets\" : [ { \"key\" : \"8629ab6dfffffff\", \"doc_count\" : 1 }, { \"key\" : \"8629857a7ffffff\", \"doc_count\" : 1 } ] } } } } . You can also restrict the geographical area by providing the coordinates of the bounding envelope in the bounds parameter. Both bounds and geo_bounding_box coordinates can be specified in any of the geopoint formats. The following query uses the well-known text (WKT) “POINT(longitude latitude)” format for the bounds parameter: . GET national_parks/_search { \"size\": 0, \"aggregations\": { \"grouped\": { \"geohex_grid\": { \"field\": \"location\", \"precision\": 6, \"bounds\": { \"top_left\": \"POINT (-120 38)\", \"bottom_right\": \"POINT (-116 36)\" } } } } } . The response contains only the two results that are within the specified bounds: . { \"took\" : 3, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 3, \"relation\" : \"eq\" }, \"max_score\" : null, \"hits\" : [ ] }, \"aggregations\" : { \"grouped\" : { \"buckets\" : [ { \"key\" : \"8629ab6dfffffff\", \"doc_count\" : 1 }, { \"key\" : \"8629857a7ffffff\", \"doc_count\" : 1 } ] } } } . The bounds parameter can be used with or without the geo_bounding_box filter; these two parameters are independent and can have any spatial relationship to each other. ",
    "url": "https://vagimeli.github.io/aggregations/geohexgrid/#filtering-requests",
    "relUrl": "/aggregations/geohexgrid/#filtering-requests"
  },"1314": {
    "doc": "GeoHex grid aggregations",
    "title": "Supported parameters",
    "content": "GeoHex grid aggregation requests support the following parameters. | Parameter | Data type | Description | . | field | String | The field that contains the geopoints. This field must be mapped as a geo_point field. If the field contains an array, all array values are aggregated. Required. | . | precision | Integer | The zoom level used to determine grid cells for bucketing results. Valid values are in the [0, 15] range. Optional. Default is 5. | . | bounds | Object | The bounding box for filtering geopoints. The bounding box is defined by the top left and bottom right vertices. The vertices are specified as geopoints in one of the following formats: - An object with a latitude and longitude- An array in the [longitude, latitude] format- A string in the “latitude,longitude” format- A Geohash - WKT See the geopoint formats for formatting examples. Optional. | . | size | Integer | The maximum number of buckets to return. When there are more buckets than size, OpenSearch returns buckets with more documents. Optional. Default is 10,000. | . | shard_size | Integer | The maximum number of buckets to return from each shard. Optional. Default is max (10, size · number of shards), which provides a more accurate count of more highly prioritized buckets. | . ",
    "url": "https://vagimeli.github.io/aggregations/geohexgrid/#supported-parameters",
    "relUrl": "/aggregations/geohexgrid/#supported-parameters"
  },"1315": {
    "doc": "Metric aggregations",
    "title": "Metric aggregations",
    "content": "Metric aggregations let you perform simple calculations such as finding the minimum, maximum, and average values of a field. ",
    "url": "https://vagimeli.github.io/aggregations/metric-agg/",
    "relUrl": "/aggregations/metric-agg/"
  },"1316": {
    "doc": "Metric aggregations",
    "title": "Types of metric aggregations",
    "content": "Metric aggregations are of two types: single-value metric aggregations and multi-value metric aggregations. Single-value metric aggregations . Single-value metric aggregations return a single metric. For example, sum, min, max, avg, cardinality, and value_count. Multi-value metric aggregations . Multi-value metric aggregations return more than one metric. For example, stats, extended_stats, matrix_stats, percentile, percentile_ranks, geo_bound, top_hits, and scripted_metric. ",
    "url": "https://vagimeli.github.io/aggregations/metric-agg/#types-of-metric-aggregations",
    "relUrl": "/aggregations/metric-agg/#types-of-metric-aggregations"
  },"1317": {
    "doc": "Metric aggregations",
    "title": "sum, min, max, avg",
    "content": "The sum, min, max, and avg metrics are single-value metric aggregations that return the sum, minimum, maximum, and average values of a field, respectively. The following example calculates the total sum of the taxful_total_price field: . GET opensearch_dashboards_sample_data_ecommerce/_search { \"size\": 0, \"aggs\": { \"sum_taxful_total_price\": { \"sum\": { \"field\": \"taxful_total_price\" } } } } . Sample Response ... \"aggregations\" : { \"sum_taxful_total_price\" : { \"value\" : 350884.12890625 } } } . In a similar fashion, you can find the minimum, maximum, and average values of a field. ",
    "url": "https://vagimeli.github.io/aggregations/metric-agg/#sum-min-max-avg",
    "relUrl": "/aggregations/metric-agg/#sum-min-max-avg"
  },"1318": {
    "doc": "Metric aggregations",
    "title": "cardinality",
    "content": "The cardinality metric is a single-value metric aggregation that counts the number of unique or distinct values of a field. The following example finds the number of unique products in an eCommerce store: . GET opensearch_dashboards_sample_data_ecommerce/_search { \"size\": 0, \"aggs\": { \"unique_products\": { \"cardinality\": { \"field\": \"products.product_id\" } } } } . Example response ... \"aggregations\" : { \"unique_products\" : { \"value\" : 7033 } } } . Cardinality count is approximate. If you have tens of thousands of products in your hypothetical store, an accurate cardinality calculation requires loading all the values into a hash set and returning its size. This approach doesn’t scale well; it requires huge amounts of memory and can cause high latencies. You can control the trade-off between memory and accuracy with the precision_threshold setting. This setting defines the threshold below which counts are expected to be close to accurate. Above this value, counts might become a bit less accurate. The default value of precision_threshold is 3,000. The maximum supported value is 40,000. GET opensearch_dashboards_sample_data_ecommerce/_search { \"size\": 0, \"aggs\": { \"unique_products\": { \"cardinality\": { \"field\": \"products.product_id\", \"precision_threshold\": 10000 } } } } . ",
    "url": "https://vagimeli.github.io/aggregations/metric-agg/#cardinality",
    "relUrl": "/aggregations/metric-agg/#cardinality"
  },"1319": {
    "doc": "Metric aggregations",
    "title": "value_count",
    "content": "The value_count metric is a single-value metric aggregation that calculates the number of values that an aggregation is based on. For example, you can use the value_count metric with the avg metric to find how many numbers the aggregation uses to calculate an average value. GET opensearch_dashboards_sample_data_ecommerce/_search { \"size\": 0, \"aggs\": { \"number_of_values\": { \"value_count\": { \"field\": \"taxful_total_price\" } } } } . Example response ... \"aggregations\" : { \"number_of_values\" : { \"value\" : 4675 } } } . ",
    "url": "https://vagimeli.github.io/aggregations/metric-agg/#value_count",
    "relUrl": "/aggregations/metric-agg/#value_count"
  },"1320": {
    "doc": "Metric aggregations",
    "title": "stats, extended_stats, matrix_stats",
    "content": "The stats metric is a multi-value metric aggregation that returns all basic metrics such as min, max, sum, avg, and value_count in one aggregation query. The following example returns the basic stats for the taxful_total_price field: . GET opensearch_dashboards_sample_data_ecommerce/_search { \"size\": 0, \"aggs\": { \"stats_taxful_total_price\": { \"stats\": { \"field\": \"taxful_total_price\" } } } } . Example response ... \"aggregations\" : { \"stats_taxful_total_price\" : { \"count\" : 4675, \"min\" : 6.98828125, \"max\" : 2250.0, \"avg\" : 75.05542864304813, \"sum\" : 350884.12890625 } } } . The extended_stats aggregation is an extended version of the stats aggregation. Apart from including basic stats, extended_stats also returns stats such as sum_of_squares, variance, and std_deviation. GET opensearch_dashboards_sample_data_ecommerce/_search { \"size\": 0, \"aggs\": { \"extended_stats_taxful_total_price\": { \"extended_stats\": { \"field\": \"taxful_total_price\" } } } } . Sample Response ... \"aggregations\" : { \"extended_stats_taxful_total_price\" : { \"count\" : 4675, \"min\" : 6.98828125, \"max\" : 2250.0, \"avg\" : 75.05542864304813, \"sum\" : 350884.12890625, \"sum_of_squares\" : 3.9367749294174194E7, \"variance\" : 2787.59157113862, \"variance_population\" : 2787.59157113862, \"variance_sampling\" : 2788.187974983536, \"std_deviation\" : 52.79764740155209, \"std_deviation_population\" : 52.79764740155209, \"std_deviation_sampling\" : 52.80329511482722, \"std_deviation_bounds\" : { \"upper\" : 180.6507234461523, \"lower\" : -30.53986616005605, \"upper_population\" : 180.6507234461523, \"lower_population\" : -30.53986616005605, \"upper_sampling\" : 180.66201887270256, \"lower_sampling\" : -30.551161586606312 } } } } . The std_deviation_bounds object provides a visual variance of the data with an interval of plus/minus two standard deviations from the mean. To set the standard deviation to a different value, say 3, set sigma to 3: . GET opensearch_dashboards_sample_data_ecommerce/_search { \"size\": 0, \"aggs\": { \"extended_stats_taxful_total_price\": { \"extended_stats\": { \"field\": \"taxful_total_price\", \"sigma\": 3 } } } } . The matrix_stats aggregation generates advanced stats for multiple fields in a matrix form. The following example returns advanced stats in a matrix form for the taxful_total_price and products.base_price fields: . GET opensearch_dashboards_sample_data_ecommerce/_search { \"size\": 0, \"aggs\": { \"matrix_stats_taxful_total_price\": { \"matrix_stats\": { \"fields\": [\"taxful_total_price\", \"products.base_price\"] } } } } . Example response ... \"aggregations\" : { \"matrix_stats_taxful_total_price\" : { \"doc_count\" : 4675, \"fields\" : [ { \"name\" : \"products.base_price\", \"count\" : 4675, \"mean\" : 34.994239430147196, \"variance\" : 360.5035285833703, \"skewness\" : 5.530161335032702, \"kurtosis\" : 131.16306324042148, \"covariance\" : { \"products.base_price\" : 360.5035285833703, \"taxful_total_price\" : 846.6489362233166 }, \"correlation\" : { \"products.base_price\" : 1.0, \"taxful_total_price\" : 0.8444765264325268 } }, { \"name\" : \"taxful_total_price\", \"count\" : 4675, \"mean\" : 75.05542864304839, \"variance\" : 2788.1879749835402, \"skewness\" : 15.812149139924037, \"kurtosis\" : 619.1235507385902, \"covariance\" : { \"products.base_price\" : 846.6489362233166, \"taxful_total_price\" : 2788.1879749835402 }, \"correlation\" : { \"products.base_price\" : 0.8444765264325268, \"taxful_total_price\" : 1.0 } } ] } } } . | Statistic | Description | . | count | The number of samples measured. | . | mean | The average value of the field measured from the sample. | . | variance | How far the values of the field measured are spread out from its mean value. The larger the variance, the more it’s spread from its mean value. | . | skewness | An asymmetric measure of the distribution of the field’s values around the mean. | . | kurtosis | A measure of the tail heaviness of a distribution. As the tail becomes lighter, kurtosis decreases. As the tail becomes heavier, kurtosis increases. To learn about kurtosis, see Wikipedia. | . | covariance | A measure of the joint variability between two fields. A positive value means their values move in the same direction and vice versa. | . | correlation | A measure of the strength of the relationship between two fields. The valid values are between [-1, 1]. A value of -1 means that the value is negatively correlated and a value of 1 means that it’s positively correlated. A value of 0 means that there’s no identifiable relationship between them. | . ",
    "url": "https://vagimeli.github.io/aggregations/metric-agg/#stats-extended_stats-matrix_stats",
    "relUrl": "/aggregations/metric-agg/#stats-extended_stats-matrix_stats"
  },"1321": {
    "doc": "Metric aggregations",
    "title": "percentile, percentile_ranks",
    "content": "Percentile is the percentage of the data that’s at or below a certain threshold value. The percentile metric is a multi-value metric aggregation that lets you find outliers in your data or figure out the distribution of your data. Like the cardinality metric, the percentile metric is also approximate. The following example calculates the percentile in relation to the taxful_total_price field: . GET opensearch_dashboards_sample_data_ecommerce/_search { \"size\": 0, \"aggs\": { \"percentile_taxful_total_price\": { \"percentiles\": { \"field\": \"taxful_total_price\" } } } } . Example response ... \"aggregations\" : { \"percentile_taxful_total_price\" : { \"values\" : { \"1.0\" : 21.984375, \"5.0\" : 27.984375, \"25.0\" : 44.96875, \"50.0\" : 64.22061688311689, \"75.0\" : 93.0, \"95.0\" : 156.0, \"99.0\" : 222.0 } } } } . Percentile rank is the percentile of values at or below a threshold grouped by a specified value. For example, if a value is greater than or equal to 80% of the values, it has a percentile rank of 80. GET opensearch_dashboards_sample_data_ecommerce/_search { \"size\": 0, \"aggs\": { \"percentile_rank_taxful_total_price\": { \"percentile_ranks\": { \"field\": \"taxful_total_price\", \"values\": [ 10, 15 ] } } } } . Example response ... \"aggregations\" : { \"percentile_rank_taxful_total_price\" : { \"values\" : { \"10.0\" : 0.055096056411283456, \"15.0\" : 0.0830092961834656 } } } } . ",
    "url": "https://vagimeli.github.io/aggregations/metric-agg/#percentile-percentile_ranks",
    "relUrl": "/aggregations/metric-agg/#percentile-percentile_ranks"
  },"1322": {
    "doc": "Metric aggregations",
    "title": "geo_bound",
    "content": "The geo_bound metric is a multi-value metric aggregation that calculates the bounding box in terms of latitude and longitude around a geo_point field. The following example returns the geo_bound metrics for the geoip.location field: . GET opensearch_dashboards_sample_data_ecommerce/_search { \"size\": 0, \"aggs\": { \"geo\": { \"geo_bounds\": { \"field\": \"geoip.location\" } } } } . Example response . \"aggregations\" : { \"geo\" : { \"bounds\" : { \"top_left\" : { \"lat\" : 52.49999997206032, \"lon\" : -118.20000001229346 }, \"bottom_right\" : { \"lat\" : 4.599999985657632, \"lon\" : 55.299999956041574 } } } } } . ",
    "url": "https://vagimeli.github.io/aggregations/metric-agg/#geo_bound",
    "relUrl": "/aggregations/metric-agg/#geo_bound"
  },"1323": {
    "doc": "Metric aggregations",
    "title": "top_hits",
    "content": "The top_hits metric is a multi-value metric aggregation that ranks the matching documents based on a relevance score for the field that’s being aggregated. You can specify the following options: . | from: The starting position of the hit. | size: The maximum size of hits to return. The default value is 3. | sort: How the matching hits are sorted. By default, the hits are sorted by the relevance score of the aggregation query. | . The following example returns the top 5 products in your eCommerce data: . GET opensearch_dashboards_sample_data_ecommerce/_search { \"size\": 0, \"aggs\": { \"top_hits_products\": { \"top_hits\": { \"size\": 5 } } } } . Example response ... \"aggregations\" : { \"top_hits_products\" : { \"hits\" : { \"total\" : { \"value\" : 4675, \"relation\" : \"eq\" }, \"max_score\" : 1.0, \"hits\" : [ { \"_index\" : \"opensearch_dashboards_sample_data_ecommerce\", \"_type\" : \"_doc\", \"_id\" : \"glMlwXcBQVLeQPrkHPtI\", \"_score\" : 1.0, \"_source\" : { \"category\" : [ \"Women's Accessories\", \"Women's Clothing\" ], \"currency\" : \"EUR\", \"customer_first_name\" : \"rania\", \"customer_full_name\" : \"rania Evans\", \"customer_gender\" : \"FEMALE\", \"customer_id\" : 24, \"customer_last_name\" : \"Evans\", \"customer_phone\" : \"\", \"day_of_week\" : \"Sunday\", \"day_of_week_i\" : 6, \"email\" : \"rania@evans-family.zzz\", \"manufacturer\" : [ \"Tigress Enterprises\" ], \"order_date\" : \"2021-02-28T14:16:48+00:00\", \"order_id\" : 583581, \"products\" : [ { \"base_price\" : 10.99, \"discount_percentage\" : 0, \"quantity\" : 1, \"manufacturer\" : \"Tigress Enterprises\", \"tax_amount\" : 0, \"product_id\" : 19024, \"category\" : \"Women's Accessories\", \"sku\" : \"ZO0082400824\", \"taxless_price\" : 10.99, \"unit_discount_amount\" : 0, \"min_price\" : 5.17, \"_id\" : \"sold_product_583581_19024\", \"discount_amount\" : 0, \"created_on\" : \"2016-12-25T14:16:48+00:00\", \"product_name\" : \"Snood - white/grey/peach\", \"price\" : 10.99, \"taxful_price\" : 10.99, \"base_unit_price\" : 10.99 }, { \"base_price\" : 32.99, \"discount_percentage\" : 0, \"quantity\" : 1, \"manufacturer\" : \"Tigress Enterprises\", \"tax_amount\" : 0, \"product_id\" : 19260, \"category\" : \"Women's Clothing\", \"sku\" : \"ZO0071900719\", \"taxless_price\" : 32.99, \"unit_discount_amount\" : 0, \"min_price\" : 17.15, \"_id\" : \"sold_product_583581_19260\", \"discount_amount\" : 0, \"created_on\" : \"2016-12-25T14:16:48+00:00\", \"product_name\" : \"Cardigan - grey\", \"price\" : 32.99, \"taxful_price\" : 32.99, \"base_unit_price\" : 32.99 } ], \"sku\" : [ \"ZO0082400824\", \"ZO0071900719\" ], \"taxful_total_price\" : 43.98, \"taxless_total_price\" : 43.98, \"total_quantity\" : 2, \"total_unique_products\" : 2, \"type\" : \"order\", \"user\" : \"rani\", \"geoip\" : { \"country_iso_code\" : \"EG\", \"location\" : { \"lon\" : 31.3, \"lat\" : 30.1 }, \"region_name\" : \"Cairo Governorate\", \"continent_name\" : \"Africa\", \"city_name\" : \"Cairo\" }, \"event\" : { \"dataset\" : \"sample_ecommerce\" } } ... } ] } } } } . ",
    "url": "https://vagimeli.github.io/aggregations/metric-agg/#top_hits",
    "relUrl": "/aggregations/metric-agg/#top_hits"
  },"1324": {
    "doc": "Metric aggregations",
    "title": "scripted_metric",
    "content": "The scripted_metric metric is a multi-value metric aggregation that returns metrics calculated from a specified script. A script has four stages: the initial stage, the map stage, the combine stage, and the reduce stage. | init_script: (OPTIONAL) Sets the initial state and executes before any collection of documents. | map_script: Checks the value of the type field and executes the aggregation on the collected documents. | combine_script: Aggregates the state returned from every shard. The aggregated value is returned to the coordinating node. | reduce_script: Provides access to the variable states; this variable combines the results from the combine_script on each shard into an array. | . The following example aggregates the different HTTP response types in web log data: . GET opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"aggregations\": { \"responses.counts\": { \"scripted_metric\": { \"init_script\": \"state.responses = ['error':0L,'success':0L,'other':0L]\", \"map_script\": \"\"\" def code = doc['response.keyword'].value; if (code.startsWith('5') || code.startsWith('4')) { state.responses.error += 1 ; } else if(code.startsWith('2')) { state.responses.success += 1; } else { state.responses.other += 1; } \"\"\", \"combine_script\": \"state.responses\", \"reduce_script\": \"\"\" def counts = ['error': 0L, 'success': 0L, 'other': 0L]; for (responses in states) { counts.error += responses['error']; counts.success += responses['success']; counts.other += responses['other']; } return counts; \"\"\" } } } } . Sample Response ... \"aggregations\" : { \"responses.counts\" : { \"value\" : { \"other\" : 0, \"success\" : 12832, \"error\" : 1242 } } } } . ",
    "url": "https://vagimeli.github.io/aggregations/metric-agg/#scripted_metric",
    "relUrl": "/aggregations/metric-agg/#scripted_metric"
  },"1325": {
    "doc": "Pipeline aggregations",
    "title": "Pipeline aggregations",
    "content": "With pipeline aggregations, you can chain aggregations by piping the results of one aggregation as an input to another for a more nuanced output. You can use pipeline aggregations to compute complex statistical and mathematical measures like derivatives, moving averages, cumulative sums, and so on. ",
    "url": "https://vagimeli.github.io/aggregations/pipeline-agg/",
    "relUrl": "/aggregations/pipeline-agg/"
  },"1326": {
    "doc": "Pipeline aggregations",
    "title": "Pipeline aggregation syntax",
    "content": "A pipeline aggregation uses the buckets_path property to access the results of other aggregations. The buckets_path property has a specific syntax: . buckets_path = &lt;AGG_NAME&gt;[&lt;AGG_SEPARATOR&gt;,&lt;AGG_NAME&gt;]*[&lt;METRIC_SEPARATOR&gt;, &lt;METRIC&gt;]; . where: . | AGG_NAME is the name of the aggregation. | AGG_SEPARATOR separates aggregations. It’s represented as &gt;. | METRIC_SEPARATOR separates aggregations from its metrics. It’s represented as .. | METRIC is the name of the metric, in case of multi-value metric aggregations. | . For example, my_sum.sum selects the sum metric of an aggregation called my_sum. popular_tags&gt;my_sum.sum nests my_sum.sum into the popular_tags aggregation. You can also specify the following additional parameters: . | gap_policy: Real-world data can contain gaps or null values. You can specify the policy to deal with such missing data with the gap_policy property. You can either set the gap_policy property to skip to skip the missing data and continue from the next available value, or insert_zeros to replace the missing values with zero and continue running. | format: The type of format for the output value. For example, yyyy-MM-dd for a date value. | . ",
    "url": "https://vagimeli.github.io/aggregations/pipeline-agg/#pipeline-aggregation-syntax",
    "relUrl": "/aggregations/pipeline-agg/#pipeline-aggregation-syntax"
  },"1327": {
    "doc": "Pipeline aggregations",
    "title": "Quick example",
    "content": "To sum all the buckets returned by the sum_total_memory aggregation: . GET opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"aggs\": { \"number_of_bytes\": { \"histogram\": { \"field\": \"bytes\", \"interval\": 10000 }, \"aggs\": { \"sum_total_memory\": { \"sum\": { \"field\": \"phpmemory\" } } } }, \"sum_copies\": { \"sum_bucket\": { \"buckets_path\": \"number_of_bytes&gt;sum_total_memory\" } } } } . Example response ... \"aggregations\" : { \"number_of_bytes\" : { \"buckets\" : [ { \"key\" : 0.0, \"doc_count\" : 13372, \"sum_total_memory\" : { \"value\" : 9.12664E7 } }, { \"key\" : 10000.0, \"doc_count\" : 702, \"sum_total_memory\" : { \"value\" : 0.0 } } ] }, \"sum_copies\" : { \"value\" : 9.12664E7 } } } . ",
    "url": "https://vagimeli.github.io/aggregations/pipeline-agg/#quick-example",
    "relUrl": "/aggregations/pipeline-agg/#quick-example"
  },"1328": {
    "doc": "Pipeline aggregations",
    "title": "Types of pipeline aggregations",
    "content": "Pipeline aggregations are of two types: . Sibling aggregations . Sibling aggregations take the output of a nested aggregation and produce new buckets or new aggregations at the same level as the nested buckets. Sibling aggregations must be a multi-bucket aggregation (have multiple grouped values for a certain field) and the metric must be a numeric value. min_bucket, max_bucket, sum_bucket, and avg_bucket are common sibling aggregations. Parent aggregations . Parent aggregations take the output of an outer aggregation and produce new buckets or new aggregations at the same level as the existing buckets. Parent aggregations must have min_doc_count set to 0 (default for histogram aggregations) and the specified metric must be a numeric value. If min_doc_count is greater than 0, some buckets are omitted, which might lead to incorrect results. derivatives and cumulative_sum are common parent aggregations. ",
    "url": "https://vagimeli.github.io/aggregations/pipeline-agg/#types-of-pipeline-aggregations",
    "relUrl": "/aggregations/pipeline-agg/#types-of-pipeline-aggregations"
  },"1329": {
    "doc": "Pipeline aggregations",
    "title": "avg_bucket, sum_bucket, min_bucket, max_bucket",
    "content": "The avg_bucket, sum_bucket, min_bucket, and max_bucket aggregations are sibling aggregations that calculate the average, sum, minimum, and maximum values of a metric in each bucket of a previous aggregation. The following example creates a date histogram with a one-month interval. The sum sub-aggregation calculates the sum of all bytes for each month. Finally, the avg_bucket aggregation uses this sum to calculate the average number of bytes per month: . POST opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"aggs\": { \"visits_per_month\": { \"date_histogram\": { \"field\": \"@timestamp\", \"interval\": \"month\" }, \"aggs\": { \"sum_of_bytes\": { \"sum\": { \"field\": \"bytes\" } } } }, \"avg_monthly_bytes\": { \"avg_bucket\": { \"buckets_path\": \"visits_per_month&gt;sum_of_bytes\" } } } } . Example response ... \"aggregations\" : { \"visits_per_month\" : { \"buckets\" : [ { \"key_as_string\" : \"2020-10-01T00:00:00.000Z\", \"key\" : 1601510400000, \"doc_count\" : 1635, \"sum_of_bytes\" : { \"value\" : 9400200.0 } }, { \"key_as_string\" : \"2020-11-01T00:00:00.000Z\", \"key\" : 1604188800000, \"doc_count\" : 6844, \"sum_of_bytes\" : { \"value\" : 3.8880434E7 } }, { \"key_as_string\" : \"2020-12-01T00:00:00.000Z\", \"key\" : 1606780800000, \"doc_count\" : 5595, \"sum_of_bytes\" : { \"value\" : 3.1445055E7 } } ] }, \"avg_monthly_bytes\" : { \"value\" : 2.6575229666666668E7 } } } . In a similar fashion, you can calculate the sum_bucket, min_bucket, and max_bucket values for the bytes per month. ",
    "url": "https://vagimeli.github.io/aggregations/pipeline-agg/#avg_bucket-sum_bucket-min_bucket-max_bucket",
    "relUrl": "/aggregations/pipeline-agg/#avg_bucket-sum_bucket-min_bucket-max_bucket"
  },"1330": {
    "doc": "Pipeline aggregations",
    "title": "stats_bucket, extended_stats_bucket",
    "content": "The stats_bucket aggregation is a sibling aggregation that returns a variety of stats (count, min, max, avg, and sum) for the buckets of a previous aggregation. The following example returns the basic stats for the buckets returned by the sum_of_bytes aggregation nested into the visits_per_month aggregation: . GET opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"aggs\": { \"visits_per_month\": { \"date_histogram\": { \"field\": \"@timestamp\", \"interval\": \"month\" }, \"aggs\": { \"sum_of_bytes\": { \"sum\": { \"field\": \"bytes\" } } } }, \"stats_monthly_bytes\": { \"stats_bucket\": { \"buckets_path\": \"visits_per_month&gt;sum_of_bytes\" } } } } . Example response ... \"stats_monthly_bytes\" : { \"count\" : 3, \"min\" : 9400200.0, \"max\" : 3.8880434E7, \"avg\" : 2.6575229666666668E7, \"sum\" : 7.9725689E7 } } } . The extended_stats aggregation is an extended version of the stats aggregation. Apart from including basic stats, extended_stats also provides stats such as sum_of_squares, variance, and std_deviation. Example response . \"stats_monthly_visits\" : { \"count\" : 3, \"min\" : 9400200.0, \"max\" : 3.8880434E7, \"avg\" : 2.6575229666666668E7, \"sum\" : 7.9725689E7, \"sum_of_squares\" : 2.588843392021381E15, \"variance\" : 1.5670496550438025E14, \"variance_population\" : 1.5670496550438025E14, \"variance_sampling\" : 2.3505744825657038E14, \"std_deviation\" : 1.251818539183616E7, \"std_deviation_population\" : 1.251818539183616E7, \"std_deviation_sampling\" : 1.5331583357780447E7, \"std_deviation_bounds\" : { \"upper\" : 5.161160045033899E7, \"lower\" : 1538858.8829943463, \"upper_population\" : 5.161160045033899E7, \"lower_population\" : 1538858.8829943463, \"upper_sampling\" : 5.723839638222756E7, \"lower_sampling\" : -4087937.0488942266 } } } } . ",
    "url": "https://vagimeli.github.io/aggregations/pipeline-agg/#stats_bucket-extended_stats_bucket",
    "relUrl": "/aggregations/pipeline-agg/#stats_bucket-extended_stats_bucket"
  },"1331": {
    "doc": "Pipeline aggregations",
    "title": "bucket_script, bucket_selector",
    "content": "The bucket_script aggregation is a parent aggregation that executes a script to perform per-bucket calculations of a previous aggregation. Make sure the metrics are of numeric type and the returned values are also numeric. Use the script parameter to add your script. The script can be inline, in a file, or in an index. To enable inline scripting, add the following line to your opensearch.yml file in the config folder: . script.inline: on . The buckets_path property consists of multiple entries. Each entry is a key and a value. The key is the name of the value that you can use in the script. The basic syntax is: . { \"bucket_script\": { \"buckets_path\": { \"my_var1\": \"the_sum\", \"my_var2\": \"the_value_count\" }, \"script\": \"params.my_var1 / params.my_var2\" } } . The following example uses the sum aggregation on the buckets generated by a date histogram. From the resultant buckets values, the percentage of RAM is calculated in an interval of 10,000 bytes in the context of a zip extension: . GET opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"aggs\": { \"sales_per_month\": { \"histogram\": { \"field\": \"bytes\", \"interval\": \"10000\" }, \"aggs\": { \"total_ram\": { \"sum\": { \"field\": \"machine.ram\" } }, \"ext-type\": { \"filter\": { \"term\": { \"extension.keyword\": \"zip\" } }, \"aggs\": { \"total_ram\": { \"sum\": { \"field\": \"machine.ram\" } } } }, \"ram-percentage\": { \"bucket_script\": { \"buckets_path\": { \"machineRam\": \"ext-type&gt;total_ram\", \"totalRam\": \"total_ram\" }, \"script\": \"params.machineRam / params.totalRam\" } } } } } } . Example response . \"aggregations\" : { \"sales_per_month\" : { \"buckets\" : [ { \"key\" : 0.0, \"doc_count\" : 13372, \"os-type\" : { \"doc_count\" : 1558, \"total_ram\" : { \"value\" : 2.0090783268864E13 } }, \"total_ram\" : { \"value\" : 1.7214228922368E14 }, \"ram-percentage\" : { \"value\" : 0.11671032934131736 } }, { \"key\" : 10000.0, \"doc_count\" : 702, \"os-type\" : { \"doc_count\" : 116, \"total_ram\" : { \"value\" : 1.622423896064E12 } }, \"total_ram\" : { \"value\" : 9.015136354304E12 }, \"ram-percentage\" : { \"value\" : 0.17996665078608862 } } ] } } } . The RAM percentage is calculated and appended at the end of each bucket. The bucket_selector aggregation is a script-based aggregation that selects buckets returned by a histogram (or date_histogram) aggregation. Use it in scenarios where you don’t want certain buckets in the output based on conditions supplied by you. The bucket_selector aggregation executes a script to decide if a bucket stays in the parent multi-bucket aggregation. The basic syntax is: . { \"bucket_selector\": { \"buckets_path\": { \"my_var1\": \"the_sum\", \"my_var2\": \"the_value_count\" }, \"script\": \"params.my_var1 / params.my_var2\" } } . The following example calculates the sum of bytes and then evaluates if this sum is greater than 20,000. If true, then the bucket is retained in the bucket list. Otherwise, it’s deleted from the final output. GET opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"aggs\": { \"bytes_per_month\": { \"date_histogram\": { \"field\": \"@timestamp\", \"calendar_interval\": \"month\" }, \"aggs\": { \"total_bytes\": { \"sum\": { \"field\": \"bytes\" } }, \"bytes_bucket_filter\": { \"bucket_selector\": { \"buckets_path\": { \"totalBytes\": \"total_bytes\" }, \"script\": \"params.totalBytes &gt; 20000\" } } } } } } . Example response . \"aggregations\" : { \"bytes_per_month\" : { \"buckets\" : [ { \"key_as_string\" : \"2020-10-01T00:00:00.000Z\", \"key\" : 1601510400000, \"doc_count\" : 1635, \"total_bytes\" : { \"value\" : 9400200.0 } }, { \"key_as_string\" : \"2020-11-01T00:00:00.000Z\", \"key\" : 1604188800000, \"doc_count\" : 6844, \"total_bytes\" : { \"value\" : 3.8880434E7 } }, { \"key_as_string\" : \"2020-12-01T00:00:00.000Z\", \"key\" : 1606780800000, \"doc_count\" : 5595, \"total_bytes\" : { \"value\" : 3.1445055E7 } } ] } } } . ",
    "url": "https://vagimeli.github.io/aggregations/pipeline-agg/#bucket_script-bucket_selector",
    "relUrl": "/aggregations/pipeline-agg/#bucket_script-bucket_selector"
  },"1332": {
    "doc": "Pipeline aggregations",
    "title": "bucket_sort",
    "content": "The bucket_sort aggregation is a parent aggregation that sorts buckets of a previous aggregation. You can specify several sort fields together with the corresponding sort order. Additionally, you can sort each bucket based on its key, count, or its sub-aggregations. You can also truncate the buckets by setting from and size parameters. Syntax . { \"bucket_sort\": { \"sort\": [ {\"sort_field_1\": {\"order\": \"asc\"}}, {\"sort_field_2\": {\"order\": \"desc\"}}, \"sort_field_3\" ], \"from\":1, \"size\":3 } } . The following example sorts the buckets of a date_histogram aggregation based on the computed total_sum values. We sort the buckets in descending order so that the buckets with the highest number of bytes are returned first. GET opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"aggs\": { \"sales_per_month\": { \"date_histogram\": { \"field\": \"@timestamp\", \"calendar_interval\": \"month\" }, \"aggs\": { \"total_bytes\": { \"sum\": { \"field\": \"bytes\" } }, \"bytes_bucket_sort\": { \"bucket_sort\": { \"sort\": [ { \"total_bytes\": { \"order\": \"desc\" } } ], \"size\": 3 } } } } } } . Example response . \"aggregations\" : { \"sales_per_month\" : { \"buckets\" : [ { \"key_as_string\" : \"2020-11-01T00:00:00.000Z\", \"key\" : 1604188800000, \"doc_count\" : 6844, \"total_bytes\" : { \"value\" : 3.8880434E7 } }, { \"key_as_string\" : \"2020-12-01T00:00:00.000Z\", \"key\" : 1606780800000, \"doc_count\" : 5595, \"total_bytes\" : { \"value\" : 3.1445055E7 } }, { \"key_as_string\" : \"2020-10-01T00:00:00.000Z\", \"key\" : 1601510400000, \"doc_count\" : 1635, \"total_bytes\" : { \"value\" : 9400200.0 } } ] } } } . You can also use this aggregation to truncate the resulting buckets without sorting. For this, just use the from and/or size parameters without sort. ",
    "url": "https://vagimeli.github.io/aggregations/pipeline-agg/#bucket_sort",
    "relUrl": "/aggregations/pipeline-agg/#bucket_sort"
  },"1333": {
    "doc": "Pipeline aggregations",
    "title": "cumulative_sum",
    "content": "The cumulative_sum aggregation is a parent aggregation that calculates the cumulative sum of each bucket of a previous aggregation. A cumulative sum is a sequence of partial sums of a given sequence. For example, the cumulative sums of the sequence {a,b,c,…} are a, a+b, a+b+c, and so on. You can use the cumulative sum to visualize the rate of change of a field over time. The following example calculates the cumulative number of bytes over a monthly basis: . GET opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"aggs\": { \"sales_per_month\": { \"date_histogram\": { \"field\": \"@timestamp\", \"calendar_interval\": \"month\" }, \"aggs\": { \"no-of-bytes\": { \"sum\": { \"field\": \"bytes\" } }, \"cumulative_bytes\": { \"cumulative_sum\": { \"buckets_path\": \"no-of-bytes\" } } } } } } . Example response ... \"aggregations\" : { \"sales_per_month\" : { \"buckets\" : [ { \"key_as_string\" : \"2020-10-01T00:00:00.000Z\", \"key\" : 1601510400000, \"doc_count\" : 1635, \"no-of-bytes\" : { \"value\" : 9400200.0 }, \"cumulative_bytes\" : { \"value\" : 9400200.0 } }, { \"key_as_string\" : \"2020-11-01T00:00:00.000Z\", \"key\" : 1604188800000, \"doc_count\" : 6844, \"no-of-bytes\" : { \"value\" : 3.8880434E7 }, \"cumulative_bytes\" : { \"value\" : 4.8280634E7 } }, { \"key_as_string\" : \"2020-12-01T00:00:00.000Z\", \"key\" : 1606780800000, \"doc_count\" : 5595, \"no-of-bytes\" : { \"value\" : 3.1445055E7 }, \"cumulative_bytes\" : { \"value\" : 7.9725689E7 } } ] } } } . ",
    "url": "https://vagimeli.github.io/aggregations/pipeline-agg/#cumulative_sum",
    "relUrl": "/aggregations/pipeline-agg/#cumulative_sum"
  },"1334": {
    "doc": "Pipeline aggregations",
    "title": "derivative",
    "content": "The derivative aggregation is a parent aggregation that calculates 1st order and 2nd order derivates of each bucket of a previous aggregation. In mathematics, the derivative of a function measures its sensitivity to change. In other words, a derivative evaluates the rate of change in some function with respect to some variable. To learn more about derivates, see Wikipedia. You can use derivates to calculate the rate of change of numeric values compared to its previous time periods. The 1st order derivative indicates whether a metric is increasing or decreasing, and by how much it’s increasing or decreasing. The following example calculates the 1st order derivative for the sum of bytes per month. The 1st order derivative is the difference between the number of bytes in the current month and the previous month: . GET opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"aggs\": { \"sales_per_month\": { \"date_histogram\": { \"field\": \"@timestamp\", \"calendar_interval\": \"month\" }, \"aggs\": { \"number_of_bytes\": { \"sum\": { \"field\": \"bytes\" } }, \"bytes_deriv\": { \"derivative\": { \"buckets_path\": \"number_of_bytes\" } } } } } } . Example response ... \"aggregations\" : { \"sales_per_month\" : { \"buckets\" : [ { \"key_as_string\" : \"2020-10-01T00:00:00.000Z\", \"key\" : 1601510400000, \"doc_count\" : 1635, \"number_of_bytes\" : { \"value\" : 9400200.0 } }, { \"key_as_string\" : \"2020-11-01T00:00:00.000Z\", \"key\" : 1604188800000, \"doc_count\" : 6844, \"number_of_bytes\" : { \"value\" : 3.8880434E7 }, \"bytes_deriv\" : { \"value\" : 2.9480234E7 } }, { \"key_as_string\" : \"2020-12-01T00:00:00.000Z\", \"key\" : 1606780800000, \"doc_count\" : 5595, \"number_of_bytes\" : { \"value\" : 3.1445055E7 }, \"bytes_deriv\" : { \"value\" : -7435379.0 } } ] } } } . The 2nd order derivative is a double derivative or a derivative of the derivative. It indicates how the rate of change of a quantity is itself changing. It’s the difference between the 1st order derivatives of adjacent buckets. To calculate a 2nd order derivative, chain one derivative aggregation to another: . GET opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"aggs\": { \"sales_per_month\": { \"date_histogram\": { \"field\": \"@timestamp\", \"calendar_interval\": \"month\" }, \"aggs\": { \"number_of_bytes\": { \"sum\": { \"field\": \"bytes\" } }, \"bytes_deriv\": { \"derivative\": { \"buckets_path\": \"number_of_bytes\" } }, \"bytes_2nd_deriv\": { \"derivative\": { \"buckets_path\": \"bytes_deriv\" } } } } } } . Example response ... \"aggregations\" : { \"sales_per_month\" : { \"buckets\" : [ { \"key_as_string\" : \"2020-10-01T00:00:00.000Z\", \"key\" : 1601510400000, \"doc_count\" : 1635, \"number_of_bytes\" : { \"value\" : 9400200.0 } }, { \"key_as_string\" : \"2020-11-01T00:00:00.000Z\", \"key\" : 1604188800000, \"doc_count\" : 6844, \"number_of_bytes\" : { \"value\" : 3.8880434E7 }, \"bytes_deriv\" : { \"value\" : 2.9480234E7 } }, { \"key_as_string\" : \"2020-12-01T00:00:00.000Z\", \"key\" : 1606780800000, \"doc_count\" : 5595, \"number_of_bytes\" : { \"value\" : 3.1445055E7 }, \"bytes_deriv\" : { \"value\" : -7435379.0 }, \"bytes_2nd_deriv\" : { \"value\" : -3.6915613E7 } } ] } } } . The first bucket doesn’t have a 1st order derivate as a derivate needs at least two points for comparison. The first and second buckets don’t have a 2nd order derivate because a 2nd order derivate needs at least two data points from the 1st order derivative. The 1st order derivative for the “2020-11-01” bucket is 2.9480234E7 and the “2020-12-01” bucket is -7435379. So, the 2nd order derivative of the “2020-12-01” bucket is -3.6915613E7 (-7435379-2.9480234E7). Theoretically, you could continue chaining derivate aggregations to calculate the third, the fourth, and even higher-order derivatives. That would, however, provide little to no value for most datasets. ",
    "url": "https://vagimeli.github.io/aggregations/pipeline-agg/#derivative",
    "relUrl": "/aggregations/pipeline-agg/#derivative"
  },"1335": {
    "doc": "Pipeline aggregations",
    "title": "moving_avg",
    "content": "A moving_avg aggregation is a parent aggregation that calculates the moving average metric. The moving_avg aggregation finds the series of averages of different windows (subsets) of a dataset. A window’s size represents the number of data points covered by the window on each iteration (specified by the window property and set to 5 by default). On each iteration, the algorithm calculates the average for all data points that fit into the window and then slides forward by excluding the first member of the previous window and including the first member from the next window. For example, given the data [1, 5, 8, 23, 34, 28, 7, 23, 20, 19], you can calculate a simple moving average with a window’s size of 5 as follows: . (1 + 5 + 8 + 23 + 34) / 5 = 14.2 (5 + 8 + 23 + 34+ 28) / 5 = 19.6 (8 + 23 + 34 + 28 + 7) / 5 = 20 so on... For more information, see Wikipedia. You can use the moving_avg aggregation to either smoothen out short-term fluctuations or to highlight longer-term trends or cycles in your time-series data. Specify a small window size (for example, window: 10) that closely follows the data to smoothen out small-scale fluctuations. Alternatively, specify a larger window size (for example, window: 100) that lags behind the actual data by a substantial amount to smoothen out all higher-frequency fluctuations or random noise, making lower frequency trends more visible. The following example nests a moving_avg aggregation into a date_histogram aggregation: . GET opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"aggs\": { \"my_date_histogram\": { \"date_histogram\": { \"field\": \"@timestamp\", \"calendar_interval\": \"month\" }, \"aggs\": { \"sum_of_bytes\": { \"sum\": { \"field\": \"bytes\" } }, \"moving_avg_of_sum_of_bytes\": { \"moving_avg\": { \"buckets_path\": \"sum_of_bytes\" } } } } } } . Example response ... \"aggregations\" : { \"my_date_histogram\" : { \"buckets\" : [ { \"key_as_string\" : \"2020-10-01T00:00:00.000Z\", \"key\" : 1601510400000, \"doc_count\" : 1635, \"sum_of_bytes\" : { \"value\" : 9400200.0 } }, { \"key_as_string\" : \"2020-11-01T00:00:00.000Z\", \"key\" : 1604188800000, \"doc_count\" : 6844, \"sum_of_bytes\" : { \"value\" : 3.8880434E7 }, \"moving_avg_of_sum_of_bytes\" : { \"value\" : 9400200.0 } }, { \"key_as_string\" : \"2020-12-01T00:00:00.000Z\", \"key\" : 1606780800000, \"doc_count\" : 5595, \"sum_of_bytes\" : { \"value\" : 3.1445055E7 }, \"moving_avg_of_sum_of_bytes\" : { \"value\" : 2.4140317E7 } } ] } } } . You can also use the moving_avg aggregation to predict future buckets. To predict buckets, add the predict property and set it to the number of predictions that you want to see. The following example adds five predictions to the preceding query: . GET opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"aggs\": { \"my_date_histogram\": { \"date_histogram\": { \"field\": \"@timestamp\", \"calendar_interval\": \"month\" }, \"aggs\": { \"sum_of_bytes\": { \"sum\": { \"field\": \"bytes\" } }, \"moving_avg_of_sum_of_bytes\": { \"moving_avg\": { \"buckets_path\": \"sum_of_bytes\", \"predict\": 5 } } } } } } . Example response . \"aggregations\" : { \"my_date_histogram\" : { \"buckets\" : [ { \"key_as_string\" : \"2020-10-01T00:00:00.000Z\", \"key\" : 1601510400000, \"doc_count\" : 1635, \"sum_of_bytes\" : { \"value\" : 9400200.0 } }, { \"key_as_string\" : \"2020-11-01T00:00:00.000Z\", \"key\" : 1604188800000, \"doc_count\" : 6844, \"sum_of_bytes\" : { \"value\" : 3.8880434E7 }, \"moving_avg_of_sum_of_bytes\" : { \"value\" : 9400200.0 } }, { \"key_as_string\" : \"2020-12-01T00:00:00.000Z\", \"key\" : 1606780800000, \"doc_count\" : 5595, \"sum_of_bytes\" : { \"value\" : 3.1445055E7 }, \"moving_avg_of_sum_of_bytes\" : { \"value\" : 2.4140317E7 } }, { \"key_as_string\" : \"2021-01-01T00:00:00.000Z\", \"key\" : 1609459200000, \"doc_count\" : 0, \"moving_avg_of_sum_of_bytes\" : { \"value\" : 2.6575229666666668E7 } }, { \"key_as_string\" : \"2021-02-01T00:00:00.000Z\", \"key\" : 1612137600000, \"doc_count\" : 0, \"moving_avg_of_sum_of_bytes\" : { \"value\" : 2.6575229666666668E7 } }, { \"key_as_string\" : \"2021-03-01T00:00:00.000Z\", \"key\" : 1614556800000, \"doc_count\" : 0, \"moving_avg_of_sum_of_bytes\" : { \"value\" : 2.6575229666666668E7 } }, { \"key_as_string\" : \"2021-04-01T00:00:00.000Z\", \"key\" : 1617235200000, \"doc_count\" : 0, \"moving_avg_of_sum_of_bytes\" : { \"value\" : 2.6575229666666668E7 } }, { \"key_as_string\" : \"2021-05-01T00:00:00.000Z\", \"key\" : 1619827200000, \"doc_count\" : 0, \"moving_avg_of_sum_of_bytes\" : { \"value\" : 2.6575229666666668E7 } } ] } } } . The moving_avg aggregation supports five models — simple, linear, exponentially weighted, holt-linear, and holt-winters. These models differ in how the values of the window are weighted. As data points become “older” (i.e., the window slides away from them), they might be weighted differently. You can specify a model of your choice by setting the model property. The model property holds the name of the model and the settings object, which you can use to provide model properties. For more information on these models, see Wikipedia. A simple model first calculates the sum of all data points in the window, and then divides that sum by the size of the window. In other words, a simple model calculates a simple arithmetic mean for each window in your dataset. The following example uses a simple model with a window size of 30: . GET opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"aggs\": { \"my_date_histogram\": { \"date_histogram\": { \"field\": \"@timestamp\", \"calendar_interval\": \"month\" }, \"aggs\": { \"sum_of_bytes\": { \"sum\": { \"field\": \"bytes\" } }, \"moving_avg_of_sum_of_bytes\": { \"moving_avg\": { \"buckets_path\": \"sum_of_bytes\", \"window\": 30, \"model\": \"simple\" } } } } } } . Example response ... \"aggregations\" : { \"my_date_histogram\" : { \"buckets\" : [ { \"key_as_string\" : \"2020-10-01T00:00:00.000Z\", \"key\" : 1601510400000, \"doc_count\" : 1635, \"sum_of_bytes\" : { \"value\" : 9400200.0 } }, { \"key_as_string\" : \"2020-11-01T00:00:00.000Z\", \"key\" : 1604188800000, \"doc_count\" : 6844, \"sum_of_bytes\" : { \"value\" : 3.8880434E7 }, \"moving_avg_of_sum_of_bytes\" : { \"value\" : 9400200.0 } }, { \"key_as_string\" : \"2020-12-01T00:00:00.000Z\", \"key\" : 1606780800000, \"doc_count\" : 5595, \"sum_of_bytes\" : { \"value\" : 3.1445055E7 }, \"moving_avg_of_sum_of_bytes\" : { \"value\" : 2.4140317E7 } } ] } } } . The following example uses a holt model. You can set the speed at which the importance decays occurs with the alpha and beta setting. The default value of alpha is 0.3 and beta is 0.1. You can specify any float value between 0-1 inclusive. GET opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"aggs\": { \"my_date_histogram\": { \"date_histogram\": { \"field\": \"@timestamp\", \"calendar_interval\": \"month\" }, \"aggs\": { \"sum_of_bytes\": { \"sum\": { \"field\": \"bytes\" } }, \"moving_avg_of_sum_of_bytes\": { \"moving_avg\": { \"buckets_path\": \"sum_of_bytes\", \"model\": \"holt\", \"settings\": { \"alpha\": 0.6, \"beta\": 0.4 } } } } } } } . Example response ... \"aggregations\" : { \"my_date_histogram\" : { \"buckets\" : [ { \"key_as_string\" : \"2020-10-01T00:00:00.000Z\", \"key\" : 1601510400000, \"doc_count\" : 1635, \"sum_of_bytes\" : { \"value\" : 9400200.0 } }, { \"key_as_string\" : \"2020-11-01T00:00:00.000Z\", \"key\" : 1604188800000, \"doc_count\" : 6844, \"sum_of_bytes\" : { \"value\" : 3.8880434E7 }, \"moving_avg_of_sum_of_bytes\" : { \"value\" : 9400200.0 } }, { \"key_as_string\" : \"2020-12-01T00:00:00.000Z\", \"key\" : 1606780800000, \"doc_count\" : 5595, \"sum_of_bytes\" : { \"value\" : 3.1445055E7 }, \"moving_avg_of_sum_of_bytes\" : { \"value\" : 2.70883404E7 } } ] } } } . ",
    "url": "https://vagimeli.github.io/aggregations/pipeline-agg/#moving_avg",
    "relUrl": "/aggregations/pipeline-agg/#moving_avg"
  },"1336": {
    "doc": "Pipeline aggregations",
    "title": "serial_diff",
    "content": "The serial_diff aggregation is a parent pipeline aggregation that computes a series of value differences between a time lag of the buckets from previous aggregations. You can use the serial_diff aggregation to find the data changes between time periods instead of finding the whole value. With the lag parameter (a positive, non-zero integer value), you can tell which previous bucket to subtract from the current one. If you don’t specify the lag parameter, OpenSearch sets it to 1. Lets say that the population of a city grows with time. If you use the serial differencing aggregation with the period of one day, you can see the daily growth. For example, you can compute a series of differences of the weekly average changes of a total price. GET opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"aggs\": { \"my_date_histogram\": { \"date_histogram\": { \"field\": \"@timestamp\", \"calendar_interval\": \"month\" }, \"aggs\": { \"the_sum\": { \"sum\": { \"field\": \"bytes\" } }, \"thirtieth_difference\": { \"serial_diff\": { \"buckets_path\": \"the_sum\", \"lag\" : 30 } } } } } } . Example response ... \"aggregations\" : { \"my_date_histogram\" : { \"buckets\" : [ { \"key_as_string\" : \"2020-10-01T00:00:00.000Z\", \"key\" : 1601510400000, \"doc_count\" : 1635, \"the_sum\" : { \"value\" : 9400200.0 } }, { \"key_as_string\" : \"2020-11-01T00:00:00.000Z\", \"key\" : 1604188800000, \"doc_count\" : 6844, \"the_sum\" : { \"value\" : 3.8880434E7 } }, { \"key_as_string\" : \"2020-12-01T00:00:00.000Z\", \"key\" : 1606780800000, \"doc_count\" : 5595, \"the_sum\" : { \"value\" : 3.1445055E7 } } ] } } } . ",
    "url": "https://vagimeli.github.io/aggregations/pipeline-agg/#serial_diff",
    "relUrl": "/aggregations/pipeline-agg/#serial_diff"
  },"1337": {
    "doc": "Language analyzers",
    "title": "Language analyzer",
    "content": "OpenSearch supports the following language values with the analyzer option: arabic, armenian, basque, bengali, brazilian, bulgarian, catalan, czech, danish, dutch, english, estonian, finnish, french, galician, german, greek, hindi, hungarian, indonesian, irish, italian, latvian, lithuanian, norwegian, persian, portuguese, romanian, russian, sorani, spanish, swedish, turkish, and thai. To use the analyzer when you map an index, specify the value within your query. For example, to map your index with the French language analyzer, specify the french value for the analyzer field: . \"analyzer\": \"french\" . Sample Request . The following query maps an index with the language analyzer set to french: . PUT my-index-000001 { \"mappings\": { \"properties\": { \"text\": { \"type\": \"text\", \"fields\": { \"french\": { \"type\": \"text\", \"analyzer\": \"french\" } } } } } } . ",
    "url": "https://vagimeli.github.io/query-dsl/analyzers/language-analyzers/#language-analyzer",
    "relUrl": "/query-dsl/analyzers/language-analyzers/#language-analyzer"
  },"1338": {
    "doc": "Language analyzers",
    "title": "Language analyzers",
    "content": " ",
    "url": "https://vagimeli.github.io/query-dsl/analyzers/language-analyzers/",
    "relUrl": "/query-dsl/analyzers/language-analyzers/"
  },"1339": {
    "doc": "Refresh search analyzer",
    "title": "Refresh search analyzer",
    "content": "With ISM installed, you can refresh search analyzers in real time with the following API: . POST /_plugins/_refresh_search_analyzers/&lt;index or alias or wildcard&gt; . For example, if you change the synonym list in your analyzer, the change takes effect without you needing to close and reopen the index. To work, the token filter must have an updateable flag of true: . { \"analyzer\": { \"my_synonyms\": { \"tokenizer\": \"whitespace\", \"filter\": [ \"synonym\" ] } }, \"filter\": { \"synonym\": { \"type\": \"synonym_graph\", \"synonyms_path\": \"synonyms.txt\", \"updateable\": true } } } . ",
    "url": "https://vagimeli.github.io/query-dsl/analyzers/refresh-analyzer/",
    "relUrl": "/query-dsl/analyzers/refresh-analyzer/"
  },"1340": {
    "doc": "Text analyzers",
    "title": "Optimizing text for searches with text analyzers",
    "content": "OpenSearch applies text analysis during indexing or searching for text fields. There is a standard analyzer that OpenSearch uses by default for text analysis. To optimize unstructured text for search, you can convert it into structured text with our text analyzers. ",
    "url": "https://vagimeli.github.io/analyzers/text-analyzers/#optimizing-text-for-searches-with-text-analyzers",
    "relUrl": "/analyzers/text-analyzers/#optimizing-text-for-searches-with-text-analyzers"
  },"1341": {
    "doc": "Text analyzers",
    "title": "Text analyzers",
    "content": "OpenSearch provides several text analyzers to convert your structured text into the format that works best for your searches. OpenSearch supports the following text analyzers: . | Standard analyzer – Parses strings into terms at word boundaries according to the Unicode text segmentation algorithm. It removes most, but not all, punctuation and converts strings to lowercase. You can remove stop words if you enable that option, but it does not remove stop words by default. | Simple analyzer – Converts strings to lowercase and removes non-letter characters when it splits a string into tokens on any non-letter character. | Whitespace analyzer – Parses strings into terms between each whitespace. | Stop analyzer – Converts strings to lowercase and removes non-letter characters by splitting strings into tokens at each non-letter character. It also removes stop words (for example, “but” or “this”) from strings. | Keyword analyzer – Receives a string as input and outputs the entire string as one term. | Pattern analyzer – Splits strings into terms using regular expressions and supports converting strings to lowercase. It also supports removing stop words. | Language analyzer – Provides analyzers specific to multiple languages. | Fingerprint analyzer – Creates a fingerprint to use as a duplicate detector. | . The full specialized text analyzers reference is in progress and will be published soon. ",
    "url": "https://vagimeli.github.io/analyzers/text-analyzers/",
    "relUrl": "/analyzers/text-analyzers/"
  },"1342": {
    "doc": "Text analyzers",
    "title": "How to use text analyzers",
    "content": "If you want to use a text analyzer, specify the name of the analyzer for the analyzer field: standard, simple, whitespace, stop, keyword, pattern, fingerprint, or language. Each analyzer consists of one tokenizer and zero or more token filters. Different analyzers have different character filters, tokenizers, and token filters. To pre-process the string before the tokenizer is applied, you can use one or more character filters. Example: Specify the standard analyzer in a simple query . GET _search { \"query\": { \"match\": { \"title\": \"A brief history of Time\", \"analyzer\": \"standard\" } } } . ",
    "url": "https://vagimeli.github.io/analyzers/text-analyzers/#how-to-use-text-analyzers",
    "relUrl": "/analyzers/text-analyzers/#how-to-use-text-analyzers"
  },"1343": {
    "doc": "Text analyzers",
    "title": "Analyzer options",
    "content": "| Option | Valid values | Description | . | analyzer | standard, simple, whitespace, stop, keyword, pattern, language, fingerprint | The analyzer you want to use for the query. Different analyzers have different character filters, tokenizers, and token filters. The stop analyzer, for example, removes stop words (for example, “an,” “but,” “this”) from the query string. For a full list of acceptable language values, see Language analyzer on this page. | . | quote_analyzer | String | This option lets you choose to use the standard analyzer without any options, such as language or other analyzers. Usage is \"quote_analyzer\": \"standard\". | . ",
    "url": "https://vagimeli.github.io/analyzers/text-analyzers/#analyzer-options",
    "relUrl": "/analyzers/text-analyzers/#analyzer-options"
  },"1344": {
    "doc": "Query DSL, aggregations, and analyzers",
    "title": "Query DSL, aggregations, and analyzers",
    "content": "Analyzers process text to make it searchable. OpenSearch provides various analyzers that let you customize the way text is split into terms and converted into a structured format. To search documents written in a different language, you can use one of the built-in language analyzers for your language of choice. The most essential search function is using a query to return relevant documents. OpenSearch provides a search language called query domain-specific language (DSL) that lets you build complex and targeted queries. Explore the query DSL documentation to learn more about the different types of queries OpenSearch supports. Aggregations let you categorize your data and analyze it to extract statistics. Use cases for aggregations include analyzing data in real time and using OpenSearch Dashboards to create visualizations. ",
    "url": "https://vagimeli.github.io/query-dsl/index/",
    "relUrl": "/query-dsl/index/"
  },"1345": {
    "doc": "Boolean queries",
    "title": "Boolean queries",
    "content": "You can perform a Boolean query with the bool query type. A Boolean query compounds query clauses so you can combine multiple search queries with Boolean logic. To narrow or broaden your search results, use the bool query clause rules. As a compound query type, bool allows you to construct an advanced query by combining several simple queries. Use the following rules to define how to combine multiple sub-query clauses within a bool query: . | Clause rule | Behavior | . | must | Logical and operator. The results must match the queries in this clause. If you have multiple queries, all of them must match. | . | must_not | Logical not operator. All matches are excluded from the results. | . | should | Logical or operator. The results must match at least one of the queries, but, optionally, they can match more than one query. Each matching should clause increases the relevancy score. You can set the minimum number of queries that must match using the minimum_number_should_match parameter. | . | minimum_number_should_match | Optional parameter for use with a should query clause. Specifies the minimum number of queries that the document must match for it to be returned in the results. The default value is 1. | . | filter | Logical and operator that is applied first to reduce your dataset before applying the queries. A query within a filter clause is a yes or no option. If a document matches the query, it is returned in the results; otherwise, it is not. The results of a filter query are generally cached to allow for a faster return. Use the filter query to filter the results based on exact matches, ranges, dates, numbers, and so on. | . Boolean query structure . The structure of a Boolean query contains the bool query type followed by clause rules, as follows: . GET _search { \"query\": { \"bool\": { \"must\": [ {} ], \"must_not\": [ {} ], \"should\": [ {} ], \"filter\": {} } } } . For example, assume you have the complete works of Shakespeare indexed in an OpenSearch cluster. You want to construct a single query that meets the following requirements: . | The text_entry field must contain the word love and should contain either life or grace. | The speaker field must not contain ROMEO. | Filter these results to the play Romeo and Juliet without affecting the relevancy score. | . Use the following query: . GET shakespeare/_search { \"query\": { \"bool\": { \"must\": [ { \"match\": { \"text_entry\": \"love\" } } ], \"should\": [ { \"match\": { \"text_entry\": \"life\" } }, { \"match\": { \"text_entry\": \"grace\" } } ], \"minimum_should_match\": 1, \"must_not\": [ { \"match\": { \"speaker\": \"ROMEO\" } } ], \"filter\": { \"term\": { \"play_name\": \"Romeo and Juliet\" } } } } } . Sample output . { \"took\": 12, \"timed_out\": false, \"_shards\": { \"total\": 4, \"successful\": 4, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 1, \"relation\": \"eq\" }, \"max_score\": 11.356054, \"hits\": [ { \"_index\": \"shakespeare\", \"_id\": \"88020\", \"_score\": 11.356054, \"_source\": { \"type\": \"line\", \"line_id\": 88021, \"play_name\": \"Romeo and Juliet\", \"speech_number\": 19, \"line_number\": \"4.5.61\", \"speaker\": \"PARIS\", \"text_entry\": \"O love! O life! not life, but love in death!\" } } ] } } . If you want to identify which of these clauses actually caused the matching results, name each query with the _name parameter. To add the _name parameter, change the field name in the match query to an object: . GET shakespeare/_search { \"query\": { \"bool\": { \"must\": [ { \"match\": { \"text_entry\": { \"query\": \"love\", \"_name\": \"love-must\" } } } ], \"should\": [ { \"match\": { \"text_entry\": { \"query\": \"life\", \"_name\": \"life-should\" } } }, { \"match\": { \"text_entry\": { \"query\": \"grace\", \"_name\": \"grace-should\" } } } ], \"minimum_should_match\": 1, \"must_not\": [ { \"match\": { \"speaker\": { \"query\": \"ROMEO\", \"_name\": \"ROMEO-must-not\" } } } ], \"filter\": { \"term\": { \"play_name\": \"Romeo and Juliet\" } } } } } . OpenSearch returns a matched_queries array that lists the queries that matched these results: . \"matched_queries\": [ \"love-must\", \"life-should\" ] . If you remove the queries not in this list, you will still see the exact same result. By examining which should clause matched, you can better understand the relevancy score of the results. You can also construct complex Boolean expressions by nesting bool queries. For example, to find a text_entry field that matches (love OR hate) AND (life OR grace) in the play Romeo and Juliet: . GET shakespeare/_search { \"query\": { \"bool\": { \"must\": [ { \"bool\": { \"should\": [ { \"match\": { \"text_entry\": \"love\" } }, { \"match\": { \"text\": \"hate\" } } ] } }, { \"bool\": { \"should\": [ { \"match\": { \"text_entry\": \"life\" } }, { \"match\": { \"text\": \"grace\" } } ] } } ], \"filter\": { \"term\": { \"play_name\": \"Romeo and Juliet\" } } } } } . Sample output . { \"took\": 10, \"timed_out\": false, \"_shards\": { \"total\": 2, \"successful\": 2, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": 1, \"max_score\": 11.37006, \"hits\": [ { \"_index\": \"shakespeare\", \"_type\": \"doc\", \"_id\": \"88020\", \"_score\": 11.37006, \"_source\": { \"type\": \"line\", \"line_id\": 88021, \"play_name\": \"Romeo and Juliet\", \"speech_number\": 19, \"line_number\": \"4.5.61\", \"speaker\": \"PARIS\", \"text_entry\": \"O love! O life! not life, but love in death!\" } } ] } } . ",
    "url": "https://vagimeli.github.io/query-dsl/compound/bool/",
    "relUrl": "/query-dsl/compound/bool/"
  },"1346": {
    "doc": "Compound queries",
    "title": "Compound queries",
    "content": "Compound queries serve as wrappers for multiple leaf or compound clauses either to combine their results or to modify their behavior. OpenSearch supports the following compound query types: . | Boolean: Combines multiple query clauses with Boolean logic. To learn more, see Boolean queries. | Constant score: Wraps a query or a filter and assigns a constant score to all matching documents. This score is equal to the boost value. | Disjunction max: Returns documents that match one or more query clauses. If a document matches multiple query clauses, it is assigned a higher relevance score. The relevance score is calculated using the highest score from any matching clause and, optionally, the scores from the other matching clauses multiplied by the tiebreaker value. | Function score: Recalculates the relevance score of documents that are returned by a query using a function that you define. | Boosting: Changes the relevance score of documents without removing them from the search results. Returns documents that match a positive query, but downgrades the relevance of documents in the results that match a negative query. | . ",
    "url": "https://vagimeli.github.io/query-dsl/compound/",
    "relUrl": "/query-dsl/compound/"
  },"1347": {
    "doc": "Full-text queries",
    "title": "Full-text queries",
    "content": "This page lists all full-text query types and common options. There are many optional fields that you can use to create subtle search behaviors, so we recommend that you test out some basic query types against representative indexes and verify the output before you perform more advanced or complex searches with multiple options. OpenSearch uses the Apache Lucene search library, which provides highly efficient data structures and algorithms for ingesting, indexing, searching, and aggregating data. To learn more about search query classes, see Lucene query JavaDocs. The full-text query types shown in this section use the standard analyzer, which analyzes text automatically when the query is submitted. You can also analyze fields when you index them. To learn more about how to convert unstructured text into structured text that is optimized for search, see Optimizing text for searches with text analyzers. . | Query types . | Match | Multi-match | Match Boolean prefix | Match phrase | Match phrase prefix | Query string | Simple query string | Match all | . | Advanced filter options . | Wildcard options | Fuzzy query options | Synonyms in a multiple terms search | Other advanced options | . | . Common terms queries and the optional query field cutoff_frequency are now deprecated. ",
    "url": "https://vagimeli.github.io/query-dsl/full-text/",
    "relUrl": "/query-dsl/full-text/"
  },"1348": {
    "doc": "Full-text queries",
    "title": "Query types",
    "content": "OpenSearch Query DSL provides multiple query types that you can use in your searches. Match . Use the match query for full-text search of a specific document field. The match query analyzes the provided search string and returns documents that match any of the string’s terms. You can use Boolean query operators to combine searches. The following example shows a basic match search for the title field set to the value wind: . GET _search { \"query\": { \"match\": { \"title\": \"wind\" } } } . For an example that uses curl, try: . curl --insecure -XGET -u 'admin:admin' https://&lt;host&gt;:&lt;port&gt;/&lt;index&gt;/_search \\ -H \"content-type: application/json\" \\ -d '{ \"query\": { \"match\": { \"title\": \"wind\" } } }' . The query accepts the following options. For descriptions of each, see Advanced filter options. GET _search { \"query\": { \"match\": { \"title\": { \"query\": \"wind\", \"fuzziness\": \"AUTO\", \"fuzzy_transpositions\": true, \"operator\": \"or\", \"minimum_should_match\": 1, \"analyzer\": \"standard\", \"zero_terms_query\": \"none\", \"lenient\": false, \"prefix_length\": 0, \"max_expansions\": 50, \"boost\": 1 } } } } . Multi-match . You can use the multi_match query type to search multiple fields. Multi-match operation functions similarly to the match operation. The ^ lets you “boost” certain fields. Boosts are multipliers that weigh matches in one field more heavily than matches in other fields. In the following example, a match for “wind” in the title field influences _score four times as much as a match in the plot field. The result is that films like The Wind Rises and Gone with the Wind are near the top of the search results, and films like Twister and Sharknado, which presumably have “wind” in their plot summaries, are near the bottom. GET _search { \"query\": { \"multi_match\": { \"query\": \"wind\", \"fields\": [\"title^4\", \"plot\"] } } } . The query accepts the following options. For descriptions of each, see Advanced filter options. GET _search { \"query\": { \"multi_match\": { \"query\": \"wind\", \"fields\": [\"title^4\", \"description\"], \"type\": \"most_fields\", \"operator\": \"and\", \"minimum_should_match\": 3, \"tie_breaker\": 0.0, \"analyzer\": \"standard\", \"boost\": 1, \"fuzziness\": \"AUTO\", \"fuzzy_transpositions\": true, \"lenient\": false, \"prefix_length\": 0, \"max_expansions\": 50, \"auto_generate_synonyms_phrase_query\": true, \"zero_terms_query\": \"none\" } } } . Match Boolean prefix . The match_bool_prefix query analyzes the provided search string and creates a bool query from the string’s terms. It uses every term except the last term as a whole word for matching. The last term is used as a prefix. The match_bool_prefix query returns documents that contain either the whole-word terms or terms that start with the prefix term, in any order. GET _search { \"query\": { \"match_bool_prefix\": { \"title\": \"rises wi\" } } } . The query accepts the following options. For descriptions of each, see Advanced filter options. GET _search { \"query\": { \"match_bool_prefix\": { \"title\": { \"query\": \"rises wi\", \"fuzziness\": \"AUTO\", \"fuzzy_transpositions\": true, \"max_expansions\": 50, \"prefix_length\": 0, \"operator\": \"or\", \"minimum_should_match\": 2, \"analyzer\": \"standard\" } } } } . For more reference information about prefix queries, see the Lucene documentation. Match phrase . Use the match_phrase query to match documents that contain an exact phrase in a specified order. You can add flexibility to phrase matching by providing the slop parameter. Creates a phrase query that matches a sequence of terms. GET _search { \"query\": { \"match_phrase\": { \"title\": \"the wind rises\" } } } . The query accepts the following options. For descriptions of each, see Advanced filter options. GET _search { \"query\": { \"match_phrase\": { \"title\": { \"query\": \"wind rises the\", \"slop\": 3, \"analyzer\": \"standard\", \"zero_terms_query\": \"none\" } } } } . Match phrase prefix . Use the match_phrase_prefix query to specify a phrase to match in order. The documents that contain the phrase you specify will be returned. The last partial term in the phrase is interpreted as a prefix, so any documents that contain phrases that begin with the phrase and prefix of the last term will be returned. Similar to match phrase, but creates a prefix query out of the last term in the query string. GET _search { \"query\": { \"match_phrase_prefix\": { \"title\": \"the wind ri\" } } } . The query accepts the following options. For descriptions of each, see Advanced filter options. GET _search { \"query\": { \"match_phrase_prefix\": { \"title\": { \"query\": \"the wind ri\", \"analyzer\": \"standard\", \"max_expansions\": 50, \"slop\": 3 } } } } . Query string . The query string query splits text based on operators and analyzes each individually. If you search using the HTTP request parameters (i.e. _search?q=wind), OpenSearch creates a query string query. GET _search { \"query\": { \"query_string\": { \"query\": \"the wind AND (rises OR rising)\" } } } . The query accepts the following options. For descriptions of each, see Advanced filter options. GET _search { \"query\": { \"query_string\": { \"query\": \"the wind AND (rises OR rising)\", \"default_field\": \"title\", \"type\": \"best_fields\", \"fuzziness\": \"AUTO\", \"fuzzy_transpositions\": true, \"fuzzy_max_expansions\": 50, \"fuzzy_prefix_length\": 0, \"minimum_should_match\": 1, \"default_operator\": \"or\", \"analyzer\": \"standard\", \"lenient\": false, \"boost\": 1, \"allow_leading_wildcard\": true, \"enable_position_increments\": true, \"phrase_slop\": 3, \"max_determinized_states\": 10000, \"time_zone\": \"-08:00\", \"quote_field_suffix\": \"\", \"quote_analyzer\": \"standard\", \"analyze_wildcard\": false, \"auto_generate_synonyms_phrase_query\": true } } } . Simple query string . Use the simple_query_string type to specify directly in the query string multiple arguments delineated by regular expressions. Searches with this type will discard any invalid portions of the string. GET _search { \"query\": { \"simple_query_string\": { \"query\": \"\\\"rises wind the\\\"~4 | *ising~2\", \"fields\": [\"title\"] } } } . | Special character | Behavior | . | + | Acts as the and operator. | . | | Acts as the or operator. | . | * | Acts as a wildcard. | . | \"\" | Wraps several terms into a phrase. | . | () | Wraps a clause for precedence. | . | ~n | When used after a term (for example, wnid~3), sets fuzziness. When used after a phrase, sets slop. Advanced filter options. | . | - | Negates the term. | . The query accepts the following options. For descriptions of each, see Advanced filter options. GET _search { \"query\": { \"simple_query_string\": { \"query\": \"\\\"rises wind the\\\"~4 | *ising~2\", \"fields\": [\"title\"], \"flags\": \"ALL\", \"fuzzy_transpositions\": true, \"fuzzy_max_expansions\": 50, \"fuzzy_prefix_length\": 0, \"minimum_should_match\": 1, \"default_operator\": \"or\", \"analyzer\": \"standard\", \"lenient\": false, \"quote_field_suffix\": \"\", \"analyze_wildcard\": false, \"auto_generate_synonyms_phrase_query\": true } } } . Match all . The match_all query type will return all documents. This type can be useful in testing large document sets if you need to return the entire set. GET _search { \"query\": { \"match_all\": {} } } . ",
    "url": "https://vagimeli.github.io/query-dsl/full-text/#query-types",
    "relUrl": "/query-dsl/full-text/#query-types"
  },"1349": {
    "doc": "Full-text queries",
    "title": "Advanced filter options",
    "content": "You can filter your query results by using some of the optional query fields, such as wildcards, fuzzy query fields, and synonyms. You can also use analyzers as optional query fields. To learn more, see How to use text analyzers. Wildcard options . | Option | Valid values | Description | . | allow_leading_wildcard | Boolean | Whether * and ? are allowed as the first character of a search term. The default is true. | . | analyze_wildcard | Boolean | Whether OpenSearch should attempt to analyze wildcard terms. Some analyzers do a poor job at this task, so the default is false. | . Fuzzy query options . | Option | Valid values | Description | . | fuzziness | AUTO, 0, or a positive integer | The number of character edits (insert, delete, substitute) that it takes to change one word to another when determining whether a term matched a value. For example, the distance between wined and wind is 1. The default, AUTO, chooses a value based on the length of each term and is a good choice for most use cases. | . | fuzzy_transpositions | Boolean | Setting fuzzy_transpositions to true (default) adds swaps of adjacent characters to the insert, delete, and substitute operations of the fuzziness option. For example, the distance between wind and wnid is 1 if fuzzy_transpositions is true (swap “n” and “i”) and 2 if it is false (delete “n”, insert “n”). If fuzzy_transpositions is false, rewind and wnid have the same distance (2) from wind, despite the more human-centric opinion that wnid is an obvious typo. The default is a good choice for most use cases. | . | fuzzy_max_expansions | Positive integer | Fuzzy queries “expand to” a number of matching terms that are within the distance specified in fuzziness. Then OpenSearch tries to match those terms against its indexes. | . Synonyms in a multiple terms search . You can also use synonyms with the terms query type to search for multiple terms. Use the auto_generate_synonyms_phrase_query Boolean field. By default it is set to true. It automatically generates phrase queries for multiple term synonyms. For example, if you have the synonym \"ba, batting average\" and search for “ba,” OpenSearch searches for ba OR \"batting average\" when the option is true or ba OR (batting AND average) when the option is false. To learn more about the multiple terms query type, see Terms. For more reference information about phrase queries, see the Lucene documentation. Other advanced options . You can also use the following optional query fields to filter your query results. | Option | Valid values | Description | . | boost | Floating-point | Boosts the clause by the given multiplier. Useful for weighing clauses in compound queries. The default is 1.0. | . | enable_position_increments | Boolean | When true, result queries are aware of position increments. This setting is useful when the removal of stop words leaves an unwanted “gap” between terms. The default is true. | . | fields | String array | The list of fields to search (e.g. \"fields\": [\"title^4\", \"description\"]). If unspecified, defaults to the index.query.default_field setting, which defaults to [\"*\"]. | . | flags | String | A |-delimited string of flags to enable (e.g., AND|OR|NOT). The default is ALL. You can explicitly set the value for default_field. For example, to return all titles, set it to \"default_field\": \"title\". | . | lenient | Boolean | Setting lenient to true lets you ignore data type mismatches between the query and the document field. For example, a query string of “8.2” could match a field of type float. The default is false. | . | low_freq_operator | and, or | The operator for low-frequency terms. The default is or. See also operator in this table. | . | max_determinized_states | Positive integer | The maximum number of “states” (a measure of complexity) that Lucene can create for query strings that contain regular expressions (e.g. \"query\": \"/wind.+?/\"). Larger numbers allow for queries that use more memory. The default is 10,000. | . | max_expansions | Positive integer | max_expansions specifies the maximum number of terms to which the query can expand. The default is 50. | . | minimum_should_match | Positive or negative integer, positive or negative percentage, combination | If the query string contains multiple search terms and you used the or operator, the number of terms that need to match for the document to be considered a match. For example, if minimum_should_match is 2, “wind often rising” does not match “The Wind Rises.” If minimum_should_match is 1, it matches. | . | operator | or, and | If the query string contains multiple search terms, whether all terms need to match (and) or only one term needs to match (or) for a document to be considered a match. | . | phrase_slop | 0 (default) or a positive integer | See slop. | . | prefix_length | 0 (default) or a positive integer | The number of leading characters that are not considered in fuzziness. | . | quote_field_suffix | String | This option lets you search different fields depending on whether terms are wrapped in quotes. For example, if quote_field_suffix is \".exact\" and you search for \"lightly\" (in quotes) in the title field, OpenSearch searches the title.exact field. This second field might use a different type (e.g. keyword rather than text) or a different analyzer. The default is null. | . | rewrite | constant_score, scoring_boolean, constant_score_boolean, top_terms_N, top_terms_boost_N, top_terms_blended_freqs_N | Determines how OpenSearch rewrites and scores multi-term queries. The default is constant_score. | . | slop | 0 (default) or a positive integer | Controls the degree to which words in a query can be misordered and still be considered a match. From the Lucene documentation: “The number of other words permitted between words in query phrase. For example, to switch the order of two words requires two moves (the first move places the words atop one another), so to permit re-orderings of phrases, the slop must be at least two. A value of zero requires an exact match.” | . | tie_breaker | 0.0 (default) to 1.0 | Changes the way OpenSearch scores searches. For example, a type of best_fields typically uses the highest score from any one field. If you specify a tie_breaker value between 0.0 and 1.0, the score changes to highest score + tie_breaker * score for all other matching fields. If you specify a value of 1.0, OpenSearch adds together the scores for all matching fields (effectively defeating the purpose of best_fields). | . | time_zone | UTC offset hours | Specifies the number of hours to offset the desired time zone from UTC. You need to indicate the time zone offset number if the query string contains a date range. For example, set time_zone\": \"-08:00\" for a query with a date range such as \"query\": \"wind rises release_date[2012-01-01 TO 2014-01-01]\"). The default time zone format used to specify number of offset hours is UTC. | . | type | best_fields, most_fields, cross_fields, phrase, phrase_prefix | Determines how OpenSearch executes the query and scores the results. The default is best_fields. | . | zero_terms_query | none, all | If the analyzer removes all terms from a query string, whether to match no documents (default) or all documents. For example, the stop analyzer removes all terms from the string “an but this.” | . ",
    "url": "https://vagimeli.github.io/query-dsl/full-text/#advanced-filter-options",
    "relUrl": "/query-dsl/full-text/#advanced-filter-options"
  },"1350": {
    "doc": "Query string queries",
    "title": "Query string queries",
    "content": "A query_string query parses the query string based on the query_string syntax. It lets you create powerful yet concise queries that can incorporate wildcards and search multiple fields. ",
    "url": "https://vagimeli.github.io/query-dsl/full-text/query-string/",
    "relUrl": "/query-dsl/full-text/query-string/"
  },"1351": {
    "doc": "Query string queries",
    "title": "Example",
    "content": "The following query searches for the speaker KING in the play name that ends with well: . GET shakespeare/_search { \"query\": { \"query_string\": { \"query\": \"speaker:KING AND play_name: *well\" } } } . ",
    "url": "https://vagimeli.github.io/query-dsl/full-text/query-string/#example",
    "relUrl": "/query-dsl/full-text/query-string/#example"
  },"1352": {
    "doc": "Query string queries",
    "title": "Parameters",
    "content": "The following table lists the parameters that query_string query supports. All parameters except query are optional. | Parameter | Data type | Description | . | query | String | The query string to use for search. Required. | . | fields | String array | The list of fields to search (for example, \"fields\": [\"title^4\", \"description\"]). Supports wildcards. | . | default_field | String | The field in which to search if the field is not specified in the query string. Supports wildcards. Defaults to the value specified in the index.query.default_field index setting. By default, the index.query.default_field is *, which means extract all fields eligible for term query and filter the metadata fields. The extracted fields are combined into a query if the prefix is not specified. Eligible fields do not include nested documents. Searching all eligible fields could be a resource-intensive operation. The indices.query.bool.max_clause_count search setting defines the maximum value for the product of the number of fields and the number of terms that can be queried at one time. The default value for indices.query.bool.max_clause_count is 4,096. | . | allow_leading_wildcard | Boolean | Specifies whether * and ? are allowed as first characters of a search term. Default is true. | . | analyze_wildcard | Boolean | Specifies whether OpenSearch should attempt to analyze wildcard terms. Default is false. | . | analyzer | String | The analyzer used to tokenize the query string text. Default is the index-time analyzer specified for the default_field. If no analyzer is specified for the default_field, the analyzer is the default analyzer for the index. | . | quote_analyzer | String | The analyzer used to tokenize quoted text in the query string. Overrides the analyzer parameter for quoted text. Default is the search_quote_analyzer specified for the default_field. | . | quote_field_suffix | String | This option lets you search for exact matches (surrounded with quotation marks) using a different analysis method than non-exact matches use. For example, if quote_field_suffix is .exact and you search for \\\"lightly\\\" in the title field, OpenSearch searches for the word lightly in the title.exact field. This second field might use a different type (for example, keyword rather than text) or a different analyzer. | . | phrase_slop | Integer | The maximum number of words that are allowed between the matched words. If phrase_slop is 2, a maximum of two words is allowed between matched words in a phrase. Transposed words have a slop of 2. Default is 0 (an exact phrase match where matched words must be next to each other). | . | minimum_should_match | Positive or negative integer, positive or negative percentage, combination | If the query string contains multiple search terms and you used the or operator, the number of terms that need to match for the document to be considered a match. For example, if minimum_should_match is 2, “wind often rising” does not match “The Wind Rises.” If minimum_should_match is 1, it matches. | . | rewrite | String | Determines how OpenSearch rewrites and scores multi-term queries. Valid values are constant_score, scoring_boolean, constant_score_boolean, top_terms_N, top_terms_boost_N, and top_terms_blended_freqs_N. Default is constant_score. | . | auto_generate_synonyms_phrase_query | Boolean | Specifies whether to create match queries automatically for multi-term synonyms. Default is true. | . | boost | Floating-point | Boosts the clause by the given multiplier. Values less than 1.0 decrease relevance, and values greater than 1.0 increase relevance. Default is 1.0. | . | default_operator | String | The default Boolean operator used if no operators are specified. Valid values are:- OR: The string to be is interpreted as to OR be- AND: The string to be is interpreted as to AND be Default is OR. | . | enable_position_increments | Boolean | When true, resulting queries are aware of position increments. This setting is useful when the removal of stop words leaves an unwanted “gap” between terms. Default is true. | . | fuzziness | String | The number of character edits (insert, delete, substitute) that it takes to change one word to another when determining whether a term matched a value. For example, the distance between wined and wind is 1. Valid values are non-negative integers or AUTO. The default, AUTO, chooses a value based on the length of each term and is a good choice for most use cases. | . | fuzzy_transpositions | Boolean | Setting fuzzy_transpositions to true (default) adds swaps of adjacent characters to the insert, delete, and substitute operations of the fuzziness option. For example, the distance between wind and wnid is 1 if fuzzy_transpositions is true (swap “n” and “i”) and 2 if it is false (delete “n”, insert “n”). If fuzzy_transpositions is false, rewind and wnid have the same distance (2) from wind, despite the more human-centric opinion that wnid is an obvious typo. The default is a good choice for most use cases. | . | fuzzy_max_expansions | Positive integer | The maximum number of terms the fuzzy query will match. Default is 50. | . | lenient | Boolean | Setting lenient to true lets you ignore data type mismatches between the query and the document field. For example, a query string of “8.2” could match a field of type float. Default is false. | . | max_determinized_states | Positive integer | The maximum number of “states” (a measure of complexity) that Lucene can create for query strings that contain regular expressions (for example, \"query\": \"/wind.+?/\"). Larger numbers allow for queries that use more memory. Default is 10,000. | . | time_zone | String | Specifies the number of hours to offset the desired time zone from UTC. You need to indicate the time zone offset number if the query string contains a date range. For example, set time_zone\": \"-08:00\" for a query with a date range such as \"query\": \"wind rises release_date[2012-01-01 TO 2014-01-01]\"). The default time zone format used to specify number of offset hours is UTC. | . ",
    "url": "https://vagimeli.github.io/query-dsl/full-text/query-string/#parameters",
    "relUrl": "/query-dsl/full-text/query-string/#parameters"
  },"1353": {
    "doc": "Geo-bounding box queries",
    "title": "Geo-bounding box queries",
    "content": "To search for documents that contain geopoint fields, use a geo-bounding box query. The geo-bounding box query returns documents whose geopoints are within the bounding box specified in the query. A document with multiple geopoints matches the query if at least one geopoint is within the bounding box. ",
    "url": "https://vagimeli.github.io/query-dsl/geo-and-xy/geo-bounding-box/",
    "relUrl": "/query-dsl/geo-and-xy/geo-bounding-box/"
  },"1354": {
    "doc": "Geo-bounding box queries",
    "title": "Example",
    "content": "You can use a geo-bounding box query to search for documents that contain geopoints. Create a mapping with the point field mapped as geo_point: . PUT testindex1 { \"mappings\": { \"properties\": { \"point\": { \"type\": \"geo_point\" } } } } . Index three geopoints as objects with latitudes and longitudes: . PUT testindex1/_doc/1 { \"point\": { \"lat\": 74.00, \"lon\": 40.71 } } PUT testindex1/_doc/2 { \"point\": { \"lat\": 72.64, \"lon\": 22.62 } } PUT testindex1/_doc/3 { \"point\": { \"lat\": 75.00, \"lon\": 28.00 } } . Search for all documents and filter the documents whose points lie within the rectangle defined in the query: . GET testindex1/_search { \"query\": { \"bool\": { \"must\": { \"match_all\": {} }, \"filter\": { \"geo_bounding_box\": { \"point\": { \"top_left\": { \"lat\": 75, \"lon\": 28 }, \"bottom_right\": { \"lat\": 73, \"lon\": 41 } } } } } } } . The response contains the matching document: . { \"took\" : 20, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 1, \"relation\" : \"eq\" }, \"max_score\" : 1.0, \"hits\" : [ { \"_index\" : \"testindex1\", \"_id\" : \"1\", \"_score\" : 1.0, \"_source\" : { \"point\" : { \"lat\" : 74.0, \"lon\" : 40.71 } } } ] } } . The preceding response does not include the document with a geopoint of \"lat\": 75.00, \"lon\": 28.00 because of the geopoint’s limited precision. ",
    "url": "https://vagimeli.github.io/query-dsl/geo-and-xy/geo-bounding-box/#example",
    "relUrl": "/query-dsl/geo-and-xy/geo-bounding-box/#example"
  },"1355": {
    "doc": "Geo-bounding box queries",
    "title": "Precision",
    "content": "Geopoint coordinates are always rounded down at index time. At query time, the upper boundaries of the bounding box are rounded down, and the lower boundaries are rounded up. Therefore, the documents with geopoints that lie on the lower and left edges of the bounding box might not be included in the results due to rounding error. On the other hand, geopoints that lie on the upper and right edges of the bounding box might be included in the results even though they are outside the boundaries. The rounding error is less than 4.20 × 10−8 degrees for latitude and less than 8.39 × 10−8 degrees for longitude (around 1 cm). ",
    "url": "https://vagimeli.github.io/query-dsl/geo-and-xy/geo-bounding-box/#precision",
    "relUrl": "/query-dsl/geo-and-xy/geo-bounding-box/#precision"
  },"1356": {
    "doc": "Geo-bounding box queries",
    "title": "Specifying the bounding box",
    "content": "You can specify the bounding box by providing any of the following combinations of its vertex coordinates: . | top_left and bottom_right | top_right and bottom_left | top, left, bottom, and right | . The following example shows how to specify the bounding box using the top, left, bottom, and right coordinates: . GET testindex1/_search { \"query\": { \"bool\": { \"must\": { \"match_all\": {} }, \"filter\": { \"geo_bounding_box\": { \"point\": { \"top\": 75, \"left\": 28, \"bottom\": 73, \"right\": 41 } } } } } } . ",
    "url": "https://vagimeli.github.io/query-dsl/geo-and-xy/geo-bounding-box/#specifying-the-bounding-box",
    "relUrl": "/query-dsl/geo-and-xy/geo-bounding-box/#specifying-the-bounding-box"
  },"1357": {
    "doc": "Geo-bounding box queries",
    "title": "Request fields",
    "content": "Geo-bounding box queries accept the following fields. | Field | Data type | Description | . | _name | String | The name of the filter. Optional. | . | validation_method | String | The validation method. Valid values are IGNORE_MALFORMED (accept geopoints with invalid coordinates), COERCE (try to coerce coordinates to valid values), and STRICT (return an error when coordinates are invalid). Default is STRICT. | . | type | String | Specifies how to execute the filter. Valid values are indexed (index the filter) and memory (execute the filter in memory). Default is memory. | . | ignore_unmapped | Boolean | Specifies whether to ignore an unmapped field. If set to true, the query does not return any documents that have an unmapped field. If set to false, an exception is thrown when the field is unmapped. Default is false. | . ",
    "url": "https://vagimeli.github.io/query-dsl/geo-and-xy/geo-bounding-box/#request-fields",
    "relUrl": "/query-dsl/geo-and-xy/geo-bounding-box/#request-fields"
  },"1358": {
    "doc": "Geo-bounding box queries",
    "title": "Accepted formats",
    "content": "You can specify coordinates of the bounding box vertices in any format that the geopoint field type accepts. Using a geohash to specify the bounding box . If you use a geohash to specify the bounding box, the geohash is treated as a rectangle. The upper-left vertex of the bounding box corresponds to the upper-left vertex of the top_left geohash, and the lower-right vertex of the bounding box corresponds to the lower-right vertex of the bottom_right geohash. The following example shows how to use a geohash to specify the same bounding box as the previous examples: . GET testindex1/_search { \"query\": { \"bool\": { \"must\": { \"match_all\": {} }, \"filter\": { \"geo_bounding_box\": { \"point\": { \"top_left\": \"ut7ftjkfxm34\", \"bottom_right\": \"uuvpkcprc4rc\" } } } } } } . To specify a bounding box that covers the whole area of a geohash, provide that geohash as both top_left and bottom_right parameters of the bounding box: . GET testindex1/_search { \"query\": { \"bool\": { \"must\": { \"match_all\": {} }, \"filter\": { \"geo_bounding_box\": { \"point\": { \"top_left\": \"ut\", \"bottom_right\": \"ut\" } } } } } } . ",
    "url": "https://vagimeli.github.io/query-dsl/geo-and-xy/geo-bounding-box/#accepted-formats",
    "relUrl": "/query-dsl/geo-and-xy/geo-bounding-box/#accepted-formats"
  },"1359": {
    "doc": "Geographic and xy queries",
    "title": "Geographic and xy queries",
    "content": "Geographic and xy queries let you search fields that contain points and shapes on a map or coordinate plane. Geographic queries work on geospatial data, while xy queries work on two-dimensional coordinate data. Out of all geographic queries, the geoshape query is very similar to the xy query, but the former searches geographic fields, while the latter searches Cartesian fields. ",
    "url": "https://vagimeli.github.io/query-dsl/geo-and-xy/",
    "relUrl": "/query-dsl/geo-and-xy/"
  },"1360": {
    "doc": "Geographic and xy queries",
    "title": "xy queries",
    "content": "xy queries search for documents that contain geometries in a Cartesian coordinate system. These geometries can be specified in xy_point fields, which support points, and xy_shape fields, which support points, lines, circles, and polygons. xy queries return documents that contain: . | xy shapes and xy points that have one of four spatial relations to the provided shape: INTERSECTS, DISJOINT, WITHIN, or CONTAINS. | xy points that intersect the provided shape. | . ",
    "url": "https://vagimeli.github.io/query-dsl/geo-and-xy/#xy-queries",
    "relUrl": "/query-dsl/geo-and-xy/#xy-queries"
  },"1361": {
    "doc": "Geographic and xy queries",
    "title": "Geographic queries",
    "content": "Geographic queries search for documents that contain geospatial geometries. These geometries can be specified in geo_point fields, which support points on a map, and geo_shape fields, which support points, lines, circles, and polygons. OpenSearch provides the following geographic query types: . | Geo-bounding box queries: Return documents with geopoint field values that are within a bounding box. | Geodistance queries return documents with geopoints that are within a specified distance from the provided geopoint. | Geopolygon queries return documents with geopoints that are within a polygon. | Geoshape queries return documents that contain: . | geoshapes and geopoints that have one of four spatial relations to the provided shape: INTERSECTS, DISJOINT, WITHIN, or CONTAINS. | geopoints that intersect the provided shape. | . | . ",
    "url": "https://vagimeli.github.io/query-dsl/geo-and-xy/#geographic-queries",
    "relUrl": "/query-dsl/geo-and-xy/#geographic-queries"
  },"1362": {
    "doc": "xy queries",
    "title": "xy queries",
    "content": "To search for documents that contain xy point and xy shape fields, use an xy query. ",
    "url": "https://vagimeli.github.io/query-dsl/geo-and-xy/xy/",
    "relUrl": "/query-dsl/geo-and-xy/xy/"
  },"1363": {
    "doc": "xy queries",
    "title": "Spatial relations",
    "content": "When you provide an xy shape to the xy query, the xy fields are matched using the following spatial relations to the provided shape. | Relation | Description | Supporting xy Field Type | . | INTERSECTS | (Default) Matches documents whose xy point or xy shape intersects the shape provided in the query. | xy_point, xy_shape | . | DISJOINT | Matches documents whose xy shape does not intersect with the shape provided in the query. | xy_shape | . | WITHIN | Matches documents whose xy shape is completely within the shape provided in the query. | xy_shape | . | CONTAINS | Matches documents whose xy shape completely contains the shape provided in the query. | xy_shape | . The following examples illustrate searching for documents that contain xy shapes. To learn how to search for documents that contain xy points, see the Querying xy points section. ",
    "url": "https://vagimeli.github.io/query-dsl/geo-and-xy/xy/#spatial-relations",
    "relUrl": "/query-dsl/geo-and-xy/xy/#spatial-relations"
  },"1364": {
    "doc": "xy queries",
    "title": "Defining the shape in an xy query",
    "content": "You can define the shape in an xy query either by providing a new shape definition at query time or by referencing the name of a shape pre-indexed in another index. Using a new shape definition . To provide a new shape to an xy query, define it in the xy_shape field. The following example illustrates searching for documents with xy shapes that match an xy shape defined at query time. First, create an index and map the geometry field as an xy_shape: . PUT testindex { \"mappings\": { \"properties\": { \"geometry\": { \"type\": \"xy_shape\" } } } } . Index a document with a point and a document with a polygon: . PUT testindex/_doc/1 { \"geometry\": { \"type\": \"point\", \"coordinates\": [0.5, 3.0] } } PUT testindex/_doc/2 { \"geometry\" : { \"type\" : \"polygon\", \"coordinates\" : [ [[2.5, 6.0], [0.5, 4.5], [1.5, 2.0], [3.5, 3.5], [2.5, 6.0]] ] } } . Define an envelope—a bounding rectangle in the [[minX, maxY], [maxX, minY]] format. Search for documents with xy points or shapes that intersect that envelope: . GET testindex/_search { \"query\": { \"xy_shape\": { \"geometry\": { \"shape\": { \"type\": \"envelope\", \"coordinates\": [ [ 0.0, 6.0], [ 4.0, 2.0] ] }, \"relation\": \"WITHIN\" } } } } . The following image depicts the example. Both the point and the polygon are within the bounding envelope. The response contains both documents: . { \"took\" : 363, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 2, \"relation\" : \"eq\" }, \"max_score\" : 0.0, \"hits\" : [ { \"_index\" : \"testindex\", \"_id\" : \"1\", \"_score\" : 0.0, \"_source\" : { \"geometry\" : { \"type\" : \"point\", \"coordinates\" : [ 0.5, 3.0 ] } } }, { \"_index\" : \"testindex\", \"_id\" : \"2\", \"_score\" : 0.0, \"_source\" : { \"geometry\" : { \"type\" : \"polygon\", \"coordinates\" : [ [ [ 2.5, 6.0 ], [ 0.5, 4.5 ], [ 1.5, 2.0 ], [ 3.5, 3.5 ], [ 2.5, 6.0 ] ] ] } } } ] } } . Using a pre-indexed shape definition . When constructing an xy query, you can also reference the name of a shape pre-indexed in another index. Using this method, you can define an xy shape at index time and refer to it by name, providing the following parameters in the indexed_shape object. | Parameter | Description | . | index | The name of the index that contains the pre-indexed shape. | . | id | The document ID of the document that contains the pre-indexed shape. | . | path | The field name of the field that contains the pre-indexed shape as a path. | . The following example illustrates referencing the name of a shape pre-indexed in another index. In this example, the index pre-indexed-shapes contains the shape that defines the boundaries, and the index testindex contains the shapes whose locations are checked against those boundaries. First, create an index pre-indexed-shapes and map the geometry field for this index as an xy_shape: . PUT pre-indexed-shapes { \"mappings\": { \"properties\": { \"geometry\": { \"type\": \"xy_shape\" } } } } . Index an envelope that specifies the boundaries and name it rectangle: . PUT pre-indexed-shapes/_doc/rectangle { \"geometry\": { \"type\": \"envelope\", \"coordinates\" : [ [ 0.0, 6.0], [ 4.0, 2.0] ] } } . Index a document with a point and a document with a polygon into the index testindex: . PUT testindex/_doc/1 { \"geometry\": { \"type\": \"point\", \"coordinates\": [0.5, 3.0] } } PUT testindex/_doc/2 { \"geometry\" : { \"type\" : \"polygon\", \"coordinates\" : [ [[2.5, 6.0], [0.5, 4.5], [1.5, 2.0], [3.5, 3.5], [2.5, 6.0]] ] } } . Search for documents with shapes that intersect rectangle in the index testindex using a filter: . GET testindex/_search { \"query\": { \"bool\": { \"filter\": { \"xy_shape\": { \"geometry\": { \"indexed_shape\": { \"index\": \"pre-indexed-shapes\", \"id\": \"rectangle\", \"path\": \"geometry\" } } } } } } } . The preceding query uses the default spatial relation INTERSECTS and returns both the point and the polygon: . { \"took\" : 26, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 2, \"relation\" : \"eq\" }, \"max_score\" : 0.0, \"hits\" : [ { \"_index\" : \"testindex\", \"_id\" : \"1\", \"_score\" : 0.0, \"_source\" : { \"geometry\" : { \"type\" : \"point\", \"coordinates\" : [ 0.5, 3.0 ] } } }, { \"_index\" : \"testindex\", \"_id\" : \"2\", \"_score\" : 0.0, \"_source\" : { \"geometry\" : { \"type\" : \"polygon\", \"coordinates\" : [ [ [ 2.5, 6.0 ], [ 0.5, 4.5 ], [ 1.5, 2.0 ], [ 3.5, 3.5 ], [ 2.5, 6.0 ] ] ] } } } ] } } . ",
    "url": "https://vagimeli.github.io/query-dsl/geo-and-xy/xy/#defining-the-shape-in-an-xy-query",
    "relUrl": "/query-dsl/geo-and-xy/xy/#defining-the-shape-in-an-xy-query"
  },"1365": {
    "doc": "xy queries",
    "title": "Querying xy points",
    "content": "You can also use an xy query to search for documents that contain xy points. Create a mapping with point as xy_point: . PUT testindex1 { \"mappings\": { \"properties\": { \"point\": { \"type\": \"xy_point\" } } } } . Index three points: . PUT testindex1/_doc/1 { \"point\": \"1.0, 1.0\" } PUT testindex1/_doc/2 { \"point\": \"2.0, 0.0\" } PUT testindex1/_doc/3 { \"point\": \"-2.0, 2.0\" } . Search for points that lie within the circle with the center at (0, 0) and a radius of 2: . GET testindex1/_search { \"query\": { \"xy_shape\": { \"point\": { \"shape\": { \"type\": \"circle\", \"coordinates\": [0.0, 0.0], \"radius\": 2 } } } } } . xy point only supports the default INTERSECTS spatial relation, so you don’t need to provide the relation parameter. The following image depicts the example. Points 1 and 2 are within the circle, and point 3 is outside the circle. The response returns documents 1 and 2: . { \"took\" : 575, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 2, \"relation\" : \"eq\" }, \"max_score\" : 0.0, \"hits\" : [ { \"_index\" : \"testindex1\", \"_id\" : \"1\", \"_score\" : 0.0, \"_source\" : { \"point\" : \"1.0, 1.0\" } }, { \"_index\" : \"testindex1\", \"_id\" : \"2\", \"_score\" : 0.0, \"_source\" : { \"point\" : \"2.0, 0.0\" } } ] } } . ",
    "url": "https://vagimeli.github.io/query-dsl/geo-and-xy/xy/#querying-xy-points",
    "relUrl": "/query-dsl/geo-and-xy/xy/#querying-xy-points"
  },"1366": {
    "doc": "Query DSL",
    "title": "Query DSL",
    "content": "OpenSearch provides a search language called query domain-specific language (DSL) that you can use to search your data. Query DSL is a flexible language with a JSON interface. With query DSL, you need to specify a query in the query parameter of the search. One of the simplest searches in OpenSearch uses the match_all query, which matches all documents in an index: . GET testindex/_search { \"query\": { \"match_all\": { } } } . A query can consist of many query clauses. You can combine query clauses to produce complex queries. Broadly, you can classify queries into two categories—leaf queries and compound queries: . | Leaf queries: Leaf queries search for a specified value in a certain field or fields. You can use leaf queries on their own. They include the following query types: . | Full-text queries: Use full-text queries to search text documents. For an analyzed text field search, full-text queries split the query string into terms with the same analyzer that was used when the field was indexed. For an exact value search, full-text queries look for the specified value without applying text analysis. To learn more, see Full-text queries. | Term-level queries: Use term-level queries to search documents for an exact specified term, such as an ID or value range. Term-level queries do not analyze search terms or sort results by relevance score. To learn more, see Term-level queries. | Geographic and xy queries: Use geographic queries to search documents that include geographic data. Use xy queries to search documents that include points and shapes in a two-dimensional coordinate system. To learn more, see Geographic and xy queries. | Joining queries: Use joining queries to search nested fields or return parent and child documents that match a specific query. Types of joining queries include nested, has_child, has_parent, and parent_id queries. | Span queries: Use span queries to perform precise positional searches. Span queries are low-level, specific queries that provide control over the order and proximity of specified query terms. They are primarily used to search legal documents. To learn more, see Span queries. | Specialized queries: Specialized queries include all other query types (distance_feature, more_like_this, percolate, rank_feature, script, script_score, wrapper, and pinned_query). | . | Compound queries: Compound queries serve as wrappers for multiple leaf or compound clauses either to combine their results or to modify their behavior. They include the Boolean, disjunction max, constant score, function score, and boosting query types. To learn more, see Compound queries. | . ",
    "url": "https://vagimeli.github.io/query-dsl/",
    "relUrl": "/query-dsl/"
  },"1367": {
    "doc": "Query DSL",
    "title": "A note on Unicode special characters in text fields",
    "content": "Due to word boundaries associated with Unicode special characters, the Unicode standard analyzer cannot index a text field type value as a whole value when it includes one of these special characters. As a result, a text field value that includes a special character is parsed by the standard analyzer as multiple values separated by the special character, effectively tokenizing the different elements on either side of it. This can lead to unintentional filtering of documents and potentially compromise control over their access. The examples below illustrate values containing special characters that will be parsed improperly by the standard analyzer. In this example, the existence of the hyphen/minus sign in the value prevents the analyzer from distinguishing between the two different users for user.id and interprets them as one and the same: . { \"bool\": { \"must\": { \"match\": { \"user.id\": \"User-1\" } } } } . { \"bool\": { \"must\": { \"match\": { \"user.id\": \"User-2\" } } } } . To avoid this circumstance when using either query DSL or the REST API, you can use a custom analyzer or map the field as keyword, which performs an exact-match search. See Keyword field type for the latter option. For a list of characters that should be avoided for text field types, see Word Boundaries. ",
    "url": "https://vagimeli.github.io/query-dsl/#a-note-on-unicode-special-characters-in-text-fields",
    "relUrl": "/query-dsl/#a-note-on-unicode-special-characters-in-text-fields"
  },"1368": {
    "doc": "Query and filter context",
    "title": "Query and filter context",
    "content": "Queries consist of query clauses, which can be run in a filter context or query context. A query clause in a filter context asks the question “Does the document match the query clause?” and returns matching documents. A query clause in a query context asks the question “How well does the document match the query clause?”, returns matching documents, and provides the relevance of each document in the form of a relevance score. ",
    "url": "https://vagimeli.github.io/query-dsl/query-filter-context/",
    "relUrl": "/query-dsl/query-filter-context/"
  },"1369": {
    "doc": "Query and filter context",
    "title": "Relevance score",
    "content": "A relevance score measures how well a document matches a query. It is a positive floating-point number that OpenSearch records in the _score metadata field for each document: . \"hits\" : [ { \"_index\" : \"shakespeare\", \"_id\" : \"32437\", \"_score\" : 18.781435, \"_source\" : { \"type\" : \"line\", \"line_id\" : 32438, \"play_name\" : \"Hamlet\", \"speech_number\" : 3, \"line_number\" : \"1.1.3\", \"speaker\" : \"BERNARDO\", \"text_entry\" : \"Long live the king!\" } }, ... A higher score indicates a more relevant document. While different query types calculate relevance scores differently, all query types take into account whether a query clause is run in a filter or query context. Use query clauses that you want to affect the relevance score in a query context, and use all other query clauses in a filter context. ",
    "url": "https://vagimeli.github.io/query-dsl/query-filter-context/#relevance-score",
    "relUrl": "/query-dsl/query-filter-context/#relevance-score"
  },"1370": {
    "doc": "Query and filter context",
    "title": "Filter context",
    "content": "A query clause in a filter context asks the question “Does the document match the query clause?”, which has a binary answer. For example, if you have an index with student data, you might use a filter context to answer the following questions about a student: . | Is the student’s honors status set to true? | Is the student’s graduation_year in the 2020–2022 range? | . With a filter context, OpenSearch returns matching documents without calculating a relevance score. Thus, you should use a filter context for fields with exact values. To run a query clause in a filter context, pass it to a filter parameter. For example, the following Boolean query searches for students who graduated with honors in 2020–2022: . GET students/_search { \"query\": { \"bool\": { \"filter\": [ { \"term\": { \"honors\": true }}, { \"range\": { \"graduation_year\": { \"gte\": 2020, \"lte\": 2022 }}} ] } } } . To improve performance, OpenSearch caches frequently used filters. ",
    "url": "https://vagimeli.github.io/query-dsl/query-filter-context/#filter-context",
    "relUrl": "/query-dsl/query-filter-context/#filter-context"
  },"1371": {
    "doc": "Query and filter context",
    "title": "Query context",
    "content": "A query clause in a query context asks the question “How well does the document match the query clause?”, which does not have a binary answer. A query context is suitable for a full-text search, where you not only want to receive matching documents but also to determine the relevance of each document. For example, if you have an index with the complete works of Shakespeare, you might use a query context for the following searches: . | Find documents that contain the word dream, including its various forms (dreaming or dreams) and synonyms (contemplate). | Find documents that match the words long live king. | . With a query context, every matching document contains a relevance score in the _score field, which you can use to sort documents by relevance. To run a query clause in a query context, pass it to a query parameter. For example, the following query searches for documents that match the words long live king in the shakespeare index: . GET shakespeare/_search { \"query\": { \"match\": { \"text_entry\": \"long live king\" } } } . Relevance scores are single-precision floating-point numbers with 24-bit significand precision. A loss of precision may occur if a score calculation exceeds the significand precision. ",
    "url": "https://vagimeli.github.io/query-dsl/query-filter-context/#query-context",
    "relUrl": "/query-dsl/query-filter-context/#query-context"
  },"1372": {
    "doc": "Span queries",
    "title": "Span queries",
    "content": "You can use span queries to perform precise positional searches. Span queries are low-level, specific queries that provide control over the order and proximity of specified query terms. They are primarily used to search legal documents and patents. Span queries include the following query types: . | Span containing: Wraps a list of span queries and only returns spans that match a second span query. | Span field masking: Combines span_near or span_or across different fields. | Span first: Matches spans close to the beginning of the field. | Span multi-term: Provides a wrapper around the following query types: term, range, prefix, wildcard, regexp or fuzzy. | Span near: Matches spans that are near each other. Wraps multiple span queries that must match within the specified slop distance of each other, and optionally in the same order. Slop represents the maximum number of intervening unmatched positions and indicates whether matches are required to be returned in order. | Span not: Provides a wrapper for another span query and excludes any documents that match the internal query. | Span or: Provides a wrapper for multiple span queries and includes any documents that match any of the specified queries. | Span term: Functions in the same way as a term query, but is designed to be used with other span queries. | Span within: Used with other span queries to return a single span query if its span is within the spans that are returned by a list of other span queries. | . ",
    "url": "https://vagimeli.github.io/query-dsl/span-query/",
    "relUrl": "/query-dsl/span-query/"
  },"1373": {
    "doc": "Term-level and full-text queries compared",
    "title": "Term-level and full-text queries compared",
    "content": "You can use both term-level and full-text queries to search text, but while term-level queries are usually used to search structured data, full-text queries are used for full-text search. The main difference between term-level and full-text queries is that term-level queries search documents for an exact specified term, while full-text queries analyze the query string. The following table summarizes the differences between term-level and full-text queries. |   | Term-level queries | Full-text queries | . | Description | Term-level queries answer which documents match a query. | Full-text queries answer how well the documents match a query. | . | Analyzer | The search term isn’t analyzed. This means that the term query searches for your search term as it is. | The search term is analyzed by the same analyzer that was used for the specific document field at the time it was indexed. This means that your search term goes through the same analysis process as the document’s field. | . | Relevance | Term-level queries simply return documents that match without sorting them based on the relevance score. They still calculate the relevance score, but this score is the same for all the documents that are returned. | Full-text queries calculate a relevance score for each match and sort the results by decreasing order of relevance. | . | Use Case | Use term-level queries when you want to match exact values such as numbers, dates, or tags and don’t need the matches to be sorted by relevance. | Use full-text queries to match text fields and sort by relevance after taking into account factors like casing and stemming variants. | . OpenSearch uses the BM25 ranking algorithm to calculate relevance scores. To learn more, see Okapi BM25. ",
    "url": "https://vagimeli.github.io/query-dsl/term-vs-full-text/",
    "relUrl": "/query-dsl/term-vs-full-text/"
  },"1374": {
    "doc": "Term-level and full-text queries compared",
    "title": "Should I use a full-text or a term-level query?",
    "content": "To clarify the difference between full-text and term-level queries, consider the following two examples that search for a specific text phrase. The complete works of Shakespeare are indexed in an OpenSearch cluster. Example: Phrase search . In this example, you’ll search the complete works of Shakespeare for the phrase “To be, or not to be” in the text_entry field. First, use a term-level query for this search: . GET shakespeare/_search { \"query\": { \"term\": { \"text_entry\": \"To be, or not to be\" } } } . The response contains no matches, indicated by zero hits: . { \"took\" : 3, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 0, \"relation\" : \"eq\" }, \"max_score\" : null, \"hits\" : [ ] } } . This is because the term “To be, or not to be” is searched literally in the inverted index, where only the analyzed values of the text fields are stored. Term-level queries aren’t suited for searching analyzed text fields because they often yield unexpected results. When working with text data, use term-level queries only for fields mapped as keyword. Now search for the same phrase using a full-text query: . GET shakespeare/_search { \"query\": { \"match\": { \"text_entry\": \"To be, or not to be\" } } } . The search query “To be, or not to be” is analyzed and tokenized into an array of tokens just like the text_entry field of the documents. The full-text query takes an intersection of tokens between the search query and the text_entry fields for all the documents, and then sorts the results by relevance score: . { \"took\" : 19, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 10000, \"relation\" : \"gte\" }, \"max_score\" : 17.419369, \"hits\" : [ { \"_index\" : \"shakespeare\", \"_id\" : \"34229\", \"_score\" : 17.419369, \"_source\" : { \"type\" : \"line\", \"line_id\" : 34230, \"play_name\" : \"Hamlet\", \"speech_number\" : 19, \"line_number\" : \"3.1.64\", \"speaker\" : \"HAMLET\", \"text_entry\" : \"To be, or not to be: that is the question:\" } }, { \"_index\" : \"shakespeare\", \"_id\" : \"109930\", \"_score\" : 14.883024, \"_source\" : { \"type\" : \"line\", \"line_id\" : 109931, \"play_name\" : \"A Winters Tale\", \"speech_number\" : 23, \"line_number\" : \"4.4.153\", \"speaker\" : \"PERDITA\", \"text_entry\" : \"Not like a corse; or if, not to be buried,\" } }, { \"_index\" : \"shakespeare\", \"_id\" : \"103117\", \"_score\" : 14.782743, \"_source\" : { \"type\" : \"line\", \"line_id\" : 103118, \"play_name\" : \"Twelfth Night\", \"speech_number\" : 53, \"line_number\" : \"1.3.95\", \"speaker\" : \"SIR ANDREW\", \"text_entry\" : \"will not be seen; or if she be, its four to one\" } } ] } } ... For a list of all full-text queries, see Full-text queries. Example: Exact term search . If you want to search for an exact term like “HAMLET” in the speaker field and don’t need the results to be sorted by relevance score, a term-level query is more efficient: . GET shakespeare/_search { \"query\": { \"term\": { \"speaker\": \"HAMLET\" } } } . The response contains document matches: . { \"took\" : 5, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 1582, \"relation\" : \"eq\" }, \"max_score\" : 4.2540946, \"hits\" : [ { \"_index\" : \"shakespeare\", \"_id\" : \"32700\", \"_score\" : 4.2540946, \"_source\" : { \"type\" : \"line\", \"line_id\" : 32701, \"play_name\" : \"Hamlet\", \"speech_number\" : 9, \"line_number\" : \"1.2.66\", \"speaker\" : \"HAMLET\", \"text_entry\" : \"[Aside] A little more than kin, and less than kind.\" } }, { \"_index\" : \"shakespeare\", \"_id\" : \"32702\", \"_score\" : 4.2540946, \"_source\" : { \"type\" : \"line\", \"line_id\" : 32703, \"play_name\" : \"Hamlet\", \"speech_number\" : 11, \"line_number\" : \"1.2.68\", \"speaker\" : \"HAMLET\", \"text_entry\" : \"Not so, my lord; I am too much i' the sun.\" } }, { \"_index\" : \"shakespeare\", \"_id\" : \"32709\", \"_score\" : 4.2540946, \"_source\" : { \"type\" : \"line\", \"line_id\" : 32710, \"play_name\" : \"Hamlet\", \"speech_number\" : 13, \"line_number\" : \"1.2.75\", \"speaker\" : \"HAMLET\", \"text_entry\" : \"Ay, madam, it is common.\" } } ] } } ... The term-level queries provide exact matches. So if you search for “Hamlet”, you don’t receive any matches, because “HAMLET” is a keyword field and is stored in OpenSearch literally and not in an analyzed form. The search query “HAMLET” is also searched literally. So to get a match for this field, we need to enter the exact same characters. ",
    "url": "https://vagimeli.github.io/query-dsl/term-vs-full-text/#should-i-use-a-full-text-or-a-term-level-query",
    "relUrl": "/query-dsl/term-vs-full-text/#should-i-use-a-full-text-or-a-term-level-query"
  },"1375": {
    "doc": "Term-level queries",
    "title": "Term-level queries",
    "content": "Term-level queries search an index for documents that contain an exact search term. Documents returned by a term-level query are not sorted by their relevance scores. When working with text data, use term-level queries for fields mapped as keyword only. Term-level queries are not suited for searching analyzed text fields. To return analyzed fields, use a full-text query. ",
    "url": "https://vagimeli.github.io/query-dsl/term/",
    "relUrl": "/query-dsl/term/"
  },"1376": {
    "doc": "Term-level queries",
    "title": "Term-level query types",
    "content": "The following table lists all term-level query types. | Query type | Description | . | term | Searches for documents with an exact term in a specific field. | . | terms | Searches for documents with one or more terms in a specific field. | . | terms_set | Searches for documents that match a minimum number of terms in a specific field. | . | ids | Searches for documents by document ID. | . | range | Searches for documents with field values in a specific range. | . | prefix | Searches for documents with terms that begin with a specific prefix. | . | exists | Searches for documents with any indexed value in a specific field. | . | fuzzy | Searches for documents with terms that are similar to the search term within the maximum allowed Levenshtein distance. The Levenshtein distance measures the number of one-character changes needed to change one term to another term. | . | wildcard | Searches for documents with terms that match a wildcard pattern. | . | regexp | Searches for documents with terms that match a regular expression. | . ",
    "url": "https://vagimeli.github.io/query-dsl/term/#term-level-query-types",
    "relUrl": "/query-dsl/term/#term-level-query-types"
  },"1377": {
    "doc": "Term-level queries",
    "title": "Term",
    "content": "Use the term query to search for an exact term in a field. GET shakespeare/_search { \"query\": { \"term\": { \"line_id\": { \"value\": \"61809\" } } } } . copy . ",
    "url": "https://vagimeli.github.io/query-dsl/term/#term",
    "relUrl": "/query-dsl/term/#term"
  },"1378": {
    "doc": "Term-level queries",
    "title": "Terms",
    "content": "Use the terms query to search for multiple terms in the same field. GET shakespeare/_search { \"query\": { \"terms\": { \"line_id\": [ \"61809\", \"61810\" ] } } } . copy . You get back documents that match any of the terms. ",
    "url": "https://vagimeli.github.io/query-dsl/term/#terms",
    "relUrl": "/query-dsl/term/#terms"
  },"1379": {
    "doc": "Term-level queries",
    "title": "Terms set",
    "content": "With a terms set query, you can search for documents that match a minimum number of exact terms in a specified field. The terms_set query is similar to the terms query, but you can specify the minimum number of matching terms that are required to return a document. You can specify this number either in a field in the index or with a script. As an example, consider an index that contains students with classes they have taken. When setting up the mapping for this index, you need to provide a numeric field that specifies the minimum number of matching terms that are required to return a document: . PUT students { \"mappings\": { \"properties\": { \"name\": { \"type\": \"keyword\" }, \"classes\": { \"type\": \"keyword\" }, \"min_required\": { \"type\": \"integer\" } } } } . copy . Next, index two documents that correspond to students: . PUT students/_doc/1 { \"name\": \"Mary Major\", \"classes\": [ \"CS101\", \"CS102\", \"MATH101\" ], \"min_required\": 2 } . copy . PUT students/_doc/2 { \"name\": \"John Doe\", \"classes\": [ \"CS101\", \"MATH101\", \"ENG101\" ], \"min_required\": 2 } . copy . Now search for students who have taken at least two of the following classes: CS101, CS102, MATH101: . GET students/_search { \"query\": { \"terms_set\": { \"classes\": { \"terms\": [ \"CS101\", \"CS102\", \"MATH101\" ], \"minimum_should_match_field\": \"min_required\" } } } } . copy . The response contains both students: . { \"took\" : 44, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 2, \"relation\" : \"eq\" }, \"max_score\" : 1.4544616, \"hits\" : [ { \"_index\" : \"students\", \"_id\" : \"1\", \"_score\" : 1.4544616, \"_source\" : { \"name\" : \"Mary Major\", \"classes\" : [ \"CS101\", \"CS102\", \"MATH101\" ], \"min_required\" : 2 } }, { \"_index\" : \"students\", \"_id\" : \"2\", \"_score\" : 0.5013843, \"_source\" : { \"name\" : \"John Doe\", \"classes\" : [ \"CS101\", \"MATH101\", \"ENG101\" ], \"min_required\" : 2 } } ] } } . To specify the minimum number of terms a document should match with a script, provide the script in the minimum_should_match_script field: . GET students/_search { \"query\": { \"terms_set\": { \"classes\": { \"terms\": [ \"CS101\", \"CS102\", \"MATH101\" ], \"minimum_should_match_script\": { \"source\": \"Math.min(params.num_terms, doc['min_required'].value)\" } } } } } . copy . ",
    "url": "https://vagimeli.github.io/query-dsl/term/#terms-set",
    "relUrl": "/query-dsl/term/#terms-set"
  },"1380": {
    "doc": "Term-level queries",
    "title": "IDs",
    "content": "Use the ids query to search for one or more document ID values. GET shakespeare/_search { \"query\": { \"ids\": { \"values\": [ 34229, 91296 ] } } } . copy . ",
    "url": "https://vagimeli.github.io/query-dsl/term/#ids",
    "relUrl": "/query-dsl/term/#ids"
  },"1381": {
    "doc": "Term-level queries",
    "title": "Range",
    "content": "You can search for a range of values in a field with the range query. To search for documents where the line_id value is &gt;= 10 and &lt;= 20: . GET shakespeare/_search { \"query\": { \"range\": { \"line_id\": { \"gte\": 10, \"lte\": 20 } } } } . copy . | Parameter | Behavior | . | gte | Greater than or equal to. | . | gt | Greater than. | . | lte | Less than or equal to. | . | lt | Less than. | . In addition to the range query parameters, you can provide date formats or relation operators such as “contains” or “within.” To see the supported field types for range queries, see Range query optional parameters. To see all date formats, see Formats. Assume that you have a products index and you want to find all the products that were added in the year 2019: . GET products/_search { \"query\": { \"range\": { \"created\": { \"gte\": \"2019/01/01\", \"lte\": \"2019/12/31\" } } } } . copy . Specify relative dates by using date math. To subtract 1 year and 1 day from the specified date, use the following query: . GET products/_search { \"query\": { \"range\": { \"created\": { \"gte\": \"2019/01/01||-1y-1d\" } } } } . copy . The first date that we specify is the anchor date or the starting point for the date math. Add two trailing pipe symbols. You could then add one day (+1d) or subtract two weeks (-2w). This math expression is relative to the anchor date that you specify. You could also round off dates by adding a forward slash to the date or time unit. To find products added in the last year and rounded off by month: . GET products/_search { \"query\": { \"range\": { \"created\": { \"gte\": \"now-1y/M\" } } } } . copy . The keyword now refers to the current date and time. ",
    "url": "https://vagimeli.github.io/query-dsl/term/#range",
    "relUrl": "/query-dsl/term/#range"
  },"1382": {
    "doc": "Term-level queries",
    "title": "Prefix",
    "content": "Use the prefix query to search for terms that begin with a specific prefix. GET shakespeare/_search { \"query\": { \"prefix\": { \"speaker\": \"KING\" } } } . copy . ",
    "url": "https://vagimeli.github.io/query-dsl/term/#prefix",
    "relUrl": "/query-dsl/term/#prefix"
  },"1383": {
    "doc": "Term-level queries",
    "title": "Exists",
    "content": "Use the exists query to search for documents that contain a specific field. GET shakespeare/_search { \"query\": { \"exists\": { \"field\": \"speaker\" } } } . copy . ",
    "url": "https://vagimeli.github.io/query-dsl/term/#exists",
    "relUrl": "/query-dsl/term/#exists"
  },"1384": {
    "doc": "Term-level queries",
    "title": "Fuzzy",
    "content": "A fuzzy query searches for documents with terms that are similar to the search term within the maximum allowed Levenshtein distance. The Levenshtein distance measures the number of one-character changes needed to change one term to another term. These changes include: . | Replacements: cat to bat | Insertions: cat to cats | Deletions: cat to at | Transpositions: cat to act | . A fuzzy query creates a list of all possible expansions of the search term that fall within the Levenshtein distance. You can specify the maximum number of such expansions in the max_expansions field. Then is searches for documents that match any of the expansions. The following example query searches for the speaker HALET (misspelled HAMLET). The maximum edit distance is not specified, so the default AUTO edit distance is used: . GET shakespeare/_search { \"query\": { \"fuzzy\": { \"speaker\": { \"value\": \"HALET\" } } } } . copy . The response contains all documents where HAMLET is the speaker. The following example query searches for the word cat with advanced parameters: . GET shakespeare/_search { \"query\": { \"fuzzy\": { \"speaker\": { \"value\": \"HALET\", \"fuzziness\": \"2\", \"max_expansions\": 40, \"prefix_length\": 0, \"transpositions\": true, \"rewrite\": \"constant_score\" } } } } . copy . ",
    "url": "https://vagimeli.github.io/query-dsl/term/#fuzzy",
    "relUrl": "/query-dsl/term/#fuzzy"
  },"1385": {
    "doc": "Term-level queries",
    "title": "Wildcard",
    "content": "Use wildcard queries to search for terms that match a wildcard pattern. | Feature | Behavior | . | * | Specifies all valid values. | . | ? | Specifies a single valid value. | . To search for terms that start with H and end with Y: . GET shakespeare/_search { \"query\": { \"wildcard\": { \"speaker\": { \"value\": \"H*Y\" } } } } . copy . If we change * to ?, we get no matches, because ? refers to a single character. Wildcard queries tend to be slow because they need to iterate over a lot of terms. Avoid placing wildcard characters at the beginning of a query because it could be a very expensive operation in terms of both resources and time. ",
    "url": "https://vagimeli.github.io/query-dsl/term/#wildcard",
    "relUrl": "/query-dsl/term/#wildcard"
  },"1386": {
    "doc": "Term-level queries",
    "title": "Regexp",
    "content": "Use the regexp query to search for terms that match a regular expression. This regular expression matches any single uppercase or lowercase letter: . GET shakespeare/_search { \"query\": { \"regexp\": { \"play_name\": \"[a-zA-Z]amlet\" } } } . copy . A few important notes: . | Regular expressions are applied to the terms in the field (i.e. tokens), not the entire field. | Regular expressions use the Lucene syntax, which differs from more standardized implementations. Test thoroughly to ensure that you receive the results you expect. To learn more, see the Lucene documentation. | regexp queries can be expensive operations and require the search.allow_expensive_queries setting to be set to true. Before making frequent regexp queries, test their impact on cluster performance and examine alternative queries for achieving similar results. | . ",
    "url": "https://vagimeli.github.io/query-dsl/term/#regexp",
    "relUrl": "/query-dsl/term/#regexp"
  },"1387": {
    "doc": "Flat object",
    "title": "Flat object field type",
    "content": "In OpenSearch, you don’t have to specify a mapping before indexing documents. If you don’t specify a mapping, OpenSearch uses dynamic mapping to map every field and its subfields in the document automatically. When you ingest documents such as logs, you may not know every field’s subfield name and type in advance. In this case, dynamically mapping all new subfields can quickly lead to a “mapping explosion,” where the growing number of fields may degrade the performance of your cluster. The flat object field type solves this problem by treating the entire JSON object as a string. Subfields within the JSON object are accessible using standard dot path notation, but they are not indexed for fast lookup. The maximum field value length in the dot notation is 224 − 1. The flat object field type provides the following benefits: . | Efficient reads: Fetching performance is similar to that of a keyword field. | Memory efficiency: Storing the entire complex JSON object in one field without indexing all of its subfields reduces the number of fields in an index. | Space efficiency: OpenSearch does not create an inverted index for subfields in flat objects, thereby saving space. | Compatibility for migration: You can migrate your data from systems that support similar flat types to OpenSearch. | . Mapping a field as a flat object applies when a field and its subfields are mostly read and not used as search criteria because the subfields are not indexed. Flat objects are useful for objects with a large number of fields or when you don’t know the keys in advance. Flat objects support exact match queries with and without dot path notation. For a complete list of supported query types, see Supported queries. Searching for a specific value of a nested field in a document may be inefficient because it may require a full scan of the index, which can be an expensive operation. Flat objects do not support: . | Type-specific parsing. | Numerical operations, such as numerical comparison or numerical sorting. | Text analysis. | Highlighting. | Aggregations of subfields using dot notation. | Filtering by subfields. | . ",
    "url": "https://vagimeli.github.io/field-types/flat-object/#flat-object-field-type",
    "relUrl": "/field-types/flat-object/#flat-object-field-type"
  },"1388": {
    "doc": "Flat object",
    "title": "Supported queries",
    "content": "The flat object field type supports the following queries: . | Term | Terms | Terms set | Prefix | Range | Match | Multi-match | Query string | Simple query string | Exists | . ",
    "url": "https://vagimeli.github.io/field-types/flat-object/#supported-queries",
    "relUrl": "/field-types/flat-object/#supported-queries"
  },"1389": {
    "doc": "Flat object",
    "title": "Limitations",
    "content": "The following limitations apply to flat objects in OpenSearch 2.7: . | Flat objects do not support open parameters. | Painless scripting and wildcard queries are not supported for retrieving values of subfields. | . This functionality is planned for a future release. ",
    "url": "https://vagimeli.github.io/field-types/flat-object/#limitations",
    "relUrl": "/field-types/flat-object/#limitations"
  },"1390": {
    "doc": "Flat object",
    "title": "Using flat object",
    "content": "The following example illustrates mapping a field as a flat object, indexing documents with flat object fields, and searching for leaf values of the flat object in those documents. Only the root field of a document can be defined as a flat object. You cannot define an object that is part of another JSON object as a flat object because when a flat object is flattened to a string, the nested architecture of the leaves is lost. First, create a mapping for your index, where issue is of type flat_object: . PUT /test-index/ { \"mappings\": { \"properties\": { \"issue\": { \"type\": \"flat_object\" } } } } . copy . Next, index two documents with flat object fields: . PUT /test-index/_doc/1 { \"issue\": { \"number\": \"123456\", \"labels\": { \"version\": \"2.1\", \"backport\": [ \"2.0\", \"1.3\" ], \"category\": { \"type\": \"API\", \"level\": \"enhancement\" } } } } . copy . PUT /test-index/_doc/2 { \"issue\": { \"number\": \"123457\", \"labels\": { \"version\": \"2.2\", \"category\": { \"type\": \"API\", \"level\": \"bug\" } } } } . copy . To search for a leaf value of the flat object, use either a GET or a POST request. Even if you don’t know the field names, you can search for a leaf value in the entire flat object. For example, the following request searches for all issues labeled as bugs: . GET /test-index/_search { \"query\": { \"match\": {\"issue\": \"bug\"} } } . Alternatively, if you know the subfield name in which to search, provide the field’s path in dot notation: . GET /test-index/_search { \"query\": { \"match\": {\"issue.labels.category.level\": \"bug\"} } } . copy . In both cases, the response is the same and contains document 2: . { \"took\": 1, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 1, \"relation\": \"eq\" }, \"max_score\": 1.0303539, \"hits\": [ { \"_index\": \"test-index\", \"_id\": \"2\", \"_score\": 1.0303539, \"_source\": { \"issue\": { \"number\": \"123457\", \"labels\": { \"version\": \"2.2\", \"category\": { \"type\": \"API\", \"level\": \"bug\" } } } } } ] } } . Using a prefix query, you can search for all issues for the versions that start with 2.: . GET /test-index/_search { \"query\": { \"prefix\": {\"issue.labels.version\": \"2.\"} } } . With a range query, you can search for all issues for versions 2.0–2.1: . GET /test-index/_search { \"query\": { \"range\": { \"issue\": { \"gte\": \"2.0\", \"lte\": \"2.1\" } } } } . ",
    "url": "https://vagimeli.github.io/field-types/flat-object/#using-flat-object",
    "relUrl": "/field-types/flat-object/#using-flat-object"
  },"1391": {
    "doc": "Flat object",
    "title": "Flat object",
    "content": " ",
    "url": "https://vagimeli.github.io/field-types/flat-object/",
    "relUrl": "/field-types/flat-object/"
  },"1392": {
    "doc": "Mappings and field types",
    "title": "Mappings and field types",
    "content": "You can define how documents and their fields are stored and indexed by creating a mapping. The mapping specifies the list of fields for a document. Every field in the document has a field type, which corresponds to the type of data the field contains. For example, you may want to specify that the year field should be of type date. To learn more, see Supported field types. If you’re just starting to build out your cluster and data, you may not know exactly how your data should be stored. In those cases, you can use dynamic mappings, which tell OpenSearch to dynamically add data and its fields. However, if you know exactly what types your data falls under and want to enforce that standard, then you can use explicit mappings. For example, if you want to indicate that year should be of type text instead of an integer, and age should be an integer, you can do so with explicit mappings. By using dynamic mapping, OpenSearch might interpret both year and age as integers. This section provides an example for how to create an index mapping and how to add a document to it that will get ip_range validated. | Dynamic mapping | Explicit mapping . | Response | . | Mapping example usage . | Create an index with an ip mapping | . | Get a mapping | . ",
    "url": "https://vagimeli.github.io/field-types/index/",
    "relUrl": "/field-types/index/"
  },"1393": {
    "doc": "Mappings and field types",
    "title": "Dynamic mapping",
    "content": "When you index a document, OpenSearch adds fields automatically with dynamic mapping. You can also explicitly add fields to an index mapping. Dynamic mapping types . | Type | Description | . | null | A null field can’t be indexed or searched. When a field is set to null, OpenSearch behaves as if that field has no values. | . | boolean | OpenSearch accepts true and false as boolean values. An empty string is equal to false. | . | float | A single-precision 32-bit floating point number. | . | double | A double-precision 64-bit floating point number. | . | integer | A signed 32-bit number. | . | object | Objects are standard JSON objects, which can have fields and mappings of their own. For example, a movies object can have additional properties such as title, year, and director. | . | array | Arrays in OpenSearch can only store values of one type, such as an array of just integers or strings. Empty arrays are treated as though they are fields with no values. | . | text | A string sequence of characters that represent full-text values. | . | keyword | A string sequence of structured characters, such as an email address or ZIP code. | . | date detection string | Enabled by default, if new string fields match a date’s format, then the string is processed as a date field. For example, date: \"2012/03/11\" is processed as a date. | . | numeric detection string | If disabled, OpenSearch may automatically process numeric values as strings when they should be processed as numbers. When enabled, OpenSearch can process strings into long, integer, short, byte, double, float, half_float, scaled_float, and unsigned_long. Default is disabled. | . ",
    "url": "https://vagimeli.github.io/field-types/index/#dynamic-mapping",
    "relUrl": "/field-types/index/#dynamic-mapping"
  },"1394": {
    "doc": "Mappings and field types",
    "title": "Explicit mapping",
    "content": "If you know exactly what your field data types need to be, you can specify them in your request body when creating your index. PUT sample-index1 { \"mappings\": { \"properties\": { \"year\": { \"type\" : \"text\" }, \"age\": { \"type\" : \"integer\" }, \"director\":{ \"type\" : \"text\" } } } } . Response . { \"acknowledged\": true, \"shards_acknowledged\": true, \"index\": \"sample-index1\" } . To add mappings to an existing index or data stream, you can send a request to the _mapping endpoint using the PUT or POST HTTP method: . POST sample-index1/_mapping { \"properties\": { \"year\": { \"type\" : \"text\" }, \"age\": { \"type\" : \"integer\" }, \"director\":{ \"type\" : \"text\" } } } . You cannot change the mapping of an existing field, you can only modify the field’s mapping parameters. ",
    "url": "https://vagimeli.github.io/field-types/index/#explicit-mapping",
    "relUrl": "/field-types/index/#explicit-mapping"
  },"1395": {
    "doc": "Mappings and field types",
    "title": "Mapping example usage",
    "content": "The following example shows how to create a mapping to specify that OpenSearch should ignore any documents with malformed IP addresses that do not conform to the ip data type. You accomplish this by setting the ignore_malformed parameter to true. Create an index with an ip mapping . To create an index, use a PUT request: . PUT /test-index { \"mappings\" : { \"properties\" : { \"ip_address\" : { \"type\" : \"ip\", \"ignore_malformed\": true } } } } . You can add a document that has a malformed IP address to your index: . PUT /test-index/_doc/1 { \"ip_address\" : \"malformed ip address\" } . This indexed IP address does not throw an error because ignore_malformed is set to true. You can query the index using the following request: . GET /test-index/_search . The response shows that the ip_address field is ignored in the indexed document: . { \"took\": 14, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 1, \"relation\": \"eq\" }, \"max_score\": 1, \"hits\": [ { \"_index\": \"test-index\", \"_id\": \"1\", \"_score\": 1, \"_ignored\": [ \"ip_address\" ], \"_source\": { \"ip_address\": \"malformed ip address\" } } ] } } . ",
    "url": "https://vagimeli.github.io/field-types/index/#mapping-example-usage",
    "relUrl": "/field-types/index/#mapping-example-usage"
  },"1396": {
    "doc": "Mappings and field types",
    "title": "Get a mapping",
    "content": "To get all mappings for one or more indexes, use the following request: . GET &lt;index&gt;/_mapping . In the above request, &lt;index&gt; may be an index name or a comma-separated list of index names. To get all mappings for all indexes, use the following request: . GET _mapping . To get a mapping for a specific field, provide the index name and the field name: . GET _mapping/field/&lt;fields&gt; GET /&lt;index&gt;/_mapping/field/&lt;fields&gt; . Both &lt;index&gt; and &lt;fields&gt; can be specified as one value or a comma-separated list. For example, the following request retrieves the mapping for the year and age fields in sample-index1: . GET sample-index1/_mapping/field/year,age . The response contains the specified fields: . { \"sample-index1\" : { \"mappings\" : { \"year\" : { \"full_name\" : \"year\", \"mapping\" : { \"year\" : { \"type\" : \"text\" } } }, \"age\" : { \"full_name\" : \"age\", \"mapping\" : { \"age\" : { \"type\" : \"integer\" } } } } } } . ",
    "url": "https://vagimeli.github.io/field-types/index/#get-a-mapping",
    "relUrl": "/field-types/index/#get-a-mapping"
  },"1397": {
    "doc": "Alias",
    "title": "Alias field type",
    "content": "An alias field type creates another name for an existing field. You can use aliases in thesearch and field capabilities API operations, with some exceptions. To set up an alias, you need to specify the original field name in the path parameter. ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/alias/#alias-field-type",
    "relUrl": "/field-types/supported-field-types/alias/#alias-field-type"
  },"1398": {
    "doc": "Alias",
    "title": "Example",
    "content": "PUT movies { \"mappings\" : { \"properties\" : { \"year\" : { \"type\" : \"date\" }, \"release_date\" : { \"type\" : \"alias\", \"path\" : \"year\" } } } } . copy . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/alias/#example",
    "relUrl": "/field-types/supported-field-types/alias/#example"
  },"1399": {
    "doc": "Alias",
    "title": "Parameters",
    "content": "| Parameter | Description | . | path | The full path to the original field, including all parent objects. For example, parent.child.field_name. Required. | . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/alias/#parameters",
    "relUrl": "/field-types/supported-field-types/alias/#parameters"
  },"1400": {
    "doc": "Alias",
    "title": "Alias field",
    "content": "Alias fields must obey the following rules: . | An alias field can only have one original field. | In nested objects, the alias must have the same nesting level as the original field. | . To change the field that the alias references, update the mappings. Note that aliases in any previously stored percolator queries will still reference the original field. ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/alias/#alias-field",
    "relUrl": "/field-types/supported-field-types/alias/#alias-field"
  },"1401": {
    "doc": "Alias",
    "title": "Original field",
    "content": "The original field for an alias must obey the following rules: . | The original field must be created before the alias is created. | The original field cannot be an object or another alias. | . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/alias/#original-field",
    "relUrl": "/field-types/supported-field-types/alias/#original-field"
  },"1402": {
    "doc": "Alias",
    "title": "Using aliases in search API operations",
    "content": "You can use aliases in the following read operations of the search API: . | Queries | Sorts | Aggregations | stored_fields | docvalue_fields | Suggestions | Highlights | Scripts that access field values | . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/alias/#using-aliases-in-search-api-operations",
    "relUrl": "/field-types/supported-field-types/alias/#using-aliases-in-search-api-operations"
  },"1403": {
    "doc": "Alias",
    "title": "Using aliases in field capabilities API operations",
    "content": "To use an alias in the field capabilities API, specify it in the fields parameter. GET movies/_field_caps?fields=release_date . copy . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/alias/#using-aliases-in-field-capabilities-api-operations",
    "relUrl": "/field-types/supported-field-types/alias/#using-aliases-in-field-capabilities-api-operations"
  },"1404": {
    "doc": "Alias",
    "title": "Exceptions",
    "content": "You cannot use aliases in the following situations: . | In write requests, such as update requests. | In multi-fields or as a target of copy_to. | As a _source parameter for filtering results. | In APIs that take field names, such as term vectors. | In terms, more_like_this, and geo_shape queries (aliases are not supported when retrieving documents). | . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/alias/#exceptions",
    "relUrl": "/field-types/supported-field-types/alias/#exceptions"
  },"1405": {
    "doc": "Alias",
    "title": "Wildcards",
    "content": "In search and field capabilities wildcard queries, both the original field and the alias are matched against the wildcard pattern. GET movies/_field_caps?fields=release* . copy . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/alias/#wildcards",
    "relUrl": "/field-types/supported-field-types/alias/#wildcards"
  },"1406": {
    "doc": "Alias",
    "title": "Alias",
    "content": " ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/alias/",
    "relUrl": "/field-types/supported-field-types/alias/"
  },"1407": {
    "doc": "Autocomplete field types",
    "title": "Autocomplete field types",
    "content": "The following table lists all autocomplete field types that OpenSearch supports. | Field data type | Description | . | completion | A completion suggester that provides autocomplete functionality using prefix completion. You need to upload a list of all possible completions into the index before using this feature. | . | search_as_you_type | Provides search-as-you-type functionality using both prefix and infix completion. | . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/autocomplete/",
    "relUrl": "/field-types/supported-field-types/autocomplete/"
  },"1408": {
    "doc": "Binary",
    "title": "Binary field type",
    "content": "A binary field type contains a binary value in Base64 encoding that is not searchable. ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/binary/#binary-field-type",
    "relUrl": "/field-types/supported-field-types/binary/#binary-field-type"
  },"1409": {
    "doc": "Binary",
    "title": "Example",
    "content": "Create a mapping with a binary field: . PUT testindex { \"mappings\" : { \"properties\" : { \"binary_value\" : { \"type\" : \"binary\" } } } } . copy . Index a document with a binary value: . PUT testindex/_doc/1 { \"binary_value\" : \"bGlkaHQtd29rfx4=\" } . copy . Use = as a padding character. Embedded newline characters are not allowed. ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/binary/#example",
    "relUrl": "/field-types/supported-field-types/binary/#example"
  },"1410": {
    "doc": "Binary",
    "title": "Parameters",
    "content": "The following table lists the parameters accepted by binary field types. All parameters are optional. | Parameter | Description | . | doc_values | A Boolean value that specifies whether the field should be stored on disk so that it can be used for aggregations, sorting, or scripting. Optional. Default is false. | . | store | A Boolean value that specifies whether the field value should be stored and can be retrieved separately from the _source field. Optional. Default is false. | . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/binary/#parameters",
    "relUrl": "/field-types/supported-field-types/binary/#parameters"
  },"1411": {
    "doc": "Binary",
    "title": "Binary",
    "content": " ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/binary/",
    "relUrl": "/field-types/supported-field-types/binary/"
  },"1412": {
    "doc": "Boolean",
    "title": "Boolean field type",
    "content": "A Boolean field type takes true or false values, or \"true\" or \"false\" strings. You can also pass an empty string (\"\") in place of a false value. ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/boolean/#boolean-field-type",
    "relUrl": "/field-types/supported-field-types/boolean/#boolean-field-type"
  },"1413": {
    "doc": "Boolean",
    "title": "Example",
    "content": "Create a mapping where a, b, and c are Boolean fields: . PUT testindex { \"mappings\" : { \"properties\" : { \"a\" : { \"type\" : \"boolean\" }, \"b\" : { \"type\" : \"boolean\" }, \"c\" : { \"type\" : \"boolean\" } } } } . copy . Index a document with Boolean values: . PUT testindex/_doc/1 { \"a\" : true, \"b\" : \"true\", \"c\" : \"\" } . copy . As a result, a and b will be set to true, and c will be set to false. Search for all documents where c is false: . GET testindex/_search { \"query\": { \"term\" : { \"c\" : false } } } . copy . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/boolean/#example",
    "relUrl": "/field-types/supported-field-types/boolean/#example"
  },"1414": {
    "doc": "Boolean",
    "title": "Parameters",
    "content": "The following table lists the parameters accepted by Boolean field types. All parameters are optional. | Parameter | Description | . | boost | A floating-point value that specifies the weight of this field toward the relevance score. Values above 1.0 increase the field’s relevance. Values between 0.0 and 1.0 decrease the field’s relevance. Default is 1.0. | . | doc_values | A Boolean value that specifies whether the field should be stored on disk so that it can be used for aggregations, sorting or scripting. Default is false. | . | index | A Boolean value that specifies whether the field should be searchable. Default is true. | . | meta | Accepts metadata for this field. | . | null_value | A value to be used in place of null. Must be of the same type as the field. If this parameter is not specified, the field is treated as missing when its value is null. Default is null. | . | store | A Boolean value that specifies whether the field value should be stored and can be retrieved separately from the _source field. Default is false. | . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/boolean/#parameters",
    "relUrl": "/field-types/supported-field-types/boolean/#parameters"
  },"1415": {
    "doc": "Boolean",
    "title": "Boolean values in aggregations and scripts",
    "content": "In aggregations on Boolean fields, key returns numeric values (1 for true or 0 for false), and key_as_string returns strings (\"true\" or \"false\"). Scripts return true and false for Boolean values. Example . Run a terms aggregation query on the field a: . GET testindex/_search { \"aggs\": { \"agg1\": { \"terms\": { \"field\": \"a\" } } }, \"script_fields\": { \"a\": { \"script\": { \"lang\": \"painless\", \"source\": \"doc['a'].value\" } } } } . copy . The script returns the value of a as true, key returns the value of a as 1, and key_as_string returns the value of a as \"true\": . { \"took\" : 1133, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 1, \"relation\" : \"eq\" }, \"max_score\" : 1.0, \"hits\" : [ { \"_index\" : \"testindex\", \"_type\" : \"_doc\", \"_id\" : \"1\", \"_score\" : 1.0, \"fields\" : { \"a\" : [ true ] } } ] }, \"aggregations\" : { \"agg1\" : { \"doc_count_error_upper_bound\" : 0, \"sum_other_doc_count\" : 0, \"buckets\" : [ { \"key\" : 1, \"key_as_string\" : \"true\", \"doc_count\" : 1 } ] } } } . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/boolean/#boolean-values-in-aggregations-and-scripts",
    "relUrl": "/field-types/supported-field-types/boolean/#boolean-values-in-aggregations-and-scripts"
  },"1416": {
    "doc": "Boolean",
    "title": "Boolean",
    "content": " ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/boolean/",
    "relUrl": "/field-types/supported-field-types/boolean/"
  },"1417": {
    "doc": "Completion",
    "title": "Completion field type",
    "content": "A completion field type provides autocomplete functionality through a completion suggester. The completion suggester is a prefix suggester, so it matches the beginning of text only. A completion suggester creates an in-memory data structure, which provides faster lookups but leads to increased memory usage. You need to upload a list of all possible completions into the index before using this feature. ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/completion/#completion-field-type",
    "relUrl": "/field-types/supported-field-types/completion/#completion-field-type"
  },"1418": {
    "doc": "Completion",
    "title": "Example",
    "content": "Create a mapping with a completion field: . PUT chess_store { \"mappings\": { \"properties\": { \"suggestions\": { \"type\": \"completion\" }, \"product\": { \"type\": \"keyword\" } } } } . copy . Index suggestions into OpenSearch: . PUT chess_store/_doc/1 { \"suggestions\": { \"input\": [\"Books on openings\", \"Books on endgames\"], \"weight\" : 10 } } . copy . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/completion/#example",
    "relUrl": "/field-types/supported-field-types/completion/#example"
  },"1419": {
    "doc": "Completion",
    "title": "Parameters",
    "content": "The following table lists the parameters accepted by completion fields. | Parameter | Description | . | input | A list of possible completions as a string or array of strings. Cannot contain \\u0000 (null), \\u001f (information separator one), or \\u001e (information separator two). Required. | . | weight | A positive integer or a positive integer string for ranking suggestions. Optional. | . Multiple suggestions can be indexed as follows: . PUT chess_store/_doc/2 { \"suggestions\": [ { \"input\": \"Chess set\", \"weight\": 20 }, { \"input\": \"Chess pieces\", \"weight\": 10 }, { \"input\": \"Chess board\", \"weight\": 5 } ] } . copy . As an alternative, you can use the following shorthand notation (note that you cannot provide the weight parameter in this notation): . PUT chess_store/_doc/3 { \"suggestions\" : [ \"Chess clock\", \"Chess timer\" ] } . copy . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/completion/#parameters",
    "relUrl": "/field-types/supported-field-types/completion/#parameters"
  },"1420": {
    "doc": "Completion",
    "title": "Querying completion field types",
    "content": "To query completion field types, specify the prefix that you want to search for and the name of the field in which to look for suggestions. Query the index for suggestions that start with the word “chess”: . GET chess_store/_search { \"suggest\": { \"product-suggestions\": { \"prefix\": \"chess\", \"completion\": { \"field\": \"suggestions\" } } } } . copy . The response contains autocomplete suggestions: . { \"took\" : 3, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 0, \"relation\" : \"eq\" }, \"max_score\" : null, \"hits\" : [ ] }, \"suggest\" : { \"product-suggestions\" : [ { \"text\" : \"chess\", \"offset\" : 0, \"length\" : 5, \"options\" : [ { \"text\" : \"Chess set\", \"_index\" : \"chess_store\", \"_type\" : \"_doc\", \"_id\" : \"2\", \"_score\" : 20.0, \"_source\" : { \"suggestions\" : [ { \"input\" : \"Chess set\", \"weight\" : 20 }, { \"input\" : \"Chess pieces\", \"weight\" : 10 }, { \"input\" : \"Chess board\", \"weight\" : 5 } ] } }, { \"text\" : \"Chess clock\", \"_index\" : \"chess_store\", \"_type\" : \"_doc\", \"_id\" : \"3\", \"_score\" : 1.0, \"_source\" : { \"suggestions\" : [ \"Chess clock\", \"Chess timer\" ] } } ] } ] } } . In the response, the _score field contains the value of the weight parameter that was set up at index time. The text field is populated with the suggestion’s input parameter. By default, the response contains the whole document, including the _source field, which may impact performance. To return only the suggestions field, you can specify that in the _source parameter. You can also restrict the number of returned suggestions by specifying the size parameter. GET chess_store/_search { \"_source\": \"suggestions\", \"suggest\": { \"product-suggestions\": { \"prefix\": \"chess\", \"completion\": { \"field\": \"suggestions\", \"size\" : 3 } } } } . copy . The response contains the suggestions: . { \"took\" : 5, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 0, \"relation\" : \"eq\" }, \"max_score\" : null, \"hits\" : [ ] }, \"suggest\" : { \"product-suggestions\" : [ { \"text\" : \"chess\", \"offset\" : 0, \"length\" : 5, \"options\" : [ { \"text\" : \"Chess set\", \"_index\" : \"chess_store\", \"_type\" : \"_doc\", \"_id\" : \"2\", \"_score\" : 20.0, \"_source\" : { \"suggestions\" : [ { \"input\" : \"Chess set\", \"weight\" : 20 }, { \"input\" : \"Chess pieces\", \"weight\" : 10 }, { \"input\" : \"Chess board\", \"weight\" : 5 } ] } }, { \"text\" : \"Chess clock\", \"_index\" : \"chess_store\", \"_type\" : \"_doc\", \"_id\" : \"3\", \"_score\" : 1.0, \"_source\" : { \"suggestions\" : [ \"Chess clock\", \"Chess timer\" ] } } ] } ] } } . To take advantage of source filtering, use the suggest functionality on the _search endpoint. The _suggest endpoint does not support source filtering. ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/completion/#querying-completion-field-types",
    "relUrl": "/field-types/supported-field-types/completion/#querying-completion-field-types"
  },"1421": {
    "doc": "Completion",
    "title": "Completion query parameters",
    "content": "The following table lists the parameters accepted by the completion suggester query. | Parameter | Description | . | field | A string that specifies the field on which to run the query. Required. | . | size | An integer that specifies the maximum number of returned suggestions. Optional. Default is 5. | . | skip_duplicates | A Boolean value that specifies whether to skip duplicate suggestions. Optional. Default is false. | . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/completion/#completion-query-parameters",
    "relUrl": "/field-types/supported-field-types/completion/#completion-query-parameters"
  },"1422": {
    "doc": "Completion",
    "title": "Fuzzy completion query",
    "content": "To allow for fuzzy matching, you can specify the fuzziness parameter for the completion query. In this case, even if the user mistypes a search term, the completion query still returns results. Additionally, the longer the prefix that matches the query, the higher the document’s score. GET chess_store/_search { \"suggest\": { \"product-suggestions\": { \"prefix\": \"chesc\", \"completion\": { \"field\": \"suggestions\", \"size\" : 3, \"fuzzy\" : { \"fuzziness\" : \"AUTO\" } } } } } . copy . To use all default fuzziness options, specify \"fuzzy\": {} or \"fuzzy\": true. The following table lists the parameters accepted by the fuzzy completion suggester query. All of the parameters are optional. | Parameter | Description | . | fuzziness | Fuzziness can be set as one of the following: 1. An integer that specifies the maximum allowed Levenshtein distance for this edit. 2. AUTO: Strings of 0–2 characters must match exactly, strings of 3–5 characters allow 1 edit, and strings longer than 5 characters allow 2 edits. Default is AUTO. | . | min_length | An integer that specifies the minimum length the input must be to start returning suggestions. If the search term is shorter than min_length, no suggestions are returned. Default is 3. | . | prefix_length | An integer that specifies the minimum length the matched prefix must be to start returning suggestions. If the prefix of prefix_length is not matched, but the search term is still within the Levenshtein distance, no suggestions are returned. Default is 1. | . | transpositions | A Boolean value that specifies to count transpositions (interchanges of adjacent characters) as one edit instead of two. Example: The suggestion’s input parameter is abcde and the fuzziness is 1. If transpositions is set to true, abdce will match, but if transpositions is set to false, abdce will not match. Default is true. | . | unicode_aware | A Boolean value that specifies whether to use Unicode code points when measuring the edit distance, transposition, and length. If unicode_aware is set to true, the measurement is slower. Default is false, in which case distances are measured in bytes. | . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/completion/#fuzzy-completion-query",
    "relUrl": "/field-types/supported-field-types/completion/#fuzzy-completion-query"
  },"1423": {
    "doc": "Completion",
    "title": "Regex queries",
    "content": "You can use a regular expression to define the prefix for the completion suggester query. For example, to search for strings that start with “a” and have a “d” later on, use the following query: . GET chess_store/_search { \"suggest\": { \"product-suggestions\": { \"regex\": \"a.*d\", \"completion\": { \"field\": \"suggestions\" } } } } . copy . The response matches the string “abcde”: . { \"took\" : 2, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 0, \"relation\" : \"eq\" }, \"max_score\" : null, \"hits\" : [ ] }, \"suggest\" : { \"product-suggestions\" : [ { \"text\" : \"a.*d\", \"offset\" : 0, \"length\" : 4, \"options\" : [ { \"text\" : \"abcde\", \"_index\" : \"chess_store\", \"_type\" : \"_doc\", \"_id\" : \"2\", \"_score\" : 20.0, \"_source\" : { \"suggestions\" : [ { \"input\" : \"abcde\", \"weight\" : 20 } ] } } ] } ] } } . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/completion/#regex-queries",
    "relUrl": "/field-types/supported-field-types/completion/#regex-queries"
  },"1424": {
    "doc": "Completion",
    "title": "Completion",
    "content": " ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/completion/",
    "relUrl": "/field-types/supported-field-types/completion/"
  },"1425": {
    "doc": "Date",
    "title": "Date field type",
    "content": "A date in OpenSearch can be represented as one of the following: . | A long value that corresponds to milliseconds since the epoch (the value must be non-negative). Dates are stored in this form internally. | A formatted string. | An integer value that corresponds to seconds since the epoch (the value must be non-negative). | . To represent date ranges, there is a date range field type. ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/date/#date-field-type",
    "relUrl": "/field-types/supported-field-types/date/#date-field-type"
  },"1426": {
    "doc": "Date",
    "title": "Example",
    "content": "Create a mapping with a date field and two date formats: . PUT testindex { \"mappings\" : { \"properties\" : { \"release_date\" : { \"type\" : \"date\", \"format\" : \"strict_date_optional_time||epoch_millis\" } } } } . copy . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/date/#example",
    "relUrl": "/field-types/supported-field-types/date/#example"
  },"1427": {
    "doc": "Date",
    "title": "Parameters",
    "content": "The following table lists the parameters accepted by date field types. All parameters are optional. | Parameter | Description | . | boost | A floating-point value that specifies the weight of this field toward the relevance score. Values above 1.0 increase the field’s relevance. Values between 0.0 and 1.0 decrease the field’s relevance. Default is 1.0. | . | doc_values | A Boolean value that specifies whether the field should be stored on disk so that it can be used for aggregations, sorting, or scripting. Default is false. | . | format | The format for parsing dates. Default is strict_date_optional_time||epoch_millis. | . | ignore_malformed | A Boolean value that specifies to ignore malformed values and not to throw an exception. Default is false. | . | index | A Boolean value that specifies whether the field should be searchable. Default is true. | . | locale | A region- and language-specific way of representing the date. Default is ROOT (a region- and language-neutral locale). | . | meta | Accepts metadata for this field. | . | null_value | A value to be used in place of null. Must be of the same type as the field. If this parameter is not specified, the field is treated as missing when its value is null. Default is null. | . | store | A Boolean value that specifies whether the field value should be stored and can be retrieved separately from the _source field. Default is false. | . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/date/#parameters",
    "relUrl": "/field-types/supported-field-types/date/#parameters"
  },"1428": {
    "doc": "Date",
    "title": "Formats",
    "content": "OpenSearch has built-in date formats, but you can also create your own custom formats. The default format is strict_date_optional_time||epoch_millis. You can specify multiple date formats, separated by ||. ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/date/#formats",
    "relUrl": "/field-types/supported-field-types/date/#formats"
  },"1429": {
    "doc": "Date",
    "title": "Built-in formats",
    "content": "Most of the date formats have a strict_ counterpart. When the format starts with strict_, the date must have the correct number of digits specified in the format. For example, if the format is set to strict_year_month_day (“yyyy-MM-dd”), both month and day have to be two-digit numbers. So, “2020-06-09” is valid, while “2020-6-9” is invalid. Epoch is defined as 00:00:00 UTC on January 1, 1970. y: year Y: week-based year M: month w: ordinal week of the year from 01 to 53 d: day D: ordinal day of the year from 001 to 365 (366 for leap years) e: ordinal day of the week from 1 (Monday) to 7 (Sunday) H: hour from 0 to 23 m: minute s: second S: fraction of a second Z: time zone offset (for example, +0400; -0400; -04:00) . Numeric date formats . | Format name and description | Examples | . | epoch_millis The number of milliseconds since the epoch. Minimum is -263. Maximum is 263 − 1. | 1553391286000 | . | epoch_second The number of seconds since the epoch. Minimum is -263 ÷ 1000. Maximum is (263 − 1) ÷ 1000. | 1553391286 | . Basic date formats . Components of basic date formats are not separated by a delimiter. For example, “20190323”. | Format name and description | Pattern and examples | . | Dates |   | . | basic_date_time A basic date and time separated by T. | “yyyyMMddTHHmmss.SSSZ”“20190323T213446.123-04:00” | . | basic_date_time_no_millis A basic date and time without milliseconds, separated by T. | “yyyyMMddTHHmmssZ”“20190323T213446-04:00” | . | basic_date A date with a four-digit year, two-digit month, and two-digit day. | “yyyyMMdd”“20190323” | . | Times |   | . | basic_time A time with a two-digit hour, two-digit minute, two-digit second, three-digit millisecond, and time zone offset. | “HHmmss.SSSZ” “213446.123-04:00” | . | basic_time_no_millis A basic time without milliseconds. | “HHmmssZ” “213446-04:00” | . | T times |   | . | basic_t_time A basic time preceded by T. | “THHmmss.SSSZ” “T213446.123-04:00” | . | basic_t_time_no_millis A basic time without milliseconds, preceded by T. | “THHmmssZ” “T213446-04:00” | . | Ordinal dates |   | . | basic_ordinal_date_time A full ordinal date and time. | “yyyyDDDTHHmmss.SSSZ”“2019082T213446.123-04:00” | . | basic_ordinal_date_time_no_millis A full ordinal date and time without milliseconds. | “yyyyDDDTHHmmssZ”“2019082T213446-04:00” | . | basic_ordinal_date A date with a four-digit year and three-digit ordinal day of the year. | “yyyyDDD” “2019082” | . | Week-based dates |   | . | basic_week_date_time strict_basic_week_date_time A full week-based date and time separated by T. | “YYYYWwweTHHmmss.SSSZ” “2019W126213446.123-04:00” | . | basic_week_date_time_no_millis strict_basic_week_date_time_no_millis A basic week-based year date and time without milliseconds, separated by T. | “YYYYWwweTHHmmssZ” “2019W126213446-04:00” | . | basic_week_date strict_basic_week_date A full week-based date with a four-digit week-based year, two-digit ordinal week of the year, and one-digit ordinal day of the week separated by W. | “YYYYWwwe” “2019W126” | . Full date formats . Components of full date formats are separated by a - delimiter for date and : delimiter for time. For example, “2019-03-23T21:34”. | Format name and description | Pattern and examples | . | Dates |   | . | date_optional_timestrict_date_optional_time A generic full date and time. Year is required. Month, day, and time are optional. Time is separated from date by T. | Multiple patterns. “2019-03-23T21:34:46.123456789-04:00” “2019-03-23T21:34:46” “2019-03-23T21:34” “2019” | . | strict_date_optional_time_nanos A generic full date and time. Year is required. Month, day, and time are optional. If time is specified, it must contain hours, minutes, and seconds, but fraction of a second is optional. Fraction of a second is one to nine digits long and has nanosecond resolution. Time is separated from date by T. | Multiple patterns. “2019-03-23T21:34:46.123456789-04:00” “2019-03-23T21:34:46” “2019” | . | date_time strict_date_time A full date and time separated by T. | “yyyy-MM-ddTHH:mm:ss.SSSZ” “2019-03-23T21:34:46.123-04:00” | . | date_time_no_millis strict_date_time_no_millis A full date and time without milliseconds, separated by T. | “yyyy-MM-dd’T’HH:mm:ssZ” “2019-03-23T21:34:46-04:00” | . | date_hour_minute_second_fraction strict_date_hour_minute_second_fraction A full date, two-digit hour, two-digit minute, two-digit second, and one- to nine-digit fraction of a second separated by T. | “yyyy-MM-ddTHH:mm:ss.SSSSSSSSS”“2019-03-23T21:34:46.123456789” “2019-03-23T21:34:46.1” | . | date_hour_minute_second_millis strict_date_hour_minute_second_millis A full date, two-digit hour, two-digit minute, two-digit second, and three-digit millisecond separated by T. | “yyyy-MM-ddTHH:mm:ss.SSS” “2019-03-23T21:34:46.123” | . | date_hour_minute_second strict_date_hour_minute_second A full date, two-digit hour, two-digit minute, and two-digit second separated by T. | “yyyy-MM-ddTHH:mm:ss”“2019-03-23T21:34:46” | . | date_hour_minute strict_date_hour_minute A full date, two-digit hour, and two-digit minute. | “yyyy-MM-ddTHH:mm” “2019-03-23T21:34” | . | date_hour strict_date_hour A full date and two-digit hour, separated by T. | “yyyy-MM-ddTHH” “2019-03-23T21” | . | date strict_date A four-digit year, two-digit month, and two-digit day. | “yyyy-MM-dd” “2019-03-23” | . | year_month_day strict_year_month_day A four-digit year, two-digit month, and two-digit day. | “yyyy-MM-dd” “2019-03-23” | . | year_month strict_year_month A four-digit year and two-digit month. | “yyyy-MM” “2019-03” | . | year strict_year A four-digit year. | “yyyy” “2019” | . | Times |   | . | time strict_time A two-digit hour, two-digit minute, two-digit second, one- to nine-digit fraction of a second, and time zone offset. | “HH:mm:ss.SSSSSSSSSZ” “21:34:46.123456789-04:00” “21:34:46.1-04:00” | . | time_no_millis strict_time_no_millis A two-digit hour, two-digit minute, two-digit second, and time zone offset. | “HH:mm:ssZ” “21:34:46-04:00” | . | hour_minute_second_fraction strict_hour_minute_second_fraction A two-digit hour, two-digit minute, two-digit second, and one- to nine-digit fraction of a second. | “HH:mm:ss.SSSSSSSSS” “21:34:46.1” “21:34:46.123456789” | . | hour_minute_second_millis strict_hour_minute_second_millis A two-digit hour, two-digit minute, two-digit second, and three-digit millisecond. | “HH:mm:ss.SSS” “21:34:46.123” | . | hour_minute_second strict_hour_minute_second A two-digit hour, two-digit minute, and two-digit second. | “HH:mm:ss” “21:34:46” | . | hour_minute strict_hour_minute A two-digit hour and two-digit minute. | “HH:mm” “21:34” | . | hour strict_hour A two-digit hour. | “HH” “21” | . | T times |   | . | t_time strict_t_time A two-digit hour, two-digit minute, two-digit second, one- to nine-digit fraction of a second, and time zone offset, preceded by T. | “THH:mm:ss.SSSSSSSSSZ”“T21:34:46.123456789-04:00” “T21:34:46.1-04:00” | . | t_time_no_millis strict_t_time_no_millis A two-digit hour, two-digit minute, two-digit second, and time zone offset, preceded by T. | “THH:mm:ssZ” “T21:34:46-04:00” | . | Ordinal dates |   | . | ordinal_date_time strict_ordinal_date_time A full ordinal date and time separated by T. | “yyyy-DDDTHH:mm:ss.SSSZ” “2019-082T21:34:46.123-04:00” | . | ordinal_date_time_no_millis strict_ordinal_date_time_no_millis A full ordinal date and time without milliseconds, separated by T. | “yyyy-DDDTHH:mm:ssZ” “2019-082T21:34:46-04:00” | . | ordinal_date strict_ordinal_date A full ordinal date with a four-digit year and three-digit ordinal day of the year. | “yyyy-DDD” “2019-082” | . | Week-based dates |   | . | week_date_time strict_week_date_time A full week-based date and time separated by T. Week date is a four-digit week-based year, two-digit ordinal week of the year, and one-digit ordinal day of the week. Time is a two-digit hour, two-digit minute, two-digit second, one- to nine-digit fraction of a second, and a time zone offset. | “YYYY-Www-eTHH:mm:ss.SSSSSSSSSZ” “2019-W12-6T21:34:46.1-04:00” “2019-W12-6T21:34:46.123456789-04:00” | . | week_date_time_no_millis strict_week_date_time_no_millis A full week-based date and time without milliseconds, separated by T. Week date is a four-digit week-based year, two-digit ordinal week of the year, and one-digit ordinal day of the week. Time is a two-digit hour, two-digit minute, two-digit second, and time zone offset. | “YYYY-Www-eTHH:mm:ssZ” “2019-W12-6T21:34:46-04:00” | . | week_date strict_week_date A full week-based date with a four-digit week-based year, two-digit ordinal week of the year, and one-digit ordinal day of the week. | “YYYY-Www-e” “2019-W12-6” | . | weekyear_week_day strict_weekyear_week_day A four-digit week-based year, two-digit ordinal week of the year, and one digit day of the week. | “YYYY-‘W’ww-e” “2019-W12-6” | . | weekyear_week strict_weekyear_week A four-digit week-based year and two-digit ordinal week of the year. | “YYYY-Www” “2019-W12” | . | weekyear strict_weekyear A four-digit week-based year. | “YYYY” “2019” | . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/date/#built-in-formats",
    "relUrl": "/field-types/supported-field-types/date/#built-in-formats"
  },"1430": {
    "doc": "Date",
    "title": "Custom formats",
    "content": "You can create custom formats for date fields. For example, the following request specifies a date in the common “MM/dd/yyyy” format: . PUT testindex { \"mappings\" : { \"properties\" : { \"release_date\" : { \"type\" : \"date\", \"format\" : \"MM/dd/yyyy\" } } } } . copy . Index a document with a date: . PUT testindex/_doc/21 { \"release_date\" : \"03/21/2019\" } . copy . When searching for an exact date, provide that date in the same format: . GET testindex/_search { \"query\" : { \"match\": { \"release_date\" : { \"query\": \"03/21/2019\" } } } } . copy . Range queries by default use the field’s mapped format. You can also specify the range of dates in a different format by providing the format parameter: . GET testindex/_search { \"query\": { \"range\": { \"release_date\": { \"gte\": \"2019-01-01\", \"lte\": \"2019-12-31\", \"format\": \"yyyy-MM-dd\" } } } } . copy . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/date/#custom-formats",
    "relUrl": "/field-types/supported-field-types/date/#custom-formats"
  },"1431": {
    "doc": "Date",
    "title": "Date math",
    "content": "The date field type supports using date math to specify durations in queries. For example, the gt, gte, lt, and lte parameters in range queries and the from and to parameters in date range aggregations accept date math expressions. A date math expression contains a fixed date, optionally followed by one or more mathematical expressions. The fixed date may be either now (current date and time in milliseconds since the epoch) or a string ending with || that specifies a date (for example, 2022-05-18||). The date must be in the strict_date_optional_time||epoch_millis format. Date math supports the following mathematical operators. | Operator | Description | Example | . | + | Addition | +1M: Add 1 month. | . | - | Subtraction | -1y: Subtract 1 year. | . | / | Rounding down | /h: Round to the beginning of the hour. | . Date math supports the following time units: . y: Years M: Months w: Weeks d: Days h or H: Hours m: Minutes s: Seconds . Example expressions . The following example expressions illustrate using date math: . | now+1M: The current date and time in milliseconds since the epoch, plus 1 month. | 2022-05-18||/M: 05/18/2022, rounded to the beginning of the month. Resolves to 2022-05-01. | 2022-05-18T15:23||/h: 15:23 on 05/18/2022, rounded to the beginning of the hour. Resolves to 2022-05-18T15. | 2022-05-18T15:23:17.789||+2M-1d/d: 15:23:17.789 on 05/18/2022 plus 2 months minus 1 day, rounded to the beginning of the day. Resolves to 2022-07-17. | . Using date math in a range query . The following example illustrates using date math in a range query. Set up an index with release_date mapped as date: . PUT testindex { \"mappings\" : { \"properties\" : { \"release_date\" : { \"type\" : \"date\" } } } } . copy . Index two documents into the index: . PUT testindex/_doc/1 { \"release_date\": \"2022-09-14\" } . copy . PUT testindex/_doc/2 { \"release_date\": \"2022-11-15\" } . copy . The following query searches for documents with release_date within 2 months and 1 day of 09/14/2022. The lower boundary of the range is rounded to the beginning of the day on 09/14/2022: . GET testindex/_search { \"query\": { \"range\": { \"release_date\": { \"gte\": \"2022-09-14T15:23||/d\", \"lte\": \"2022-09-14||+2M+1d\" } } } } . copy . The response contains both documents: . { \"took\" : 1, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 2, \"relation\" : \"eq\" }, \"max_score\" : 1.0, \"hits\" : [ { \"_index\" : \"testindex\", \"_id\" : \"2\", \"_score\" : 1.0, \"_source\" : { \"release_date\" : \"2022-11-14\" } }, { \"_index\" : \"testindex\", \"_id\" : \"1\", \"_score\" : 1.0, \"_source\" : { \"release_date\" : \"2022-09-14\" } } ] } } . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/date/#date-math",
    "relUrl": "/field-types/supported-field-types/date/#date-math"
  },"1432": {
    "doc": "Date",
    "title": "Date",
    "content": " ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/date/",
    "relUrl": "/field-types/supported-field-types/date/"
  },"1433": {
    "doc": "Geopoint",
    "title": "Geopoint field type",
    "content": "A geopoint field type contains a geographic point specified by latitude and longitude. ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/geo-point/#geopoint-field-type",
    "relUrl": "/field-types/supported-field-types/geo-point/#geopoint-field-type"
  },"1434": {
    "doc": "Geopoint",
    "title": "Example",
    "content": "Create a mapping with a geopoint field type: . PUT testindex1 { \"mappings\": { \"properties\": { \"point\": { \"type\": \"geo_point\" } } } } . copy . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/geo-point/#example",
    "relUrl": "/field-types/supported-field-types/geo-point/#example"
  },"1435": {
    "doc": "Geopoint",
    "title": "Formats",
    "content": "Geopoints can be indexed in the following formats: . | An object with a latitude and longitude | . PUT testindex1/_doc/1 { \"point\": { \"lat\": 40.71, \"lon\": 74.00 } } . copy . | A string in the “latitude,longitude” format | . PUT testindex1/_doc/2 { \"point\": \"40.71,74.00\" } . copy . | A geohash | . PUT testindex1/_doc/3 { \"point\": \"txhxegj0uyp3\" } . copy . | An array in the [longitude, latitude] format | . PUT testindex1/_doc/4 { \"point\": [74.00, 40.71] } . copy . | A Well-Known Text POINT in the “POINT(longitude latitude)” format | . PUT testindex1/_doc/5 { \"point\": \"POINT (74.00 40.71)\" } . copy . | GeoJSON format, where the coordinates are in the [longitude, latitude] format | . PUT testindex1/_doc/6 { \"point\": { \"type\": \"Point\", \"coordinates\": [74.00, 40.71] } } . copy . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/geo-point/#formats",
    "relUrl": "/field-types/supported-field-types/geo-point/#formats"
  },"1436": {
    "doc": "Geopoint",
    "title": "Parameters",
    "content": "The following table lists the parameters accepted by geopoint field types. All parameters are optional. | Parameter | Description | . | ignore_malformed | A Boolean value that specifies to ignore malformed values and not to throw an exception. Valid values for latitude are [-90, 90]. Valid values for longitude are [-180, 180]. Default is false. | . | ignore_z_value | Specific to points with three coordinates. If ignore_z_value is true, the third coordinate is not indexed but is still stored in the _source field. If ignore_z_value is false, an exception is thrown. | . | null_value | A value to be used in place of null. Must be of the same type as the field. If this parameter is not specified, the field is treated as missing when its value is null. Default is null. | . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/geo-point/#parameters",
    "relUrl": "/field-types/supported-field-types/geo-point/#parameters"
  },"1437": {
    "doc": "Geopoint",
    "title": "Geopoint",
    "content": " ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/geo-point/",
    "relUrl": "/field-types/supported-field-types/geo-point/"
  },"1438": {
    "doc": "Geoshape",
    "title": "Geoshape field type",
    "content": "A geoshape field type contains a geographic shape, such as a polygon or a collection of geographic points. To index a geoshape, OpenSearch tesselates the shape into a triangular mesh and stores each triangle in a BKD tree. This provides a 10-7decimal degree of precision, which represents near-perfect spatial resolution. Performance of this process is mostly impacted by the number of vertices in a polygon you are indexing. ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/geo-shape/#geoshape-field-type",
    "relUrl": "/field-types/supported-field-types/geo-shape/#geoshape-field-type"
  },"1439": {
    "doc": "Geoshape",
    "title": "Example",
    "content": "Create a mapping with a geoshape field type: . PUT testindex { \"mappings\": { \"properties\": { \"location\": { \"type\": \"geo_shape\" } } } } . copy . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/geo-shape/#example",
    "relUrl": "/field-types/supported-field-types/geo-shape/#example"
  },"1440": {
    "doc": "Geoshape",
    "title": "Formats",
    "content": "Geoshapes can be indexed in the following formats: . | GeoJSON | Well-Known Text (WKT) | . In both GeoJSON and WKT, the coordinates must be specified in the longitude, latitude order within coordinate arrays. Note that the longitude comes first in this format. ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/geo-shape/#formats",
    "relUrl": "/field-types/supported-field-types/geo-shape/#formats"
  },"1441": {
    "doc": "Geoshape",
    "title": "Geoshape types",
    "content": "The following table describes the possible geoshape types and their relationship to the GeoJSON and WKT types. | OpenSearch type | GeoJSON type | WKT type | Description | . | point | Point | POINT | A geographic point specified by latitude and longitude. OpenSearch uses World Geodetic System (WGS84) coordinates. | . | linestring | LineString | LINESTRING | A line specified by two or more points. May be a straight line or a path of connected line segments. | . | polygon | Polygon | POLYGON | A polygon specified by a list of vertices in coordinate form. The polygon must be closed, meaning the last point must be the same as the first point. Therefore, to create an n-gon, n+1 vertices are required. The minimum number of vertices is four, which creates a triangle. | . | multipoint | MultiPoint | MULTIPOINT | An array of discrete related points that are not connected. | . | multilinestring | MultiLineString | MULTILINESTRING | An array of linestrings. | . | multipolygon | MultiPolygon | MULTIPOLYGON | An array of polygons. | . | geometrycollection | GeometryCollection | GEOMETRYCOLLECTION | A collection of geoshapes that may be of different types. | . | envelope | N/A | BBOX | A bounding rectangle specified by upper-left and lower-right vertices. | . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/geo-shape/#geoshape-types",
    "relUrl": "/field-types/supported-field-types/geo-shape/#geoshape-types"
  },"1442": {
    "doc": "Geoshape",
    "title": "Point",
    "content": "A point is a single pair of coordinates specified by latitude and longitude. Index a point in GeoJSON format: . PUT testindex/_doc/1 { \"location\" : { \"type\" : \"point\", \"coordinates\" : [74.00, 40.71] } } . copy . Index a point in WKT format: . PUT testindex/_doc/1 { \"location\" : \"POINT (74.0060 40.7128)\" } . copy . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/geo-shape/#point",
    "relUrl": "/field-types/supported-field-types/geo-shape/#point"
  },"1443": {
    "doc": "Geoshape",
    "title": "Linestring",
    "content": "A linestring is a line specified by two or more points. If the points are collinear, the linestring is a straight line. Otherwise, the linestring represents a path made of line segments. Index a linestring in GeoJSON format: . PUT testindex/_doc/2 { \"location\" : { \"type\" : \"linestring\", \"coordinates\" : [[74.0060, 40.7128], [71.0589, 42.3601]] } } . copy . Index a linestring in WKT format: . PUT testindex/_doc/2 { \"location\" : \"LINESTRING (74.0060 40.7128, 71.0589 42.3601)\" } . copy . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/geo-shape/#linestring",
    "relUrl": "/field-types/supported-field-types/geo-shape/#linestring"
  },"1444": {
    "doc": "Geoshape",
    "title": "Polygon",
    "content": "A polygon is specified by a list of vertices in coordinate form. The polygon must be closed, meaning the last point must be the same as the first point. In the following example, a triangle is created using four points. GeoJSON requires that you list the vertices of the polygon counterclockwise. WKT does not impose a specific order on vertices. Index a polygon (triangle) in GeoJSON format: . PUT testindex/_doc/3 { \"location\" : { \"type\" : \"polygon\", \"coordinates\" : [ [[74.0060, 40.7128], [71.0589, 42.3601], [73.7562, 42.6526], [74.0060, 40.7128]] ] } } . copy . Index a polygon (triangle) in WKT format: . PUT testindex/_doc/3 { \"location\" : \"POLYGON ((74.0060 40.7128, 71.0589 42.3601, 73.7562 42.6526, 74.0060 40.7128))\" } . copy . The polygon may have holes inside. In this case, the coordinates field will contain multiple arrays. The first array represents the outer polygon, and each subsequent array represents a hole. Holes are represented as polygons and specified as arrays of coordinates. GeoJSON requires that you list the vertices of the polygon counterclockwise and the vertices of the hole clockwise. WKT does not impose a specific order on vertices. Index a polygon (triangle) with a triangular hole in GeoJSON format: . PUT testindex/_doc/4 { \"location\" : { \"type\" : \"polygon\", \"coordinates\" : [ [[74.0060, 40.7128], [71.0589, 42.3601], [73.7562, 42.6526], [74.0060, 40.7128]], [[72.6734,41.7658], [72.6506, 41.5623], [73.0515, 41.5582], [72.6734, 41.7658]] ] } } . copy . Index a polygon (triangle) with a triangular hole in WKT format: . PUT testindex/_doc/4 { \"location\" : \"POLYGON ((40.7128 74.0060, 42.3601 71.0589, 42.6526 73.7562, 40.7128 74.0060), (41.7658 72.6734, 41.5623 72.6506, 41.5582 73.0515, 41.7658 72.6734))\" } . copy . In OpenSearch, you can specify a polygon by listing its vertices clockwise or counterclockwise. This works well for polygons that do not cross the date line (are narrower than 180°). However, a polygon that crosses the date line (is wider than 180°) might be ambiguous because WKT does not impose a specific order on vertices. Thus, you must specify polygons that cross the date line by listing their vertices counterclockwise. You can define an orientation parameter to specify the vertex traversal order at mapping time: . PUT testindex { \"mappings\": { \"properties\": { \"location\": { \"type\": \"geo_shape\", \"orientation\" : \"left\" } } } } . copy . Subsequently indexed documents can override the orientation setting: . PUT testindex/_doc/3 { \"location\" : { \"type\" : \"polygon\", \"orientation\" : \"cw\", \"coordinates\" : [ [[74.0060, 40.7128], [71.0589, 42.3601], [73.7562, 42.6526], [74.0060, 40.7128]] ] } } . copy . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/geo-shape/#polygon",
    "relUrl": "/field-types/supported-field-types/geo-shape/#polygon"
  },"1445": {
    "doc": "Geoshape",
    "title": "Multipoint",
    "content": "A multipoint is an array of discrete related points that are not connected. Index a multipoint in GeoJSON format: . PUT testindex/_doc/6 { \"location\" : { \"type\" : \"multipoint\", \"coordinates\" : [ [74.0060, 40.7128], [71.0589, 42.3601] ] } } . copy . Index a multipoint in WKT format: . PUT testindex/_doc/6 { \"location\" : \"MULTIPOINT (74.0060 40.7128, 71.0589 42.3601)\" } . copy . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/geo-shape/#multipoint",
    "relUrl": "/field-types/supported-field-types/geo-shape/#multipoint"
  },"1446": {
    "doc": "Geoshape",
    "title": "Multilinestring",
    "content": "A multilinestring is an array of linestrings. Index a linestring in GeoJSON format: . PUT testindex/_doc/2 { \"location\" : { \"type\" : \"multilinestring\", \"coordinates\" : [ [[74.0060, 40.7128], [71.0589, 42.3601]], [[73.7562, 42.6526], [72.6734, 41.7658]] ] } } . copy . Index a linestring in WKT format: . PUT testindex/_doc/2 { \"location\" : \"MULTILINESTRING ((74.0060 40.7128, 71.0589 42.3601), (73.7562 42.6526, 72.6734 41.7658))\" } . copy . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/geo-shape/#multilinestring",
    "relUrl": "/field-types/supported-field-types/geo-shape/#multilinestring"
  },"1447": {
    "doc": "Geoshape",
    "title": "Multipolygon",
    "content": "A multipolygon is an array of polygons. In this example, the first polygon contains a hole, and the second does not. Index a multipolygon in GeoJSON format: . PUT testindex/_doc/4 { \"location\" : { \"type\" : \"multipolygon\", \"coordinates\" : [ [ [[74.0060, 40.7128], [71.0589, 42.3601], [73.7562, 42.6526], [74.0060, 40.7128]], [[72.6734,41.7658], [72.6506, 41.5623], [73.0515, 41.5582], [72.6734, 41.7658]] ], [ [[73.9776, 40.7614], [73.9554, 40.7827], [73.9631, 40.7812], [73.9776, 40.7614]] ] ] } } . copy . Index a multipolygon in WKT format: . PUT testindex/_doc/4 { \"location\" : \"MULTIPOLYGON (((40.7128 74.0060, 42.3601 71.0589, 42.6526 73.7562, 40.7128 74.0060), (41.7658 72.6734, 41.5623 72.6506, 41.5582 73.0515, 41.7658 72.6734)), ((73.9776 40.7614, 73.9554 40.7827, 73.9631 40.7812, 73.9776 40.7614)))\" } . copy . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/geo-shape/#multipolygon",
    "relUrl": "/field-types/supported-field-types/geo-shape/#multipolygon"
  },"1448": {
    "doc": "Geoshape",
    "title": "Geometry collection",
    "content": "A geometry collection is a collection of geoshapes that may be of different types. Index a geometry collection in GeoJSON format: . PUT testindex/_doc/7 { \"location\" : { \"type\": \"geometrycollection\", \"geometries\": [ { \"type\": \"point\", \"coordinates\": [74.0060, 40.7128] }, { \"type\": \"linestring\", \"coordinates\": [[73.7562, 42.6526], [72.6734, 41.7658]] } ] } } . copy . Index a geometry collection in WKT format: . PUT testindex/_doc/7 { \"location\" : \"GEOMETRYCOLLECTION (POINT (74.0060 40.7128), LINESTRING(73.7562 42.6526, 72.6734 41.7658))\" } . copy . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/geo-shape/#geometry-collection",
    "relUrl": "/field-types/supported-field-types/geo-shape/#geometry-collection"
  },"1449": {
    "doc": "Geoshape",
    "title": "Envelope",
    "content": "An envelope is a bounding rectangle specified by upper-left and lower-right vertices. The GeoJSON format is [[minLon, maxLat], [maxLon, minLat]]. Index an envelope in GeoJSON format: . PUT testindex/_doc/2 { \"location\" : { \"type\" : \"envelope\", \"coordinates\" : [[71.0589, 42.3601], [74.0060, 40.7128]] } } . copy . In WKT format, use BBOX (minLon, maxLon, maxLat, minLat). Index an envelope in WKT BBOX format: . PUT testindex/_doc/8 { \"location\" : \"BBOX (71.0589, 74.0060, 42.3601, 40.7128)\" } . copy . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/geo-shape/#envelope",
    "relUrl": "/field-types/supported-field-types/geo-shape/#envelope"
  },"1450": {
    "doc": "Geoshape",
    "title": "Parameters",
    "content": "The following table lists the parameters accepted by geoshape field types. All parameters are optional. | Parameter | Description | . | coerce | A Boolean value that specifies whether to automatically close unclosed linear rings. Default is false. | . | ignore_malformed | A Boolean value that specifies to ignore malformed GeoJSON or WKT geoshapes and not to throw an exception. Default is false (throw an exception when geoshapes are malformed). | . | ignore_z_value | Specific to points with three coordinates. If ignore_z_value is true, the third coordinate is not indexed but is still stored in the _source field. If ignore_z_value is false, an exception is thrown. Default is true. | . | orientation | Specifies the traversal order of the vertices in the geoshape’s list of coordinates. orientation takes the following values: 1. RIGHT: counterclockwise. Specify RIGHT orientation by using one of the following strings (uppercase or lowercase): right, counterclockwise, ccw. 2. LEFT: clockwise. Specify LEFT orientation by using one of the following strings (uppercase or lowercase): left, clockwise, cw. This value can be overridden by individual documents. Default is RIGHT. | . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/geo-shape/#parameters",
    "relUrl": "/field-types/supported-field-types/geo-shape/#parameters"
  },"1451": {
    "doc": "Geoshape",
    "title": "Geoshape",
    "content": " ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/geo-shape/",
    "relUrl": "/field-types/supported-field-types/geo-shape/"
  },"1452": {
    "doc": "Geographic field types",
    "title": "Geographic field types",
    "content": "The following table lists all geographic field types that OpenSearch supports. | Field data type | Description | . | geo_point | A geographic point specified by latitude and longitude. | . | geo_shape | A geographic shape, such as a polygon or a collection of geographic points. | . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/geographic/",
    "relUrl": "/field-types/supported-field-types/geographic/"
  },"1453": {
    "doc": "Supported field types",
    "title": "Supported field types",
    "content": "You can specify data types for your fields when creating a mapping. The following table lists all data field types that OpenSearch supports. | Field data type | Description | . | alias | An additional name for an existing field. | . | binary | A binary value in Base64 encoding. | . | Numeric | byte, double, float, half_float, integer, long, scaled_float, short. | . | boolean | A Boolean value. | . | date | A date value as a formatted string, a long value, or an integer. | . | ip | An IP address in IPv4 or IPv6 format. | . | Range | integer_range, long_range,double_range, float_range, date_range,ip_range. | . | Object | object, nested, join. | . | String | keyword, text, token_count. | . | Autocomplete | completion, search_as_you_type. | . | Geographic | geo_point, geo_shape. | . | Rank | rank_feature, rank_features. | . | percolator | Specifies to treat this field as a query. | . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/index/",
    "relUrl": "/field-types/supported-field-types/index/"
  },"1454": {
    "doc": "Supported field types",
    "title": "Arrays",
    "content": "There is no dedicated array field type in OpenSearch. Instead, you can pass an array of values into any field. All values in the array must have the same field type. PUT testindex1/_doc/1 { \"number\": 1 } PUT testindex1/_doc/2 { \"number\": [1, 2, 3] } . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/index/#arrays",
    "relUrl": "/field-types/supported-field-types/index/#arrays"
  },"1455": {
    "doc": "Supported field types",
    "title": "Multifields",
    "content": "Multifields are used to index the same field differently. Strings are often mapped as text for full-text queries and keyword for exact-value queries. Multifields can be created using the fields parameter. For example, you can map a book title to be of type text and keep a title.raw subfield of type keyword. PUT books { \"mappings\" : { \"properties\" : { \"title\" : { \"type\" : \"text\", \"fields\" : { \"raw\" : { \"type\" : \"keyword\" } } } } } } . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/index/#multifields",
    "relUrl": "/field-types/supported-field-types/index/#multifields"
  },"1456": {
    "doc": "Supported field types",
    "title": "Null value",
    "content": "Setting a field’s value to null, an empty array, or an array of null values makes this field equivalent to an empty field. Therefore, you cannot search for documents that have null in this field. To make a field searchable for null values, you can specify its null_value parameter in the index’s mappings. Then, all null values passed to this field will be replaced with the specified null_value. The null_value parameter must be of the same type as the field. For example, if your field is a string, the null_value for this field must also be a string. Example . Create a mapping to replace null values in the emergency_phone field with the string “NONE”: . PUT testindex { \"mappings\": { \"properties\": { \"name\": { \"type\": \"keyword\" }, \"emergency_phone\": { \"type\": \"keyword\", \"null_value\": \"NONE\" } } } } . Index three documents into testindex. The emergency_phone fields of documents 1 and 3 contain null, while the emergency_phone field of document 2 has an empty array: . PUT testindex/_doc/1 { \"name\": \"Akua Mansa\", \"emergency_phone\": null } . PUT testindex/_doc/2 { \"name\": \"Diego Ramirez\", \"emergency_phone\" : [] } . PUT testindex/_doc/3 { \"name\": \"Jane Doe\", \"emergency_phone\": [null, null] } . Search for people who do not have an emergency phone: . GET testindex/_search { \"query\": { \"term\": { \"emergency_phone\": \"NONE\" } } } . The response contains documents 1 and 3 but not document 2 because only explicit null values are replaced with the string “NONE”: . { \"took\" : 1, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 2, \"relation\" : \"eq\" }, \"max_score\" : 0.18232156, \"hits\" : [ { \"_index\" : \"testindex\", \"_type\" : \"_doc\", \"_id\" : \"1\", \"_score\" : 0.18232156, \"_source\" : { \"name\" : \"Akua Mansa\", \"emergency_phone\" : null } }, { \"_index\" : \"testindex\", \"_type\" : \"_doc\", \"_id\" : \"3\", \"_score\" : 0.18232156, \"_source\" : { \"name\" : \"Jane Doe\", \"emergency_phone\" : [ null, null ] } } ] } } . The _source field still contains explicit null values because it is not affected by the null_value. ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/index/#null-value",
    "relUrl": "/field-types/supported-field-types/index/#null-value"
  },"1457": {
    "doc": "IP address",
    "title": "IP address field type",
    "content": "An ip field type contains an IP address in IPv4 or IPv6 format. To represent IP address ranges, there is an IP range field type. ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/ip/#ip-address-field-type",
    "relUrl": "/field-types/supported-field-types/ip/#ip-address-field-type"
  },"1458": {
    "doc": "IP address",
    "title": "Example",
    "content": "Create a mapping with an IP address: . PUT testindex { \"mappings\" : { \"properties\" : { \"ip_address\" : { \"type\" : \"ip\" } } } } . copy . Index a document with an IP address: . PUT testindex/_doc/1 { \"ip_address\" : \"10.24.34.0\" } . copy . Query an index for a specific IP address: . GET testindex/_doc/1 { \"query\": { \"term\": { \"ip_address\": \"10.24.34.0\" } } } . copy . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/ip/#example",
    "relUrl": "/field-types/supported-field-types/ip/#example"
  },"1459": {
    "doc": "IP address",
    "title": "Searching for an IP address and its associated network mask",
    "content": "You can query an index for an IP address in Classless Inter-Domain Routing (CIDR) notation. Using CIDR notation, specify the IP address and the prefix length (0–32), separated by /. For example, the prefix length of 24 will match all IP addresses with the same initial 24 bits. Example query in IPv4 format . GET testindex/_search { \"query\": { \"term\": { \"ip_address\": \"10.24.34.0/24\" } } } . copy . Example query in IPv6 format . GET testindex/_search { \"query\": { \"term\": { \"ip_address\": \"2001:DB8::/24\" } } } . copy . If you use an IP address in IPv6 format in a query_string query, you need to escape : characters because they are parsed as special characters. You can accomplish this by wrapping the IP address in quotation marks and escaping those quotation marks with \\. GET testindex/_search { \"query\" : { \"query_string\": { \"query\": \"ip_address:\\\"2001:DB8::/24\\\"\" } } } . copy . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/ip/#searching-for-an-ip-address-and-its-associated-network-mask",
    "relUrl": "/field-types/supported-field-types/ip/#searching-for-an-ip-address-and-its-associated-network-mask"
  },"1460": {
    "doc": "IP address",
    "title": "Parameters",
    "content": "The following table lists the parameters accepted by ip field types. All parameters are optional. | Parameter | Description | . | boost | A floating-point value that specifies the weight of this field toward the relevance score. Values above 1.0 increase the field’s relevance. Values between 0.0 and 1.0 decrease the field’s relevance. Default is 1.0. | . | doc_values | A Boolean value that specifies if the field should be stored on disk so that it can be used for aggregations, sorting, or scripting. Default is false. | . | ignore_malformed | A Boolean value that specifies to ignore malformed values and not to throw an exception. Default is false. | . | index | A Boolean value that specifies whether the field should be searchable. Default is true. | . | null_value | A value to be used in place of null. Must be of the same type as the field. If this parameter is not specified, the field is treated as missing when its value is null. Default is null. | . | store | A Boolean value that specifies whether the field value should be stored and can be retrieved separately from the _source field. Default is false. | . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/ip/#parameters",
    "relUrl": "/field-types/supported-field-types/ip/#parameters"
  },"1461": {
    "doc": "IP address",
    "title": "IP address",
    "content": " ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/ip/",
    "relUrl": "/field-types/supported-field-types/ip/"
  },"1462": {
    "doc": "Join",
    "title": "Join field type",
    "content": "A join field type establishes a parent/child relationship between documents in the same index. ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/join/#join-field-type",
    "relUrl": "/field-types/supported-field-types/join/#join-field-type"
  },"1463": {
    "doc": "Join",
    "title": "Example",
    "content": "Create a mapping to establish a parent-child relationship between products and their brands: . PUT testindex1 { \"mappings\": { \"properties\": { \"product_to_brand\": { \"type\": \"join\", \"relations\": { \"brand\": \"product\" } } } } } . copy . Then, index a parent document with a join field type: . PUT testindex1/_doc/1 { \"name\": \"Brand 1\", \"product_to_brand\": { \"name\": \"brand\" } } . copy . You can also use a shortcut without object notation to index a parent document: . PUT testindex1/_doc/1 { \"name\": \"Brand 1\", \"product_to_brand\" : \"brand\" } . copy . When indexing child documents, you have to specify the routing query parameter because parent and child documents in the same relation have to be indexed on the same shard. Each child document refers to its parent’s ID in the parent field. Index two child documents, one for each parent: . PUT testindex1/_doc/3?routing=1 { \"name\": \"Product 1\", \"product_to_brand\": { \"name\": \"product\", \"parent\": \"1\" } } . copy . PUT testindex1/_doc/4?routing=1 { \"name\": \"Product 2\", \"product_to_brand\": { \"name\": \"product\", \"parent\": \"1\" } } . copy . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/join/#example",
    "relUrl": "/field-types/supported-field-types/join/#example"
  },"1464": {
    "doc": "Join",
    "title": "Querying a join field",
    "content": "When you query a join field, the response contains subfields that specify whether the returned document is a parent or a child. For child objects, the parent ID is also returned. Search for all documents . GET testindex1/_search { \"query\": { \"match_all\": {} } } . copy . The response indicates whether a document is a parent or a child: . { \"took\" : 4, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 3, \"relation\" : \"eq\" }, \"max_score\" : 1.0, \"hits\" : [ { \"_index\" : \"testindex1\", \"_type\" : \"_doc\", \"_id\" : \"1\", \"_score\" : 1.0, \"_source\" : { \"name\" : \"Brand 1\", \"product_to_brand\" : { \"name\" : \"brand\" } } }, { \"_index\" : \"testindex1\", \"_type\" : \"_doc\", \"_id\" : \"3\", \"_score\" : 1.0, \"_routing\" : \"1\", \"_source\" : { \"name\" : \"Product 1\", \"product_to_brand\" : { \"name\" : \"product\", \"parent\" : \"1\" } } }, { \"_index\" : \"testindex1\", \"_type\" : \"_doc\", \"_id\" : \"4\", \"_score\" : 1.0, \"_routing\" : \"1\", \"_source\" : { \"name\" : \"Product 2\", \"product_to_brand\" : { \"name\" : \"product\", \"parent\" : \"1\" } } } ] } } . Search for all children of a parent . Find all products associated with Brand 1: . GET testindex1/_search { \"query\" : { \"has_parent\": { \"parent_type\":\"brand\", \"query\": { \"match\" : { \"name\": \"Brand 1\" } } } } } . copy . The response contains Product 1 and Product 2, which are associated with Brand 1: . { \"took\" : 7, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 2, \"relation\" : \"eq\" }, \"max_score\" : 1.0, \"hits\" : [ { \"_index\" : \"testindex1\", \"_type\" : \"_doc\", \"_id\" : \"3\", \"_score\" : 1.0, \"_routing\" : \"1\", \"_source\" : { \"name\" : \"Product 1\", \"product_to_brand\" : { \"name\" : \"product\", \"parent\" : \"1\" } } }, { \"_index\" : \"testindex1\", \"_type\" : \"_doc\", \"_id\" : \"4\", \"_score\" : 1.0, \"_routing\" : \"1\", \"_source\" : { \"name\" : \"Product 2\", \"product_to_brand\" : { \"name\" : \"product\", \"parent\" : \"1\" } } } ] } } . Search for the parent of a child . Find the parent of Product 1: . GET testindex1/_search { \"query\" : { \"has_child\": { \"type\":\"product\", \"query\": { \"match\" : { \"name\": \"Product 1\" } } } } } . copy . The response returns Brand 1 as Product 1’s parent: . { \"took\" : 4, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 1, \"relation\" : \"eq\" }, \"max_score\" : 1.0, \"hits\" : [ { \"_index\" : \"testindex1\", \"_type\" : \"_doc\", \"_id\" : \"1\", \"_score\" : 1.0, \"_source\" : { \"name\" : \"Brand 1\", \"product_to_brand\" : { \"name\" : \"brand\" } } } ] } } . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/join/#querying-a-join-field",
    "relUrl": "/field-types/supported-field-types/join/#querying-a-join-field"
  },"1465": {
    "doc": "Join",
    "title": "Parent with many children",
    "content": "One parent can have many children. Create a mapping with multiple children: . PUT testindex1 { \"mappings\": { \"properties\": { \"parent_to_child\": { \"type\": \"join\", \"relations\": { \"parent\": [\"child 1\", \"child 2\"] } } } } } . copy . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/join/#parent-with-many-children",
    "relUrl": "/field-types/supported-field-types/join/#parent-with-many-children"
  },"1466": {
    "doc": "Join",
    "title": "Join field type notes",
    "content": ". | There can only be one join field mapping in an index. | You need to provide the routing parameter when retrieving, updating, or deleting a child document. This is because parent and child documents in the same relation have to be indexed on the same shard. | Multiple parents are not supported. | You can add a child document to an existing document only if the existing document is already marked as a parent. | You can add a new relation to an existing join field. | . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/join/#join-field-type-notes",
    "relUrl": "/field-types/supported-field-types/join/#join-field-type-notes"
  },"1467": {
    "doc": "Join",
    "title": "Join",
    "content": " ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/join/",
    "relUrl": "/field-types/supported-field-types/join/"
  },"1468": {
    "doc": "Keyword",
    "title": "Keyword field type",
    "content": "A keyword field type contains a string that is not analyzed. It allows only exact, case-sensitive matches. If you need to use a field for full-text search, map it as text instead. ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/keyword/#keyword-field-type",
    "relUrl": "/field-types/supported-field-types/keyword/#keyword-field-type"
  },"1469": {
    "doc": "Keyword",
    "title": "Example",
    "content": "Create a mapping with a keyword field: . PUT movies { \"mappings\" : { \"properties\" : { \"genre\" : { \"type\" : \"keyword\" } } } } . copy . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/keyword/#example",
    "relUrl": "/field-types/supported-field-types/keyword/#example"
  },"1470": {
    "doc": "Keyword",
    "title": "Parameters",
    "content": "The following table lists the parameters accepted by keyword field types. All parameters are optional. | Parameter | Description | . | boost | A floating-point value that specifies the weight of this field toward the relevance score. Values above 1.0 increase the field’s relevance. Values between 0.0 and 1.0 decrease the field’s relevance. Default is 1.0. | . | doc_values | A Boolean value that specifies whether the field should be stored on disk so that it can be used for aggregations, sorting, or scripting. Default is false. | . | eager_global_ordinals | Specifies whether global ordinals should be loaded eagerly on refresh. If the field is often used for aggregations, this parameter should be set to true. Default is false. | . | fields | To index the same string in several ways (for example, as a keyword and text), provide the fields parameter. You can specify one version of the field to be used for search and another to be used for sorting and aggregations. | . | ignore_above | Any string longer than this integer value should not be indexed. Default is 2147483647. Default dynamic mapping creates a keyword subfield for which ignore_above is set to 256. | . | index | A Boolean value that specifies whether the field should be searchable. Default is true. | . | index_options | Information to be stored in the index that will be considered when calculating relevance scores. Can be set to freqs for term frequency. Default is docs. | . | meta | Accepts metadata for this field. | . | normalizer | Specifies how to preprocess this field before indexing (for example, make it lowercase). Default is null (no preprocessing). | . | norms | A Boolean value that specifies whether the field length should be used when calculating relevance scores. Default is false. | . | null_value | A value to be used in place of null. Must be of the same type as the field. If this parameter is not specified, the field is treated as missing when its value is null. Default is null. | . | similarity | The ranking algorithm for calculating relevance scores. Default is BM25. | . | split_queries_on_whitespace | A Boolean value that specifies whether full-text queries should be split on whitespace. Default is false. | . | store | A Boolean value that specifies whether the field value should be stored and can be retrieved separately from the _source field. Default is false. | . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/keyword/#parameters",
    "relUrl": "/field-types/supported-field-types/keyword/#parameters"
  },"1471": {
    "doc": "Keyword",
    "title": "Keyword",
    "content": " ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/keyword/",
    "relUrl": "/field-types/supported-field-types/keyword/"
  },"1472": {
    "doc": "Nested",
    "title": "Nested field type",
    "content": "A nested field type is a special type of object field type. Any object field can take an array of objects. Each of the objects in the array is dynamically mapped as an object field type and stored in flattened form. This means that the objects in the array are broken down into individual fields, and values for each field across all objects are stored together. It is sometimes necessary to use the nested type to preserve a nested object as a whole so that you can perform a search on it. ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/nested/#nested-field-type",
    "relUrl": "/field-types/supported-field-types/nested/#nested-field-type"
  },"1473": {
    "doc": "Nested",
    "title": "Flattened form",
    "content": "By default, each of the nested objects is dynamically mapped as object field type. Any object field can take an array of objects. PUT testindex1/_doc/100 { \"patients\": [ {\"name\" : \"John Doe\", \"age\" : 56, \"smoker\" : true}, {\"name\" : \"Mary Major\", \"age\" : 85, \"smoker\" : false} ] } . copy . When these objects are stored, they are flattened, so their internal representation has an array of all values for each field: . { \"patients.name\" : [\"John Doe\", \"Mary Major\"], \"patients.age\" : [56, 85], \"smoker\" : [true, false] } . Some queries will work correctly in this representation. If you search for patients older than 75 OR smokers, document 100 should match. GET testindex1/_search { \"query\": { \"bool\": { \"should\": [ { \"term\": { \"patients.smoker\": true } }, { \"range\": { \"patients.age\": { \"gte\": 75 } } } ] } } } . copy . The query correctly returns document 100: . { \"took\" : 3, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 1, \"relation\" : \"eq\" }, \"max_score\" : 1.3616575, \"hits\" : [ { \"_index\" : \"testindex1\", \"_type\" : \"_doc\", \"_id\" : \"100\", \"_score\" : 1.3616575, \"_source\" : { \"patients\" : [ { \"name\" : \"John Doe\", \"age\" : \"56\", \"smoker\" : true }, { \"name\" : \"Mary Major\", \"age\" : \"85\", \"smoker\" : false } ] } } ] } } . Alternatively, if you search for patients older than 75 AND smokers, document 100 should not match. GET testindex1/_search { \"query\": { \"bool\": { \"must\": [ { \"term\": { \"patients.smoker\": true } }, { \"range\": { \"patients.age\": { \"gte\": 75 } } } ] } } } . copy . However, this query still incorrectly returns document 100. This is because the relation between age and smoking was lost when arrays of values for individual fields were created. ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/nested/#flattened-form",
    "relUrl": "/field-types/supported-field-types/nested/#flattened-form"
  },"1474": {
    "doc": "Nested",
    "title": "Nested field type",
    "content": "Nested objects are stored as separate documents, and the parent object has references to its children. To mark objects as nested, create a mapping with a nested field type. PUT testindex1 { \"mappings\" : { \"properties\": { \"patients\": { \"type\" : \"nested\" } } } } . copy . Then, index a document with a nested field type: . PUT testindex1/_doc/100 { \"patients\": [ {\"name\" : \"John Doe\", \"age\" : 56, \"smoker\" : true}, {\"name\" : \"Mary Major\", \"age\" : 85, \"smoker\" : false} ] } . copy . Now if you run the same query to search for patients older than 75 AND smokers, nothing is returned, which is correct. { \"took\" : 3, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 0, \"relation\" : \"eq\" }, \"max_score\" : null, \"hits\" : [ ] } } . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/nested/#nested-field-type-1",
    "relUrl": "/field-types/supported-field-types/nested/#nested-field-type-1"
  },"1475": {
    "doc": "Nested",
    "title": "Parameters",
    "content": "The following table lists the parameters accepted by object field types. All parameters are optional. | Parameter | Description | . | dynamic | Specifies whether new fields can be dynamically added to this object. Valid values are true, false, and strict. Default is true. | . | include_in_parent | A Boolean value that specifies whether all fields in the child nested object should also be added to the parent document in flattened form. Default is false. | . | incude_in_root | A Boolean value that specifies whether all fields in the child nested object should also be added to the root document in flattened form. Default is false. | . | properties | Fields of this object, which can be of any supported type. New properties can be dynamically added to this object if dynamic is set to true. | . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/nested/#parameters",
    "relUrl": "/field-types/supported-field-types/nested/#parameters"
  },"1476": {
    "doc": "Nested",
    "title": "Nested",
    "content": " ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/nested/",
    "relUrl": "/field-types/supported-field-types/nested/"
  },"1477": {
    "doc": "Numeric field types",
    "title": "Numeric field types",
    "content": "The following table lists all numeric field types that OpenSearch supports. | Field data type | Description | . | byte | A signed 8-bit integer. Minimum is -128. Maximum is 127. | . | double | A double-precision 64-bit IEEE 754 floating-point value. Minimum magnitude is 2-1074 . Maximum magnitude is (2 − 2-52) · 21023. The number of significant bits is 53. The number of significant digits is 15.95. | . | float | A single-precision 32-bit IEEE 754 floating-point value. Minimum magnitude is 2-149 . Maximum magnitude is (2 − 2-23) · 2127. The number of significant bits is 24. The number of significant digits is 7.22. | . | half_float | A half-precision 16-bit IEEE 754 floating-point value. Minimum magnitude is 2-24 . Maximum magnitude is 65504. The number of significant bits is 11. The number of significant digits is 3.31. | . | integer | A signed 32-bit integer. Minimum is -231. Maximum is 231 − 1. | . | long | A signed 64-bit integer. Minimum is -263. Maximum is 263 − 1. | . | short | A signed 16-bit integer. Minimum is -215. Maximum is 215 − 1. | . | scaled_float | A floating-point value that is multiplied by the double scale factor and stored as a long value. | . Integer, long, float, and double field types have corresponding range field types. If your numeric field contains an identifier such as an ID, you can map this field as a keyword to optimize for faster term-level queries. If you need to use range queries on this field, you can map this field as a numeric field type in addition to a keyword field type. ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/numeric/",
    "relUrl": "/field-types/supported-field-types/numeric/"
  },"1478": {
    "doc": "Numeric field types",
    "title": "Example",
    "content": "Create a mapping where integer_value is an integer field: . PUT testindex { \"mappings\" : { \"properties\" : { \"integer_value\" : { \"type\" : \"integer\" } } } } . copy . Index a document with an integer value: . PUT testindex/_doc/1 { \"integer_value\" : 123 } . copy . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/numeric/#example",
    "relUrl": "/field-types/supported-field-types/numeric/#example"
  },"1479": {
    "doc": "Numeric field types",
    "title": "Scaled float field type",
    "content": "A scaled float field type is a floating-point value that is multiplied by the scale factor and stored as a long value. It takes all optional parameters taken by number field types, plus an additional scaling_factor parameter. The scale factor is required when creating a scaled float. Scaled floats are useful for saving disk space. Larger scaling_factor values lead to better accuracy but higher space overhead. ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/numeric/#scaled-float-field-type",
    "relUrl": "/field-types/supported-field-types/numeric/#scaled-float-field-type"
  },"1480": {
    "doc": "Numeric field types",
    "title": "Scaled float example",
    "content": "Create a mapping where scaled is a scaled_float field: . PUT testindex { \"mappings\" : { \"properties\" : { \"scaled\" : { \"type\" : \"scaled_float\", \"scaling_factor\" : 10 } } } } . copy . Index a document with a scaled_float value: . PUT testindex/_doc/1 { \"scaled\" : 2.3 } . copy . The scaled value will be stored as 23. ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/numeric/#scaled-float-example",
    "relUrl": "/field-types/supported-field-types/numeric/#scaled-float-example"
  },"1481": {
    "doc": "Numeric field types",
    "title": "Parameters",
    "content": "The following table lists the parameters accepted by numeric field types. All parameters are optional. | Parameter | Description | . | boost | A floating-point value that specifies the weight of this field toward the relevance score. Values above 1.0 increase the field’s relevance. Values between 0.0 and 1.0 decrease the field’s relevance. Default is 1.0. | . | coerce | A Boolean value that signals to truncate decimals for integer values and to convert strings to numeric values. Default is true. | . | doc_values | A Boolean value that specifies whether the field should be stored on disk so that it can be used for aggregations, sorting, or scripting. Default is false. | . | ignore_malformed | A Boolean value that specifies to ignore malformed values and not to throw an exception. Default is false. | . | index | A Boolean value that specifies whether the field should be searchable. Default is true. | . | meta | Accepts metadata for this field. | . | null_value | A value to be used in place of null. Must be of the same type as the field. If this parameter is not specified, the field is treated as missing when its value is null. Default is null. | . | store | A Boolean value that specifies whether the field value should be stored and can be retrieved separately from the _source field. Default is false. | . Scaled float has an additional required parameter: scaling_factor. | Parameter | Description | . | scaling_factor | A double value that is multiplied by the field value and rounded to the nearest long. Required. | . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/numeric/#parameters",
    "relUrl": "/field-types/supported-field-types/numeric/#parameters"
  },"1482": {
    "doc": "Object field types",
    "title": "Object field types",
    "content": "The following table lists all object field types that OpenSearch supports. | Field data type | Description | . | object | A JSON object. | . | nested | Used when objects in an array need to be indexed independently as separate documents. | . | flat_object | A JSON object treated as a string. | . | join | Establishes a parent-child relationship between documents in the same index. | . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/object-fields/",
    "relUrl": "/field-types/supported-field-types/object-fields/"
  },"1483": {
    "doc": "Object",
    "title": "Object field type",
    "content": "An object field type contains a JSON object (a set of name/value pairs). A value in a JSON object may be another JSON object. It is not necessary to specify object as the type when mapping object fields because object is the default type. ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/object/#object-field-type",
    "relUrl": "/field-types/supported-field-types/object/#object-field-type"
  },"1484": {
    "doc": "Object",
    "title": "Example",
    "content": "Create a mapping with an object field: . PUT testindex1/_mappings { \"properties\": { \"patient\": { \"properties\" : { \"name\" : { \"type\" : \"text\" }, \"id\" : { \"type\" : \"keyword\" } } } } } . copy . Index a document with an object field: . PUT testindex1/_doc/1 { \"patient\": { \"name\" : \"John Doe\", \"id\" : \"123456\" } } . copy . Nested objects are stored as flat key/value pairs internally. To refer to a field in a nested object, use parent field.child field (for example, patient.id). Search for a patient with ID 123456: . GET testindex1/_search { \"query\": { \"term\" : { \"patient.id\" : \"123456\" } } } . copy . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/object/#example",
    "relUrl": "/field-types/supported-field-types/object/#example"
  },"1485": {
    "doc": "Object",
    "title": "Parameters",
    "content": "The following table lists the parameters accepted by object field types. All parameters are optional. | Parameter | Description | . | dynamic | Specifies whether new fields can be dynamically added to this object. Valid values are true, false, and strict. Default is true. | . | enabled | A Boolean value that specifies whether the JSON contents of the object should be parsed. If enabled is set to false, the object’s contents are not indexed or searchable, but they are still retrievable from the _source field. Default is true. | . | properties | Fields of this object, which can be of any supported type. New properties can be dynamically added to this object if dynamic is set to true. | . The dynamic parameter . The dynamic parameter specifies whether new fields can be dynamically added to an object that is already indexed. For example, you can initially create a mapping with a patient object that has only one field: . PUT testindex1/_mappings { \"properties\": { \"patient\": { \"properties\" : { \"name\" : { \"type\" : \"text\" } } } } } . copy . Then you index a document with a new id field in patient: . PUT testindex1/_doc/1 { \"patient\": { \"name\" : \"John Doe\", \"id\" : \"123456\" } } . copy . As a result, the field id is added to the mappings: . { \"testindex1\" : { \"mappings\" : { \"properties\" : { \"patient\" : { \"properties\" : { \"id\" : { \"type\" : \"text\", \"fields\" : { \"keyword\" : { \"type\" : \"keyword\", \"ignore_above\" : 256 } } }, \"name\" : { \"type\" : \"text\" } } } } } } } . The dynamic parameter has the following valid values. | Value | Description | . | true | New fields can be added to the mapping dynamically. This is the default. | . | false | New fields cannot be added to the mapping dynamically. If a new field is detected, it is not indexed or searchable. However, it is still retrievable from the _source field. | . | strict | When new fields are added to the mapping dynamically, an exception is thrown. To add a new field to an object, you have to add it to the mapping first. | . Inner objects inherit the dynamic parameter value from their parent unless they declare their own dynamic parameter value. ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/object/#parameters",
    "relUrl": "/field-types/supported-field-types/object/#parameters"
  },"1486": {
    "doc": "Object",
    "title": "Object",
    "content": " ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/object/",
    "relUrl": "/field-types/supported-field-types/object/"
  },"1487": {
    "doc": "Percolator",
    "title": "Percolator field type",
    "content": "A percolator field type specifies to treat this field as a query. Any JSON object field can be marked as a percolator field. Normally, documents are indexed and searches are run against them. When you use a percolator field, you store a search, and later the percolate query matches documents to that search. ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/percolator/#percolator-field-type",
    "relUrl": "/field-types/supported-field-types/percolator/#percolator-field-type"
  },"1488": {
    "doc": "Percolator",
    "title": "Example",
    "content": "A customer is searching for a table priced at $400 or less and wants to create an alert for this search. Create a mapping assigning a percolator field type to the query field: . PUT testindex1 { \"mappings\": { \"properties\": { \"search\": { \"properties\": { \"query\": { \"type\": \"percolator\" } } }, \"price\": { \"type\": \"float\" }, \"item\": { \"type\": \"text\" } } } } . copy . Index a query: . PUT testindex1/_doc/1 { \"search\": { \"query\": { \"bool\": { \"filter\": [ { \"match\": { \"item\": { \"query\": \"table\" } } }, { \"range\": { \"price\": { \"lte\": 400.00 } } } ] } } } } . copy . Fields referenced in the query must already exist in the mapping. Run a percolate query to search for matching documents: . GET testindex1/_search { \"query\" : { \"bool\" : { \"filter\" : { \"percolate\" : { \"field\" : \"search.query\", \"document\" : { \"item\" : \"Mahogany table\", \"price\": 399.99 } } } } } } . copy . The response contains the originally indexed query: . { \"took\" : 30, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 1, \"relation\" : \"eq\" }, \"max_score\" : 0.0, \"hits\" : [ { \"_index\" : \"testindex1\", \"_type\" : \"_doc\", \"_id\" : \"1\", \"_score\" : 0.0, \"_source\" : { \"search\" : { \"query\" : { \"bool\" : { \"filter\" : [ { \"match\" : { \"item\" : { \"query\" : \"table\" } } }, { \"range\" : { \"price\" : { \"lte\" : 400.0 } } } ] } } } }, \"fields\" : { \"_percolator_document_slot\" : [ 0 ] } } ] } } . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/percolator/#example",
    "relUrl": "/field-types/supported-field-types/percolator/#example"
  },"1489": {
    "doc": "Percolator",
    "title": "Percolator",
    "content": " ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/percolator/",
    "relUrl": "/field-types/supported-field-types/percolator/"
  },"1490": {
    "doc": "Range field types",
    "title": "Range field types",
    "content": "The following table lists all range field types that OpenSearch supports. | Field data type | Description | . | integer_range | A range of integer values. | . | long_range | A range of long values. | . | double_range | A range of double values. | . | float_range | A range of float values. | . | ip_range | A range of IP addresses in IPv4 or IPv6 format. Start and end IP addresses may be in different formats. | . | date_range | A range of date values. Start and end dates may be in different formats. Internally, all dates are stored as unsigned 64-bit integers representing milliseconds since the epoch. | . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/range/",
    "relUrl": "/field-types/supported-field-types/range/"
  },"1491": {
    "doc": "Range field types",
    "title": "Example",
    "content": "Create a mapping with a double range and a date range: . PUT testindex { \"mappings\" : { \"properties\" : { \"gpa\" : { \"type\" : \"double_range\" }, \"graduation_date\" : { \"type\" : \"date_range\", \"format\" : \"strict_year_month||strict_year_month_day\" } } } } . copy . Index a document with a double range and a date range: . PUT testindex/_doc/1 { \"gpa\" : { \"gte\" : 1.0, \"lte\" : 4.0 }, \"graduation_date\" : { \"gte\" : \"2019-05-01\", \"lte\" : \"2019-05-15\" } } . copy . You can use a Term query or a Range query to search for values within range fields. Term query . A term query takes a value and matches all range fields for which the value is within the range. The following query will return document 1 because 3.5 is within the range [1.0, 4.0]: . GET testindex/_search { \"query\" : { \"term\" : { \"gpa\" : { \"value\" : 3.5 } } } } . copy . Range query . A range query on a range field returns documents within that range. Along with the field to be matched, you can further specify a date format or relational operators with the following optional parameters: . | Parameter | Description | . | format | A format for dates in this query. Default is the field’s mapped format. | . | relation | Provides a relation between the query’s date range and the document’s date range. There are three types of relations that you can specify: 1. intersects matches documents for which there are dates that belong to both the query’s date range and document’s date range. This is the default. 2. contains matches documents for which the query’s date range is a subset of the document’s date range. 3. within matches documents for which the document’s date range is a subset of the query’s date range. | . To use a date format other than the field’s mapped format in a query, specify it in the format field. For a full description of range query usage, including all range query parameters, see Range query. Query for all graduation dates in 2019, providing the date range in a “MM/dd/yyyy” format: . GET testindex1/_search { \"query\": { \"range\": { \"graduation_date\": { \"gte\": \"01/01/2019\", \"lte\": \"12/31/2019\", \"format\": \"MM/dd/yyyy\", \"relation\" : \"within\" } } } } . copy . The above query will return document 1 for the within and intersects relations but will not return it for the contains relation. IP address ranges . You can specify IP address ranges in two formats: as a range and in CIDR notation. Create a mapping with an IP address range: . PUT testindex { \"mappings\" : { \"properties\" : { \"ip_address_range\" : { \"type\" : \"ip_range\" }, \"ip_address_cidr\" : { \"type\" : \"ip_range\" } } } } . copy . Index a document with IP address ranges in both formats: . PUT testindex/_doc/2 { \"ip_address_range\" : { \"gte\" : \"10.24.34.0\", \"lte\" : \"10.24.35.255\" }, \"ip_address_cidr\" : \"10.24.34.0/24\" } . copy . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/range/#example",
    "relUrl": "/field-types/supported-field-types/range/#example"
  },"1492": {
    "doc": "Range field types",
    "title": "Parameters",
    "content": "The following table lists the parameters accepted by range field types. All parameters are optional. | Parameter | Description | . | boost | A floating-point value that specifies the weight of this field toward the relevance score. Values above 1.0 increase the field’s relevance. Values between 0.0 and 1.0 decrease the field’s relevance. Default is 1.0. | . | coerce | A Boolean value that signals to truncate decimals for integer values and to convert strings to numeric values. Default is true. | . | index | A Boolean value that specifies whether the field should be searchable. Default is true. | . | store | A Boolean value that specifies whether the field value should be stored and can be retrieved separately from the _source field. Default is false. | . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/range/#parameters",
    "relUrl": "/field-types/supported-field-types/range/#parameters"
  },"1493": {
    "doc": "Rank field types",
    "title": "Rank field types",
    "content": "The following table lists all rank field types that OpenSearch supports. | Field data type | Description | . | rank_feature | Boosts or decreases the relevance score of documents. | . | rank_features | Boosts or decreases the relevance score of documents. Used when the list of features is sparse. | . Rank feature and rank features fields can be queried with rank feature queries only. They do not support aggregating or sorting. ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/rank/",
    "relUrl": "/field-types/supported-field-types/rank/"
  },"1494": {
    "doc": "Rank field types",
    "title": "Rank feature",
    "content": "A rank feature field type uses a positive float value to boost or decrease the relevance score of a document in a rank_feature query. By default, this value boosts the relevance score. To decrease the relevance score, set the optional positive_score_impact parameter to false. Example . Create a mapping with a rank feature field: . PUT chessplayers { \"mappings\": { \"properties\": { \"name\" : { \"type\" : \"text\" }, \"rating\": { \"type\": \"rank_feature\" }, \"age\": { \"type\": \"rank_feature\", \"positive_score_impact\": false } } } } . copy . Index three documents with a rank_feature field that boosts the score (rating) and a rank_feature field that decreases the score (age): . PUT testindex1/_doc/1 { \"name\" : \"John Doe\", \"rating\" : 2554, \"age\" : 75 } . copy . PUT testindex1/_doc/2 { \"name\" : \"Kwaku Mensah\", \"rating\" : 2067, \"age\": 10 } . copy . PUT testindex1/_doc/3 { \"name\" : \"Nikki Wolf\", \"rating\" : 1864, \"age\" : 22 } . copy . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/rank/#rank-feature",
    "relUrl": "/field-types/supported-field-types/rank/#rank-feature"
  },"1495": {
    "doc": "Rank field types",
    "title": "Rank feature query",
    "content": "Using a rank feature query, you can rank players by rating, by age, or by both rating and age. If you rank players by rating, higher-rated players will have higher relevance scores. If you rank players by age, younger players will have higher relevance scores. Use a rank feature query to search for players based on age and rating: . GET chessplayers/_search { \"query\": { \"bool\": { \"should\": [ { \"rank_feature\": { \"field\": \"rating\" } }, { \"rank_feature\": { \"field\": \"age\" } } ] } } } . copy . When ranked by both age and rating, younger players and players who are more highly ranked score better: . { \"took\" : 2, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 3, \"relation\" : \"eq\" }, \"max_score\" : 1.2093145, \"hits\" : [ { \"_index\" : \"chessplayers\", \"_type\" : \"_doc\", \"_id\" : \"2\", \"_score\" : 1.2093145, \"_source\" : { \"name\" : \"Kwaku Mensah\", \"rating\" : 1967, \"age\" : 10 } }, { \"_index\" : \"chessplayers\", \"_type\" : \"_doc\", \"_id\" : \"3\", \"_score\" : 1.0150313, \"_source\" : { \"name\" : \"Nikki Wolf\", \"rating\" : 1864, \"age\" : 22 } }, { \"_index\" : \"chessplayers\", \"_type\" : \"_doc\", \"_id\" : \"1\", \"_score\" : 0.8098284, \"_source\" : { \"name\" : \"John Doe\", \"rating\" : 2554, \"age\" : 75 } } ] } } . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/rank/#rank-feature-query",
    "relUrl": "/field-types/supported-field-types/rank/#rank-feature-query"
  },"1496": {
    "doc": "Rank field types",
    "title": "Rank features",
    "content": "A rank features field type is similar to the rank feature field type, but it is more suitable for a sparse list of features. A rank features field can index numeric feature vectors that are later used to boost or decrease documents’ relevance scores in rank_feature queries. Example . Create a mapping with a rank features field: . PUT testindex1 { \"mappings\": { \"properties\": { \"correlations\": { \"type\": \"rank_features\" } } } } . copy . To index a document with a rank features field, use a hashmap with string keys and positive float values: . PUT testindex1/_doc/1 { \"correlations\": { \"young kids\" : 1, \"older kids\" : 15, \"teens\" : 25.9 } } . copy . PUT testindex1/_doc/2 { \"correlations\": { \"teens\": 10, \"adults\": 95.7 } } . copy . Query the documents using a rank feature query: . GET testindex1/_search { \"query\": { \"rank_feature\": { \"field\": \"correlations.teens\" } } } . copy . The response is ranked by relevance score: . { \"took\" : 123, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 2, \"relation\" : \"eq\" }, \"max_score\" : 0.6258503, \"hits\" : [ { \"_index\" : \"testindex1\", \"_type\" : \"_doc\", \"_id\" : \"1\", \"_score\" : 0.6258503, \"_source\" : { \"correlations\" : { \"young kids\" : 1, \"older kids\" : 15, \"teens\" : 25.9 } } }, { \"_index\" : \"testindex1\", \"_type\" : \"_doc\", \"_id\" : \"2\", \"_score\" : 0.39263803, \"_source\" : { \"correlations\" : { \"teens\" : 10, \"adults\" : 95.7 } } } ] } } . Rank feature and rank features fields use top nine significant bits for precision, leading to about 0.4% relative error. Values are stored with a relative precision of 2−8 = 0.00390625. ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/rank/#rank-features",
    "relUrl": "/field-types/supported-field-types/rank/#rank-features"
  },"1497": {
    "doc": "Search as you type",
    "title": "Search-as-you-type field type",
    "content": "A search-as-you-type field type provides search-as-you-type functionality using both prefix and infix completion. ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/search-as-you-type/#search-as-you-type-field-type",
    "relUrl": "/field-types/supported-field-types/search-as-you-type/#search-as-you-type-field-type"
  },"1498": {
    "doc": "Search as you type",
    "title": "Example",
    "content": "Mapping a search-as-you-type field creates n-gram subfields of this field, where n is in the range [2, max_shingle_size]. Additionally, it creates an index prefix subfield. Create a mapping with a search-as-you-type field: . PUT books { \"mappings\": { \"properties\": { \"suggestions\": { \"type\": \"search_as_you_type\" } } } } . copy . In addition to the suggestions field, this creates suggestions._2gram, suggestions._3gram, and suggestions._index_prefix fields. Index a document with a search-as-you-type field: . PUT books/_doc/1 { \"suggestions\": \"one two three four\" } . copy . To match terms in any order, use a bool_prefix or multi-match query. These queries rank the documents in which search terms are in the specified order higher than the documents in which terms are out of order. GET books/_search { \"query\": { \"multi_match\": { \"query\": \"tw one\", \"type\": \"bool_prefix\", \"fields\": [ \"suggestions\", \"suggestions._2gram\", \"suggestions._3gram\" ] } } } . copy . The response contains the matching document: . { \"took\" : 13, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 1, \"relation\" : \"eq\" }, \"max_score\" : 1.0, \"hits\" : [ { \"_index\" : \"books\", \"_type\" : \"_doc\", \"_id\" : \"1\", \"_score\" : 1.0, \"_source\" : { \"suggestions\" : \"one two three four\" } } ] } } . To match terms in order, use a match_phrase_prefix query: . GET books/_search { \"query\": { \"match_phrase_prefix\": { \"suggestions\": \"two th\" } } } . copy . The response contains the matching document: . { \"took\" : 23, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 1, \"relation\" : \"eq\" }, \"max_score\" : 0.4793051, \"hits\" : [ { \"_index\" : \"books\", \"_type\" : \"_doc\", \"_id\" : \"1\", \"_score\" : 0.4793051, \"_source\" : { \"suggestions\" : \"one two three four\" } } ] } } . To match the last terms exactly, use a match_phrase query: . GET books/_search { \"query\": { \"match_phrase\": { \"suggestions\": \"four\" } } } . copy . Response: . { \"took\" : 2, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 1, \"relation\" : \"eq\" }, \"max_score\" : 0.2876821, \"hits\" : [ { \"_index\" : \"books\", \"_type\" : \"_doc\", \"_id\" : \"1\", \"_score\" : 0.2876821, \"_source\" : { \"suggestions\" : \"one two three four\" } } ] } } . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/search-as-you-type/#example",
    "relUrl": "/field-types/supported-field-types/search-as-you-type/#example"
  },"1499": {
    "doc": "Search as you type",
    "title": "Parameters",
    "content": "The following table lists the parameters accepted by search-as-you-type field types. All parameters are optional. | Parameter | Description | . | analyzer | The analyzer to be used for this field. By default, it will be used at index time and at search time. To override it at search time, set the search_analyzer parameter. Default is the standard analyzer, which uses grammar-based tokenization and is based on the Unicode Text Segmentation algorithm. Configures the root field and subfields. | . | index | A Boolean value that specifies whether the field should be searchable. Default is true. Configures the root field and subfields. | . | index_options | Specifies the information to be stored in the index for search and highlighting. Valid values: docs (doc number only), freqs (doc number and term frequencies), positions (doc number, term frequencies, and term positions), offsets (doc number, term frequencies, term positions, and start and end character offsets). Default is positions. Configures the root field and subfields. | . | max_shingle_size | An integer that specifies the maximum n-gram size. Valid values are in the range [2, 4]. N-grams to be created are in the range [2, max_shingle_size]. Default is 3, which creates a 2-gram and a 3-gram. Larger max_shingle_size values work better for more specific queries but lead to a larger index size. | . | norms | A Boolean value that specifies whether the field length should be used when calculating relevance scores. Configures the root field and n-gram subfields (default is false). Does not configure the prefix subfield (in the prefix subfield, norms is false). | . | search_analyzer | The analyzer to be used at search time. Default is the analyzer specified in the analyzer parameter. Configures the root field and subfields. | . | search_quote_analyzer | The analyzer to be used at search time with phrases. Default is the analyzer specified in the analyzer parameter. Configures the root field and subfields. | . | similarity | The ranking algorithm for calculating relevance scores. Default is BM25. Configures the root field and subfields. | . | store | A Boolean value that specifies whether the field value should be stored and can be retrieved separately from the _source field. Default is false. Configures the root field only. | . | term_vector | A Boolean value that specifies whether a term vector for this field should be stored. Default is no. Configures the root field and n-gram subfields. Does not configure the prefix subfield. | . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/search-as-you-type/#parameters",
    "relUrl": "/field-types/supported-field-types/search-as-you-type/#parameters"
  },"1500": {
    "doc": "Search as you type",
    "title": "Search as you type",
    "content": " ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/search-as-you-type/",
    "relUrl": "/field-types/supported-field-types/search-as-you-type/"
  },"1501": {
    "doc": "String field types",
    "title": "String field types",
    "content": "The following table lists all string field types that OpenSearch supports. | Field data type | Description | . | keyword | A string that is not analyzed. Useful for exact-value search. | . | text | A string that is analyzed. Useful for full-text search. | . | token_count | Counts the number of tokens in a string. | . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/string/",
    "relUrl": "/field-types/supported-field-types/string/"
  },"1502": {
    "doc": "Text",
    "title": "Text field type",
    "content": "A text field type contains a string that is analyzed. It is used for full-text search because it allows partial matches. Searches with multiple terms can match some but not all of them. Depending on the analyzer, results can be case insensitive, stemmed, stopwords removed, synonyms applied, etc. If you need to use a field for exact-value search, map it as a keyword instead. ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/text/#text-field-type",
    "relUrl": "/field-types/supported-field-types/text/#text-field-type"
  },"1503": {
    "doc": "Text",
    "title": "Example",
    "content": "Create a mapping with a text field: . PUT movies { \"mappings\" : { \"properties\" : { \"title\" : { \"type\" : \"text\" } } } } . copy . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/text/#example",
    "relUrl": "/field-types/supported-field-types/text/#example"
  },"1504": {
    "doc": "Text",
    "title": "Parameters",
    "content": "The following table lists the parameters accepted by text field types. All parameters are optional. | Parameter | Description | . | analyzer | The analyzer to be used for this field. By default, it will be used at index time and at search time. To override it at search time, set the search_analyzer parameter. Default is the standard analyzer, which uses grammar-based tokenization and is based on the Unicode Text Segmentation algorithm. | . | boost | A floating-point value that specifies the weight of this field toward the relevance score. Values above 1.0 increase the field’s relevance. Values between 0.0 and 1.0 decrease the field’s relevance. Default is 1.0. | . | eager_global_ordinals | Specifies whether global ordinals should be loaded eagerly on refresh. If the field is often used for aggregations, this parameter should be set to true. Default is false. | . | fielddata | A Boolean value that specifies whether to access analyzed tokens for this field for sorting, aggregation, and scripting. Default is false. | . | fielddata_frequency_filter | A JSON object that specifies to load into memory only those analyzed tokens whose document frequency is between the min and max values (provided as either an absolute number or a percentage). Frequency is computed per segment. Parameters: min, max, min_segment_size. Default is to load all analyzed tokens. | . | fields | To index the same string in several ways (for example, as a keyword and text), provide the fields parameter. You can specify one version of the field to be used for search and another to be used for sorting and aggregations. | . | index | A Boolean value that specifies whether the field should be searchable. Default is true. | . | index_options | Specifies the information to be stored in the index for search and highlighting. Valid values: docs (doc number only), freqs (doc number and term frequencies), positions (doc number, term frequencies, and term positions), offsets (doc number, term frequencies, term positions, and start and end character offsets). Default is positions. | . | index_phrases | A Boolean value that specifies to index 2-grams separately. 2-grams are combinations of two consecutive words in this field’s string. Leads to faster exact phrase queries with no slop but a larger index. Works best when stopwords are not removed. Default is false. | . | index_prefixes | A JSON object that specifies to index term prefixes separately. The number of characters in the prefix is between min_chars and max_chars, inclusive. Leads to faster prefix searches but a larger index. Optional parameters: min_chars, max_chars. Default min_chars is 2, max_chars is 5. | . | meta | Accepts metadata for this field. | . | norms | A Boolean value that specifies whether the field length should be used when calculating relevance scores. Default is false. | . | position_increment_gap | When text fields are analyzed, they are assigned positions. If a field contained an array of strings, and these positions were consecutive, this would lead to potentially matching across different array elements. To prevent this, an artificial gap is inserted between consecutive array elements. You can change this gap by specifying an integer position_increment_gap. Note: If slop is greater than position_element_gap, matching across different array elements may occur. Default is 100. | . | similarity | The ranking algorithm for calculating relevance scores. Default is BM25. | . | term_vector | A Boolean value that specifies whether a term vector for this field should be stored. Default is no. | . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/text/#parameters",
    "relUrl": "/field-types/supported-field-types/text/#parameters"
  },"1505": {
    "doc": "Text",
    "title": "Term vector parameter",
    "content": "A term vector is produced during analysis. It contains: . | A list of terms. | The ordinal position of each term. | The start and end character offsets of the search string within the field. | Payloads (if available). Each term can have custom binary data associated with the term’s position. | . The term_vector field contains a JSON object that accepts the following parameters: . | Parameter | Stored values | . | no | None. This is the default. | . | yes | Terms in the field. | . | with_offsets | Terms and character offsets. | . | with_positions_offsets | Terms, positions, and character offsets. | . | with_positions_offsets_payloads | Terms, positions, character offsets, and payloads. | . | with_positions | Terms and positions. | . | with_positions_payloads | Terms, positions, and payloads. | . Storing positions is useful for proximity queries. Storing character offsets is useful for highlighting. Term vector parameter example . Create a mapping with a text field that stores character offsets in a term vector: . PUT testindex { \"mappings\" : { \"properties\" : { \"dob\" : { \"type\" : \"text\", \"term_vector\": \"with_positions_offsets\" } } } } . copy . Index a document with a text field: . PUT testindex/_doc/1 { \"dob\" : \"The patient's date of birth.\" } . copy . Query for “date of birth” and highlight it in the original field: . GET testindex/_search { \"query\": { \"match\": { \"text\": \"date of birth\" } }, \"highlight\": { \"fields\": { \"text\": {} } } } . copy . The words “date of birth” are highlighted in the response: . { \"took\" : 854, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 1, \"relation\" : \"eq\" }, \"max_score\" : 0.8630463, \"hits\" : [ { \"_index\" : \"testindex\", \"_type\" : \"_doc\", \"_id\" : \"1\", \"_score\" : 0.8630463, \"_source\" : { \"text\" : \"The patient's date of birth.\" }, \"highlight\" : { \"text\" : [ \"The patient's &lt;em&gt;date&lt;/em&gt; &lt;em&gt;of&lt;/em&gt; &lt;em&gt;birth&lt;/em&gt;.\" ] } } ] } } . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/text/#term-vector-parameter",
    "relUrl": "/field-types/supported-field-types/text/#term-vector-parameter"
  },"1506": {
    "doc": "Text",
    "title": "Text",
    "content": " ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/text/",
    "relUrl": "/field-types/supported-field-types/text/"
  },"1507": {
    "doc": "Token count",
    "title": "Token count field type",
    "content": "A token count field type stores the number of analyzed tokens in a string. ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/token-count/#token-count-field-type",
    "relUrl": "/field-types/supported-field-types/token-count/#token-count-field-type"
  },"1508": {
    "doc": "Token count",
    "title": "Example",
    "content": "Create a mapping with a token count field: . PUT testindex { \"mappings\": { \"properties\": { \"sentence\": { \"type\": \"text\", \"fields\": { \"num_words\": { \"type\": \"token_count\", \"analyzer\": \"english\" } } } } } } . copy . Index three documents with text fields: . PUT testindex/_doc/1 { \"sentence\": \"To be, or not to be: that is the question.\" } . copy . PUT testindex/_doc/2 { \"sentence\": \"All the world’s a stage, and all the men and women are merely players.\" } . copy . PUT testindex/_doc/3 { \"sentence\": \"Now is the winter of our discontent.\" } . copy . Search for sentences with fewer than 10 words: . GET testindex/_search { \"query\": { \"range\": { \"sentence.num_words\": { \"lt\": 10 } } } } . copy . The response contains one matching sentence: . { \"took\" : 8, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 1, \"relation\" : \"eq\" }, \"max_score\" : 1.0, \"hits\" : [ { \"_index\" : \"testindex\", \"_type\" : \"_doc\", \"_id\" : \"3\", \"_score\" : 1.0, \"_source\" : { \"sentence\" : \"Now is the winter of our discontent.\" } } ] } } . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/token-count/#example",
    "relUrl": "/field-types/supported-field-types/token-count/#example"
  },"1509": {
    "doc": "Token count",
    "title": "Parameters",
    "content": "The following table lists the parameters accepted by token count field types. The analyzer parameter is required; all other parameters are optional. | Parameter | Description | . | analyzer | The analyzer to be used for this field. Specify an analyzer without token filters for optimal performance. Required. | . | boost | A floating-point value that specifies the weight of this field toward the relevance score. Values above 1.0 increase the field’s relevance. Values between 0.0 and 1.0 decrease the field’s relevance. Default is 1.0. | . | doc_values | A Boolean value that specifies whether the field should be stored on disk so that it can be used for aggregations, sorting, or scripting. Default is false. | . | enable_position_increments | A Boolean value that specifies whether position increments should be counted. To avoid removing stopwords, set this field to false. Default is true. | . | index | A Boolean value that specifies whether the field should be searchable. Default is true. | . | null_value | A value to be used in place of null. Must be of the same type as the field. If this parameter is not specified, the field is treated as missing when its value is null. Default is null. | . | store | A Boolean value that specifies whether the field value should be stored and can be retrieved separately from the _source field. Default is false. | . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/token-count/#parameters",
    "relUrl": "/field-types/supported-field-types/token-count/#parameters"
  },"1510": {
    "doc": "Token count",
    "title": "Token count",
    "content": " ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/token-count/",
    "relUrl": "/field-types/supported-field-types/token-count/"
  },"1511": {
    "doc": "xy point",
    "title": "xy point field type",
    "content": "An xy point field type contains a point in a two-dimensional Cartesian coordinate system, specified by x and y coordinates. It is based on the Lucene XYPoint field type. The xy point field type is similar to the geopoint field type, but does not have the range limitations of geopoint. The coordinates of an xy point are single-precision floating-point values. For information about the range and precision of floating-point values, see Numeric field types. ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/xy-point/#xy-point-field-type",
    "relUrl": "/field-types/supported-field-types/xy-point/#xy-point-field-type"
  },"1512": {
    "doc": "xy point",
    "title": "Example",
    "content": "Create a mapping with an xy point field type: . PUT testindex1 { \"mappings\": { \"properties\": { \"point\": { \"type\": \"xy_point\" } } } } . copy . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/xy-point/#example",
    "relUrl": "/field-types/supported-field-types/xy-point/#example"
  },"1513": {
    "doc": "xy point",
    "title": "Formats",
    "content": "xy points can be indexed in the following formats: . | An object with x and y coordinates | . PUT testindex1/_doc/1 { \"point\": { \"x\": 0.5, \"y\": 4.5 } } . copy . | A string in the “x, y” format | . PUT testindex1/_doc/2 { \"point\": \"0.5, 4.5\" } . copy . | An array in the [x, y] format | . PUT testindex1/_doc/3 { \"point\": [0.5, 4.5] } . copy . | A well-known text (WKT) POINT in the “POINT(x y)” format | . PUT testindex1/_doc/4 { \"point\": \"POINT (0.5 4.5)\" } . copy . | GeoJSON format | . PUT testindex1/_doc/5 { \"point\" : { \"type\" : \"Point\", \"coordinates\" : [0.5, 4.5] } } . copy . In all xy point formats, the coordinates must be specified in the x, y order. ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/xy-point/#formats",
    "relUrl": "/field-types/supported-field-types/xy-point/#formats"
  },"1514": {
    "doc": "xy point",
    "title": "Parameters",
    "content": "The following table lists the parameters accepted by xy point field types. All parameters are optional. | Parameter | Description | . | ignore_malformed | A Boolean value that specifies to ignore malformed values and not to throw an exception. Default is false. | . | ignore_z_value | Specific to points with three coordinates. If ignore_z_value is true, the third coordinate is not indexed but is still stored in the _source field. If ignore_z_value is false, an exception is thrown. | . | null_value | A value to be used in place of null. The value must be of the same type as the field. If this parameter is not specified, the field is treated as missing when its value is null. Default is null. | . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/xy-point/#parameters",
    "relUrl": "/field-types/supported-field-types/xy-point/#parameters"
  },"1515": {
    "doc": "xy point",
    "title": "xy point",
    "content": " ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/xy-point/",
    "relUrl": "/field-types/supported-field-types/xy-point/"
  },"1516": {
    "doc": "xy shape",
    "title": "xy shape field type",
    "content": "An xy shape field type contains a shape, such as a polygon or a collection of xy points. It is based on the Lucene XYShape field type. To index an xy shape, OpenSearch tessellates the shape into a triangular mesh and stores each triangle in a BKD tree (a set of balanced k-dimensional trees). This provides a 10-7decimal degree of precision, which represents near-perfect spatial resolution. The xy shape field type is similar to the geoshape field type, but it represents shapes on the Cartesian plane, which is not based on the Earth-fixed terrestrial reference system. The coordinates of an xy shape are single-precision floating-point values. For information about the range and precision of floating-point values, see Numeric field types. ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/xy-shape/#xy-shape-field-type",
    "relUrl": "/field-types/supported-field-types/xy-shape/#xy-shape-field-type"
  },"1517": {
    "doc": "xy shape",
    "title": "Example",
    "content": "Create a mapping with an xy shape field type: . PUT testindex { \"mappings\": { \"properties\": { \"location\": { \"type\": \"xy_shape\" } } } } . copy . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/xy-shape/#example",
    "relUrl": "/field-types/supported-field-types/xy-shape/#example"
  },"1518": {
    "doc": "xy shape",
    "title": "Formats",
    "content": "xy shapes can be indexed in the following formats: . | GeoJSON | Well-known text (WKT) | . In both GeoJSON and WKT, the coordinates must be specified in the x, y order within coordinate arrays. ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/xy-shape/#formats",
    "relUrl": "/field-types/supported-field-types/xy-shape/#formats"
  },"1519": {
    "doc": "xy shape",
    "title": "xy shape types",
    "content": "The following table describes the possible xy shape types and their relationship to the GeoJSON and WKT types. | OpenSearch type | GeoJSON type | WKT type | Description | . | point | Point | POINT | A geographic point specified by the x and y coordinates. | . | linestring | LineString | LINESTRING | A line specified by two or more points. May be a straight line or a path of connected line segments. | . | polygon | Polygon | POLYGON | A polygon specified by a list of vertices in coordinate form. The polygon must be closed, meaning the last point must be the same as the first point. Therefore, to create an n-gon, n+1 vertices are required. The minimum number of vertices is four, which creates a triangle. | . | multipoint | MultiPoint | MULTIPOINT | An array of discrete related points that are not connected. | . | multilinestring | MultiLineString | MULTILINESTRING | An array of linestrings. | . | multipolygon | MultiPolygon | MULTIPOLYGON | An array of polygons. | . | geometrycollection | GeometryCollection | GEOMETRYCOLLECTION | A collection of xy shapes that may be of different types. | . | envelope | N/A | BBOX | A bounding rectangle specified by upper-left and lower-right vertices. | . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/xy-shape/#xy-shape-types",
    "relUrl": "/field-types/supported-field-types/xy-shape/#xy-shape-types"
  },"1520": {
    "doc": "xy shape",
    "title": "Point",
    "content": "A point is specified by a single pair of coordinates. Index a point in GeoJSON format: . PUT testindex/_doc/1 { \"location\" : { \"type\" : \"point\", \"coordinates\" : [0.5, 4.5] } } . copy . Index a point in WKT format: . PUT testindex/_doc/1 { \"location\" : \"POINT (0.5 4.5)\" } . copy . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/xy-shape/#point",
    "relUrl": "/field-types/supported-field-types/xy-shape/#point"
  },"1521": {
    "doc": "xy shape",
    "title": "Linestring",
    "content": "A linestring is a line specified by two or more points. If the points are collinear, the linestring is a straight line. Otherwise, the linestring represents a path made of line segments. Index a linestring in GeoJSON format: . PUT testindex/_doc/2 { \"location\" : { \"type\" : \"linestring\", \"coordinates\" : [[0.5, 4.5], [-1.5, 2.3]] } } . copy . Index a linestring in WKT format: . PUT testindex/_doc/2 { \"location\" : \"LINESTRING (0.5 4.5, -1.5 2.3)\" } . copy . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/xy-shape/#linestring",
    "relUrl": "/field-types/supported-field-types/xy-shape/#linestring"
  },"1522": {
    "doc": "xy shape",
    "title": "Polygon",
    "content": "A polygon is specified by a list of vertices in coordinate form. The polygon must be closed, meaning the last point must be the same as the first point. In the following example, a triangle is created using four points. GeoJSON requires that you list the vertices of the polygon counterclockwise. WKT does not impose a specific order on vertices. Index a polygon (triangle) in GeoJSON format: . PUT testindex/_doc/3 { \"location\" : { \"type\" : \"polygon\", \"coordinates\" : [ [[0.5, 4.5], [2.5, 6.0], [1.5, 2.0], [0.5, 4.5]] ] } } . copy . Index a polygon (triangle) in WKT format: . PUT testindex/_doc/3 { \"location\" : \"POLYGON ((0.5 4.5, 2.5 6.0, 1.5 2.0, 0.5 4.5))\" } . copy . The polygon may have holes inside. In this case, the coordinates field will contain multiple arrays. The first array represents the outer polygon, and each subsequent array represents a hole. Holes are represented as polygons and specified as arrays of coordinates. GeoJSON requires that you list the vertices of the polygon counterclockwise and the vertices of the hole clockwise. WKT does not impose a specific order on vertices. Index a polygon (triangle) with a triangular hole in GeoJSON format: . PUT testindex/_doc/4 { \"location\" : { \"type\" : \"polygon\", \"coordinates\" : [ [[0.5, 4.5], [2.5, 6.0], [1.5, 2.0], [0.5, 4.5]], [[1.0, 4.5], [1.5, 4.5], [1.5, 4.0], [1.0, 4.5]] ] } } . copy . Index a polygon (triangle) with a triangular hole in WKT format: . PUT testindex/_doc/4 { \"location\" : \"POLYGON ((0.5 4.5, 2.5 6.0, 1.5 2.0, 0.5 4.5), (1.0 4.5, 1.5 4.5, 1.5 4.0, 1.0 4.5))\" } . copy . By default, the vertices of the polygon are traversed in a counterclockwise order. You can define an orientation parameter to specify the vertex traversal order at mapping time: . PUT testindex { \"mappings\": { \"properties\": { \"location\": { \"type\": \"xy_shape\", \"orientation\" : \"left\" } } } } . copy . Subsequently indexed documents can override the orientation setting: . PUT testindex/_doc/3 { \"location\" : { \"type\" : \"polygon\", \"orientation\" : \"cw\", \"coordinates\" : [ [[0.5, 4.5], [2.5, 6.0], [1.5, 2.0], [0.5, 4.5]] ] } } . copy . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/xy-shape/#polygon",
    "relUrl": "/field-types/supported-field-types/xy-shape/#polygon"
  },"1523": {
    "doc": "xy shape",
    "title": "Multipoint",
    "content": "A multipoint is an array of discrete related points that are not connected. Index a multipoint in GeoJSON format: . PUT testindex/_doc/6 { \"location\" : { \"type\" : \"multipoint\", \"coordinates\" : [ [0.5, 4.5], [2.5, 6.0] ] } } . copy . Index a multipoint in WKT format: . PUT testindex/_doc/6 { \"location\" : \"MULTIPOINT (0.5 4.5, 2.5 6.0)\" } . copy . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/xy-shape/#multipoint",
    "relUrl": "/field-types/supported-field-types/xy-shape/#multipoint"
  },"1524": {
    "doc": "xy shape",
    "title": "Multilinestring",
    "content": "A multilinestring is an array of linestrings. Index a multilinestring in GeoJSON format: . PUT testindex/_doc/2 { \"location\" : { \"type\" : \"multilinestring\", \"coordinates\" : [ [[0.5, 4.5], [2.5, 6.0]], [[1.5, 2.0], [3.5, 3.5]] ] } } . copy . Index a linestring in WKT format: . PUT testindex/_doc/2 { \"location\" : \"MULTILINESTRING ((0.5 4.5, 2.5 6.0), (1.5 2.0, 3.5 3.5))\" } . copy . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/xy-shape/#multilinestring",
    "relUrl": "/field-types/supported-field-types/xy-shape/#multilinestring"
  },"1525": {
    "doc": "xy shape",
    "title": "Multipolygon",
    "content": "A multipolygon is an array of polygons. In this example, the first polygon contains a hole, and the second does not. Index a multipolygon in GeoJSON format: . PUT testindex/_doc/4 { \"location\" : { \"type\" : \"multipolygon\", \"coordinates\" : [ [ [[0.5, 4.5], [2.5, 6.0], [1.5, 2.0], [0.5, 4.5]], [[1.0, 4.5], [1.5, 4.5], [1.5, 4.0], [1.0, 4.5]] ], [ [[2.0, 0.0], [1.0, 2.0], [3.0, 1.0], [2.0, 0.0]] ] ] } } . copy . Index a multipolygon in WKT format: . PUT testindex/_doc/4 { \"location\" : \"MULTIPOLYGON (((0.5 4.5, 2.5 6.0, 1.5 2.0, 0.5 4.5), (1.0 4.5, 1.5 4.5, 1.5 4.0, 1.0 4.5)), ((2.0 0.0, 1.0 2.0, 3.0 1.0, 2.0 0.0)))\" } . copy . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/xy-shape/#multipolygon",
    "relUrl": "/field-types/supported-field-types/xy-shape/#multipolygon"
  },"1526": {
    "doc": "xy shape",
    "title": "Geometry collection",
    "content": "A geometry collection is a collection of xy shapes that may be of different types. Index a geometry collection in GeoJSON format: . PUT testindex/_doc/7 { \"location\" : { \"type\": \"geometrycollection\", \"geometries\": [ { \"type\": \"point\", \"coordinates\": [0.5, 4.5] }, { \"type\": \"linestring\", \"coordinates\": [[2.5, 6.0], [1.5, 2.0]] } ] } } . copy . Index a geometry collection in WKT format: . PUT testindex/_doc/7 { \"location\" : \"GEOMETRYCOLLECTION (POINT (0.5 4.5), LINESTRING(2.5 6.0, 1.5 2.0))\" } . copy . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/xy-shape/#geometry-collection",
    "relUrl": "/field-types/supported-field-types/xy-shape/#geometry-collection"
  },"1527": {
    "doc": "xy shape",
    "title": "Envelope",
    "content": "An envelope is a bounding rectangle specified by upper-left and lower-right vertices. The GeoJSON format is [[minX, maxY], [maxX, minY]]. Index an envelope in GeoJSON format: . PUT testindex/_doc/2 { \"location\" : { \"type\" : \"envelope\", \"coordinates\" : [[3.0, 2.0], [6.0, 0.0]] } } . copy . In WKT format, use BBOX (minX, maxY, maxX, minY). Index an envelope in WKT BBOX format: . PUT testindex/_doc/8 { \"location\" : \"BBOX (3.0, 2.0, 6.0, 0.0)\" } . copy . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/xy-shape/#envelope",
    "relUrl": "/field-types/supported-field-types/xy-shape/#envelope"
  },"1528": {
    "doc": "xy shape",
    "title": "Parameters",
    "content": "The following table lists the parameters accepted by xy shape field types. All parameters are optional. | Parameter | Description | . | coerce | A Boolean value that specifies whether to automatically close unclosed linear rings. Default is false. | . | ignore_malformed | A Boolean value that specifies to ignore malformed GeoJSON or WKT xy shapes and not to throw an exception. Default is false (throw an exception when xy shapes are malformed). | . | ignore_z_value | Specific to points with three coordinates. If ignore_z_value is true, the third coordinate is not indexed but is still stored in the _source field. If ignore_z_value is false, an exception is thrown. Default is true. | . | orientation | Specifies the traversal order of the vertices in the xy shape’s list of coordinates. orientation takes the following values: 1. RIGHT: counterclockwise. Specify RIGHT orientation by using one of the following strings (uppercase or lowercase): right, counterclockwise, ccw. 2. LEFT: clockwise. Specify LEFT orientation by using one of the following strings (uppercase or lowercase): left, clockwise, cw. This value can be overridden by individual documents. Default is RIGHT. | . ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/xy-shape/#parameters",
    "relUrl": "/field-types/supported-field-types/xy-shape/#parameters"
  },"1529": {
    "doc": "xy shape",
    "title": "xy shape",
    "content": " ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/xy-shape/",
    "relUrl": "/field-types/supported-field-types/xy-shape/"
  },"1530": {
    "doc": "Cartesian field types",
    "title": "Cartesian field types",
    "content": "Cartesian field types facilitate indexing and searching of points and shapes in a two-dimensional Cartesian coordinate system. Cartesian field types are similar to geographic field types, except they represent points and shapes on the Cartesian plane, which is not based on the Earth-fixed terrestrial reference system. Calculating distances on a plane is more efficient than calculating distances on a sphere, so distance sorting is faster for Cartesian field types. Cartesian field types work well for spatial applications like virtual reality, computer-aided design (CAD), and amusement park and sporting venue mapping. The coordinates for the Cartesian field types are single-precision floating-point values. For information about the range and precision of floating-point values, see Numeric field types. The following table lists all Cartesian field types that OpenSearch supports. | Field Data type | Description | . | xy_point | A point in a two-dimensional Cartesian coordinate system, specified by x and y coordinates. | . | xy_shape | A shape, such as a polygon or a collection of xy points, in a two-dimensional Cartesian coordinate system. | . Currently, OpenSearch supports indexing and searching of Cartesian field types but not aggregations on Cartesian field types. If you’d like to see aggregations implemented, open a GitHub issue. ",
    "url": "https://vagimeli.github.io/field-types/supported-field-types/xy/",
    "relUrl": "/field-types/supported-field-types/xy/"
  },"1531": {
    "doc": "Getting started with the high-level .NET client",
    "title": "Getting started with the high-level .NET client (OpenSearch.Client)",
    "content": "OpenSearch.Client is a high-level .NET client. It provides strongly typed requests and responses as well as Query DSL. It frees you from constructing raw JSON requests and parsing raw JSON responses by providing models that parse and serialize/deserialize requests and responses automatically. OpenSearch.Client also exposes the OpenSearch.Net low-level client if you need it. For the client’s complete API documentation, see the OpenSearch.Client API documentation. This getting started guide illustrates how to connect to OpenSearch, index documents, and run queries. For the client source code, see the opensearch-net repo. ",
    "url": "https://vagimeli.github.io/clients/OSC-dot-net/#getting-started-with-the-high-level-net-client-opensearchclient",
    "relUrl": "/clients/OSC-dot-net/#getting-started-with-the-high-level-net-client-opensearchclient"
  },"1532": {
    "doc": "Getting started with the high-level .NET client",
    "title": "Installing OpenSearch.Client",
    "content": "To install OpenSearch.Client, download the OpenSearch.Client NuGet package and add it to your project in an IDE of your choice. In Microsoft Visual Studio, follow the steps below: . | In the Solution Explorer panel, right-click on your solution or project and select Manage NuGet Packages for Solution. | Search for the OpenSearch.Client NuGet package, and select Install. | . Alternatively, you can add OpenSearch.Client to your .csproj file: . &lt;Project&gt; ... &lt;ItemGroup&gt; &lt;PackageReference Include=\"OpenSearch.Client\" Version=\"1.0.0\" /&gt; &lt;/ItemGroup&gt; &lt;/Project&gt; . copy . ",
    "url": "https://vagimeli.github.io/clients/OSC-dot-net/#installing-opensearchclient",
    "relUrl": "/clients/OSC-dot-net/#installing-opensearchclient"
  },"1533": {
    "doc": "Getting started with the high-level .NET client",
    "title": "Example",
    "content": "The following example illustrates connecting to OpenSearch, indexing documents, and sending queries on the data. It uses the Student class to represent one student, which is equivalent to one document in the index. public class Student { public int Id { get; init; } public string FirstName { get; init; } public string LastName { get; init; } public int GradYear { get; init; } public double Gpa { get; init; } } . copy . By default, OpenSearch.Client uses camel case to convert property names to field names. ",
    "url": "https://vagimeli.github.io/clients/OSC-dot-net/#example",
    "relUrl": "/clients/OSC-dot-net/#example"
  },"1534": {
    "doc": "Getting started with the high-level .NET client",
    "title": "Connecting to OpenSearch",
    "content": "Use the default constructor when creating an OpenSearchClient object to connect to the default OpenSearch host (http://localhost:9200). var client = new OpenSearchClient(); . copy . To connect to your OpenSearch cluster through a single node with a known address, specify this address when creating an instance of OpenSearch.Client: . var nodeAddress = new Uri(\"http://myserver:9200\"); var client = new OpenSearchClient(nodeAddress); . copy . You can also connect to OpenSearch through multiple nodes. Connecting to your OpenSearch cluster with a node pool provides advantages like load balancing and cluster failover support. To connect to your OpenSearch cluster using multiple nodes, specify their addresses and create a ConnectionSettings object for the OpenSearch.Client instance: . var nodes = new Uri[] { new Uri(\"http://myserver1:9200\"), new Uri(\"http://myserver2:9200\"), new Uri(\"http://myserver3:9200\") }; var pool = new StaticConnectionPool(nodes); var settings = new ConnectionSettings(pool); var client = new OpenSearchClient(settings); . copy . ",
    "url": "https://vagimeli.github.io/clients/OSC-dot-net/#connecting-to-opensearch",
    "relUrl": "/clients/OSC-dot-net/#connecting-to-opensearch"
  },"1535": {
    "doc": "Getting started with the high-level .NET client",
    "title": "Using ConnectionSettings",
    "content": "ConnectionConfiguration is used to pass configuration options to the low-level OpenSearch.Net client. ConnectionSettings inherits from ConnectionConfiguration and provides additional configuration options. To set the address of the node and the default index name for requests that don’t specify the index name, create a ConnectionSettings object: . var node = new Uri(\"http://myserver:9200\"); var config = new ConnectionSettings(node).DefaultIndex(\"students\"); var client = new OpenSearchClient(config); . copy . ",
    "url": "https://vagimeli.github.io/clients/OSC-dot-net/#using-connectionsettings",
    "relUrl": "/clients/OSC-dot-net/#using-connectionsettings"
  },"1536": {
    "doc": "Getting started with the high-level .NET client",
    "title": "Indexing one document",
    "content": "Create one instance of Student: . var student = new Student { Id = 100, FirstName = \"Paulo\", LastName = \"Santos\", Gpa = 3.93, GradYear = 2021 }; . copy . To index one document, you can use either fluent lambda syntax or object initializer syntax. Index this Student into the students index using fluent lambda syntax: . var response = client.Index(student, i =&gt; i.Index(\"students\")); . copy . Index this Student into the students index using object initializer syntax: . var response = client.Index(new IndexRequest&lt;Student&gt;(student, \"students\")); . copy . ",
    "url": "https://vagimeli.github.io/clients/OSC-dot-net/#indexing-one-document",
    "relUrl": "/clients/OSC-dot-net/#indexing-one-document"
  },"1537": {
    "doc": "Getting started with the high-level .NET client",
    "title": "Indexing many documents",
    "content": "You can index many documents from a collection at the same time by using the OpenSearch.Client’s IndexMany method: . var studentArray = new Student[] { new() {Id = 200, FirstName = \"Shirley\", LastName = \"Rodriguez\", Gpa = 3.91, GradYear = 2019}, new() {Id = 300, FirstName = \"Nikki\", LastName = \"Wolf\", Gpa = 3.87, GradYear = 2020} }; var manyResponse = client.IndexMany(studentArray, \"students\"); . copy . ",
    "url": "https://vagimeli.github.io/clients/OSC-dot-net/#indexing-many-documents",
    "relUrl": "/clients/OSC-dot-net/#indexing-many-documents"
  },"1538": {
    "doc": "Getting started with the high-level .NET client",
    "title": "Searching for a document",
    "content": "To search for a student indexed above, you want to construct a query that is analogous to the following Query DSL query: . GET students/_search { \"query\" : { \"match\": { \"lastName\": \"Santos\" } } } . The query above is a shorthand version of the following explicit query: . GET students/_search { \"query\" : { \"match\": { \"lastName\": { \"query\": \"Santos\" } } } } . In OpenSearch.Client, this query looks like this: . var searchResponse = client.Search&lt;Student&gt;(s =&gt; s .Index(\"students\") .Query(q =&gt; q .Match(m =&gt; m .Field(fld =&gt; fld.LastName) .Query(\"Santos\")))); . copy . You can print out the results by accessing the documents in the response: . if (searchResponse.IsValid) { foreach (var s in searchResponse.Documents) { Console.WriteLine($\"{s.Id} {s.LastName} {s.FirstName} {s.Gpa} {s.GradYear}\"); } } . copy . The response contains one document, which corresponds to the correct student: . 100 Santos Paulo 3.93 2021 . ",
    "url": "https://vagimeli.github.io/clients/OSC-dot-net/#searching-for-a-document",
    "relUrl": "/clients/OSC-dot-net/#searching-for-a-document"
  },"1539": {
    "doc": "Getting started with the high-level .NET client",
    "title": "Using OpenSearch.Client methods asynchronously",
    "content": "For applications that require asynchronous code, all method calls in OpenSearch.Client have asynchronous counterparts: . // synchronous method var response = client.Index(student, i =&gt; i.Index(\"students\")); // asynchronous method var response = await client.IndexAsync(student, i =&gt; i.Index(\"students\")); . ",
    "url": "https://vagimeli.github.io/clients/OSC-dot-net/#using-opensearchclient-methods-asynchronously",
    "relUrl": "/clients/OSC-dot-net/#using-opensearchclient-methods-asynchronously"
  },"1540": {
    "doc": "Getting started with the high-level .NET client",
    "title": "Falling back on the low-level OpenSearch.Net client",
    "content": "OpenSearch.Client exposes the low-level the OpenSearch.Net client you can use if anything is missing: . var lowLevelClient = client.LowLevel; var searchResponseLow = lowLevelClient.Search&lt;SearchResponse&lt;Student&gt;&gt;(\"students\", PostData.Serializable( new { query = new { match = new { lastName = new { query = \"Santos\" } } } })); if (searchResponseLow.IsValid) { foreach (var s in searchResponseLow.Documents) { Console.WriteLine($\"{s.Id} {s.LastName} {s.FirstName} {s.Gpa} {s.GradYear}\"); } } . copy . ",
    "url": "https://vagimeli.github.io/clients/OSC-dot-net/#falling-back-on-the-low-level-opensearchnet-client",
    "relUrl": "/clients/OSC-dot-net/#falling-back-on-the-low-level-opensearchnet-client"
  },"1541": {
    "doc": "Getting started with the high-level .NET client",
    "title": "Sample program",
    "content": "The following is a complete sample program that illustrates all of the concepts described above. It uses the Student class defined above. using OpenSearch.Client; using OpenSearch.Net; namespace NetClientProgram; internal class Program { private static IOpenSearchClient osClient = new OpenSearchClient(); public static void Main(string[] args) { Console.WriteLine(\"Indexing one student......\"); var student = new Student { Id = 100, FirstName = \"Paulo\", LastName = \"Santos\", Gpa = 3.93, GradYear = 2021 }; var response = osClient.Index(student, i =&gt; i.Index(\"students\")); Console.WriteLine(response.IsValid ? \"Response received\" : \"Error\"); Console.WriteLine(\"Searching for one student......\"); SearchForOneStudent(); Console.WriteLine(\"Searching using low-level client......\"); SearchLowLevel(); Console.WriteLine(\"Indexing an array of Student objects......\"); var studentArray = new Student[] { new() { Id = 200, FirstName = \"Shirley\", LastName = \"Rodriguez\", Gpa = 3.91, GradYear = 2019}, new() { Id = 300, FirstName = \"Nikki\", LastName = \"Wolf\", Gpa = 3.87, GradYear = 2020} }; var manyResponse = osClient.IndexMany(studentArray, \"students\"); Console.WriteLine(manyResponse.IsValid ? \"Response received\" : \"Error\"); } private static void SearchForOneStudent() { var searchResponse = osClient.Search&lt;Student&gt;(s =&gt; s .Index(\"students\") .Query(q =&gt; q .Match(m =&gt; m .Field(fld =&gt; fld.LastName) .Query(\"Santos\")))); PrintResponse(searchResponse); } private static void SearchLowLevel() { // Search for the student using the low-level client var lowLevelClient = osClient.LowLevel; var searchResponseLow = lowLevelClient.Search&lt;SearchResponse&lt;Student&gt;&gt; (\"students\", PostData.Serializable( new { query = new { match = new { lastName = new { query = \"Santos\" } } } })); PrintResponse(searchResponseLow); } private static void PrintResponse(ISearchResponse&lt;Student&gt; response) { if (response.IsValid) { foreach (var s in response.Documents) { Console.WriteLine($\"{s.Id} {s.LastName} \" + $\"{s.FirstName} {s.Gpa} {s.GradYear}\"); } } else { Console.WriteLine(\"Student not found.\"); } } } . copy . ",
    "url": "https://vagimeli.github.io/clients/OSC-dot-net/#sample-program",
    "relUrl": "/clients/OSC-dot-net/#sample-program"
  },"1542": {
    "doc": "Getting started with the high-level .NET client",
    "title": "Getting started with the high-level .NET client",
    "content": " ",
    "url": "https://vagimeli.github.io/clients/OSC-dot-net/",
    "relUrl": "/clients/OSC-dot-net/"
  },"1543": {
    "doc": "More advanced features of the high-level .NET client",
    "title": "More advanced features of the high-level .NET client (OpenSearch.Client)",
    "content": "The following example illustrates more advanced features of OpenSearch.Client. For a simple example, see the Getting started guide. This example uses the following Student class. public class Student { public int Id { get; init; } public string FirstName { get; init; } public string LastName { get; init; } public int GradYear { get; init; } public double Gpa { get; init; } } . copy . ",
    "url": "https://vagimeli.github.io/clients/OSC-example/#more-advanced-features-of-the-high-level-net-client-opensearchclient",
    "relUrl": "/clients/OSC-example/#more-advanced-features-of-the-high-level-net-client-opensearchclient"
  },"1544": {
    "doc": "More advanced features of the high-level .NET client",
    "title": "Mappings",
    "content": "OpenSearch uses dynamic mapping to infer field types of the documents that are indexed. However, to have more control over the schema of your document, you can pass an explicit mapping to OpenSearch. You can define data types for some or all fields of your document in this mapping. Similarly, OpenSearch.Client uses auto mapping to infer field data types based on the types of the class’s properties. To use auto mapping, create a students index using the AutoMap’s default constructor: . var createResponse = await osClient.Indices.CreateAsync(\"students\", c =&gt; c.Map(m =&gt; m.AutoMap&lt;Student&gt;())); . copy . If you use auto mapping, Id and GradYear are mapped as integers, Gpa is mapped as a double, and FirstName and LastName are mapped as text with a keyword subfield. If you want to search for FirstName and LastName and allow only case-sensitive full matches, you can suppress analyzing by mapping these fields as keyword only. In Query DSL, you can accomplish this using the following query: . PUT students { \"mappings\" : { \"properties\" : { \"firstName\" : { \"type\" : \"keyword\" }, \"lastName\" : { \"type\" : \"keyword\" } } } } . In OpenSearch.Client, you can use fluid lambda syntax to mark these fields as keywords: . var createResponse = await osClient.Indices.CreateAsync(index, c =&gt; c.Map(m =&gt; m.AutoMap&lt;Student&gt;() .Properties&lt;Student&gt;(p =&gt; p .Keyword(k =&gt; k.Name(f =&gt; f.FirstName)) .Keyword(k =&gt; k.Name(f =&gt; f.LastName))))); . copy . ",
    "url": "https://vagimeli.github.io/clients/OSC-example/#mappings",
    "relUrl": "/clients/OSC-example/#mappings"
  },"1545": {
    "doc": "More advanced features of the high-level .NET client",
    "title": "Settings",
    "content": "In addition to mappings, you can specify settings like the number of primary and replica shards when creating an index. The following query sets the number of primary shards to 1 and the number of replica shards to 2: . PUT students { \"mappings\" : { \"properties\" : { \"firstName\" : { \"type\" : \"keyword\" }, \"lastName\" : { \"type\" : \"keyword\" } } }, \"settings\": { \"number_of_shards\": 1, \"number_of_replicas\": 2 } } . In OpenSearch.Client, the equivalent of the above query is the following: . var createResponse = await osClient.Indices.CreateAsync(index, c =&gt; c.Map(m =&gt; m.AutoMap&lt;Student&gt;() .Properties&lt;Student&gt;(p =&gt; p .Keyword(k =&gt; k.Name(f =&gt; f.FirstName)) .Keyword(k =&gt; k.Name(f =&gt; f.LastName)))) .Settings(s =&gt; s.NumberOfShards(1).NumberOfReplicas(2))); . copy . ",
    "url": "https://vagimeli.github.io/clients/OSC-example/#settings",
    "relUrl": "/clients/OSC-example/#settings"
  },"1546": {
    "doc": "More advanced features of the high-level .NET client",
    "title": "Indexing multiple documents using the Bulk API",
    "content": "In addition to indexing one document using Index and IndexDocument and indexing multiple documents using IndexMany, you can gain more control over document indexing by using Bulk or BulkAll. Indexing documents individually is inefficient because it creates an HTTP request for every document sent. The BulkAll helper frees you from handling retry, chunking or back off request functionality. It automatically retries if the request fails, backs off if the server is down, and controls how many documents are sent in one HTTP request. In the following example, BulkAll is configured with the index name, number of back off retries, and back off time. Additionally, the maximum degrees of parallelism setting controls the number of parallel HTTP requests containing the data. Finally, the size parameter signals how many documents are sent in one HTTP request. We recommend setting the size to 100–1000 documents in production. BulkAll takes a stream of data and returns an Observable that you can use to observe the background operation. var bulkAll = osClient.BulkAll(ReadData(), r =&gt; r .Index(index) .BackOffRetries(2) .BackOffTime(\"30s\") .MaxDegreeOfParallelism(4) .Size(100)); . copy . ",
    "url": "https://vagimeli.github.io/clients/OSC-example/#indexing-multiple-documents-using-the-bulk-api",
    "relUrl": "/clients/OSC-example/#indexing-multiple-documents-using-the-bulk-api"
  },"1547": {
    "doc": "More advanced features of the high-level .NET client",
    "title": "Searching with Boolean query",
    "content": "OpenSearch.Client exposes full OpenSearch query capability. In addition to simple searches that use the match query, you can create a more complex Boolean query to search for students who graduated in 2022 and sort them by last name. In the example below, search is limited to 10 documents, and the scroll API is used to control the pagination of results. var gradResponse = await osClient.SearchAsync&lt;Student&gt;(s =&gt; s .Index(index) .From(0) .Size(10) .Scroll(\"1m\") .Query(q =&gt; q .Bool(b =&gt; b .Filter(f =&gt; f .Term(t =&gt; t.Field(fld =&gt; fld.GradYear).Value(2022))))) .Sort(srt =&gt; srt.Ascending(f =&gt; f.LastName))); . copy . The response contains the Documents property with matching documents from OpenSearch. The data is in the form of deserialized JSON objects of Student type, so you can access their properties in a strongly typed fashion. All serialization and deserialization is handled by OpenSearch.Client. ",
    "url": "https://vagimeli.github.io/clients/OSC-example/#searching-with-boolean-query",
    "relUrl": "/clients/OSC-example/#searching-with-boolean-query"
  },"1548": {
    "doc": "More advanced features of the high-level .NET client",
    "title": "Aggregations",
    "content": "OpenSearch.Client includes the full OpenSearch query functionality, including aggregations. In addition to grouping search results into buckets (for example, grouping students by GPA ranges), you can calculate metrics like sum or average. The following query calculates the average GPA of all students in the index. Setting Size to 0 means OpenSearch will only return the aggregation, not the actual documents. var aggResponse = await osClient.SearchAsync&lt;Student&gt;(s =&gt; s .Index(index) .Size(0) .Aggregations(a =&gt; a .Average(\"average gpa\", avg =&gt; avg.Field(fld =&gt; fld.Gpa)))); . copy . ",
    "url": "https://vagimeli.github.io/clients/OSC-example/#aggregations",
    "relUrl": "/clients/OSC-example/#aggregations"
  },"1549": {
    "doc": "More advanced features of the high-level .NET client",
    "title": "Sample program for creating an index and indexing data",
    "content": "The following program creates an index, reads a stream of student records from a comma-separated file and indexes this data into OpenSearch. using OpenSearch.Client; namespace NetClientProgram; internal class Program { private const string index = \"students\"; public static IOpenSearchClient osClient = new OpenSearchClient(); public static async Task Main(string[] args) { // Check if the index with the name \"students\" exists var existResponse = await osClient.Indices.ExistsAsync(index); if (!existResponse.Exists) // There is no index with this name { // Create an index \"students\" // Map FirstName and LastName as keyword var createResponse = await osClient.Indices.CreateAsync(index, c =&gt; c.Map(m =&gt; m.AutoMap&lt;Student&gt;() .Properties&lt;Student&gt;(p =&gt; p .Keyword(k =&gt; k.Name(f =&gt; f.FirstName)) .Keyword(k =&gt; k.Name(f =&gt; f.LastName)))) .Settings(s =&gt; s.NumberOfShards(1).NumberOfReplicas(1))); if (!createResponse.IsValid &amp;&amp; !createResponse.Acknowledged) { throw new Exception(\"Create response is invalid.\"); } // Take a stream of data and send it to OpenSearch var bulkAll = osClient.BulkAll(ReadData(), r =&gt; r .Index(index) .BackOffRetries(2) .BackOffTime(\"20s\") .MaxDegreeOfParallelism(4) .Size(10)); // Wait until the data upload is complete. // FromMinutes specifies a timeout. // r is a response object that is returned as the data is indexed. bulkAll.Wait(TimeSpan.FromMinutes(10), r =&gt; Console.WriteLine(\"Data chunk indexed\")); } } // Reads student data in the form \"Id,FirsName,LastName,GradYear,Gpa\" public static IEnumerable&lt;Student&gt; ReadData() { var file = new StreamReader(\"C:\\\\search\\\\students.csv\"); string s; while ((s = file.ReadLine()) is not null) { yield return new Student(s); } } } . copy . ",
    "url": "https://vagimeli.github.io/clients/OSC-example/#sample-program-for-creating-an-index-and-indexing-data",
    "relUrl": "/clients/OSC-example/#sample-program-for-creating-an-index-and-indexing-data"
  },"1550": {
    "doc": "More advanced features of the high-level .NET client",
    "title": "Sample program for search",
    "content": "The following program searches students by name and graduation date and calculates the average GPA. using OpenSearch.Client; namespace NetClientProgram; internal class Program { private const string index = \"students\"; public static IOpenSearchClient osClient = new OpenSearchClient(); public static async Task Main(string[] args) { await SearchByName(); await SearchByGradDate(); await CalculateAverageGpa(); } private static async Task SearchByName() { Console.WriteLine(\"Searching for name......\"); var nameResponse = await osClient.SearchAsync&lt;Student&gt;(s =&gt; s .Index(index) .Query(q =&gt; q .Match(m =&gt; m .Field(fld =&gt; fld.FirstName) .Query(\"Zhang\")))); if (!nameResponse.IsValid) { throw new Exception(\"Aggregation query response is not valid.\"); } foreach (var s in nameResponse.Documents) { Console.WriteLine($\"{s.Id} {s.LastName} \" + $\"{s.FirstName} {s.Gpa} {s.GradYear}\"); } } private static async Task SearchByGradDate() { Console.WriteLine(\"Searching for grad date......\"); // Search for all students who graduated in 2022 var gradResponse = await osClient.SearchAsync&lt;Student&gt;(s =&gt; s .Index(index) .From(0) .Size(2) .Scroll(\"1m\") .Query(q =&gt; q .Bool(b =&gt; b .Filter(f =&gt; f .Term(t =&gt; t.Field(fld =&gt; fld.GradYear).Value(2022))))) .Sort(srt =&gt; srt.Ascending(f =&gt; f.LastName)) .Size(10)); if (!gradResponse.IsValid) { throw new Exception(\"Grad date query response is not valid.\"); } while (gradResponse.Documents.Any()) { foreach (var data in gradResponse.Documents) { Console.WriteLine($\"{data.Id} {data.LastName} {data.FirstName} \" + $\"{data.Gpa} {data.GradYear}\"); } gradResponse = osClient.Scroll&lt;Student&gt;(\"1m\", gradResponse.ScrollId); } } public static async Task CalculateAverageGpa() { Console.WriteLine(\"Calculating average GPA......\"); // Search and aggregate // Size 0 means documents are not returned, only aggregation is returned var aggResponse = await osClient.SearchAsync&lt;Student&gt;(s =&gt; s .Index(index) .Size(0) .Aggregations(a =&gt; a .Average(\"average gpa\", avg =&gt; avg.Field(fld =&gt; fld.Gpa)))); if (!aggResponse.IsValid) throw new Exception(\"Aggregation response not valid\"); var avg = aggResponse.Aggregations.Average(\"average gpa\").Value; Console.WriteLine($\"Average GPA is {avg}\"); } } . copy . ",
    "url": "https://vagimeli.github.io/clients/OSC-example/#sample-program-for-search",
    "relUrl": "/clients/OSC-example/#sample-program-for-search"
  },"1551": {
    "doc": "More advanced features of the high-level .NET client",
    "title": "More advanced features of the high-level .NET client",
    "content": " ",
    "url": "https://vagimeli.github.io/clients/OSC-example/",
    "relUrl": "/clients/OSC-example/"
  },"1552": {
    "doc": "Low-level .NET client",
    "title": "Low-level .NET client (OpenSearch.Net)",
    "content": "OpenSearch.Net is a low-level .NET client that provides the foundational layer of communication with OpenSearch. It is dependency free, and it can handle round-robin load balancing, transport, and the basic request/response cycle. OpenSearch.Net contains all OpenSearch API endpoints as methods. When using OpenSearch.Net, you need to construct the queries yourself. This getting started guide illustrates how to connect to OpenSearch, index documents, and run queries. For the client source code, see the opensearch-net repo. ",
    "url": "https://vagimeli.github.io/clients/OpenSearch-dot-net/#low-level-net-client-opensearchnet",
    "relUrl": "/clients/OpenSearch-dot-net/#low-level-net-client-opensearchnet"
  },"1553": {
    "doc": "Low-level .NET client",
    "title": "Example",
    "content": "The following example illustrates connecting to OpenSearch, indexing documents, and sending queries on the data. It uses the Student class to represent one student, which is equivalent to one document in the index. public class Student { public int Id { get; init; } public string FirstName { get; init; } public string LastName { get; init; } public int GradYear { get; init; } public double Gpa { get; init; } } . copy . ",
    "url": "https://vagimeli.github.io/clients/OpenSearch-dot-net/#example",
    "relUrl": "/clients/OpenSearch-dot-net/#example"
  },"1554": {
    "doc": "Low-level .NET client",
    "title": "Installing the Opensearch.Net client",
    "content": "To install Opensearch.Net, download the Opensearch.Net NuGet package and add it to your project in an IDE of your choice. In Microsoft Visual Studio, follow the steps below: . | In the Solution Explorer panel, right-click on your solution or project and select Manage NuGet Packages for Solution. | Search for the OpenSearch.Net NuGet package, and select Install. | . Alternatively, you can add OpenSearch.Net to your .csproj file: . &lt;Project&gt; ... &lt;ItemGroup&gt; &lt;PackageReference Include=\"Opensearch.Net\" Version=\"1.0.0\" /&gt; &lt;/ItemGroup&gt; &lt;/Project&gt; . copy . ",
    "url": "https://vagimeli.github.io/clients/OpenSearch-dot-net/#installing-the-opensearchnet-client",
    "relUrl": "/clients/OpenSearch-dot-net/#installing-the-opensearchnet-client"
  },"1555": {
    "doc": "Low-level .NET client",
    "title": "Connecting to OpenSearch",
    "content": "Use the default constructor when creating an OpenSearchLowLevelClient object to connect to the default OpenSearch host (http://localhost:9200). var client = new OpenSearchLowLevelClient(); . copy . To connect to your OpenSearch cluster through a single node with a known address, create a ConnectionConfiguration object with that address and pass it to the OpenSearch.Net constructor: . var nodeAddress = new Uri(\"http://myserver:9200\"); var config = new ConnectionConfiguration(nodeAddress); var client = new OpenSearchLowLevelClient(config); . copy . You can also use a connection pool to manage the nodes in the cluster. Additionally, you can set up a connection configuration to have OpenSearch return the response as formatted JSON. var uri = new Uri(\"http://localhost:9200\"); var connectionPool = new SingleNodeConnectionPool(uri); var settings = new ConnectionConfiguration(connectionPool).PrettyJson(); var client = new OpenSearchLowLevelClient(settings); . copy . To connect to your OpenSearch cluster using multiple nodes, create a connection pool with their addresses. In this example, a SniffingConnectionPool is used because it keeps track of nodes being removed or added to the cluster, so it works best for clusters that scale automatically. var uris = new[] { new Uri(\"http://localhost:9200\"), new Uri(\"http://localhost:9201\"), new Uri(\"http://localhost:9202\") }; var connectionPool = new SniffingConnectionPool(uris); var settings = new ConnectionConfiguration(connectionPool).PrettyJson(); var client = new OpenSearchLowLevelClient(settings); . copy . ",
    "url": "https://vagimeli.github.io/clients/OpenSearch-dot-net/#connecting-to-opensearch",
    "relUrl": "/clients/OpenSearch-dot-net/#connecting-to-opensearch"
  },"1556": {
    "doc": "Low-level .NET client",
    "title": "Connecting to Amazon OpenSearch Service",
    "content": "The following example illustrates connecting to Amazon OpenSearch Service: . using OpenSearch.Client; using OpenSearch.Net.Auth.AwsSigV4; namespace Application { class Program { static void Main(string[] args) { var endpoint = new Uri(\"https://search-xxx.region.es.amazonaws.com\"); var connection = new AwsSigV4HttpConnection(RegionEndpoint.APSoutheast2, service: AwsSigV4HttpConnection.OpenSearchService); var config = new ConnectionSettings(endpoint, connection); var client = new OpenSearchClient(config); Console.WriteLine($\"{client.RootNodeInfo().Version.Distribution}: {client.RootNodeInfo().Version.Number}\"); } } } . copy . ",
    "url": "https://vagimeli.github.io/clients/OpenSearch-dot-net/#connecting-to-amazon-opensearch-service",
    "relUrl": "/clients/OpenSearch-dot-net/#connecting-to-amazon-opensearch-service"
  },"1557": {
    "doc": "Low-level .NET client",
    "title": "Connecting to Amazon OpenSearch Serverless",
    "content": "The following example illustrates connecting to Amazon OpenSearch Serverless Service: . using OpenSearch.Client; using OpenSearch.Net.Auth.AwsSigV4; namespace Application { class Program { static void Main(string[] args) { var endpoint = new Uri(\"https://search-xxx.region.aoss.amazonaws.com\"); var connection = new AwsSigV4HttpConnection(RegionEndpoint.APSoutheast2, service: AwsSigV4HttpConnection.OpenSearchServerlessService); var config = new ConnectionSettings(endpoint, connection); var client = new OpenSearchClient(config); Console.WriteLine($\"{client.RootNodeInfo().Version.Distribution}: {client.RootNodeInfo().Version.Number}\"); } } } . copy . ",
    "url": "https://vagimeli.github.io/clients/OpenSearch-dot-net/#connecting-to-amazon-opensearch-serverless",
    "relUrl": "/clients/OpenSearch-dot-net/#connecting-to-amazon-opensearch-serverless"
  },"1558": {
    "doc": "Low-level .NET client",
    "title": "Using ConnectionSettings",
    "content": "ConnectionConfiguration is used to pass configuration options to the OpenSearch.Net client. ConnectionSettings inherits from ConnectionConfiguration and provides additional configuration options. The following example uses ConnectionSettings to: . | Set the default index name for requests that don’t specify the index name. | Enable gzip-compressed requests and responses. | Signal to OpenSearch to return formatted JSON. | Make field names lowercase. | . var uri = new Uri(\"http://localhost:9200\"); var connectionPool = new SingleNodeConnectionPool(uri); var settings = new ConnectionSettings(connectionPool) .DefaultIndex(\"students\") .EnableHttpCompression() .PrettyJson() .DefaultFieldNameInferrer(f =&gt; f.ToLower()); var client = new OpenSearchLowLevelClient(settings); . copy . ",
    "url": "https://vagimeli.github.io/clients/OpenSearch-dot-net/#using-connectionsettings",
    "relUrl": "/clients/OpenSearch-dot-net/#using-connectionsettings"
  },"1559": {
    "doc": "Low-level .NET client",
    "title": "Indexing one document",
    "content": "To index a document, you first need to create an instance of the Student class: . var student = new Student { Id = 100, FirstName = \"Paulo\", LastName = \"Santos\", Gpa = 3.93, GradYear = 2021 }; . copy . Alternatively, you can create an instance of Student using an anonymous type: . var student = new { Id = 100, FirstName = \"Paulo\", LastName = \"Santos\", Gpa = 3.93, GradYear = 2021 }; . copy . Next, upload this Student into the students index using the Index method: . var response = client.Index&lt;StringResponse&gt;(\"students\", \"100\", PostData.Serializable(student)); Console.WriteLine(response.Body); . copy . The generic type parameter of the Index method specifies the response body type. In the example above, the response is a string. ",
    "url": "https://vagimeli.github.io/clients/OpenSearch-dot-net/#indexing-one-document",
    "relUrl": "/clients/OpenSearch-dot-net/#indexing-one-document"
  },"1560": {
    "doc": "Low-level .NET client",
    "title": "Indexing many documents using the Bulk API",
    "content": "To index many documents, use the Bulk API to bundle many operations into one request: . var studentArray = new object[] { new {index = new { _index = \"students\", _type = \"_doc\", _id = \"200\"}}, new { Id = 200, FirstName = \"Shirley\", LastName = \"Rodriguez\", Gpa = 3.91, GradYear = 2019 }, new {index = new { _index = \"students\", _type = \"_doc\", _id = \"300\"}}, new { Id = 300, FirstName = \"Nikki\", LastName = \"Wolf\", Gpa = 3.87, GradYear = 2020 } }; var manyResponse = client.Bulk&lt;StringResponse&gt;(PostData.MultiJson(studentArray)); . copy . You can send the request body as an anonymous object, string, byte array, or stream in APIs that take a body. For APIs that take multiline JSON, you can send the body as a list of bytes or a list of objects, like in the example above. The PostData class has static methods to send the body in all of these forms. ",
    "url": "https://vagimeli.github.io/clients/OpenSearch-dot-net/#indexing-many-documents-using-the-bulk-api",
    "relUrl": "/clients/OpenSearch-dot-net/#indexing-many-documents-using-the-bulk-api"
  },"1561": {
    "doc": "Low-level .NET client",
    "title": "Searching for a document",
    "content": "To construct a Query DSL query, use anonymous types within the request body. The following query searches for all students who graduated in 2021: . var searchResponseLow = client.Search&lt;StringResponse&gt;(\"students\", PostData.Serializable( new { from = 0, size = 20, query = new { term = new { gradYear = new { value = 2019 } } } })); Console.WriteLine(searchResponseLow.Body); . copy . Alternatively, you can use strings to construct the request. When using strings, you have to escape the \" character: . var searchResponse = client.Search&lt;StringResponse&gt;(\"students\", @\" { \"\"query\"\": { \"\"match\"\": { \"\"lastName\"\": { \"\"query\"\": \"\"Santos\"\" } } } }\"); Console.WriteLine(searchResponse.Body); . copy . ",
    "url": "https://vagimeli.github.io/clients/OpenSearch-dot-net/#searching-for-a-document",
    "relUrl": "/clients/OpenSearch-dot-net/#searching-for-a-document"
  },"1562": {
    "doc": "Low-level .NET client",
    "title": "Using OpenSearch.Net methods asynchronously",
    "content": "For applications that require asynchronous code, all method calls in OpenSearch.Client have asynchronous counterparts: . // synchronous method var response = client.Index&lt;StringResponse&gt;(\"students\", \"100\", PostData.Serializable(student)); // asynchronous method var response = client.IndexAsync&lt;StringResponse&gt;(\"students\", \"100\", PostData.Serializable(student)); . copy . ",
    "url": "https://vagimeli.github.io/clients/OpenSearch-dot-net/#using-opensearchnet-methods-asynchronously",
    "relUrl": "/clients/OpenSearch-dot-net/#using-opensearchnet-methods-asynchronously"
  },"1563": {
    "doc": "Low-level .NET client",
    "title": "Handling exceptions",
    "content": "By default, OpenSearch.Net does not throw exceptions when an operation is unsuccessful. In particular, OpenSearch.Net does not throw exceptions if the response status code has one of the expected values for this request. For example, the following query searches for a document in an index that does not exist: . var searchResponse = client.Search&lt;StringResponse&gt;(\"students1\", @\" { \"\"query\"\": { \"\"match\"\": { \"\"lastName\"\": { \"\"query\"\": \"\"Santos\"\" } } } }\"); Console.WriteLine(searchResponse.Body); . copy . The response contains an error status code 404, which is one of the expected error codes for search requests, so no exception is thrown. You can see the status code in the status field: . { \"error\" : { \"root_cause\" : [ { \"type\" : \"index_not_found_exception\", \"reason\" : \"no such index [students1]\", \"index\" : \"students1\", \"resource.id\" : \"students1\", \"resource.type\" : \"index_or_alias\", \"index_uuid\" : \"_na_\" } ], \"type\" : \"index_not_found_exception\", \"reason\" : \"no such index [students1]\", \"index\" : \"students1\", \"resource.id\" : \"students1\", \"resource.type\" : \"index_or_alias\", \"index_uuid\" : \"_na_\" }, \"status\" : 404 } . To configure OpenSearch.Net to throw exceptions, turn on the ThrowExceptions() setting on ConnectionConfiguration: . var uri = new Uri(\"http://localhost:9200\"); var connectionPool = new SingleNodeConnectionPool(uri); var settings = new ConnectionConfiguration(connectionPool) .PrettyJson().ThrowExceptions(); var client = new OpenSearchLowLevelClient(settings); . copy . You can use the following properties of the response object to determine response success: . Console.WriteLine(\"Success: \" + searchResponse.Success); Console.WriteLine(\"SuccessOrKnownError: \" + searchResponse.SuccessOrKnownError); Console.WriteLine(\"Original Exception: \" + searchResponse.OriginalException); . | Success returns true if the response code is in the 2xx range or the response code has one of the expected values for this request. | SuccessOrKnownError returns true if the response is successful or the response code is in the 400–501 or 505–599 ranges. If SuccessOrKnownError is true, the request is not retried. | OriginalException holds the original exception for the unsuccessful responses. | . ",
    "url": "https://vagimeli.github.io/clients/OpenSearch-dot-net/#handling-exceptions",
    "relUrl": "/clients/OpenSearch-dot-net/#handling-exceptions"
  },"1564": {
    "doc": "Low-level .NET client",
    "title": "Sample program",
    "content": "The following program creates an index, indexes data, and searches for documents. using OpenSearch.Net; using OpenSearch.Client; namespace NetClientProgram; internal class Program { public static void Main(string[] args) { // Create a client with custom settings var uri = new Uri(\"http://localhost:9200\"); var connectionPool = new SingleNodeConnectionPool(uri); var settings = new ConnectionSettings(connectionPool) .PrettyJson(); var client = new OpenSearchLowLevelClient(settings); Console.WriteLine(\"Indexing one student......\"); var student = new Student { Id = 100, FirstName = \"Paulo\", LastName = \"Santos\", Gpa = 3.93, GradYear = 2021 };v var response = client.Index&lt;StringResponse&gt;(\"students\", \"100\", PostData.Serializable(student)); Console.WriteLine(response.Body); Console.WriteLine(\"Indexing many students......\"); var studentArray = new object[] { new { index = new { _index = \"students\", _type = \"_doc\", _id = \"200\"}}, new { Id = 200, FirstName = \"Shirley\", LastName = \"Rodriguez\", Gpa = 3.91, GradYear = 2019}, new { index = new { _index = \"students\", _type = \"_doc\", _id = \"300\"}}, new { Id = 300, FirstName = \"Nikki\", LastName = \"Wolf\", Gpa = 3.87, GradYear = 2020} }; var manyResponse = client.Bulk&lt;StringResponse&gt;(PostData.MultiJson(studentArray)); Console.WriteLine(manyResponse.Body); Console.WriteLine(\"Searching for students who graduated in 2019......\"); var searchResponseLow = client.Search&lt;StringResponse&gt;(\"students\", PostData.Serializable( new { from = 0, size = 20, query = new { term = new { gradYear = new { value = 2019 } } } })); Console.WriteLine(searchResponseLow.Body); Console.WriteLine(\"Searching for a student with the last name Santos......\"); var searchResponse = client.Search&lt;StringResponse&gt;(\"students\", @\" { \"\"query\"\": { \"\"match\"\": { \"\"lastName\"\": { \"\"query\"\": \"\"Santos\"\" } } } }\"); Console.WriteLine(searchResponse.Body); } } . copy . ",
    "url": "https://vagimeli.github.io/clients/OpenSearch-dot-net/#sample-program",
    "relUrl": "/clients/OpenSearch-dot-net/#sample-program"
  },"1565": {
    "doc": "Low-level .NET client",
    "title": "Low-level .NET client",
    "content": " ",
    "url": "https://vagimeli.github.io/clients/OpenSearch-dot-net/",
    "relUrl": "/clients/OpenSearch-dot-net/"
  },"1566": {
    "doc": ".NET client considerations",
    "title": ".NET client considerations and best practices",
    "content": "The following sections provide information regarding the considerations and best practices for using .NET clients. ",
    "url": "https://vagimeli.github.io/clients/dot-net-conventions/#net-client-considerations-and-best-practices",
    "relUrl": "/clients/dot-net-conventions/#net-client-considerations-and-best-practices"
  },"1567": {
    "doc": ".NET client considerations",
    "title": "Registering OpenSearch.Client as a singleton",
    "content": "As a rule, you should set up your OpenSearch.Client as a singleton. OpenSearch.Client manages connections to the server and the states of the nodes in a cluster. Additionally, each client uses a lot of configuration for its setup. Therefore, it is beneficial to create an OpenSearch.Client instance once and reuse it for all OpenSearch operations. The client is thread safe, so the same instance can be shared by multiple threads. ",
    "url": "https://vagimeli.github.io/clients/dot-net-conventions/#registering-opensearchclient-as-a-singleton",
    "relUrl": "/clients/dot-net-conventions/#registering-opensearchclient-as-a-singleton"
  },"1568": {
    "doc": ".NET client considerations",
    "title": "Exceptions",
    "content": "The following are the types of exceptions that may be thrown by .NET clients: . | OpenSearchClientException is a known exception that occurs either in the request pipeline (for example, timeout reached) or in OpenSearch (for example, malformed query). If it is an OpenSearch exception, the ServerError response property contains the error that OpenSearch returns. | UnexpectedOpenSearchClientException is an unknown exception (for example, an error during deserialization) and is a subclass of OpenSearchClientException. | System exceptions are thrown when the API is not used properly. | . ",
    "url": "https://vagimeli.github.io/clients/dot-net-conventions/#exceptions",
    "relUrl": "/clients/dot-net-conventions/#exceptions"
  },"1569": {
    "doc": ".NET client considerations",
    "title": "Nodes",
    "content": "To create a node, pass a Uri object into its constructor: . var uri = new Uri(\"http://example.org/opensearch\"); var node = new Node(uri); . copy . When first created, a node is master eligible, and its HoldsData property is set to true. The AbsolutePath property of the node created above is \"/opensearch/\": A trailing forward slash is appended so that the paths can be easily combined. If not specified, the default Port is 80. Nodes are considered equal if they have the same endpoint. Metadata is not taken into account when checking nodes for equality. ",
    "url": "https://vagimeli.github.io/clients/dot-net-conventions/#nodes",
    "relUrl": "/clients/dot-net-conventions/#nodes"
  },"1570": {
    "doc": ".NET client considerations",
    "title": "Connection pools",
    "content": "Connection pools are instances of IConnectionPool and are responsible for managing the nodes in the OpenSearch cluster. We recommend creating a singleton client with a single ConnectionSettings object. The lifetime of both the client and its ConnectionSettings is the lifetime of the application. The following are connection pool types. | SingleNodeConnectionPool | . SingleNodeConnectionPool is the default connection pool that is used if no connection pool is passed to the ConnectionSettings constructor. Use SingleNodeConnectionPool if you have only one node in the cluster or if your cluster has a load balancer as an entry point. SingleNodeConnectionPool does not support sniffing or pinging and does not mark nodes as dead or alive. | CloudConnectionPool | . CloudConnectionPool is a subclass of SingleNodeConnectionPool that takes a Cloud ID and credentials. Like SingleNodeConnectionPool, CloudConnectionPool does not support sniffing or pinging. | StaticConnectionPool | . StaticConnectionPool is used for a small cluster when you do not want to turn on sniffing to learn about cluster topology. StaticConnectionPool does not support sniffing, but can support pinging. | SniffingConnectionPool | . SniffingConnectionPool is a subclass of StaticConnectionPool. It is thread safe and supports sniffing and pinging. SniffingConnectionPool can be reseeded at run time, and you can specify node roles when seeding. | StickyConnectionPool | . StickyConnectionPool is set up to return the first live node, which then persists between requests. It can be seeded using an enumerable of Uri or Node objects. StickyConnectionPool does not support sniffing but supports pinging. | StickySniffingConnectionPool | . StickySniffingConnectionPool is a subclass of SniffingConnectionPool. Like StickyConnectionPool, it returns the first live node2, which then persists between requests. StickySniffingConnectionPool supports sniffing and sorting so that each instance of your application can favor a different node. Nodes have weights associated with them and can be sorted by weight. ",
    "url": "https://vagimeli.github.io/clients/dot-net-conventions/#connection-pools",
    "relUrl": "/clients/dot-net-conventions/#connection-pools"
  },"1571": {
    "doc": ".NET client considerations",
    "title": "Retries",
    "content": "If a request does not succeed, it is automatically retried. By default, the number of retries is the number of nodes known to OpenSearch.Client in your cluster. The number of retries is also limited by the timeout parameter, so OpenSearch.Client retries requests as many times as possible within the timeout period. To set the maximum number of retries, specify the number in the MaximumRetries property on the ConnectionSettings object. var settings = new ConnectionSettings(connectionPool).MaximumRetries(5); . copy . You can also set a RequestTimeout that specifies a timeout for a single request and a MaxRetryTimeout that specifies the time limit for all retry attempts. In the example below, RequestTimeout is set to 4 seconds, and MaxRetryTimeout is set to 12 seconds, so the maximum number of attempts for a query is 3. var settings = new ConnectionSettings(connectionPool) .RequestTimeout(TimeSpan.FromSeconds(4)) .MaxRetryTimeout(TimeSpan.FromSeconds(12)); . copy . ",
    "url": "https://vagimeli.github.io/clients/dot-net-conventions/#retries",
    "relUrl": "/clients/dot-net-conventions/#retries"
  },"1572": {
    "doc": ".NET client considerations",
    "title": "Failover",
    "content": "If you are using a connection pool with multiple nodes, a request is retried if it returns a 502 (Bad Gateway), 503 (Service Unavailable), or 504 (Gateway Timeout) HTTP error response code. If the response code is an error code in the 400–501 or 505–599 ranges, the request is not retried. A response is considered valid if the response code is in the 2xx range or the response code has one of the expected values for this request. For example, 404 (Not Found) is a valid response for a request that checks whether an index exists. ",
    "url": "https://vagimeli.github.io/clients/dot-net-conventions/#failover",
    "relUrl": "/clients/dot-net-conventions/#failover"
  },"1573": {
    "doc": ".NET client considerations",
    "title": ".NET client considerations",
    "content": " ",
    "url": "https://vagimeli.github.io/clients/dot-net-conventions/",
    "relUrl": "/clients/dot-net-conventions/"
  },"1574": {
    "doc": ".NET clients",
    "title": ".NET clients",
    "content": "OpenSearch has two .NET clients: a low-level OpenSearch.Net client and a high-level OpenSearch.Client client. OpenSearch.Net is a low-level .NET client that provides the foundational layer of communication with OpenSearch. It is dependency free, and it can handle round-robin load balancing, transport, and the basic request/response cycle. OpenSearch.Net contains methods for all OpenSearch API endpoints. OpenSearch.Client is a high-level .NET client on top of OpenSearch.Net. It provides strongly typed requests and responses as well as Query DSL. It frees you from constructing raw JSON requests and parsing raw JSON responses by supplying models that parse and serialize/deserialize requests and responses automatically. OpenSearch.Client also exposes the OpenSearch.Net low-level client if you need it. OpenSearch.Client includes the following advanced functionality: . | Automapping: Given a C# type, OpenSearch.Client can infer the correct mapping to send to OpenSearch. | Operator overloading in queries. | Type and index inference. | . You can use both .NET clients in a console program, a .NET core, an ASP.NET core, or in worker services. To get started with OpenSearch.Client, follow the instructions in Getting started with the high-level .NET client or in More advanced features of the high-level .NET client, a slightly more advanced walkthrough. ",
    "url": "https://vagimeli.github.io/clients/dot-net/",
    "relUrl": "/clients/dot-net/"
  },"1575": {
    "doc": "Go client",
    "title": "Go client",
    "content": "The OpenSearch Go client lets you connect your Go application with the data in your OpenSearch cluster. This getting started guide illustrates how to connect to OpenSearch, index documents, and run queries. For the client’s complete API documentation and additional examples, see the Go client API documentation. For the client source code, see the opensearch-go repo. ",
    "url": "https://vagimeli.github.io/clients/go/",
    "relUrl": "/clients/go/"
  },"1576": {
    "doc": "Go client",
    "title": "Setup",
    "content": "If you’re starting a new project, create a new module by running the following command: . go mod init &lt;mymodulename&gt; . copy . To add the Go client to your project, import it like any other module: . go get github.com/opensearch-project/opensearch-go . copy . ",
    "url": "https://vagimeli.github.io/clients/go/#setup",
    "relUrl": "/clients/go/#setup"
  },"1577": {
    "doc": "Go client",
    "title": "Connecting to OpenSearch",
    "content": "To connect to the default OpenSearch host, create a client object with the address https://localhost:9200 if you are using the Security plugin: . client, err := opensearch.NewClient(opensearch.Config{ Transport: &amp;http.Transport{ TLSClientConfig: &amp;tls.Config{InsecureSkipVerify: true}, }, Addresses: []string{\"https://localhost:9200\"}, Username: \"admin\", // For testing only. Don't store credentials in code. Password: \"admin\", }) . copy . If you are not using the Security plugin, create a client object with the address http://localhost:9200: . client, err := opensearch.NewClient(opensearch.Config{ Transport: &amp;http.Transport{ TLSClientConfig: &amp;tls.Config{InsecureSkipVerify: true}, }, Addresses: []string{\"http://localhost:9200\"}, }) . copy . ",
    "url": "https://vagimeli.github.io/clients/go/#connecting-to-opensearch",
    "relUrl": "/clients/go/#connecting-to-opensearch"
  },"1578": {
    "doc": "Go client",
    "title": "Connecting to Amazon OpenSearch Service",
    "content": "The following example illustrates connecting to Amazon OpenSearch Service: . package main import ( \"context\" \"log\" \"github.com/aws/aws-sdk-go-v2/aws\" \"github.com/aws/aws-sdk-go-v2/config\" opensearch \"github.com/opensearch-project/opensearch-go/v2\" opensearchapi \"github.com/opensearch-project/opensearch-go/v2/opensearchapi\" requestsigner \"github.com/opensearch-project/opensearch-go/v2/signer/awsv2\" ) const endpoint = \"\" // e.g. https://opensearch-domain.region.com or Amazon OpenSearch Serverless endpoint func main() { ctx := context.Background() awsCfg, err := config.LoadDefaultConfig(ctx, config.WithRegion(\"&lt;AWS_REGION&gt;\"), config.WithCredentialsProvider( getCredentialProvider(\"&lt;AWS_ACCESS_KEY&gt;\", \"&lt;AWS_SECRET_ACCESS_KEY&gt;\", \"&lt;AWS_SESSION_TOKEN&gt;\"), ), ) if err != nil { log.Fatal(err) // Do not log.fatal in a production ready app. } // Create an AWS request Signer and load AWS configuration using default config folder or env vars. signer, err := requestsigner.NewSignerWithService(awsCfg, \"es\") if err != nil { log.Fatal(err) // Do not log.fatal in a production ready app. } // Create an opensearch client and use the request-signer client, err := opensearch.NewClient(opensearch.Config{ Addresses: []string{endpoint}, Signer: signer, }) if err != nil { log.Fatal(\"client creation err\", err) } } func getCredentialProvider(accessKey, secretAccessKey, token string) aws.CredentialsProviderFunc { return func(ctx context.Context) (aws.Credentials, error) { c := &amp;aws.Credentials{ AccessKeyID: accessKey, SecretAccessKey: secretAccessKey, SessionToken: token, } return *c, nil } } . copy . ",
    "url": "https://vagimeli.github.io/clients/go/#connecting-to-amazon-opensearch-service",
    "relUrl": "/clients/go/#connecting-to-amazon-opensearch-service"
  },"1579": {
    "doc": "Go client",
    "title": "Connecting to Amazon OpenSearch Serverless",
    "content": "The following example illustrates connecting to Amazon OpenSearch Serverless Service: . package main import ( \"context\" \"log\" \"github.com/aws/aws-sdk-go-v2/aws\" \"github.com/aws/aws-sdk-go-v2/config\" opensearch \"github.com/opensearch-project/opensearch-go/v2\" opensearchapi \"github.com/opensearch-project/opensearch-go/v2/opensearchapi\" requestsigner \"github.com/opensearch-project/opensearch-go/v2/signer/awsv2\" ) const endpoint = \"\" // e.g. https://opensearch-domain.region.com or Amazon OpenSearch Serverless endpoint func main() { ctx := context.Background() awsCfg, err := config.LoadDefaultConfig(ctx, config.WithRegion(\"&lt;AWS_REGION&gt;\"), config.WithCredentialsProvider( getCredentialProvider(\"&lt;AWS_ACCESS_KEY&gt;\", \"&lt;AWS_SECRET_ACCESS_KEY&gt;\", \"&lt;AWS_SESSION_TOKEN&gt;\"), ), ) if err != nil { log.Fatal(err) // Do not log.fatal in a production ready app. } // Create an AWS request Signer and load AWS configuration using default config folder or env vars. signer, err := requestsigner.NewSignerWithService(awsCfg, \"aoss\") if err != nil { log.Fatal(err) // Do not log.fatal in a production ready app. } // Create an opensearch client and use the request-signer client, err := opensearch.NewClient(opensearch.Config{ Addresses: []string{endpoint}, Signer: signer, }) if err != nil { log.Fatal(\"client creation err\", err) } } func getCredentialProvider(accessKey, secretAccessKey, token string) aws.CredentialsProviderFunc { return func(ctx context.Context) (aws.Credentials, error) { c := &amp;aws.Credentials{ AccessKeyID: accessKey, SecretAccessKey: secretAccessKey, SessionToken: token, } return *c, nil } } . copy . The Go client constructor takes an opensearch.Config{} type, which can be customized using options such as a list of OpenSearch node addresses or a username and password combination. To connect to multiple OpenSearch nodes, specify them in the Addresses parameter: . var ( urls = []string{\"http://localhost:9200\", \"http://localhost:9201\", \"http://localhost:9202\"} ) client, err := opensearch.NewClient(opensearch.Config{ Transport: &amp;http.Transport{ TLSClientConfig: &amp;tls.Config{InsecureSkipVerify: true}, }, Addresses: urls, }) . copy . The Go client retries requests for a maximum of three times by default. To customize the number of retries, set the MaxRetries parameter. Additionally, you can change the list of response codes for which a request is retried by setting the RetryOnStatus parameter. The following code snippet creates a new Go client with custom MaxRetries and RetryOnStatus values: . client, err := opensearch.NewClient(opensearch.Config{ Transport: &amp;http.Transport{ TLSClientConfig: &amp;tls.Config{InsecureSkipVerify: true}, }, Addresses: []string{\"http://localhost:9200\"}, MaxRetries: 5, RetryOnStatus: []int{502, 503, 504}, }) . copy . ",
    "url": "https://vagimeli.github.io/clients/go/#connecting-to-amazon-opensearch-serverless",
    "relUrl": "/clients/go/#connecting-to-amazon-opensearch-serverless"
  },"1580": {
    "doc": "Go client",
    "title": "Creating an index",
    "content": "To create an OpenSearch index, use the IndicesCreateRequest method. You can use the following code to construct a JSON object with custom settings : . settings := strings.NewReader(`{ 'settings': { 'index': { 'number_of_shards': 1, 'number_of_replicas': 0 } } }`) res := opensearchapi.IndicesCreateRequest{ Index: \"go-test-index1\", Body: settings, } . copy . ",
    "url": "https://vagimeli.github.io/clients/go/#creating-an-index",
    "relUrl": "/clients/go/#creating-an-index"
  },"1581": {
    "doc": "Go client",
    "title": "Indexing a document",
    "content": "You can index a document into OpenSearch using the IndexRequest method: . document := strings.NewReader(`{ \"title\": \"Moneyball\", \"director\": \"Bennett Miller\", \"year\": \"2011\" }`) docId := \"1\" req := opensearchapi.IndexRequest{ Index: \"go-test-index1\", DocumentID: docId, Body: document, } insertResponse, err := req.Do(context.Background(), client) . copy . ",
    "url": "https://vagimeli.github.io/clients/go/#indexing-a-document",
    "relUrl": "/clients/go/#indexing-a-document"
  },"1582": {
    "doc": "Go client",
    "title": "Performing bulk operations",
    "content": "You can perform several operations at the same time by using the Bulk method of the client. The operations may be of the same type or of different types. blk, err := client.Bulk( strings.NewReader(` { \"index\" : { \"_index\" : \"go-test-index1\", \"_id\" : \"2\" } } { \"title\" : \"Interstellar\", \"director\" : \"Christopher Nolan\", \"year\" : \"2014\"} { \"create\" : { \"_index\" : \"go-test-index1\", \"_id\" : \"3\" } } { \"title\" : \"Star Trek Beyond\", \"director\" : \"Justin Lin\", \"year\" : \"2015\"} { \"update\" : {\"_id\" : \"3\", \"_index\" : \"go-test-index1\" } } { \"doc\" : {\"year\" : \"2016\"} } `), ) . copy . ",
    "url": "https://vagimeli.github.io/clients/go/#performing-bulk-operations",
    "relUrl": "/clients/go/#performing-bulk-operations"
  },"1583": {
    "doc": "Go client",
    "title": "Searching for documents",
    "content": "The easiest way to search for documents is to construct a query string. The following code uses a multi_match query to search for “miller” in the title and director fields. It boosts the documents where “miller” appears in the title field: . content := strings.NewReader(`{ \"size\": 5, \"query\": { \"multi_match\": { \"query\": \"miller\", \"fields\": [\"title^2\", \"director\"] } } }`) search := opensearchapi.SearchRequest{ Index: []string{\"go-test-index1\"}, Body: content, } searchResponse, err := search.Do(context.Background(), client) . copy . ",
    "url": "https://vagimeli.github.io/clients/go/#searching-for-documents",
    "relUrl": "/clients/go/#searching-for-documents"
  },"1584": {
    "doc": "Go client",
    "title": "Deleting a document",
    "content": "You can delete a document using the DeleteRequest method: . delete := opensearchapi.DeleteRequest{ Index: \"go-test-index1\", DocumentID: \"1\", } deleteResponse, err := delete.Do(context.Background(), client) . copy . ",
    "url": "https://vagimeli.github.io/clients/go/#deleting-a-document",
    "relUrl": "/clients/go/#deleting-a-document"
  },"1585": {
    "doc": "Go client",
    "title": "Deleting an index",
    "content": "You can delete an index using the IndicesDeleteRequest method: . deleteIndex := opensearchapi.IndicesDeleteRequest{ Index: []string{\"go-test-index1\"}, } deleteIndexResponse, err := deleteIndex.Do(context.Background(), client) . copy . ",
    "url": "https://vagimeli.github.io/clients/go/#deleting-an-index",
    "relUrl": "/clients/go/#deleting-an-index"
  },"1586": {
    "doc": "Go client",
    "title": "Sample program",
    "content": "The following sample program creates a client, adds an index with non-default settings, inserts a document, performs bulk operations, searches for the document, deletes the document, and then deletes the index: . package main import ( \"os\" \"context\" \"crypto/tls\" \"fmt\" opensearch \"github.com/opensearch-project/opensearch-go\" opensearchapi \"github.com/opensearch-project/opensearch-go/opensearchapi\" \"net/http\" \"strings\" ) const IndexName = \"go-test-index1\" func main() { // Initialize the client with SSL/TLS enabled. client, err := opensearch.NewClient(opensearch.Config{ Transport: &amp;http.Transport{ TLSClientConfig: &amp;tls.Config{InsecureSkipVerify: true}, }, Addresses: []string{\"https://localhost:9200\"}, Username: \"admin\", // For testing only. Don't store credentials in code. Password: \"admin\", }) if err != nil { fmt.Println(\"cannot initialize\", err) os.Exit(1) } // Print OpenSearch version information on console. fmt.Println(client.Info()) // Define index settings. settings := strings.NewReader(`{ 'settings': { 'index': { 'number_of_shards': 1, 'number_of_replicas': 2 } } }`) // Create an index with non-default settings. res := opensearchapi.IndicesCreateRequest{ Index: IndexName, Body: settings, } fmt.Println(\"Creating index\") fmt.Println(res) // Add a document to the index. document := strings.NewReader(`{ \"title\": \"Moneyball\", \"director\": \"Bennett Miller\", \"year\": \"2011\" }`) docId := \"1\" req := opensearchapi.IndexRequest{ Index: IndexName, DocumentID: docId, Body: document, } insertResponse, err := req.Do(context.Background(), client) if err != nil { fmt.Println(\"failed to insert document \", err) os.Exit(1) } fmt.Println(\"Inserting a document\") fmt.Println(insertResponse) defer insertResponse.Body.Close() // Perform bulk operations. blk, err := client.Bulk( strings.NewReader(` { \"index\" : { \"_index\" : \"go-test-index1\", \"_id\" : \"2\" } } { \"title\" : \"Interstellar\", \"director\" : \"Christopher Nolan\", \"year\" : \"2014\"} { \"create\" : { \"_index\" : \"go-test-index1\", \"_id\" : \"3\" } } { \"title\" : \"Star Trek Beyond\", \"director\" : \"Justin Lin\", \"year\" : \"2015\"} { \"update\" : {\"_id\" : \"3\", \"_index\" : \"go-test-index1\" } } { \"doc\" : {\"year\" : \"2016\"} } `), ) if err != nil { fmt.Println(\"failed to perform bulk operations\", err) os.Exit(1) } fmt.Println(\"Performing bulk operations\") fmt.Println(blk) // Search for the document. content := strings.NewReader(`{ \"size\": 5, \"query\": { \"multi_match\": { \"query\": \"miller\", \"fields\": [\"title^2\", \"director\"] } } }`) search := opensearchapi.SearchRequest{ Index: []string{IndexName}, Body: content, } searchResponse, err := search.Do(context.Background(), client) if err != nil { fmt.Println(\"failed to search document \", err) os.Exit(1) } fmt.Println(\"Searching for a document\") fmt.Println(searchResponse) defer searchResponse.Body.Close() // Delete the document. delete := opensearchapi.DeleteRequest{ Index: IndexName, DocumentID: docId, } deleteResponse, err := delete.Do(context.Background(), client) if err != nil { fmt.Println(\"failed to delete document \", err) os.Exit(1) } fmt.Println(\"Deleting a document\") fmt.Println(deleteResponse) defer deleteResponse.Body.Close() // Delete the previously created index. deleteIndex := opensearchapi.IndicesDeleteRequest{ Index: []string{IndexName}, } deleteIndexResponse, err := deleteIndex.Do(context.Background(), client) if err != nil { fmt.Println(\"failed to delete index \", err) os.Exit(1) } fmt.Println(\"Deleting the index\") fmt.Println(deleteIndexResponse) defer deleteIndexResponse.Body.Close() } . copy . ",
    "url": "https://vagimeli.github.io/clients/go/#sample-program",
    "relUrl": "/clients/go/#sample-program"
  },"1587": {
    "doc": "Language clients",
    "title": "OpenSearch language clients",
    "content": "OpenSearch provides clients in JavaScript, Python, Ruby, Java, PHP, .NET, Go and Rust. ",
    "url": "https://vagimeli.github.io/clients/index/#opensearch-language-clients",
    "relUrl": "/clients/index/#opensearch-language-clients"
  },"1588": {
    "doc": "Language clients",
    "title": "OpenSearch clients",
    "content": "OpenSearch provides clients for the following programming languages and platforms: . | Python . | OpenSearch high-level Python client | OpenSearch low-level Python client | opensearch-py-ml client | . | Java . | OpenSearch Java client | . | JavaScript . | OpenSearch JavaScript (Node.js) client | . | Go . | OpenSearch Go client | . | Ruby . | OpenSearch Ruby client | . | PHP . | OpenSearch PHP client | . | .NET . | OpenSearch .NET clients | . | Rust . | OpenSearch Rust client | . | . All clients are compatible with any version of OpenSearch. ",
    "url": "https://vagimeli.github.io/clients/index/#opensearch-clients",
    "relUrl": "/clients/index/#opensearch-clients"
  },"1589": {
    "doc": "Language clients",
    "title": "Legacy clients",
    "content": "Most clients that work with Elasticsearch OSS 7.10.2 should work with OpenSearch, but the latest versions of those clients might include license or version checks that artificially break compatibility. This page includes recommendations around which versions of those clients to use for best compatibility with OpenSearch. | Client | Recommended version | . | Elasticsearch Java low-level REST client | 7.13.4 | . | Elasticsearch Java high-level REST client | 7.13.4 | . | Elasticsearch Python client | 7.13.4 | . | Elasticsearch Node.js client | 7.13.0 | . | Elasticsearch Ruby client | 7.13.0 | . If you test a legacy client and verify that it works, please submit a PR and add it to this table. ",
    "url": "https://vagimeli.github.io/clients/index/#legacy-clients",
    "relUrl": "/clients/index/#legacy-clients"
  },"1590": {
    "doc": "Language clients",
    "title": "Language clients",
    "content": " ",
    "url": "https://vagimeli.github.io/clients/index/",
    "relUrl": "/clients/index/"
  },"1591": {
    "doc": "Java high-level REST client",
    "title": "Java high-level REST client",
    "content": "The OpenSearch Java high-level REST client will be deprecated starting with OpenSearch version 3.0.0 and will be removed in a future release. We recommend switching to the Java client instead. The OpenSearch Java high-level REST client lets you interact with your OpenSearch clusters and indices through Java methods and data structures rather than HTTP methods and JSON. ",
    "url": "https://vagimeli.github.io/clients/java-rest-high-level/",
    "relUrl": "/clients/java-rest-high-level/"
  },"1592": {
    "doc": "Java high-level REST client",
    "title": "Setup",
    "content": "To start using the OpenSearch Java high-level REST client, ensure that you have the following dependency in your project’s pom.xml file: . &lt;dependency&gt; &lt;groupId&gt;org.opensearch.client&lt;/groupId&gt; &lt;artifactId&gt;opensearch-rest-high-level-client&lt;/artifactId&gt; &lt;version&gt;2.7.0&lt;/version&gt; &lt;/dependency&gt; . You can now start your OpenSearch cluster. The OpenSearch 1.x high-level REST client works with the 1.x versions of OpenSearch. ",
    "url": "https://vagimeli.github.io/clients/java-rest-high-level/#setup",
    "relUrl": "/clients/java-rest-high-level/#setup"
  },"1593": {
    "doc": "Java high-level REST client",
    "title": "Security",
    "content": "Before using the REST client in your Java application, you must configure the application’s truststore to connect to the Security plugin. If you are using self-signed certificates or demo configurations, you can use the following command to create a custom truststore and add in root authority certificates. If you’re using certificates from a trusted Certificate Authority (CA), you don’t need to configure the truststore. keytool -import &lt;path-to-cert&gt; -alias &lt;alias-to-call-cert&gt; -keystore &lt;truststore-name&gt; . You can now point your Java client to the truststore and set basic authentication credentials that can access a secure cluster (refer to the sample code below on how to do so). If you run into issues when configuring security, see common issues and troubleshoot TLS. ",
    "url": "https://vagimeli.github.io/clients/java-rest-high-level/#security",
    "relUrl": "/clients/java-rest-high-level/#security"
  },"1594": {
    "doc": "Java high-level REST client",
    "title": "Sample program",
    "content": "This code example uses basic credentials that come with the default OpenSearch configuration. If you’re using the OpenSearch Java high-level REST client with your own OpenSearch cluster, be sure to change the code to use your own credentials. import org.apache.http.HttpHost; import org.apache.http.auth.AuthScope; import org.apache.http.auth.UsernamePasswordCredentials; import org.apache.http.client.CredentialsProvider; import org.apache.http.impl.client.BasicCredentialsProvider; import org.apache.http.impl.nio.client.HttpAsyncClientBuilder; import org.opensearch.action.admin.indices.delete.DeleteIndexRequest; import org.opensearch.action.delete.DeleteRequest; import org.opensearch.action.delete.DeleteResponse; import org.opensearch.action.get.GetRequest; import org.opensearch.action.get.GetResponse; import org.opensearch.action.index.IndexRequest; import org.opensearch.action.index.IndexResponse; import org.opensearch.action.support.master.AcknowledgedResponse; import org.opensearch.client.RequestOptions; import org.opensearch.client.RestClient; import org.opensearch.client.RestClientBuilder; import org.opensearch.client.RestHighLevelClient; import org.opensearch.client.indices.CreateIndexRequest; import org.opensearch.client.indices.CreateIndexResponse; import org.opensearch.common.settings.Settings; import java.io.IOException; import java.util.HashMap; public class RESTClientSample { public static void main(String[] args) throws IOException { //Point to keystore with appropriate certificates for security. System.setProperty(\"javax.net.ssl.trustStore\", \"/full/path/to/keystore\"); System.setProperty(\"javax.net.ssl.trustStorePassword\", \"password-to-keystore\"); //Establish credentials to use basic authentication. //Only for demo purposes. Don't specify your credentials in code. final CredentialsProvider credentialsProvider = new BasicCredentialsProvider(); credentialsProvider.setCredentials(AuthScope.ANY, new UsernamePasswordCredentials(\"admin\", \"admin\")); //Create a client. RestClientBuilder builder = RestClient.builder(new HttpHost(\"localhost\", 9200, \"https\")) .setHttpClientConfigCallback(new RestClientBuilder.HttpClientConfigCallback() { @Override public HttpAsyncClientBuilder customizeHttpClient(HttpAsyncClientBuilder httpClientBuilder) { return httpClientBuilder.setDefaultCredentialsProvider(credentialsProvider); } }); RestHighLevelClient client = new RestHighLevelClient(builder); //Create a non-default index with custom settings and mappings. CreateIndexRequest createIndexRequest = new CreateIndexRequest(\"custom-index\"); createIndexRequest.settings(Settings.builder() //Specify in the settings how many shards you want in the index.put(\"index.number_of_shards\", 4) .put(\"index.number_of_replicas\", 3) ); //Create a set of maps for the index's mappings. HashMap&lt;String, String&gt; typeMapping = new HashMap&lt;String,String&gt;(); typeMapping.put(\"type\", \"integer\"); HashMap&lt;String, Object&gt; ageMapping = new HashMap&lt;String, Object&gt;(); ageMapping.put(\"age\", typeMapping); HashMap&lt;String, Object&gt; mapping = new HashMap&lt;String, Object&gt;(); mapping.put(\"properties\", ageMapping); createIndexRequest.mapping(mapping); CreateIndexResponse createIndexResponse = client.indices().create(createIndexRequest, RequestOptions.DEFAULT); //Adding data to the index. IndexRequest request = new IndexRequest(\"custom-index\"); //Add a document to the custom-index we created. request.id(\"1\"); //Assign an ID to the document. HashMap&lt;String, String&gt; stringMapping = new HashMap&lt;String, String&gt;(); stringMapping.put(\"message:\", \"Testing Java REST client\"); request.source(stringMapping); //Place your content into the index's source. IndexResponse indexResponse = client.index(request, RequestOptions.DEFAULT); //Getting back the document GetRequest getRequest = new GetRequest(\"custom-index\", \"1\"); GetResponse response = client.get(getRequest, RequestOptions.DEFAULT); System.out.println(response.getSourceAsString()); //Delete the document DeleteRequest deleteDocumentRequest = new DeleteRequest(\"custom-index\", \"1\"); //Index name followed by the ID. DeleteResponse deleteResponse = client.delete(deleteDocumentRequest, RequestOptions.DEFAULT); //Delete the index DeleteIndexRequest deleteIndexRequest = new DeleteIndexRequest(\"custom-index\"); //Index name. AcknowledgedResponse deleteIndexResponse = client.indices().delete(deleteIndexRequest, RequestOptions.DEFAULT); client.close(); } } . ",
    "url": "https://vagimeli.github.io/clients/java-rest-high-level/#sample-program",
    "relUrl": "/clients/java-rest-high-level/#sample-program"
  },"1595": {
    "doc": "Java high-level REST client",
    "title": "Elasticsearch OSS Java high-level REST client",
    "content": "We recommend using the OpenSearch client to connect to OpenSearch clusters, but if you must use the Elasticsearch OSS Java high-level REST client, version 7.10.2 of the Elasticsearch OSS client also works with the 1.x versions of OpenSearch. Migrating to the OpenSearch Java high-level REST client . Migrating from the Elasticsearch OSS client to the OpenSearch high-level REST client is as simple as changing your Maven dependency to one that references OpenSearch’s dependency. Afterward, change all references of org.elasticsearch to org.opensearch, and you’re ready to start submitting requests to your OpenSearch cluster. ",
    "url": "https://vagimeli.github.io/clients/java-rest-high-level/#elasticsearch-oss-java-high-level-rest-client",
    "relUrl": "/clients/java-rest-high-level/#elasticsearch-oss-java-high-level-rest-client"
  },"1596": {
    "doc": "Java client",
    "title": "Java client",
    "content": "The OpenSearch Java client allows you to interact with your OpenSearch clusters through Java methods and data structures rather than HTTP methods and raw JSON. For example, you can submit requests to your cluster using objects to create indexes, add data to documents, or complete some other operation using the client’s built-in methods. For the client’s complete API documentation and additional examples, see the javadoc. This getting started guide illustrates how to connect to OpenSearch, index documents, and run queries. For the client source code, see the opensearch-java repo. ",
    "url": "https://vagimeli.github.io/clients/java/",
    "relUrl": "/clients/java/"
  },"1597": {
    "doc": "Java client",
    "title": "Installing the client using Apache HttpClient 5 Transport",
    "content": "To start using the OpenSearch Java client, you need to provide a transport. The default ApacheHttpClient5TransportBuilder transport comes with the Java client. To use the OpenSearch Java client with the default transport, add it to your pom.xml file as a dependency: . &lt;dependency&gt; &lt;groupId&gt;org.opensearch.client&lt;/groupId&gt; &lt;artifactId&gt;opensearch-java&lt;/artifactId&gt; &lt;version&gt;2.4.0&lt;/version&gt; &lt;/dependency&gt; . copy . If you’re using Gradle, add the following dependencies to your project: . dependencies { implementation 'org.opensearch.client:opensearch-java:2.4.0' } . copy . You can now start your OpenSearch cluster. ",
    "url": "https://vagimeli.github.io/clients/java/#installing-the-client-using-apache-httpclient-5-transport",
    "relUrl": "/clients/java/#installing-the-client-using-apache-httpclient-5-transport"
  },"1598": {
    "doc": "Java client",
    "title": "Installing the client using RestClient Transport",
    "content": "Alternatively, you can create a Java client by using the RestClient-based transport. In this case, make sure that you have the following dependencies in your project’s pom.xml file: . &lt;dependency&gt; &lt;groupId&gt;org.opensearch.client&lt;/groupId&gt; &lt;artifactId&gt;opensearch-rest-client&lt;/artifactId&gt; &lt;version&gt;2.7.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.opensearch.client&lt;/groupId&gt; &lt;artifactId&gt;opensearch-java&lt;/artifactId&gt; &lt;version&gt;2.4.0&lt;/version&gt; &lt;/dependency&gt; . copy . If you’re using Gradle, add the following dependencies to your project” . dependencies { implementation 'org.opensearch.client:opensearch-rest-client: 2.7.0' implementation 'org.opensearch.client:opensearch-java:2.4.0' } . copy . You can now start your OpenSearch cluster. ",
    "url": "https://vagimeli.github.io/clients/java/#installing-the-client-using-restclient-transport",
    "relUrl": "/clients/java/#installing-the-client-using-restclient-transport"
  },"1599": {
    "doc": "Java client",
    "title": "Security",
    "content": "Before using the REST client in your Java application, you must configure the application’s truststore to connect to the Security plugin. If you are using self-signed certificates or demo configurations, you can use the following command to create a custom truststore and add in root authority certificates. If you’re using certificates from a trusted Certificate Authority (CA), you don’t need to configure the truststore. keytool -import &lt;path-to-cert&gt; -alias &lt;alias-to-call-cert&gt; -keystore &lt;truststore-name&gt; . copy . You can now point your Java client to the truststore and set basic authentication credentials that can access a secure cluster (refer to the sample code below on how to do so). If you run into issues when configuring security, see common issues and troubleshoot TLS. ",
    "url": "https://vagimeli.github.io/clients/java/#security",
    "relUrl": "/clients/java/#security"
  },"1600": {
    "doc": "Java client",
    "title": "Sample data",
    "content": "This section uses a class called IndexData, which is a simple Java class that stores basic data and methods. For your own OpenSearch cluster, you might find that you need a more robust class to store your data. IndexData class . static class IndexData { private String firstName; private String lastName; public IndexData(String firstName, String lastName) { this.firstName = firstName; this.lastName = lastName; } public String getFirstName() { return firstName; } public void setFirstName(String firstName) { this.firstName = firstName; } public String getLastName() { return lastName; } public void setLastName(String lastName) { this.lastName = lastName; } @Override public String toString() { return String.format(\"IndexData{first name='%s', last name='%s'}\", firstName, lastName); } } . copy . ",
    "url": "https://vagimeli.github.io/clients/java/#sample-data",
    "relUrl": "/clients/java/#sample-data"
  },"1601": {
    "doc": "Java client",
    "title": "Initializing the client with SSL and TLS enabled using Apache HttpClient 5 Transport",
    "content": "This code example uses basic credentials that come with the default OpenSearch configuration. If you’re using the Java client with your own OpenSearch cluster, be sure to change the code so that it uses your own credentials. The following sample code initializes a client with SSL and TLS enabled: . import javax.net.ssl.SSLContext; import javax.net.ssl.SSLEngine; import org.apache.hc.client5.http.auth.AuthScope; import org.apache.hc.client5.http.auth.UsernamePasswordCredentials; import org.apache.hc.client5.http.impl.auth.BasicCredentialsProvider; import org.apache.hc.client5.http.impl.nio.PoolingAsyncClientConnectionManager; import org.apache.hc.client5.http.impl.nio.PoolingAsyncClientConnectionManagerBuilder; import org.apache.hc.client5.http.ssl.ClientTlsStrategyBuilder; import org.apache.hc.core5.function.Factory; import org.apache.hc.core5.http.HttpHost; import org.apache.hc.core5.http.nio.ssl.TlsStrategy; import org.apache.hc.core5.reactor.ssl.TlsDetails; import org.apache.hc.core5.ssl.SSLContextBuilder; import org.opensearch.client.opensearch.OpenSearchClient; import org.opensearch.client.transport.OpenSearchTransport; import org.opensearch.client.transport.httpclient5.ApacheHttpClient5TransportBuilder; public class OpenSearchClientExample { public static void main(String[] args) throws Exception { System.setProperty(\"javax.net.ssl.trustStore\", \"/full/path/to/keystore\"); System.setProperty(\"javax.net.ssl.trustStorePassword\", \"password-to-keystore\"); final HttpHost host = new HttpHost(\"https\", \"localhost\", 9200); final BasicCredentialsProvider credentialsProvider = new BasicCredentialsProvider(); // Only for demo purposes. Don't specify your credentials in code. credentialsProvider.setCredentials(new AuthScope(host), new UsernamePasswordCredentials(\"admin\", \"admin\".toCharArray())); final SSLContext sslcontext = SSLContextBuilder .create() .loadTrustMaterial(null, (chains, authType) -&gt; true) .build(); final ApacheHttpClient5TransportBuilder builder = ApacheHttpClient5TransportBuilder.builder(host); builder.setHttpClientConfigCallback(httpClientBuilder -&gt; { final TlsStrategy tlsStrategy = ClientTlsStrategyBuilder.create() .setSslContext(SSLContextBuilder.create().build()) // See https://issues.apache.org/jira/browse/HTTPCLIENT-2219 .setTlsDetailsFactory(new Factory&lt;SSLEngine, TlsDetails&gt;() { @Override public TlsDetails create(final SSLEngine sslEngine) { return new TlsDetails(sslEngine.getSession(), sslEngine.getApplicationProtocol()); } }) .build(); final PoolingAsyncClientConnectionManager connectionManager = PoolingAsyncClientConnectionManagerBuilder .create() .setTlsStrategy(tlsStrategy) .build(); return httpClientBuilder .setDefaultCredentialsProvider(credentialsProvider) .setConnectionManager(connectionManager); }); final OpenSearchTransport transport = ApacheHttpClient5TransportBuilder.builder(host).build(); OpenSearchClient client = new OpenSearchClient(transport); } } . ",
    "url": "https://vagimeli.github.io/clients/java/#initializing-the-client-with-ssl-and-tls-enabled-using-apache-httpclient-5-transport",
    "relUrl": "/clients/java/#initializing-the-client-with-ssl-and-tls-enabled-using-apache-httpclient-5-transport"
  },"1602": {
    "doc": "Java client",
    "title": "Initializing the client with SSL and TLS enabled using RestClient Transport",
    "content": "This code example uses basic credentials that come with the default OpenSearch configuration. If you’re using the Java client with your own OpenSearch cluster, be sure to change the code so that it uses your own credentials. The following sample code initializes a client with SSL and TLS enabled: . import org.apache.hc.client5.http.auth.AuthScope; import org.apache.hc.client5.http.auth.UsernamePasswordCredentials; import org.apache.hc.client5.http.impl.async.HttpAsyncClientBuilder; import org.apache.hc.client5.http.impl.auth.BasicCredentialsProvider; import org.apache.hc.core5.http.HttpHost; import org.opensearch.client.RestClient; import org.opensearch.client.RestClientBuilder; import org.opensearch.client.json.jackson.JacksonJsonpMapper; import org.opensearch.client.opensearch.OpenSearchClient; import org.opensearch.client.transport.OpenSearchTransport; import org.opensearch.client.transport.rest_client.RestClientTransport; public class OpenSearchClientExample { public static void main(String[] args) throws Exception { System.setProperty(\"javax.net.ssl.trustStore\", \"/full/path/to/keystore\"); System.setProperty(\"javax.net.ssl.trustStorePassword\", \"password-to-keystore\"); final HttpHost host = new HttpHost(\"https\", \"localhost\", 9200); final BasicCredentialsProvider credentialsProvider = new BasicCredentialsProvider(); //Only for demo purposes. Don't specify your credentials in code. credentialsProvider.setCredentials(new AuthScope(host), new UsernamePasswordCredentials(\"admin\", \"admin\".toCharArray())); //Initialize the client with SSL and TLS enabled final RestClient restClient = RestClient.builder(host). setHttpClientConfigCallback(new RestClientBuilder.HttpClientConfigCallback() { @Override public HttpAsyncClientBuilder customizeHttpClient(HttpAsyncClientBuilder httpClientBuilder) { return httpClientBuilder.setDefaultCredentialsProvider(credentialsProvider); } }).build(); final OpenSearchTransport transport = new RestClientTransport(restClient, new JacksonJsonpMapper()); final OpenSearchClient client = new OpenSearchClient(transport); } } . copy . ",
    "url": "https://vagimeli.github.io/clients/java/#initializing-the-client-with-ssl-and-tls-enabled-using-restclient-transport",
    "relUrl": "/clients/java/#initializing-the-client-with-ssl-and-tls-enabled-using-restclient-transport"
  },"1603": {
    "doc": "Java client",
    "title": "Connecting to Amazon OpenSearch Service",
    "content": "The following example illustrates connecting to Amazon OpenSearch Service: . SdkHttpClient httpClient = ApacheHttpClient.builder().build(); OpenSearchClient client = new OpenSearchClient( new AwsSdk2Transport( httpClient, \"search-...us-west-2.es.amazonaws.com\", // OpenSearch endpoint, without https:// \"es\" Region.US_WEST_2, // signing service region AwsSdk2TransportOptions.builder().build() ) ); InfoResponse info = client.info(); System.out.println(info.version().distribution() + \": \" + info.version().number()); httpClient.close(); . copy . ",
    "url": "https://vagimeli.github.io/clients/java/#connecting-to-amazon-opensearch-service",
    "relUrl": "/clients/java/#connecting-to-amazon-opensearch-service"
  },"1604": {
    "doc": "Java client",
    "title": "Connecting to Amazon OpenSearch Serverless",
    "content": "The following example illustrates connecting to Amazon OpenSearch Serverless Service: . SdkHttpClient httpClient = ApacheHttpClient.builder().build(); OpenSearchClient client = new OpenSearchClient( new AwsSdk2Transport( httpClient, \"search-...us-west-2.aoss.amazonaws.com\", // OpenSearch endpoint, without https:// \"aoss\" Region.US_WEST_2, // signing service region AwsSdk2TransportOptions.builder().build() ) ); InfoResponse info = client.info(); System.out.println(info.version().distribution() + \": \" + info.version().number()); httpClient.close(); . copy . ",
    "url": "https://vagimeli.github.io/clients/java/#connecting-to-amazon-opensearch-serverless",
    "relUrl": "/clients/java/#connecting-to-amazon-opensearch-serverless"
  },"1605": {
    "doc": "Java client",
    "title": "Creating an index",
    "content": "You can create an index with non-default settings using the following code: . String index = \"sample-index\"; CreateRequest createIndexRequest = new CreateRequest.Builder().index(index).build(); client.indices().create(createIndexRequest); IndexSettings indexSettings = new IndexSettings.Builder().autoExpandReplicas(\"0-all\").build(); IndexSettingsBody settingsBody = new IndexSettingsBody.Builder().settings(indexSettings).build(); PutSettingsRequest putSettingsRequest = new PutSettingsRequest.Builder().index(index).value(settingsBody).build(); client.indices().putSettings(putSettingsRequest); . copy . ",
    "url": "https://vagimeli.github.io/clients/java/#creating-an-index",
    "relUrl": "/clients/java/#creating-an-index"
  },"1606": {
    "doc": "Java client",
    "title": "Indexing data",
    "content": "You can index data into OpenSearch using the following code: . IndexData indexData = new IndexData(\"first_name\", \"Bruce\"); IndexRequest&lt;IndexData&gt; indexRequest = new IndexRequest.Builder&lt;IndexData&gt;().index(index).id(\"1\").document(indexData).build(); client.index(indexRequest); . copy . ",
    "url": "https://vagimeli.github.io/clients/java/#indexing-data",
    "relUrl": "/clients/java/#indexing-data"
  },"1607": {
    "doc": "Java client",
    "title": "Searching for documents",
    "content": "You can search for a document using the following code: . SearchResponse&lt;IndexData&gt; searchResponse = client.search(s -&gt; s.index(index), IndexData.class); for (int i = 0; i&lt; searchResponse.hits().hits().size(); i++) { System.out.println(searchResponse.hits().hits().get(i).source()); } . copy . ",
    "url": "https://vagimeli.github.io/clients/java/#searching-for-documents",
    "relUrl": "/clients/java/#searching-for-documents"
  },"1608": {
    "doc": "Java client",
    "title": "Deleting a document",
    "content": "The following sample code deletes a document whose ID is 1: . client.delete(b -&gt; b.index(index).id(\"1\")); . copy . Deleting an index . The following sample code deletes an index: . DeleteRequest deleteRequest = new DeleteRequest.Builder().index(index).build(); DeleteResponse deleteResponse = client.indices().delete(deleteRequest); } catch (IOException e){ System.out.println(e.toString()); } finally { try { if (restClient != null) { restClient.close(); } } catch (IOException e) { System.out.println(e.toString()); } } } } . copy . ",
    "url": "https://vagimeli.github.io/clients/java/#deleting-a-document",
    "relUrl": "/clients/java/#deleting-a-document"
  },"1609": {
    "doc": "Java client",
    "title": "Sample program",
    "content": "The following sample program creates a client, adds an index with non-default settings, inserts a document, searches for the document, deletes the document, and then deletes the index: . import org.apache.http.HttpHost; import org.apache.http.auth.AuthScope; import org.apache.http.auth.UsernamePasswordCredentials; import org.apache.http.client.CredentialsProvider; import org.apache.http.impl.client.BasicCredentialsProvider; import org.apache.http.impl.nio.client.HttpAsyncClientBuilder; import org.opensearch.client.RestClient; import org.opensearch.client.RestClientBuilder; import org.opensearch.client.base.RestClientTransport; import org.opensearch.client.base.Transport; import org.opensearch.client.json.jackson.JacksonJsonpMapper; import org.opensearch.client.opensearch.OpenSearchClient; import org.opensearch.client.opensearch._global.IndexRequest; import org.opensearch.client.opensearch._global.IndexResponse; import org.opensearch.client.opensearch._global.SearchResponse; import org.opensearch.client.opensearch.indices.*; import org.opensearch.client.opensearch.indices.put_settings.IndexSettingsBody; import java.io.IOException; public class OpenSearchClientExample { public static void main(String[] args) { RestClient restClient = null; try{ System.setProperty(\"javax.net.ssl.trustStore\", \"/full/path/to/keystore\"); System.setProperty(\"javax.net.ssl.trustStorePassword\", \"password-to-keystore\"); //Only for demo purposes. Don't specify your credentials in code. final CredentialsProvider credentialsProvider = new BasicCredentialsProvider(); credentialsProvider.setCredentials(AuthScope.ANY, new UsernamePasswordCredentials(\"admin\", \"admin\")); //Initialize the client with SSL and TLS enabled restClient = RestClient.builder(new HttpHost(\"localhost\", 9200, \"https\")). setHttpClientConfigCallback(new RestClientBuilder.HttpClientConfigCallback() { @Override public HttpAsyncClientBuilder customizeHttpClient(HttpAsyncClientBuilder httpClientBuilder) { return httpClientBuilder.setDefaultCredentialsProvider(credentialsProvider); } }).build(); Transport transport = new RestClientTransport(restClient, new JacksonJsonpMapper()); OpenSearchClient client = new OpenSearchClient(transport); //Create the index String index = \"sample-index\"; CreateRequest createIndexRequest = new CreateRequest.Builder().index(index).build(); client.indices().create(createIndexRequest); //Add some settings to the index IndexSettings indexSettings = new IndexSettings.Builder().autoExpandReplicas(\"0-all\").build(); IndexSettingsBody settingsBody = new IndexSettingsBody.Builder().settings(indexSettings).build(); PutSettingsRequest putSettingsRequest = new PutSettingsRequest.Builder().index(index).value(settingsBody).build(); client.indices().putSettings(putSettingsRequest); //Index some data IndexData indexData = new IndexData(\"first_name\", \"Bruce\"); IndexRequest&lt;IndexData&gt; indexRequest = new IndexRequest.Builder&lt;IndexData&gt;().index(index).id(\"1\").document(indexData).build(); client.index(indexRequest); //Search for the document SearchResponse&lt;IndexData&gt; searchResponse = client.search(s -&gt; s.index(index), IndexData.class); for (int i = 0; i&lt; searchResponse.hits().hits().size(); i++) { System.out.println(searchResponse.hits().hits().get(i).source()); } //Delete the document client.delete(b -&gt; b.index(index).id(\"1\")); // Delete the index DeleteRequest deleteRequest = new DeleteRequest.Builder().index(index).build(); DeleteResponse deleteResponse = client.indices().delete(deleteRequest); } catch (IOException e){ System.out.println(e.toString()); } finally { try { if (restClient != null) { restClient.close(); } } catch (IOException e) { System.out.println(e.toString()); } } } } . copy . ",
    "url": "https://vagimeli.github.io/clients/java/#sample-program",
    "relUrl": "/clients/java/#sample-program"
  },"1610": {
    "doc": "Helper methods",
    "title": "Helper methods",
    "content": "Helper methods simplify the use of complicated API tasks. ",
    "url": "https://vagimeli.github.io/clients/javascript/helpers/",
    "relUrl": "/clients/javascript/helpers/"
  },"1611": {
    "doc": "Helper methods",
    "title": "Bulk helper",
    "content": "The bulk helper simplifies making complex bulk API requests. Usage . The following code creates a bulk helper instance: . const { Client } = require('@opensearch-project/opensearch') const documents = require('./docs.json') const client = new Client({ ... }) const result = await client.helpers.bulk({ datasource: documents, onDocument (doc) { return { index: { _index: 'example-index' } } } }) console.log(result) . copy . Bulk helper operations return an object with the following fields: . { total: number, failed: number, retry: number, successful: number, time: number, bytes: number, aborted: boolean } . Bulk helper configuration options . When creating a new bulk helper instance, you can use the following configuration options. | Option | Data type | Required/Default | Description | . | datasource | An array, async generator or a readable stream of strings or objects | Required | Represents the documents you need to create, delete, index, or update. | . | onDocument | Function | Required | A function to be invoked with each document in the given datasource. It returns the operation to be executed for this document. Optionally, the document can be manipulated for create and index operations by returning a new document as part of the function’s result. | . | concurrency | Integer | Optional. Default is 5. | The number of requests to be executed in parallel. | . | flushBytes | Integer | Optional. Default is 5,000,000. | Maximum bulk body size to send in bytes. | . | flushInterval | Integer | Optional. Default is 30,000. | Time in milliseconds to wait before flushing the body after the last document has been read. | . | onDrop | Function | Optional. Default is noop. | A function to be invoked for every document that can’t be indexed after reaching the maximum number of retries. | . | refreshOnCompletion | Boolean | Optional. Default is false. | Whether or not a refresh should be run on all affected indexes at the end of the bulk operation. | . | retries | Integer | Optional. Defaults to the client’s maxRetries value. | The number of times an operation is retried before onDrop is called for that document. | . | wait | Integer | Optional. Default is 5,000. | Time in milliseconds to wait before retrying an operation. | . Examples . The following examples illustrate the index, create, update, and delete bulk helper operations. Index . The index operation creates a new document if it doesn’t exist and recreates the document if it already exists. The following bulk operation indexes documents into example-index: . client.helpers.bulk({ datasource: arrayOfDocuments, onDocument (doc) { return { index: { _index: 'example-index' } } } }) . copy . The following bulk operation indexes documents into example-index with document overwrite: . client.helpers.bulk({ datasource: arrayOfDocuments, onDocument (doc) { return [ { index: { _index: 'example-index' } }, { ...doc, createdAt: new Date().toISOString() } ] } }) . copy . Create . The create operation creates a new document only if the document does not already exist. The following bulk operation creates documents in the example-index: . client.helpers.bulk({ datasource: arrayOfDocuments, onDocument (doc) { return { create: { _index: 'example-index', _id: doc.id } } } }) . copy . The following bulk operation creates documents in the example-index with document overwrite: . client.helpers.bulk({ datasource: arrayOfDocuments, onDocument (doc) { return [ { create: { _index: 'example-index', _id: doc.id } }, { ...doc, createdAt: new Date().toISOString() } ] } }) . copy . Update . The update operation updates the document with the fields being sent. The document must already exist in the index. The following bulk operation updates documents in the arrayOfDocuments: . client.helpers.bulk({ datasource: arrayOfDocuments, onDocument (doc) { // The update operation always requires a tuple to be returned, with the // first element being the action and the second being the update options. return [ { update: { _index: 'example-index', _id: doc.id } }, { doc_as_upsert: true } ] } }) . copy . The following bulk operation updates documents in the arrayOfDocuments with document overwrite: . client.helpers.bulk({ datasource: arrayOfDocuments, onDocument (doc) { return [ { update: { _index: 'example-index', _id: doc.id } }, { doc: { ...doc, createdAt: new Date().toISOString() }, doc_as_upsert: true } ] } }) . copy . Delete . The delete operation deletes a document. The following bulk operation deletes documents from the example-index: . client.helpers.bulk({ datasource: arrayOfDocuments, onDocument (doc) { return { delete: { _index: 'example-index', _id: doc.id } } } }) . copy . ",
    "url": "https://vagimeli.github.io/clients/javascript/helpers/#bulk-helper",
    "relUrl": "/clients/javascript/helpers/#bulk-helper"
  },"1612": {
    "doc": "JavaScript client",
    "title": "JavaScript client",
    "content": "The OpenSearch JavaScript (JS) client provides a safer and easier way to interact with your OpenSearch cluster. Rather than using OpenSearch from the browser and potentially exposing your data to the public, you can build an OpenSearch client that takes care of sending requests to your cluster. For the client’s complete API documentation and additional examples, see the JS client API documentation. The client contains a library of APIs that let you perform different operations on your cluster and return a standard response body. The example here demonstrates some basic operations like creating an index, adding documents, and searching your data. ",
    "url": "https://vagimeli.github.io/clients/javascript/index/",
    "relUrl": "/clients/javascript/index/"
  },"1613": {
    "doc": "JavaScript client",
    "title": "Setup",
    "content": "To add the client to your project, install it from npm: . npm install @opensearch-project/opensearch . copy . To install a specific major version of the client, run the following command: . npm install @opensearch-project/opensearch@&lt;version&gt; . copy . If you prefer to add the client manually or just want to examine the source code, see opensearch-js on GitHub. Then require the client: . const { Client } = require(\"@opensearch-project/opensearch\"); . copy . ",
    "url": "https://vagimeli.github.io/clients/javascript/index/#setup",
    "relUrl": "/clients/javascript/index/#setup"
  },"1614": {
    "doc": "JavaScript client",
    "title": "Connecting to OpenSearch",
    "content": "To connect to the default OpenSearch host, create a client object with the address https://localhost:9200 if you are using the Security plugin: . var host = \"localhost\"; var protocol = \"https\"; var port = 9200; var auth = \"admin:admin\"; // For testing only. Don't store credentials in code. var ca_certs_path = \"/full/path/to/root-ca.pem\"; // Optional client certificates if you don't want to use HTTP basic authentication. // var client_cert_path = '/full/path/to/client.pem' // var client_key_path = '/full/path/to/client-key.pem' // Create a client with SSL/TLS enabled. var { Client } = require(\"@opensearch-project/opensearch\"); var fs = require(\"fs\"); var client = new Client({ node: protocol + \"://\" + auth + \"@\" + host + \":\" + port, ssl: { ca: fs.readFileSync(ca_certs_path), // You can turn off certificate verification (rejectUnauthorized: false) if you're using // self-signed certificates with a hostname mismatch. // cert: fs.readFileSync(client_cert_path), // key: fs.readFileSync(client_key_path) }, }); . copy . ",
    "url": "https://vagimeli.github.io/clients/javascript/index/#connecting-to-opensearch",
    "relUrl": "/clients/javascript/index/#connecting-to-opensearch"
  },"1615": {
    "doc": "JavaScript client",
    "title": "Authenticating with Amazon OpenSearch Service – AWS Sigv4",
    "content": "Use the following code to authenticate with AWS V2 SDK: . const AWS = require('aws-sdk'); // V2 SDK. const { Client } = require('@opensearch-project/opensearch'); const { AwsSigv4Signer } = require('@opensearch-project/opensearch/aws'); const client = new Client({ ...AwsSigv4Signer({ region: 'us-west-2', service: 'es', // Must return a Promise that resolve to an AWS.Credentials object. // This function is used to acquire the credentials when the client start and // when the credentials are expired. // The Client will refresh the Credentials only when they are expired. // With AWS SDK V2, Credentials.refreshPromise is used when available to refresh the credentials. // Example with AWS SDK V2: getCredentials: () =&gt; new Promise((resolve, reject) =&gt; { // Any other method to acquire a new Credentials object can be used. AWS.config.getCredentials((err, credentials) =&gt; { if (err) { reject(err); } else { resolve(credentials); } }); }), }), node: 'https://search-xxx.region.es.amazonaws.com', // OpenSearch domain URL }); . copy . AWS V2 SDK for Amazon OpenSearch Serverless . const AWS = require('aws-sdk'); // V2 SDK. const { Client } = require('@opensearch-project/opensearch'); const { AwsSigv4Signer } = require('@opensearch-project/opensearch/aws'); const client = new Client({ ...AwsSigv4Signer({ region: 'us-west-2', service: 'aoss', // Must return a Promise that resolve to an AWS.Credentials object. // This function is used to acquire the credentials when the client start and // when the credentials are expired. // The Client will refresh the Credentials only when they are expired. // With AWS SDK V2, Credentials.refreshPromise is used when available to refresh the credentials. // Example with AWS SDK V2: getCredentials: () =&gt; new Promise((resolve, reject) =&gt; { // Any other method to acquire a new Credentials object can be used. AWS.config.getCredentials((err, credentials) =&gt; { if (err) { reject(err); } else { resolve(credentials); } }); }), }), node: \"https://xxx.region.aoss.amazonaws.com\" // OpenSearch domain URL }); . copy . Use the following code to authenticate with AWS V3 SDK: . const { defaultProvider } = require('@aws-sdk/credential-provider-node'); // V3 SDK. const { Client } = require('@opensearch-project/opensearch'); const { AwsSigv4Signer } = require('@opensearch-project/opensearch/aws'); const client = new Client({ ...AwsSigv4Signer({ region: 'us-east-1', service: 'es', // 'aoss' for OpenSearch Serverless // Must return a Promise that resolve to an AWS.Credentials object. // This function is used to acquire the credentials when the client start and // when the credentials are expired. // The Client will refresh the Credentials only when they are expired. // With AWS SDK V2, Credentials.refreshPromise is used when available to refresh the credentials. // Example with AWS SDK V3: getCredentials: () =&gt; { // Any other method to acquire a new Credentials object can be used. const credentialsProvider = defaultProvider(); return credentialsProvider(); }, }), node: 'https://search-xxx.region.es.amazonaws.com', // OpenSearch domain URL // node: \"https://xxx.region.aoss.amazonaws.com\" for OpenSearch Serverless }); . copy . AWS V3 SDK for Amazon OpenSearch Serverless . const { defaultProvider } = require('@aws-sdk/credential-provider-node'); // V3 SDK. const { Client } = require('@opensearch-project/opensearch'); const { AwsSigv4Signer } = require('@opensearch-project/opensearch/aws'); const client = new Client({ ...AwsSigv4Signer({ region: 'us-east-1', service: 'aoss', // Must return a Promise that resolve to an AWS.Credentials object. // This function is used to acquire the credentials when the client start and // when the credentials are expired. // The Client will refresh the Credentials only when they are expired. // With AWS SDK V2, Credentials.refreshPromise is used when available to refresh the credentials. // Example with AWS SDK V3: getCredentials: () =&gt; { // Any other method to acquire a new Credentials object can be used. const credentialsProvider = defaultProvider(); return credentialsProvider(); }, }), node: \"https://xxx.region.aoss.amazonaws.com\" // OpenSearch domain URL }); . copy . ",
    "url": "https://vagimeli.github.io/clients/javascript/index/#authenticating-with-amazon-opensearch-service--aws-sigv4",
    "relUrl": "/clients/javascript/index/#authenticating-with-amazon-opensearch-service--aws-sigv4"
  },"1616": {
    "doc": "JavaScript client",
    "title": "Creating an index",
    "content": "To create an OpenSearch index, use the indices.create() method. You can use the following code to construct a JSON object with custom settings: . var index_name = \"books\"; var settings = { settings: { index: { number_of_shards: 4, number_of_replicas: 3, }, }, }; var response = await client.indices.create({ index: index_name, body: settings, }); . copy . ",
    "url": "https://vagimeli.github.io/clients/javascript/index/#creating-an-index",
    "relUrl": "/clients/javascript/index/#creating-an-index"
  },"1617": {
    "doc": "JavaScript client",
    "title": "Indexing a document",
    "content": "You can index a document into OpenSearch using the client’s index method: . var document = { title: \"The Outsider\", author: \"Stephen King\", year: \"2018\", genre: \"Crime fiction\", }; var id = \"1\"; var response = await client.index({ id: id, index: index_name, body: document, refresh: true, }); . copy . ",
    "url": "https://vagimeli.github.io/clients/javascript/index/#indexing-a-document",
    "relUrl": "/clients/javascript/index/#indexing-a-document"
  },"1618": {
    "doc": "JavaScript client",
    "title": "Searching for documents",
    "content": "The easiest way to search for documents is to construct a query string. The following code uses a match query to search for “The Outsider” in the title field: . var query = { query: { match: { title: { query: \"The Outsider\", }, }, }, }; var response = await client.search({ index: index_name, body: query, }); . copy . ",
    "url": "https://vagimeli.github.io/clients/javascript/index/#searching-for-documents",
    "relUrl": "/clients/javascript/index/#searching-for-documents"
  },"1619": {
    "doc": "JavaScript client",
    "title": "Deleting a document",
    "content": "You can delete a document using the client’s delete method: . var response = await client.delete({ index: index_name, id: id, }); . copy . ",
    "url": "https://vagimeli.github.io/clients/javascript/index/#deleting-a-document",
    "relUrl": "/clients/javascript/index/#deleting-a-document"
  },"1620": {
    "doc": "JavaScript client",
    "title": "Deleting an index",
    "content": "You can delete an index using the indices.delete() method: . var response = await client.indices.delete({ index: index_name, }); . copy . ",
    "url": "https://vagimeli.github.io/clients/javascript/index/#deleting-an-index",
    "relUrl": "/clients/javascript/index/#deleting-an-index"
  },"1621": {
    "doc": "JavaScript client",
    "title": "Sample program",
    "content": "The following sample program creates a client, adds an index with non-default settings, inserts a document, searches for the document, deletes the document, and then deletes the index: . \"use strict\"; var host = \"localhost\"; var protocol = \"https\"; var port = 9200; var auth = \"admin:admin\"; // For testing only. Don't store credentials in code. var ca_certs_path = \"/full/path/to/root-ca.pem\"; // Optional client certificates if you don't want to use HTTP basic authentication. // var client_cert_path = '/full/path/to/client.pem' // var client_key_path = '/full/path/to/client-key.pem' // Create a client with SSL/TLS enabled. var { Client } = require(\"@opensearch-project/opensearch\"); var fs = require(\"fs\"); var client = new Client({ node: protocol + \"://\" + auth + \"@\" + host + \":\" + port, ssl: { ca: fs.readFileSync(ca_certs_path), // You can turn off certificate verification (rejectUnauthorized: false) if you're using // self-signed certificates with a hostname mismatch. // cert: fs.readFileSync(client_cert_path), // key: fs.readFileSync(client_key_path) }, }); async function search() { // Create an index with non-default settings. var index_name = \"books\"; var settings = { settings: { index: { number_of_shards: 4, number_of_replicas: 3, }, }, }; var response = await client.indices.create({ index: index_name, body: settings, }); console.log(\"Creating index:\"); console.log(response.body); // Add a document to the index. var document = { title: \"The Outsider\", author: \"Stephen King\", year: \"2018\", genre: \"Crime fiction\", }; var id = \"1\"; var response = await client.index({ id: id, index: index_name, body: document, refresh: true, }); console.log(\"Adding document:\"); console.log(response.body); // Search for the document. var query = { query: { match: { title: { query: \"The Outsider\", }, }, }, }; var response = await client.search({ index: index_name, body: query, }); console.log(\"Search results:\"); console.log(response.body.hits); // Delete the document. var response = await client.delete({ index: index_name, id: id, }); console.log(\"Deleting document:\"); console.log(response.body); // Delete the index. var response = await client.indices.delete({ index: index_name, }); console.log(\"Deleting index:\"); console.log(response.body); } search().catch(console.log); . copy . ",
    "url": "https://vagimeli.github.io/clients/javascript/index/#sample-program",
    "relUrl": "/clients/javascript/index/#sample-program"
  },"1622": {
    "doc": "JavaScript client",
    "title": "Circuit breaker",
    "content": "The memoryCircuitBreaker option can be used to prevent errors caused by a response payload being too large to fit into the heap memory available to the client. The memoryCircuitBreaker object contains two fields: . | enabled: A Boolean used to turn the circuit breaker on or off. Defaults to false. | maxPercentage: The threshold that determines whether the circuit breaker engages. Valid values are floats in the [0, 1] range that represent percentages in decimal form. Any value that exceeds that range will correct to 1.0. | . The following example instantiates a client with the circuit breaker enabled and its threshold set to 80% of the available heap size limit: . var client = new Client({ memoryCircuitBreaker: { enabled: true, maxPercentage: 0.8, }, }); . copy . ",
    "url": "https://vagimeli.github.io/clients/javascript/index/#circuit-breaker",
    "relUrl": "/clients/javascript/index/#circuit-breaker"
  },"1623": {
    "doc": "Opensearch-py-ml",
    "title": "opensearch-py-ml",
    "content": "opensearch-py-ml is a Python client that provides a suite of data analytics and natural language processing (NLP) support tools for OpenSearch. It provides data analysts with the ability to: . | Call OpenSearch indexes and manipulate them using the opensearch-py-ml DataFrame APIs. The opensearch-py-ml DataFrame wraps an OpenSearch index into an API similar to pandas, giving you the ability to process large amounts of data from OpenSearch inside a Jupyter Notebook. | Upload NLP SentenceTransformer models into OpenSearch using the ML Commons plugin. | Train and tune SentenceTransformer models with synthetic queries. | . ",
    "url": "https://vagimeli.github.io/clients/opensearch-py-ml/#opensearch-py-ml",
    "relUrl": "/clients/opensearch-py-ml/#opensearch-py-ml"
  },"1624": {
    "doc": "Opensearch-py-ml",
    "title": "Prerequisites",
    "content": "To use opensearch-py-ml, install the OpenSearch Python client. The Python client allows OpenSearch to use the Python syntax required to run DataFrames in opensearch-py-ml. ",
    "url": "https://vagimeli.github.io/clients/opensearch-py-ml/#prerequisites",
    "relUrl": "/clients/opensearch-py-ml/#prerequisites"
  },"1625": {
    "doc": "Opensearch-py-ml",
    "title": "Install opensearch-py-ml",
    "content": "To add the client to your project, install it using pip: . pip install opensearch-py-ml . copy . Then import the client into OpenSearch like any other module: . from opensearchpy import OpenSearch import opensearch_py_ml as oml . copy . ",
    "url": "https://vagimeli.github.io/clients/opensearch-py-ml/#install-opensearch-py-ml",
    "relUrl": "/clients/opensearch-py-ml/#install-opensearch-py-ml"
  },"1626": {
    "doc": "Opensearch-py-ml",
    "title": "API reference",
    "content": "For information on all opensearch-py-ml objects, functions, and methods, see the opensearch-py-ml API reference. ",
    "url": "https://vagimeli.github.io/clients/opensearch-py-ml/#api-reference",
    "relUrl": "/clients/opensearch-py-ml/#api-reference"
  },"1627": {
    "doc": "Opensearch-py-ml",
    "title": "Next steps",
    "content": "If you want to track or contribute to the development of the opensearch-py-ml client, see the opensearch-py-ml GitHub repository. For example Python notebooks to use with the client, see Examples. ",
    "url": "https://vagimeli.github.io/clients/opensearch-py-ml/#next-steps",
    "relUrl": "/clients/opensearch-py-ml/#next-steps"
  },"1628": {
    "doc": "Opensearch-py-ml",
    "title": "Opensearch-py-ml",
    "content": " ",
    "url": "https://vagimeli.github.io/clients/opensearch-py-ml/",
    "relUrl": "/clients/opensearch-py-ml/"
  },"1629": {
    "doc": "PHP client",
    "title": "PHP client",
    "content": "The OpenSearch PHP client provides a safer and easier way to interact with your OpenSearch cluster. Rather than using OpenSearch from a browser and potentially exposing your data to the public, you can build an OpenSearch client that takes care of sending requests to your cluster. The client contains a library of APIs that let you perform different operations on your cluster and return a standard response body. This getting started guide illustrates how to connect to OpenSearch, index documents, and run queries. For the client source code, see the opensearch-php repo. ",
    "url": "https://vagimeli.github.io/clients/php/",
    "relUrl": "/clients/php/"
  },"1630": {
    "doc": "PHP client",
    "title": "Setup",
    "content": "To add the client to your project, install it using composer: . composer require opensearch-project/opensearch-php . copy . To install a specific major version of the client, run the following command: . composer require opensearch-project/opensearch-php:&lt;version&gt; . copy . Then require the autload file from composer in your code: . require __DIR__ . '/vendor/autoload.php'; . copy . ",
    "url": "https://vagimeli.github.io/clients/php/#setup",
    "relUrl": "/clients/php/#setup"
  },"1631": {
    "doc": "PHP client",
    "title": "Connecting to OpenSearch",
    "content": "To connect to the default OpenSearch host, create a client object with the address https://localhost:9200 if you are using the Security plugin: . $client = (new \\OpenSearch\\ClientBuilder()) -&gt;setHosts(['https://localhost:9200']) -&gt;setBasicAuthentication('admin', 'admin') // For testing only. Don't store credentials in code. -&gt;setSSLVerification(false) // For testing only. Use certificate for validation -&gt;build(); . copy . ",
    "url": "https://vagimeli.github.io/clients/php/#connecting-to-opensearch",
    "relUrl": "/clients/php/#connecting-to-opensearch"
  },"1632": {
    "doc": "PHP client",
    "title": "Connecting to Amazon OpenSearch Service",
    "content": "The following example illustrates connecting to Amazon OpenSearch Service: . $client = (new \\OpenSearch\\ClientBuilder()) -&gt;setSigV4Region('us-east-2') -&gt;setSigV4Service('es') // Default credential provider. -&gt;setSigV4CredentialProvider(true) // Using a custom access key and secret -&gt;setSigV4CredentialProvider([ 'key' =&gt; 'awskeyid', 'secret' =&gt; 'awssecretkey', ]) -&gt;build(); . copy . ",
    "url": "https://vagimeli.github.io/clients/php/#connecting-to-amazon-opensearch-service",
    "relUrl": "/clients/php/#connecting-to-amazon-opensearch-service"
  },"1633": {
    "doc": "PHP client",
    "title": "Connecting to Amazon OpenSearch Serverless",
    "content": "The following example illustrates connecting to Amazon OpenSearch Serverless Service: . $client = (new \\OpenSearch\\ClientBuilder()) -&gt;setSigV4Region('us-east-2') -&gt;setSigV4Service('aoss') // Default credential provider. -&gt;setSigV4CredentialProvider(true) // Using a custom access key and secret -&gt;setSigV4CredentialProvider([ 'key' =&gt; 'awskeyid', 'secret' =&gt; 'awssecretkey', ]) -&gt;build(); . copy . ",
    "url": "https://vagimeli.github.io/clients/php/#connecting-to-amazon-opensearch-serverless",
    "relUrl": "/clients/php/#connecting-to-amazon-opensearch-serverless"
  },"1634": {
    "doc": "PHP client",
    "title": "Creating an index",
    "content": "To create an OpenSearch index with custom settings, use the following code: . $indexName = 'test-index-name'; // Create an index with non-default settings. $client-&gt;indices()-&gt;create([ 'index' =&gt; $indexName, 'body' =&gt; [ 'settings' =&gt; [ 'index' =&gt; [ 'number_of_shards' =&gt; 4 ] ] ] ]); . copy . ",
    "url": "https://vagimeli.github.io/clients/php/#creating-an-index",
    "relUrl": "/clients/php/#creating-an-index"
  },"1635": {
    "doc": "PHP client",
    "title": "Indexing a document",
    "content": "You can index a document into OpenSearch using the following code: . $client-&gt;create([ 'index' =&gt; $indexName, 'id' =&gt; 1, 'body' =&gt; [ 'title' =&gt; 'Moneyball', 'director' =&gt; 'Bennett Miller', 'year' =&gt; 2011 ] ]); . copy . ",
    "url": "https://vagimeli.github.io/clients/php/#indexing-a-document",
    "relUrl": "/clients/php/#indexing-a-document"
  },"1636": {
    "doc": "PHP client",
    "title": "Searching for documents",
    "content": "The following code uses a multi_match query to search for “miller” in the title and director fields. It boosts the documents where “miller” appears in the title field: . var_dump( $client-&gt;search([ 'index' =&gt; $indexName, 'body' =&gt; [ 'size' =&gt; 5, 'query' =&gt; [ 'multi_match' =&gt; [ 'query' =&gt; 'miller', 'fields' =&gt; ['title^2', 'director'] ] ] ] ]) ); . copy . ",
    "url": "https://vagimeli.github.io/clients/php/#searching-for-documents",
    "relUrl": "/clients/php/#searching-for-documents"
  },"1637": {
    "doc": "PHP client",
    "title": "Deleting a document",
    "content": "You can delete a document using the following code: . $client-&gt;delete([ 'index' =&gt; $indexName, 'id' =&gt; 1, ]); . copy . ",
    "url": "https://vagimeli.github.io/clients/php/#deleting-a-document",
    "relUrl": "/clients/php/#deleting-a-document"
  },"1638": {
    "doc": "PHP client",
    "title": "Deleting an index",
    "content": "You can delete an index using the following code: . $client-&gt;indices()-&gt;delete([ 'index' =&gt; $indexName ]); . copy . ",
    "url": "https://vagimeli.github.io/clients/php/#deleting-an-index",
    "relUrl": "/clients/php/#deleting-an-index"
  },"1639": {
    "doc": "PHP client",
    "title": "Sample program",
    "content": "The following sample program creates a client, adds an index with non-default settings, inserts a document, searches for the document, deletes the document, and then deletes the index: . &lt;?php require __DIR__ . '/vendor/autoload.php'; $client = (new \\OpenSearch\\ClientBuilder()) -&gt;setHosts(['https://localhost:9200']) -&gt;setBasicAuthentication('admin', 'admin') // For testing only. Don't store credentials in code. -&gt;setSSLVerification(false) // For testing only. Use certificate for validation -&gt;build(); $indexName = 'test-index-name'; // Print OpenSearch version information on console. var_dump($client-&gt;info()); // Create an index with non-default settings. $client-&gt;indices()-&gt;create([ 'index' =&gt; $indexName, 'body' =&gt; [ 'settings' =&gt; [ 'index' =&gt; [ 'number_of_shards' =&gt; 4 ] ] ] ]); $client-&gt;create([ 'index' =&gt; $indexName, 'id' =&gt; 1, 'body' =&gt; [ 'title' =&gt; 'Moneyball', 'director' =&gt; 'Bennett Miller', 'year' =&gt; 2011 ] ]); // Search for it var_dump( $client-&gt;search([ 'index' =&gt; $indexName, 'body' =&gt; [ 'size' =&gt; 5, 'query' =&gt; [ 'multi_match' =&gt; [ 'query' =&gt; 'miller', 'fields' =&gt; ['title^2', 'director'] ] ] ] ]) ); // Delete a single document $client-&gt;delete([ 'index' =&gt; $indexName, 'id' =&gt; 1, ]); // Delete index $client-&gt;indices()-&gt;delete([ 'index' =&gt; $indexName ]); ?&gt; . copy . ",
    "url": "https://vagimeli.github.io/clients/php/#sample-program",
    "relUrl": "/clients/php/#sample-program"
  },"1640": {
    "doc": "High-level Python client",
    "title": "High-level Python client",
    "content": "The OpenSearch high-level Python client (opensearch-dsl-py) provides wrapper classes for common OpenSearch entities, like documents, so you can work with them as Python objects. Additionally, the high-level client simplifies writing queries and supplies convenient Python methods for common OpenSearch operations. The high-level Python client supports creating and indexing documents, searching with and without filters, and updating documents using queries. This getting started guide illustrates how to connect to OpenSearch, index documents, and run queries. For the client source code, see the opensearch-dsl-py repo. ",
    "url": "https://vagimeli.github.io/clients/python-high-level/#high-level-python-client",
    "relUrl": "/clients/python-high-level/#high-level-python-client"
  },"1641": {
    "doc": "High-level Python client",
    "title": "Setup",
    "content": "To add the client to your project, install it using pip: . pip install opensearch-dsl . copy . After installing the client, you can import it like any other module: . from opensearchpy import OpenSearch from opensearch_dsl import Search . copy . ",
    "url": "https://vagimeli.github.io/clients/python-high-level/#setup",
    "relUrl": "/clients/python-high-level/#setup"
  },"1642": {
    "doc": "High-level Python client",
    "title": "Connecting to OpenSearch",
    "content": "To connect to the default OpenSearch host, create a client object with SSL enabled if you are using the Security plugin. You can use the default credentials for testing purposes: . host = 'localhost' port = 9200 auth = ('admin', 'admin') # For testing only. Don't store credentials in code. ca_certs_path = '/full/path/to/root-ca.pem' # Provide a CA bundle if you use intermediate CAs with your root CA. # Create the client with SSL/TLS enabled, but hostname verification disabled. client = OpenSearch( hosts = [{'host': host, 'port': port}], http_compress = True, # enables gzip compression for request bodies http_auth = auth, use_ssl = True, verify_certs = True, ssl_assert_hostname = False, ssl_show_warn = False, ca_certs = ca_certs_path ) . copy . If you have your own client certificates, specify them in the client_cert_path and client_key_path parameters: . host = 'localhost' port = 9200 auth = ('admin', 'admin') # For testing only. Don't store credentials in code. ca_certs_path = '/full/path/to/root-ca.pem' # Provide a CA bundle if you use intermediate CAs with your root CA. # Optional client certificates if you don't want to use HTTP basic authentication. client_cert_path = '/full/path/to/client.pem' client_key_path = '/full/path/to/client-key.pem' # Create the client with SSL/TLS enabled, but hostname verification disabled. client = OpenSearch( hosts = [{'host': host, 'port': port}], http_compress = True, # enables gzip compression for request bodies http_auth = auth, client_cert = client_cert_path, client_key = client_key_path, use_ssl = True, verify_certs = True, ssl_assert_hostname = False, ssl_show_warn = False, ca_certs = ca_certs_path ) . copy . If you are not using the Security plugin, create a client object with SSL disabled: . host = 'localhost' port = 9200 # Create the client with SSL/TLS and hostname verification disabled. client = OpenSearch( hosts = [{'host': host, 'port': port}], http_compress = True, # enables gzip compression for request bodies use_ssl = False, verify_certs = False, ssl_assert_hostname = False, ssl_show_warn = False ) . copy . ",
    "url": "https://vagimeli.github.io/clients/python-high-level/#connecting-to-opensearch",
    "relUrl": "/clients/python-high-level/#connecting-to-opensearch"
  },"1643": {
    "doc": "High-level Python client",
    "title": "Creating an index",
    "content": "To create an OpenSearch index, use the client.indices.create() method. You can use the following code to construct a JSON object with custom settings: . index_name = 'my-dsl-index' index_body = { 'settings': { 'index': { 'number_of_shards': 4 } } } response = client.indices.create(index_name, body=index_body) . copy . ",
    "url": "https://vagimeli.github.io/clients/python-high-level/#creating-an-index",
    "relUrl": "/clients/python-high-level/#creating-an-index"
  },"1644": {
    "doc": "High-level Python client",
    "title": "Indexing a document",
    "content": "You can create a class to represent the documents that you’ll index in OpenSearch by extending the Document class: . class Movie(Document): title = Text(fields={'raw': Keyword()}) director = Text() year = Text() class Index: name = index_name def save(self, ** kwargs): return super(Movie, self).save(** kwargs) . copy . To index a document, create an object of the new class and call its save() method: . # Set up the opensearch-py version of the document Movie.init(using=client) doc = Movie(meta={'id': 1}, title='Moneyball', director='Bennett Miller', year='2011') response = doc.save(using=client) . copy . ",
    "url": "https://vagimeli.github.io/clients/python-high-level/#indexing-a-document",
    "relUrl": "/clients/python-high-level/#indexing-a-document"
  },"1645": {
    "doc": "High-level Python client",
    "title": "Performing bulk operations",
    "content": "You can perform several operations at the same time by using the bulk() method of the client. The operations may be of the same type or of different types. Note that the operations must be separated by a \\n and the entire string must be a single line: . movies = '{ \"index\" : { \"_index\" : \"my-dsl-index\", \"_id\" : \"2\" } } \\n { \"title\" : \"Interstellar\", \"director\" : \"Christopher Nolan\", \"year\" : \"2014\"} \\n { \"create\" : { \"_index\" : \"my-dsl-index\", \"_id\" : \"3\" } } \\n { \"title\" : \"Star Trek Beyond\", \"director\" : \"Justin Lin\", \"year\" : \"2015\"} \\n { \"update\" : {\"_id\" : \"3\", \"_index\" : \"my-dsl-index\" } } \\n { \"doc\" : {\"year\" : \"2016\"} }' client.bulk(movies) . copy . ",
    "url": "https://vagimeli.github.io/clients/python-high-level/#performing-bulk-operations",
    "relUrl": "/clients/python-high-level/#performing-bulk-operations"
  },"1646": {
    "doc": "High-level Python client",
    "title": "Searching for documents",
    "content": "You can use the Search class to construct a query. The following code creates a Boolean query with a filter: . s = Search(using=client, index=index_name) \\ .filter(\"term\", year=\"2011\") \\ .query(\"match\", title=\"Moneyball\") response = s.execute() . copy . The preceding query is equivalent to the following query in OpenSearch domain-specific language (DSL): . GET my-dsl-index/_search { \"query\": { \"bool\": { \"must\": { \"match\": { \"title\": \"Moneyball\" } }, \"filter\": { \"term\" : { \"year\": 2011 } } } } } . ",
    "url": "https://vagimeli.github.io/clients/python-high-level/#searching-for-documents",
    "relUrl": "/clients/python-high-level/#searching-for-documents"
  },"1647": {
    "doc": "High-level Python client",
    "title": "Deleting a document",
    "content": "You can delete a document using the client.delete() method: . response = client.delete( index = 'my-dsl-index', id = '1' ) . copy . ",
    "url": "https://vagimeli.github.io/clients/python-high-level/#deleting-a-document",
    "relUrl": "/clients/python-high-level/#deleting-a-document"
  },"1648": {
    "doc": "High-level Python client",
    "title": "Deleting an index",
    "content": "You can delete an index using the client.indices.delete() method: . response = client.indices.delete( index = 'my-dsl-index' ) . copy . ",
    "url": "https://vagimeli.github.io/clients/python-high-level/#deleting-an-index",
    "relUrl": "/clients/python-high-level/#deleting-an-index"
  },"1649": {
    "doc": "High-level Python client",
    "title": "Sample program",
    "content": "The following sample program creates a client, adds an index with non-default settings, inserts a document, performs bulk operations, searches for the document, deletes the document, and then deletes the index: . from opensearchpy import OpenSearch from opensearch_dsl import Search, Document, Text, Keyword host = 'localhost' port = 9200 auth = ('admin', 'admin') # For testing only. Don't store credentials in code. ca_certs_path = 'root-ca.pem' # Create the client with SSL/TLS enabled, but hostname verification disabled. client = OpenSearch( hosts=[{'host': host, 'port': port}], http_compress=True, # enables gzip compression for request bodies # http_auth=auth, use_ssl=False, verify_certs=False, ssl_assert_hostname=False, ssl_show_warn=False, # ca_certs=ca_certs_path ) index_name = 'my-dsl-index' index_body = { 'settings': { 'index': { 'number_of_shards': 4 } } } response = client.indices.create(index_name, index_body) print('\\nCreating index:') print(response) # Create the structure of the document class Movie(Document): title = Text(fields={'raw': Keyword()}) director = Text() year = Text() class Index: name = index_name def save(self, ** kwargs): return super(Movie, self).save(** kwargs) # Set up the opensearch-py version of the document Movie.init(using=client) doc = Movie(meta={'id': 1}, title='Moneyball', director='Bennett Miller', year='2011') response = doc.save(using=client) print('\\nAdding document:') print(response) # Perform bulk operations movies = '{ \"index\" : { \"_index\" : \"my-dsl-index\", \"_id\" : \"2\" } } \\n { \"title\" : \"Interstellar\", \"director\" : \"Christopher Nolan\", \"year\" : \"2014\"} \\n { \"create\" : { \"_index\" : \"my-dsl-index\", \"_id\" : \"3\" } } \\n { \"title\" : \"Star Trek Beyond\", \"director\" : \"Justin Lin\", \"year\" : \"2015\"} \\n { \"update\" : {\"_id\" : \"3\", \"_index\" : \"my-dsl-index\" } } \\n { \"doc\" : {\"year\" : \"2016\"} }' client.bulk(movies) # Search for the document. s = Search(using=client, index=index_name) \\ .filter('term', year='2011') \\ .query('match', title='Moneyball') response = s.execute() print('\\nSearch results:') for hit in response: print(hit.meta.score, hit.title) # Delete the document. print('\\nDeleting document:') print(response) # Delete the index. response = client.indices.delete( index = index_name ) print('\\nDeleting index:') print(response) . copy . ",
    "url": "https://vagimeli.github.io/clients/python-high-level/#sample-program",
    "relUrl": "/clients/python-high-level/#sample-program"
  },"1650": {
    "doc": "High-level Python client",
    "title": "High-level Python client",
    "content": "The OpenSearch high-level Python client (opensearch-dsl-py) will be deprecated after version 2.1.0. We recommend switching to the Python client (opensearch-py), which now includes the functionality of opensearch-dsl-py. ",
    "url": "https://vagimeli.github.io/clients/python-high-level/",
    "relUrl": "/clients/python-high-level/"
  },"1651": {
    "doc": "Low-level Python client",
    "title": "Low-level Python client",
    "content": "The OpenSearch low-level Python client (opensearch-py) provides wrapper methods for the OpenSearch REST API so that you can interact with your cluster more naturally in Python. Rather than sending raw HTTP requests to a given URL, you can create an OpenSearch client for your cluster and call the client’s built-in functions. For the client’s complete API documentation and additional examples, see the opensearch-py API documentation. This getting started guide illustrates how to connect to OpenSearch, index documents, and run queries. For the client source code, see the opensearch-py repo. ",
    "url": "https://vagimeli.github.io/clients/python-low-level/",
    "relUrl": "/clients/python-low-level/"
  },"1652": {
    "doc": "Low-level Python client",
    "title": "Setup",
    "content": "To add the client to your project, install it using pip: . pip install opensearch-py . copy . After installing the client, you can import it like any other module: . from opensearchpy import OpenSearch . copy . ",
    "url": "https://vagimeli.github.io/clients/python-low-level/#setup",
    "relUrl": "/clients/python-low-level/#setup"
  },"1653": {
    "doc": "Low-level Python client",
    "title": "Connecting to OpenSearch",
    "content": "To connect to the default OpenSearch host, create a client object with SSL enabled if you are using the Security plugin. You can use the default credentials for testing purposes: . host = 'localhost' port = 9200 auth = ('admin', 'admin') # For testing only. Don't store credentials in code. ca_certs_path = '/full/path/to/root-ca.pem' # Provide a CA bundle if you use intermediate CAs with your root CA. # Create the client with SSL/TLS enabled, but hostname verification disabled. client = OpenSearch( hosts = [{'host': host, 'port': port}], http_compress = True, # enables gzip compression for request bodies http_auth = auth, use_ssl = True, verify_certs = True, ssl_assert_hostname = False, ssl_show_warn = False, ca_certs = ca_certs_path ) . copy . If you have your own client certificates, specify them in the client_cert_path and client_key_path parameters: . host = 'localhost' port = 9200 auth = ('admin', 'admin') # For testing only. Don't store credentials in code. ca_certs_path = '/full/path/to/root-ca.pem' # Provide a CA bundle if you use intermediate CAs with your root CA. # Optional client certificates if you don't want to use HTTP basic authentication. client_cert_path = '/full/path/to/client.pem' client_key_path = '/full/path/to/client-key.pem' # Create the client with SSL/TLS enabled, but hostname verification disabled. client = OpenSearch( hosts = [{'host': host, 'port': port}], http_compress = True, # enables gzip compression for request bodies http_auth = auth, client_cert = client_cert_path, client_key = client_key_path, use_ssl = True, verify_certs = True, ssl_assert_hostname = False, ssl_show_warn = False, ca_certs = ca_certs_path ) . copy . If you are not using the Security plugin, create a client object with SSL disabled: . host = 'localhost' port = 9200 # Create the client with SSL/TLS and hostname verification disabled. client = OpenSearch( hosts = [{'host': host, 'port': port}], http_compress = True, # enables gzip compression for request bodies use_ssl = False, verify_certs = False, ssl_assert_hostname = False, ssl_show_warn = False ) . copy . ",
    "url": "https://vagimeli.github.io/clients/python-low-level/#connecting-to-opensearch",
    "relUrl": "/clients/python-low-level/#connecting-to-opensearch"
  },"1654": {
    "doc": "Low-level Python client",
    "title": "Connecting to Amazon OpenSearch Service",
    "content": "The following example illustrates connecting to Amazon OpenSearch Service: . from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth import boto3 host = '' # cluster endpoint, for example: my-test-domain.us-east-1.es.amazonaws.com region = 'us-west-2' service = 'es' credentials = boto3.Session().get_credentials() auth = AWSV4SignerAuth(credentials, region, service) client = OpenSearch( hosts = [{'host': host, 'port': 443}], http_auth = auth, use_ssl = True, verify_certs = True, connection_class = RequestsHttpConnection, pool_maxsize = 20 ) . copy . ",
    "url": "https://vagimeli.github.io/clients/python-low-level/#connecting-to-amazon-opensearch-service",
    "relUrl": "/clients/python-low-level/#connecting-to-amazon-opensearch-service"
  },"1655": {
    "doc": "Low-level Python client",
    "title": "Connecting to Amazon OpenSearch Serverless",
    "content": "The following example illustrates connecting to Amazon OpenSearch Serverless Service: . from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth import boto3 host = '' # cluster endpoint, for example: my-test-domain.us-east-1.aoss.amazonaws.com region = 'us-west-2' service = 'aoss' credentials = boto3.Session().get_credentials() auth = AWSV4SignerAuth(credentials, region, service) client = OpenSearch( hosts = [{'host': host, 'port': 443}], http_auth = auth, use_ssl = True, verify_certs = True, connection_class = RequestsHttpConnection, pool_maxsize = 20 ) . copy . ",
    "url": "https://vagimeli.github.io/clients/python-low-level/#connecting-to-amazon-opensearch-serverless",
    "relUrl": "/clients/python-low-level/#connecting-to-amazon-opensearch-serverless"
  },"1656": {
    "doc": "Low-level Python client",
    "title": "Creating an index",
    "content": "To create an OpenSearch index, use the client.indices.create() method. You can use the following code to construct a JSON object with custom settings: . index_name = 'python-test-index' index_body = { 'settings': { 'index': { 'number_of_shards': 4 } } } response = client.indices.create(index_name, body=index_body) . copy . ",
    "url": "https://vagimeli.github.io/clients/python-low-level/#creating-an-index",
    "relUrl": "/clients/python-low-level/#creating-an-index"
  },"1657": {
    "doc": "Low-level Python client",
    "title": "Indexing a document",
    "content": "You can index a document using the client.index() method: . document = { 'title': 'Moneyball', 'director': 'Bennett Miller', 'year': '2011' } response = client.index( index = 'python-test-index', body = document, id = '1', refresh = True ) . copy . ",
    "url": "https://vagimeli.github.io/clients/python-low-level/#indexing-a-document",
    "relUrl": "/clients/python-low-level/#indexing-a-document"
  },"1658": {
    "doc": "Low-level Python client",
    "title": "Performing bulk operations",
    "content": "You can perform several operations at the same time by using the bulk() method of the client. The operations may be of the same type or of different types. Note that the operations must be separated by a \\n and the entire string must be a single line: . movies = '{ \"index\" : { \"_index\" : \"my-dsl-index\", \"_id\" : \"2\" } } \\n { \"title\" : \"Interstellar\", \"director\" : \"Christopher Nolan\", \"year\" : \"2014\"} \\n { \"create\" : { \"_index\" : \"my-dsl-index\", \"_id\" : \"3\" } } \\n { \"title\" : \"Star Trek Beyond\", \"director\" : \"Justin Lin\", \"year\" : \"2015\"} \\n { \"update\" : {\"_id\" : \"3\", \"_index\" : \"my-dsl-index\" } } \\n { \"doc\" : {\"year\" : \"2016\"} }' client.bulk(movies) . copy . ",
    "url": "https://vagimeli.github.io/clients/python-low-level/#performing-bulk-operations",
    "relUrl": "/clients/python-low-level/#performing-bulk-operations"
  },"1659": {
    "doc": "Low-level Python client",
    "title": "Searching for documents",
    "content": "The easiest way to search for documents is to construct a query string. The following code uses a multi-match query to search for “miller” in the title and director fields. It boosts the documents that have “miller” in the title field: . q = 'miller' query = { 'size': 5, 'query': { 'multi_match': { 'query': q, 'fields': ['title^2', 'director'] } } } response = client.search( body = query, index = 'python-test-index' ) . copy . ",
    "url": "https://vagimeli.github.io/clients/python-low-level/#searching-for-documents",
    "relUrl": "/clients/python-low-level/#searching-for-documents"
  },"1660": {
    "doc": "Low-level Python client",
    "title": "Deleting a document",
    "content": "You can delete a document using the client.delete() method: . response = client.delete( index = 'python-test-index', id = '1' ) . copy . ",
    "url": "https://vagimeli.github.io/clients/python-low-level/#deleting-a-document",
    "relUrl": "/clients/python-low-level/#deleting-a-document"
  },"1661": {
    "doc": "Low-level Python client",
    "title": "Deleting an index",
    "content": "You can delete an index using the client.indices.delete() method: . response = client.indices.delete( index = 'python-test-index' ) . copy . ",
    "url": "https://vagimeli.github.io/clients/python-low-level/#deleting-an-index",
    "relUrl": "/clients/python-low-level/#deleting-an-index"
  },"1662": {
    "doc": "Low-level Python client",
    "title": "Sample program",
    "content": "The following sample program creates a client, adds an index with non-default settings, inserts a document, performs bulk operations, searches for the document, deletes the document, and then deletes the index: . from opensearchpy import OpenSearch host = 'localhost' port = 9200 auth = ('admin', 'admin') # For testing only. Don't store credentials in code. ca_certs_path = '/full/path/to/root-ca.pem' # Provide a CA bundle if you use intermediate CAs with your root CA. # Optional client certificates if you don't want to use HTTP basic authentication. # client_cert_path = '/full/path/to/client.pem' # client_key_path = '/full/path/to/client-key.pem' # Create the client with SSL/TLS enabled, but hostname verification disabled. client = OpenSearch( hosts = [{'host': host, 'port': port}], http_compress = True, # enables gzip compression for request bodies http_auth = auth, # client_cert = client_cert_path, # client_key = client_key_path, use_ssl = True, verify_certs = True, ssl_assert_hostname = False, ssl_show_warn = False, ca_certs = ca_certs_path ) # Create an index with non-default settings. index_name = 'python-test-index' index_body = { 'settings': { 'index': { 'number_of_shards': 4 } } } response = client.indices.create(index_name, body=index_body) print('\\nCreating index:') print(response) # Add a document to the index. document = { 'title': 'Moneyball', 'director': 'Bennett Miller', 'year': '2011' } id = '1' response = client.index( index = index_name, body = document, id = id, refresh = True ) print('\\nAdding document:') print(response) # Perform bulk operations movies = '{ \"index\" : { \"_index\" : \"my-dsl-index\", \"_id\" : \"2\" } } \\n { \"title\" : \"Interstellar\", \"director\" : \"Christopher Nolan\", \"year\" : \"2014\"} \\n { \"create\" : { \"_index\" : \"my-dsl-index\", \"_id\" : \"3\" } } \\n { \"title\" : \"Star Trek Beyond\", \"director\" : \"Justin Lin\", \"year\" : \"2015\"} \\n { \"update\" : {\"_id\" : \"3\", \"_index\" : \"my-dsl-index\" } } \\n { \"doc\" : {\"year\" : \"2016\"} }' client.bulk(movies) # Search for the document. q = 'miller' query = { 'size': 5, 'query': { 'multi_match': { 'query': q, 'fields': ['title^2', 'director'] } } } response = client.search( body = query, index = index_name ) print('\\nSearch results:') print(response) # Delete the document. response = client.delete( index = index_name, id = id ) print('\\nDeleting document:') print(response) # Delete the index. response = client.indices.delete( index = index_name ) print('\\nDeleting index:') print(response) . copy . ",
    "url": "https://vagimeli.github.io/clients/python-low-level/#sample-program",
    "relUrl": "/clients/python-low-level/#sample-program"
  },"1663": {
    "doc": "Ruby client",
    "title": "Ruby client",
    "content": "The OpenSearch Ruby client allows you to interact with your OpenSearch clusters through Ruby methods rather than HTTP methods and raw JSON. For the client’s complete API documentation and additional examples, see the opensearch-transport, opensearch-api, opensearch-dsl, and opensearch-ruby gem documentation. This getting started guide illustrates how to connect to OpenSearch, index documents, and run queries. For the client source code, see the opensearch-ruby repo. ",
    "url": "https://vagimeli.github.io/clients/ruby/",
    "relUrl": "/clients/ruby/"
  },"1664": {
    "doc": "Ruby client",
    "title": "Installing the Ruby client",
    "content": "To install the Ruby gem for the Ruby client, run the following command: . gem install opensearch-ruby . copy . To use the client, import it as a module: . require 'opensearch' . copy . ",
    "url": "https://vagimeli.github.io/clients/ruby/#installing-the-ruby-client",
    "relUrl": "/clients/ruby/#installing-the-ruby-client"
  },"1665": {
    "doc": "Ruby client",
    "title": "Connecting to OpenSearch",
    "content": "To connect to the default OpenSearch host, create a client object, passing the default host address in the constructor: . client = OpenSearch::Client.new(host: 'http://localhost:9200') . copy . The following example creates a client object with a custom URL and the log option set to true. It sets the retry_on_failure parameter to retry a failed request five times rather than the default three times. Finally, it increases the timeout by setting the request_timeout parameter to 120 seconds. It then returns the basic cluster health information: . client = OpenSearch::Client.new( url: \"http://localhost:9200\", retry_on_failure: 5, request_timeout: 120, log: true ) client.cluster.health . copy . The output is as follows: . 2022-08-25 14:24:52 -0400: GET http://localhost:9200/ [status:200, request:0.048s, query:n/a] 2022-08-25 14:24:52 -0400: &lt; { \"name\" : \"opensearch\", \"cluster_name\" : \"docker-cluster\", \"cluster_uuid\" : \"Aw0F5Pt9QF6XO9vXQHIs_w\", \"version\" : { \"distribution\" : \"opensearch\", \"number\" : \"2.2.0\", \"build_type\" : \"tar\", \"build_hash\" : \"b1017fa3b9a1c781d4f34ecee411e0cdf930a515\", \"build_date\" : \"2022-08-09T02:27:25.256769336Z\", \"build_snapshot\" : false, \"lucene_version\" : \"9.3.0\", \"minimum_wire_compatibility_version\" : \"7.10.0\", \"minimum_index_compatibility_version\" : \"7.0.0\" }, \"tagline\" : \"The OpenSearch Project: https://opensearch.org/\" } 2022-08-25 14:24:52 -0400: GET http://localhost:9200/_cluster/health [status:200, request:0.018s, query:n/a] 2022-08-25 14:24:52 -0400: &lt; {\"cluster_name\":\"docker-cluster\",\"status\":\"yellow\",\"timed_out\":false,\"number_of_nodes\":1,\"number_of_data_nodes\":1,\"discovered_master\":true,\"discovered_cluster_manager\":true,\"active_primary_shards\":10,\"active_shards\":10,\"relocating_shards\":0,\"initializing_shards\":0,\"unassigned_shards\":8,\"delayed_unassigned_shards\":0,\"number_of_pending_tasks\":0,\"number_of_in_flight_fetch\":0,\"task_max_waiting_in_queue_millis\":0,\"active_shards_percent_as_number\":55.55555555555556} . ",
    "url": "https://vagimeli.github.io/clients/ruby/#connecting-to-opensearch",
    "relUrl": "/clients/ruby/#connecting-to-opensearch"
  },"1666": {
    "doc": "Ruby client",
    "title": "Connecting to Amazon OpenSearch Service",
    "content": "To connect to Amazon OpenSearch Service, first install the opensearch-aws-sigv4 gem: . gem install opensearch-aws-sigv4 . require 'opensearch-aws-sigv4' require 'aws-sigv4' signer = Aws::Sigv4::Signer.new(service: 'es', region: 'us-west-2', # signing service region access_key_id: 'key_id', secret_access_key: 'secret') client = OpenSearch::Aws::Sigv4Client.new({ host: 'https://your.amz-managed-opensearch.domain', log: true }, signer) # create an index and document index = 'prime' client.indices.create(index: index) client.index(index: index, id: '1', body: { name: 'Amazon Echo', msrp: '5999', year: 2011 }) # search for the document client.search(body: { query: { match: { name: 'Echo' } } }) # delete the document client.delete(index: index, id: '1') # delete the index client.indices.delete(index: index) . copy . ",
    "url": "https://vagimeli.github.io/clients/ruby/#connecting-to-amazon-opensearch-service",
    "relUrl": "/clients/ruby/#connecting-to-amazon-opensearch-service"
  },"1667": {
    "doc": "Ruby client",
    "title": "Connecting to Amazon OpenSearch Serverless",
    "content": "To connect to Amazon OpenSearch Serverless Service, first install the opensearch-aws-sigv4 gem: . gem install opensearch-aws-sigv4 . require 'opensearch-aws-sigv4' require 'aws-sigv4' signer = Aws::Sigv4::Signer.new(service: 'aoss', region: 'us-west-2', # signing service region access_key_id: 'key_id', secret_access_key: 'secret') client = OpenSearch::Aws::Sigv4Client.new({ host: 'https://your.amz-managed-opensearch.domain', # serverless endpoint for OpenSearch Serverless log: true }, signer) # create an index and document index = 'prime' client.indices.create(index: index) client.index(index: index, id: '1', body: { name: 'Amazon Echo', msrp: '5999', year: 2011 }) # search for the document client.search(body: { query: { match: { name: 'Echo' } } }) # delete the document client.delete(index: index, id: '1') # delete the index client.indices.delete(index: index) . copy . ",
    "url": "https://vagimeli.github.io/clients/ruby/#connecting-to-amazon-opensearch-serverless",
    "relUrl": "/clients/ruby/#connecting-to-amazon-opensearch-serverless"
  },"1668": {
    "doc": "Ruby client",
    "title": "Creating an index",
    "content": "You don’t need to create an index explicitly in OpenSearch. Once you upload a document into an index that does not exist, OpenSearch creates the index automatically. Alternatively, you can create an index explicitly to specify settings like the number of primary and replica shards. To create an index with non-default settings, create an index body hash with those settings: . index_body = { 'settings': { 'index': { 'number_of_shards': 1, 'number_of_replicas': 2 } } } client.indices.create( index: 'students', body: index_body ) . copy . ",
    "url": "https://vagimeli.github.io/clients/ruby/#creating-an-index",
    "relUrl": "/clients/ruby/#creating-an-index"
  },"1669": {
    "doc": "Ruby client",
    "title": "Mappings",
    "content": "OpenSearch uses dynamic mapping to infer field types of the documents that are indexed. However, to have more control over the schema of your document, you can pass an explicit mapping to OpenSearch. You can define data types for some or all fields of your document in this mapping. To create a mapping for an index, use the put_mapping method: . client.indices.put_mapping( index: 'students', body: { properties: { first_name: { type: 'keyword' }, last_name: { type: 'keyword' } } } ) . copy . By default, string fields are mapped as text, but in the mapping above, the first_name and last_name fields are mapped as keyword. This mapping signals to OpenSearch that these fields should not be analyzed and should support only full case-sensitive matches. You can verify the index’s mappings using the get_mapping method: . response = client.indices.get_mapping(index: 'students') . copy . If you know the mapping of your documents in advance and want to avoid mapping errors (for example, misspellings of a field name), you can set the dynamic parameter to strict: . client.indices.put_mapping( index: 'students', body: { dynamic: 'strict', properties: { first_name: { type: 'keyword' }, last_name: { type: 'keyword' }, gpa: { type: 'float'}, grad_year: { type: 'integer'} } } ) . copy . With strict mapping, you can index a document with a missing field, but you cannot index a document with a new field. For example, indexing the following document with a misspelled grad_yea field fails: . document = { first_name: 'Connor', last_name: 'James', gpa: 3.93, grad_yea: 2021 } client.index( index: 'students', body: document, id: 100, refresh: true ) . copy . OpenSearch returns a mapping error: . {\"error\":{\"root_cause\":[{\"type\":\"strict_dynamic_mapping_exception\",\"reason\":\"mapping set to strict, dynamic introduction of [grad_yea] within [_doc] is not allowed\"}],\"type\":\"strict_dynamic_mapping_exception\",\"reason\":\"mapping set to strict, dynamic introduction of [grad_yea] within [_doc] is not allowed\"},\"status\":400} . ",
    "url": "https://vagimeli.github.io/clients/ruby/#mappings",
    "relUrl": "/clients/ruby/#mappings"
  },"1670": {
    "doc": "Ruby client",
    "title": "Indexing one document",
    "content": "To index one document, use the index method: . document = { first_name: 'Connor', last_name: 'James', gpa: 3.93, grad_year: 2021 } client.index( index: 'students', body: document, id: 100, refresh: true ) . copy . ",
    "url": "https://vagimeli.github.io/clients/ruby/#indexing-one-document",
    "relUrl": "/clients/ruby/#indexing-one-document"
  },"1671": {
    "doc": "Ruby client",
    "title": "Updating a document",
    "content": "To update a document, use the update method: . client.update(index: 'students', id: 100, body: { doc: { gpa: 3.25 } }, refresh: true) . copy . ",
    "url": "https://vagimeli.github.io/clients/ruby/#updating-a-document",
    "relUrl": "/clients/ruby/#updating-a-document"
  },"1672": {
    "doc": "Ruby client",
    "title": "Deleting a document",
    "content": "To delete a document, use the delete method: . client.delete( index: 'students', id: 100, refresh: true ) . copy . ",
    "url": "https://vagimeli.github.io/clients/ruby/#deleting-a-document",
    "relUrl": "/clients/ruby/#deleting-a-document"
  },"1673": {
    "doc": "Ruby client",
    "title": "Bulk operations",
    "content": "You can perform several operations at the same time by using the bulk method. The operations may be of the same type or of different types. You can index multiple documents using the bulk method: . actions = [ { index: { _index: 'students', _id: '200' } }, { first_name: 'James', last_name: 'Rodriguez', gpa: 3.91, grad_year: 2019 }, { index: { _index: 'students', _id: '300' } }, { first_name: 'Nikki', last_name: 'Wolf', gpa: 3.87, grad_year: 2020 } ] client.bulk(body: actions, refresh: true) . copy . You can delete multiple documents as follows: . # Deleting multiple documents. actions = [ { delete: { _index: 'students', _id: 200 } }, { delete: { _index: 'students', _id: 300 } } ] client.bulk(body: actions, refresh: true) . copy . You can perform different operations when using bulk as follows: . actions = [ { index: { _index: 'students', _id: 100, data: { first_name: 'Paulo', last_name: 'Santos', gpa: 3.29, grad_year: 2022 } } }, { index: { _index: 'students', _id: 200, data: { first_name: 'Shirley', last_name: 'Rodriguez', gpa: 3.92, grad_year: 2020 } } }, { index: { _index: 'students', _id: 300, data: { first_name: 'Akua', last_name: 'Mansa', gpa: 3.95, grad_year: 2022 } } }, { index: { _index: 'students', _id: 400, data: { first_name: 'John', last_name: 'Stiles', gpa: 3.72, grad_year: 2019 } } }, { index: { _index: 'students', _id: 500, data: { first_name: 'Li', last_name: 'Juan', gpa: 3.94, grad_year: 2022 } } }, { index: { _index: 'students', _id: 600, data: { first_name: 'Richard', last_name: 'Roe', gpa: 3.04, grad_year: 2020 } } }, { update: { _index: 'students', _id: 100, data: { doc: { gpa: 3.73 } } } }, { delete: { _index: 'students', _id: 200 } } ] client.bulk(body: actions, refresh: true) . copy . In the above example, you pass the data and the header together and you denote the data with the data: key. ",
    "url": "https://vagimeli.github.io/clients/ruby/#bulk-operations",
    "relUrl": "/clients/ruby/#bulk-operations"
  },"1674": {
    "doc": "Ruby client",
    "title": "Searching for a document",
    "content": "To search for a document, use the search method. The following example searches for a student whose first or last name is “James.” It uses a multi_match query to search for two fields (first_name and last_name), and it is boosting the last_name field in relevance with a caret notation (last_name^2). q = 'James' query = { 'size': 5, 'query': { 'multi_match': { 'query': q, 'fields': ['first_name', 'last_name^2'] } } } response = client.search( body: query, index: 'students' ) . copy . If you omit the request body in the search method, your query becomes a match_all query and returns all documents in the index: . client.search(index: 'students') . copy . ",
    "url": "https://vagimeli.github.io/clients/ruby/#searching-for-a-document",
    "relUrl": "/clients/ruby/#searching-for-a-document"
  },"1675": {
    "doc": "Ruby client",
    "title": "Boolean query",
    "content": "The Ruby client exposes full OpenSearch query capability. In addition to simple searches that use the match query, you can create a more complex Boolean query to search for students who graduated in 2022 and sort them by last name. In the example below, search is limited to 10 documents. query = { 'query': { 'bool': { 'filter': { 'term': { 'grad_year': 2022 } } } }, 'sort': { 'last_name': { 'order': 'asc' } } } response = client.search(index: 'students', from: 0, size: 10, body: query) . copy . ",
    "url": "https://vagimeli.github.io/clients/ruby/#boolean-query",
    "relUrl": "/clients/ruby/#boolean-query"
  },"1676": {
    "doc": "Ruby client",
    "title": "Multi-search",
    "content": "You can bulk several queries together and perform a multi-search using the msearch method. The following code searches for students whose GPAs are outside the 3.1–3.9 range: . actions = [ {}, {query: {range: {gpa: {gt: 3.9}}}}, {}, {query: {range: {gpa: {lt: 3.1}}}} ] response = client.msearch(index: 'students', body: actions) . copy . ",
    "url": "https://vagimeli.github.io/clients/ruby/#multi-search",
    "relUrl": "/clients/ruby/#multi-search"
  },"1677": {
    "doc": "Ruby client",
    "title": "Scroll",
    "content": "You can paginate your search results using the Scroll API: . response = client.search(index: index_name, scroll: '2m', size: 2) while response['hits']['hits'].size.positive? scroll_id = response['_scroll_id'] puts(response['hits']['hits'].map { |doc| [doc['_source']['first_name'] + ' ' + doc['_source']['last_name']] }) response = client.scroll(scroll: '1m', body: { scroll_id: scroll_id }) end . copy . First, you issue a search query, specifying the scroll and size parameters. The scroll parameter tells OpenSearch how long to keep the search context. In this case, it is set to two minutes. The size parameter specifies how many documents you want to return in each request. The response to the initial search query contains a _scroll_id that you can use to get the next set of documents. To do this, you use the scroll method, again specifying the scroll parameter and passing the _scroll_id in the body. You don’t need to specify the query or index to the scroll method. The scroll method returns the next set of documents and the _scroll_id. It’s important to use the latest _scroll_id when requesting the next batch of documents because _scroll_id can change between requests. ",
    "url": "https://vagimeli.github.io/clients/ruby/#scroll",
    "relUrl": "/clients/ruby/#scroll"
  },"1678": {
    "doc": "Ruby client",
    "title": "Deleting an index",
    "content": "You can delete the index using the delete method: . response = client.indices.delete(index: index_name) . copy . ",
    "url": "https://vagimeli.github.io/clients/ruby/#deleting-an-index",
    "relUrl": "/clients/ruby/#deleting-an-index"
  },"1679": {
    "doc": "Ruby client",
    "title": "Sample program",
    "content": "The following is a complete sample program that illustrates all of the concepts described in the preceding sections. The Ruby client’s methods return responses as Ruby hashes, which are hard to read. To display JSON responses in a pretty format, the sample program uses the MultiJson.dump method. require 'opensearch' client = OpenSearch::Client.new(host: 'http://localhost:9200') # Create an index with non-default settings index_name = 'students' index_body = { 'settings': { 'index': { 'number_of_shards': 1, 'number_of_replicas': 2 } } } client.indices.create( index: index_name, body: index_body ) # Create a mapping client.indices.put_mapping( index: index_name, body: { properties: { first_name: { type: 'keyword' }, last_name: { type: 'keyword' } } } ) # Get mappings response = client.indices.get_mapping(index: index_name) puts 'Mappings for the students index:' puts MultiJson.dump(response, pretty: \"true\") # Add one document to the index puts 'Adding one document:' document = { first_name: 'Connor', last_name: 'James', gpa: 3.93, grad_year: 2021 } id = 100 client.index( index: index_name, body: document, id: id, refresh: true ) response = client.search(index: index_name) puts MultiJson.dump(response, pretty: \"true\") # Update a document puts 'Updating a document:' client.update(index: index_name, id: id, body: { doc: { gpa: 3.25 } }, refresh: true) response = client.search(index: index_name) puts MultiJson.dump(response, pretty: \"true\") print 'The updated gpa is ' puts response['hits']['hits'].map { |doc| doc['_source']['gpa'] } # Add many documents in bulk documents = [ { index: { _index: index_name, _id: '200' } }, { first_name: 'James', last_name: 'Rodriguez', gpa: 3.91, grad_year: 2019}, { index: { _index: index_name, _id: '300' } }, { first_name: 'Nikki', last_name: 'Wolf', gpa: 3.87, grad_year: 2020} ] client.bulk(body: documents, refresh: true) # Get all documents in the index response = client.search(index: index_name) puts 'All documents in the index after bulk upload:' puts MultiJson.dump(response, pretty: \"true\") # Search for a document using a multi_match query puts 'Searching for documents that match \"James\":' q = 'James' query = { 'size': 5, 'query': { 'multi_match': { 'query': q, 'fields': ['first_name', 'last_name^2'] } } } response = client.search( body: query, index: index_name ) puts MultiJson.dump(response, pretty: \"true\") # Delete the document response = client.delete( index: index_name, id: id, refresh: true ) response = client.search(index: index_name) puts 'Documents in the index after one document was deleted:' puts MultiJson.dump(response, pretty: \"true\") # Delete multiple documents actions = [ { delete: { _index: index_name, _id: 200 } }, { delete: { _index: index_name, _id: 300 } } ] client.bulk(body: actions, refresh: true) response = client.search(index: index_name) puts 'Documents in the index after all documents were deleted:' puts MultiJson.dump(response, pretty: \"true\") # Bulk several operations together actions = [ { index: { _index: index_name, _id: 100, data: { first_name: 'Paulo', last_name: 'Santos', gpa: 3.29, grad_year: 2022 } } }, { index: { _index: index_name, _id: 200, data: { first_name: 'Shirley', last_name: 'Rodriguez', gpa: 3.92, grad_year: 2020 } } }, { index: { _index: index_name, _id: 300, data: { first_name: 'Akua', last_name: 'Mansa', gpa: 3.95, grad_year: 2022 } } }, { index: { _index: index_name, _id: 400, data: { first_name: 'John', last_name: 'Stiles', gpa: 3.72, grad_year: 2019 } } }, { index: { _index: index_name, _id: 500, data: { first_name: 'Li', last_name: 'Juan', gpa: 3.94, grad_year: 2022 } } }, { index: { _index: index_name, _id: 600, data: { first_name: 'Richard', last_name: 'Roe', gpa: 3.04, grad_year: 2020 } } }, { update: { _index: index_name, _id: 100, data: { doc: { gpa: 3.73 } } } }, { delete: { _index: index_name, _id: 200 } } ] client.bulk(body: actions, refresh: true) puts 'All documents in the index after bulk operations with scrolling:' response = client.search(index: index_name, scroll: '2m', size: 2) while response['hits']['hits'].size.positive? scroll_id = response['_scroll_id'] puts(response['hits']['hits'].map { |doc| [doc['_source']['first_name'] + ' ' + doc['_source']['last_name']] }) response = client.scroll(scroll: '1m', body: { scroll_id: scroll_id }) end # Multi search actions = [ {}, {query: {range: {gpa: {gt: 3.9}}}}, {}, {query: {range: {gpa: {lt: 3.1}}}} ] response = client.msearch(index: index_name, body: actions) puts 'Multi search results:' puts MultiJson.dump(response, pretty: \"true\") # Boolean query query = { 'query': { 'bool': { 'filter': { 'term': { 'grad_year': 2022 } } } }, 'sort': { 'last_name': { 'order': 'asc' } } } response = client.search(index: index_name, from: 0, size: 10, body: query) puts 'Boolean query search results:' puts MultiJson.dump(response, pretty: \"true\") # Delete the index puts 'Deleting the index:' response = client.indices.delete(index: index_name) puts MultiJson.dump(response, pretty: \"true\") . copy . ",
    "url": "https://vagimeli.github.io/clients/ruby/#sample-program",
    "relUrl": "/clients/ruby/#sample-program"
  },"1680": {
    "doc": "Ruby client",
    "title": "Ruby AWS Sigv4 Client",
    "content": "The opensearch-aws-sigv4 gem provides the OpenSearch::Aws::Sigv4Client class, which has all features of OpenSearch::Client. The only difference between these two clients is that OpenSearch::Aws::Sigv4Client requires an instance of Aws::Sigv4::Signer during instantiation to authenticate with AWS: . require 'opensearch-aws-sigv4' require 'aws-sigv4' signer = Aws::Sigv4::Signer.new(service: 'es', region: 'us-west-2', access_key_id: 'key_id', secret_access_key: 'secret') client = OpenSearch::Aws::Sigv4Client.new({ log: true }, signer) client.cluster.health client.transport.reload_connections! client.search q: 'test' . copy . ",
    "url": "https://vagimeli.github.io/clients/ruby/#ruby-aws-sigv4-client",
    "relUrl": "/clients/ruby/#ruby-aws-sigv4-client"
  },"1681": {
    "doc": "Rust client",
    "title": "Rust client",
    "content": "The OpenSearch Rust client lets you connect your Rust application with the data in your OpenSearch cluster. For the client’s complete API documentation and additional examples, see the OpenSearch docs.rs documentation. This getting started guide illustrates how to connect to OpenSearch, index documents, and run queries. For the client source code, see the opensearch-rs repo. ",
    "url": "https://vagimeli.github.io/clients/rust/",
    "relUrl": "/clients/rust/"
  },"1682": {
    "doc": "Rust client",
    "title": "Setup",
    "content": "If you’re starting a new project, add the opensearch crate to Cargo.toml: . [dependencies] opensearch = \"1.0.0\" . copy . Additionally, you may want to add the following serde dependencies that help serialize types to JSON and deserialize JSON responses: . serde = \"~1\" serde_json = \"~1\" . copy . The Rust client uses the higher-level reqwest HTTP client library for HTTP requests, and reqwest uses the tokio platform to support asynchronous requests. If you are planning to use asynchronous functions, you need to add the tokio dependency to Cargo.toml: . tokio = { version = \"*\", features = [\"full\"] } . copy . See the Sample program section for the complete Cargo.toml file. To use the Rust client API, import the modules, structs, and enums you need: . use opensearch::OpenSearch; . copy . ",
    "url": "https://vagimeli.github.io/clients/rust/#setup",
    "relUrl": "/clients/rust/#setup"
  },"1683": {
    "doc": "Rust client",
    "title": "Connecting to OpenSearch",
    "content": "To connect to the default OpenSearch host, create a default client object that connects to OpenSearch at the address http://localhost:9200: . let client = OpenSearch::default(); . copy . To connect to an OpenSearch host that is running at a different address, create a client with the specified address: . let transport = Transport::single_node(\"http://localhost:9200\")?; let client = OpenSearch::new(transport); . copy . Alternatively, you can customize the URL and use a connection pool by creating a TransportBuilder struct and passing it to OpenSearch::new to create a new instance of the client: . let url = Url::parse(\"http://localhost:9200\")?; let conn_pool = SingleNodeConnectionPool::new(url); let transport = TransportBuilder::new(conn_pool).disable_proxy().build()?; let client = OpenSearch::new(transport); . copy . ",
    "url": "https://vagimeli.github.io/clients/rust/#connecting-to-opensearch",
    "relUrl": "/clients/rust/#connecting-to-opensearch"
  },"1684": {
    "doc": "Rust client",
    "title": "Connecting to Amazon OpenSearch Service",
    "content": "The following example illustrates connecting to Amazon OpenSearch Service: . let url = Url::parse(\"https://...\"); let service_name = \"es\"; let conn_pool = SingleNodeConnectionPool::new(url?); let region_provider = RegionProviderChain::default_provider().or_else(\"us-east-1\"); let aws_config = aws_config::from_env().region(region_provider).load().await.clone(); let transport = TransportBuilder::new(conn_pool) .auth(aws_config.clone().try_into()?) .service_name(service_name) .build()?; let client = OpenSearch::new(transport); . copy . ",
    "url": "https://vagimeli.github.io/clients/rust/#connecting-to-amazon-opensearch-service",
    "relUrl": "/clients/rust/#connecting-to-amazon-opensearch-service"
  },"1685": {
    "doc": "Rust client",
    "title": "Connecting to Amazon OpenSearch Serverless",
    "content": "The following example illustrates connecting to Amazon OpenSearch Serverless Service: . let url = Url::parse(\"https://...\"); let service_name = \"aoss\"; let conn_pool = SingleNodeConnectionPool::new(url?); let region_provider = RegionProviderChain::default_provider().or_else(\"us-east-1\"); let aws_config = aws_config::from_env().region(region_provider).load().await.clone(); let transport = TransportBuilder::new(conn_pool) .auth(aws_config.clone().try_into()?) .service_name(service_name) .build()?; let client = OpenSearch::new(transport); . copy . ",
    "url": "https://vagimeli.github.io/clients/rust/#connecting-to-amazon-opensearch-serverless",
    "relUrl": "/clients/rust/#connecting-to-amazon-opensearch-serverless"
  },"1686": {
    "doc": "Rust client",
    "title": "Creating an index",
    "content": "To create an OpenSearch index, use the create function of the opensearch::indices::Indices struct. You can use the following code to construct a JSON object with custom mappings: . let response = client .indices() .create(IndicesCreateParts::Index(\"movies\")) .body(json!({ \"mappings\" : { \"properties\" : { \"title\" : { \"type\" : \"text\" } } } })) .send() .await?; . copy . ",
    "url": "https://vagimeli.github.io/clients/rust/#creating-an-index",
    "relUrl": "/clients/rust/#creating-an-index"
  },"1687": {
    "doc": "Rust client",
    "title": "Indexing a document",
    "content": "You can index a document into OpenSearch using the client’s index function: . let response = client .index(IndexParts::IndexId(\"movies\", \"1\")) .body(json!({ \"id\": 1, \"title\": \"Moneyball\", \"director\": \"Bennett Miller\", \"year\": \"2011\" })) .send() .await?; . copy . ",
    "url": "https://vagimeli.github.io/clients/rust/#indexing-a-document",
    "relUrl": "/clients/rust/#indexing-a-document"
  },"1688": {
    "doc": "Rust client",
    "title": "Performing bulk operations",
    "content": "You can perform several operations at the same time by using the client’s bulk function. First, create the JSON body of a Bulk API call, and then pass it to the bulk function: . let mut body: Vec&lt;JsonBody&lt;_&gt;&gt; = Vec::with_capacity(4); // add the first operation and document body.push(json!({\"index\": {\"_id\": \"2\"}}).into()); body.push(json!({ \"id\": 2, \"title\": \"Interstellar\", \"director\": \"Christopher Nolan\", \"year\": \"2014\" }).into()); // add the second operation and document body.push(json!({\"index\": {\"_id\": \"3\"}}).into()); body.push(json!({ \"id\": 3, \"title\": \"Star Trek Beyond\", \"director\": \"Justin Lin\", \"year\": \"2015\" }).into()); let response = client .bulk(BulkParts::Index(\"movies\")) .body(body) .send() .await?; . copy . ",
    "url": "https://vagimeli.github.io/clients/rust/#performing-bulk-operations",
    "relUrl": "/clients/rust/#performing-bulk-operations"
  },"1689": {
    "doc": "Rust client",
    "title": "Searching for documents",
    "content": "The easiest way to search for documents is to construct a query string. The following code uses a multi_match query to search for “miller” in the title and director fields. It boosts the documents where “miller” appears in the title field: . response = client .search(SearchParts::Index(&amp;[\"movies\"])) .from(0) .size(10) .body(json!({ \"query\": { \"multi_match\": { \"query\": \"miller\", \"fields\": [\"title^2\", \"director\"] } } })) .send() .await?; . copy . You can then read the response body as JSON and iterate over the hits array to read all the _source documents: . let response_body = response.json::&lt;Value&gt;().await?; for hit in response_body[\"hits\"][\"hits\"].as_array().unwrap() { // print the source document println!(\"{}\", serde_json::to_string_pretty(&amp;hit[\"_source\"]).unwrap()); } . copy . ",
    "url": "https://vagimeli.github.io/clients/rust/#searching-for-documents",
    "relUrl": "/clients/rust/#searching-for-documents"
  },"1690": {
    "doc": "Rust client",
    "title": "Deleting a document",
    "content": "You can delete a document using the client’s delete function: . let response = client .delete(DeleteParts::IndexId(\"movies\", \"2\")) .send() .await?; . copy . ",
    "url": "https://vagimeli.github.io/clients/rust/#deleting-a-document",
    "relUrl": "/clients/rust/#deleting-a-document"
  },"1691": {
    "doc": "Rust client",
    "title": "Deleting an index",
    "content": "You can delete an index using the delete function of the opensearch::indices::Indices struct: . let response = client .indices() .delete(IndicesDeleteParts::Index(&amp;[\"movies\"])) .send() .await?; . copy . ",
    "url": "https://vagimeli.github.io/clients/rust/#deleting-an-index",
    "relUrl": "/clients/rust/#deleting-an-index"
  },"1692": {
    "doc": "Rust client",
    "title": "Sample program",
    "content": "The sample program uses the following Cargo.toml file with all dependencies described in the Setup section: . [package] name = \"os_rust_project\" version = \"0.1.0\" edition = \"2021\" # See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html [dependencies] opensearch = \"1.0.0\" tokio = { version = \"*\", features = [\"full\"] } serde = \"~1\" serde_json = \"~1\" . copy . The following sample program creates a client, adds an index with non-default mappings, inserts a document, performs bulk operations, searches for the document, deletes the document, and then deletes the index: . use opensearch::{DeleteParts, OpenSearch, IndexParts, http::request::JsonBody, BulkParts, SearchParts}; use opensearch::{indices::{IndicesDeleteParts, IndicesCreateParts}}; use serde_json::{json, Value}; #[tokio::main] async fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; { let client = OpenSearch::default(); // Create an index let mut response = client .indices() .create(IndicesCreateParts::Index(\"movies\")) .body(json!({ \"mappings\" : { \"properties\" : { \"title\" : { \"type\" : \"text\" } } } })) .send() .await?; let mut successful = response.status_code().is_success(); if successful { println!(\"Successfully created an index\"); } else { println!(\"Could not create an index\"); } // Index a single document println!(\"Indexing a single document...\"); response = client .index(IndexParts::IndexId(\"movies\", \"1\")) .body(json!({ \"id\": 1, \"title\": \"Moneyball\", \"director\": \"Bennett Miller\", \"year\": \"2011\" })) .send() .await?; successful = response.status_code().is_success(); if successful { println!(\"Successfully indexed a document\"); } else { println!(\"Could not index document\"); } // Index multiple documents using the bulk operation println!(\"Indexing multiple documents...\"); let mut body: Vec&lt;JsonBody&lt;_&gt;&gt; = Vec::with_capacity(4); // add the first operation and document body.push(json!({\"index\": {\"_id\": \"2\"}}).into()); body.push(json!({ \"id\": 2, \"title\": \"Interstellar\", \"director\": \"Christopher Nolan\", \"year\": \"2014\" }).into()); // add the second operation and document body.push(json!({\"index\": {\"_id\": \"3\"}}).into()); body.push(json!({ \"id\": 3, \"title\": \"Star Trek Beyond\", \"director\": \"Justin Lin\", \"year\": \"2015\" }).into()); response = client .bulk(BulkParts::Index(\"movies\")) .body(body) .send() .await?; let mut response_body = response.json::&lt;Value&gt;().await?; successful = response_body[\"errors\"].as_bool().unwrap() == false; if successful { println!(\"Successfully performed bulk operations\"); } else { println!(\"Could not perform bulk operations\"); } // Search for a document println!(\"Searching for a document...\"); response = client .search(SearchParts::Index(&amp;[\"movies\"])) .from(0) .size(10) .body(json!({ \"query\": { \"multi_match\": { \"query\": \"miller\", \"fields\": [\"title^2\", \"director\"] } } })) .send() .await?; response_body = response.json::&lt;Value&gt;().await?; for hit in response_body[\"hits\"][\"hits\"].as_array().unwrap() { // print the source document println!(\"{}\", serde_json::to_string_pretty(&amp;hit[\"_source\"]).unwrap()); } // Delete a document response = client .delete(DeleteParts::IndexId(\"movies\", \"2\")) .send() .await?; successful = response.status_code().is_success(); if successful { println!(\"Successfully deleted a document\"); } else { println!(\"Could not delete document\"); } // Delete the index response = client .indices() .delete(IndicesDeleteParts::Index(&amp;[\"movies\"])) .send() .await?; successful = response.status_code().is_success(); if successful { println!(\"Successfully deleted the index\"); } else { println!(\"Could not delete the index\"); } Ok(()) } . copy . ",
    "url": "https://vagimeli.github.io/clients/rust/#sample-program",
    "relUrl": "/clients/rust/#sample-program"
  },"1693": {
    "doc": "Common use cases",
    "title": "Common use cases",
    "content": "You can use Data Prepper for several different purposes, including trace analytics, log analytics, Amazon S3 log analytics, and metrics ingestion. ",
    "url": "https://vagimeli.github.io/data-prepper/common-use-cases/common-use-cases/",
    "relUrl": "/data-prepper/common-use-cases/common-use-cases/"
  },"1694": {
    "doc": "Log analytics",
    "title": "Log analytics",
    "content": "Data Prepper is an extendable, configurable, and scalable solution for log ingestion into OpenSearch and Amazon OpenSearch Service. Data Prepper supports receiving logs from Fluent Bit through the HTTP Source and processing those logs with a Grok Processor before ingesting them into OpenSearch through the OpenSearch sink. The following image shows all of the components used for log analytics with Fluent Bit, Data Prepper, and OpenSearch. In the application environment, run Fluent Bit. Fluent Bit can be containerized through Kubernetes, Docker, or Amazon Elastic Container Service (Amazon ECS). You can also run Fluent Bit as an agent on Amazon Elastic Compute Cloud (Amazon EC2). Configure the Fluent Bit http output plugin to export log data to Data Prepper. Then deploy Data Prepper as an intermediate component and configure it to send the enriched log data to your OpenSearch cluster. From there, use OpenSearch Dashboards to perform more intensive visualization and analysis. ",
    "url": "https://vagimeli.github.io/data-prepper/common-use-cases/log-analytics/",
    "relUrl": "/data-prepper/common-use-cases/log-analytics/"
  },"1695": {
    "doc": "Log analytics",
    "title": "Log analytics pipeline",
    "content": "Log analytics pipelines in Data Prepper are extremely customizable. The following image shows a simple pipeline. HTTP source . The HTTP Source accepts log data from Fluent Bit. This source accepts log data in a JSON array format and supports industry-standard encryption in the form of TLS/HTTPS and HTTP basic authentication. Processor . Data Prepper 1.2 and above come with a Grok Processor. The Grok Processor is an invaluable tool for structuring and extracting important fields from your logs, making them more queryable. The Grok Processor comes with a wide variety of default patterns that match common log formats like Apache logs or syslogs, but it can easily accept any custom patterns that cater to your specific log format. For more information about Grok features, see the documentation. Sink . There is a generic sink that writes data to OpenSearch as the destination. The OpenSearch sink has configuration options related to an OpenSearch cluster, like endpoint, SSL/username, index name, index template, and index state management. ",
    "url": "https://vagimeli.github.io/data-prepper/common-use-cases/log-analytics/#log-analytics-pipeline",
    "relUrl": "/data-prepper/common-use-cases/log-analytics/#log-analytics-pipeline"
  },"1696": {
    "doc": "Log analytics",
    "title": "Pipeline configuration",
    "content": "The following sections discuss pipeline configuration. Example pipeline with SSL and basic authentication enabled . This example pipeline configuration comes with SSL and basic authentication enabled for the http-source: . log-pipeline: source: http: ssl_certificate_file: \"/full/path/to/certfile.crt\" ssl_key_file: \"/full/path/to/keyfile.key\" authentication: http_basic: username: \"myuser\" password: \"mys3cret\" processor: - grok: match: # This will match logs with a \"log\" key against the COMMONAPACHELOG pattern (ex: { \"log\": \"actual apache log...\" } ) # You should change this to match what your logs look like. See the grok documenation to get started. log: [ \"%{COMMONAPACHELOG}\" ] sink: - opensearch: hosts: [ \"https://localhost:9200\" ] # Change to your credentials username: \"admin\" password: \"admin\" # Add a certificate file if you are accessing an OpenSearch cluster with a self-signed certificate #cert: /path/to/cert # If you are connecting to an Amazon OpenSearch Service domain without # Fine-Grained Access Control, enable these settings. Comment out the # username and password above. #aws_sigv4: true #aws_region: us-east-1 # Since we are grok matching for apache logs, it makes sense to send them to an OpenSearch index named apache_logs. # You should change this to correspond with how your OpenSearch indexes are set up. index: apache_logs . This pipeline configuration is an example of Apache log ingestion. Don’t forget that you can easily configure the Grok Processor for your own custom logs. You will need to modify the configuration for your OpenSearch cluster. The following are the main changes you need to make: . | hosts – Set to your hosts. | index – Change this to the OpenSearch index to which you want to send logs. | username – Provide your OpenSearch username. | password – Provide your OpenSearch password. | aws_sigv4 – If you use Amazon OpenSearch Service with AWS signing, set this to true. It will sign requests with the default AWS credentials provider. | aws_region – If you use Amazon OpenSearch Service with AWS signing, set this value to the AWS Region in which your cluster is hosted. | . ",
    "url": "https://vagimeli.github.io/data-prepper/common-use-cases/log-analytics/#pipeline-configuration",
    "relUrl": "/data-prepper/common-use-cases/log-analytics/#pipeline-configuration"
  },"1697": {
    "doc": "Log analytics",
    "title": "Fluent Bit",
    "content": "You will need to run Fluent Bit in your service environment. See Getting Started with Fluent Bit for installation instructions. Ensure that you can configure the Fluent Bit http output plugin to your Data Prepper HTTP source. The following is an example fluent-bit.conf that tails a log file named test.log and forwards it to a locally running Data Prepper HTTP source, which runs by default on port 2021. Note that you should adjust the file path, output Host, and Port according to how and where you have Fluent Bit and Data Prepper running. Example: Fluent Bit file without SSL and basic authentication enabled . The following is an example fluent-bit.conf file without SSL and basic authentication enabled on the HTTP source: . [INPUT] name tail refresh_interval 5 path test.log read_from_head true [OUTPUT] Name http Match * Host localhost Port 2021 URI /log/ingest Format json . If your HTTP source has SSL and basic authentication enabled, you will need to add the details of http_User, http_Passwd, tls.crt_file, and tls.key_file to the fluent-bit.conf file, as shown in the following example. Example: Fluent Bit file with SSL and basic authentication enabled . The following is an example fluent-bit.conf file with SSL and basic authentication enabled on the HTTP source: . [INPUT] name tail refresh_interval 5 path test.log read_from_head true [OUTPUT] Name http Match * Host localhost http_User myuser http_Passwd mys3cret tls On tls.crt_file /full/path/to/certfile.crt tls.key_file /full/path/to/keyfile.key Port 2021 URI /log/ingest Format json . ",
    "url": "https://vagimeli.github.io/data-prepper/common-use-cases/log-analytics/#fluent-bit",
    "relUrl": "/data-prepper/common-use-cases/log-analytics/#fluent-bit"
  },"1698": {
    "doc": "Log analytics",
    "title": "Next steps",
    "content": "See the Data Prepper Log Ingestion Demo Guide for a specific example of Apache log ingestion from FluentBit -&gt; Data Prepper -&gt; OpenSearch running through Docker. In the future, Data Prepper will offer additional sources and processors that will make more complex log analytics pipelines available. Check out the Data Prepper Project Roadmap to see what is coming. If there is a specific source, processor, or sink that you would like to include in your log analytics workflow and is not currently on the roadmap, please bring it to our attention by creating a GitHub issue. Additionally, if you are interested in contributing to Data Prepper, see our Contributing Guidelines as well as our developer guide and plugin development guide. ",
    "url": "https://vagimeli.github.io/data-prepper/common-use-cases/log-analytics/#next-steps",
    "relUrl": "/data-prepper/common-use-cases/log-analytics/#next-steps"
  },"1699": {
    "doc": "S3 logs",
    "title": "S3 logs",
    "content": "Data Prepper allows you to load logs from Amazon Simple Storage Service (Amazon S3), including traditional logs, JSON documents, and CSV logs. ",
    "url": "https://vagimeli.github.io/data-prepper/common-use-cases/s3-logs/",
    "relUrl": "/data-prepper/common-use-cases/s3-logs/"
  },"1700": {
    "doc": "S3 logs",
    "title": "Architecture",
    "content": "Data Prepper can read objects from S3 buckets using an Amazon Simple Queue Service (SQS) (Amazon SQS) queue and Amazon S3 Event Notifications. Data Prepper polls the Amazon SQS queue for S3 event notifications. When Data Prepper receives a notification that an S3 object was created, Data Prepper reads and parses that S3 object. The following diagram shows the overall architecture of the components involved. The flow of data is as follows. | A system produces logs into the S3 bucket. | S3 creates an S3 event notification in the SQS queue. | Data Prepper polls Amazon SQS for messages and then receives a message. | Data Prepper downloads the content from the S3 object. | Data Prepper sends a document to OpenSearch for the content in the S3 object. | . ",
    "url": "https://vagimeli.github.io/data-prepper/common-use-cases/s3-logs/#architecture",
    "relUrl": "/data-prepper/common-use-cases/s3-logs/#architecture"
  },"1701": {
    "doc": "S3 logs",
    "title": "Pipeline overview",
    "content": "Data Prepper supports reading data from S3 using the s3 source. The following diagram shows a conceptual outline of a Data Prepper pipeline reading from S3. ",
    "url": "https://vagimeli.github.io/data-prepper/common-use-cases/s3-logs/#pipeline-overview",
    "relUrl": "/data-prepper/common-use-cases/s3-logs/#pipeline-overview"
  },"1702": {
    "doc": "S3 logs",
    "title": "Prerequisites",
    "content": "Before Data Prepper can read log data from S3, you need the following prerequisites: . | An S3 bucket. | A log producer that writes logs to S3. The exact log producer will vary depending on your specific use case, but could include writing logs to S3 or a service such as Amazon CloudWatch. | . ",
    "url": "https://vagimeli.github.io/data-prepper/common-use-cases/s3-logs/#prerequisites",
    "relUrl": "/data-prepper/common-use-cases/s3-logs/#prerequisites"
  },"1703": {
    "doc": "S3 logs",
    "title": "Getting started",
    "content": "Use the following steps to begin loading logs from S3 with Data Prepper. | Create an SQS standard queue for your S3 event notifications. | Configure bucket notifications for SQS. Use the s3:ObjectCreated:* event type. | Grant AWS IAM permissions to Data Prepper for accessing SQS and S3. | (Recommended) Create an SQS dead-letter queue (DLQ). | (Recommended) Configure an SQS re-drive policy to move failed messages into the DLQ. | . Setting permissions for Data Prepper . To view S3 logs, Data Prepper needs access to Amazon SQS and S3. Use the following example to set up permissions: . { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"s3-access\", \"Effect\": \"Allow\", \"Action\": \"s3:GetObject\", \"Resource\": \"arn:aws:s3:::&lt;YOUR-BUCKET&gt;/*\" }, { \"Sid\": \"sqs-access\", \"Effect\": \"Allow\", \"Action\": [ \"sqs:DeleteMessage\", \"sqs:ReceiveMessage\" ], \"Resource\": \"arn:aws:sqs:&lt;YOUR-REGION&gt;:&lt;123456789012&gt;:&lt;YOUR-SQS-QUEUE&gt;\" }, { \"Sid\": \"kms-access\", \"Effect\": \"Allow\", \"Action\": \"kms:Decrypt\", \"Resource\": \"arn:aws:kms:&lt;YOUR-REGION&gt;:&lt;123456789012&gt;:key/&lt;YOUR-KMS-KEY&gt;\" } ] } . If your S3 objects or SQS queues do not use KMS, you can remove the kms:Decrypt permission. SQS dead-letter queue . The are two options for how to handle errors resulting from processing S3 objects. | Use an SQS dead-letter queue (DLQ) to track the failure. This is the recommended approach. | Delete the message from SQS. You must manually find the S3 object and correct the error. | . The following diagram shows the system architecture when using SQS with DLQ. To use an SQS dead-letter queue, perform the following steps: . | Create a new SQS standard queue to act as your DLQ. | Configure your SQS’s redrive policy to use your DLQ. Consider using a low value such as 2 or 3 for the “Maximum Receives” setting. | Configure the Data Prepper s3 source to use retain_messages for on_error. This is the default behavior. | . ",
    "url": "https://vagimeli.github.io/data-prepper/common-use-cases/s3-logs/#getting-started",
    "relUrl": "/data-prepper/common-use-cases/s3-logs/#getting-started"
  },"1704": {
    "doc": "S3 logs",
    "title": "Pipeline design",
    "content": "Create a pipeline to read logs from S3, starting with an s3 source plugin. Use the following example for guidance. s3-log-pipeline: source: s3: notification_type: sqs compression: gzip codec: newline: sqs: # Change this value to your SQS Queue URL queue_url: \"arn:aws:sqs:&lt;YOUR-REGION&gt;:&lt;123456789012&gt;:&lt;YOUR-SQS-QUEUE&gt;\" visibility_timeout: \"2m\" . Configure the following options according to your use case: . | queue_url: This the SQS queue URL and is always unique to your pipeline. | codec: The codec determines how to parse the incoming data. | visibility_timeout: Configure this value to be large enough for Data Prepper to process 10 S3 objects. However, if you make this value too large, messages that fail to process will take at least as long as the specified value before Data Prepper retries. | . The default values for each option work for the majority of use cases. For all available options for the S3 source, see s3. s3-log-pipeline: source: s3: notification_type: sqs compression: gzip codec: newline: sqs: # Change this value to your SQS Queue URL queue_url: \"arn:aws:sqs:&lt;YOUR-REGION&gt;:&lt;123456789012&gt;:&lt;YOUR-SQS-QUEUE&gt;\" visibility_timeout: \"2m\" aws: # Specify the correct region region: \"&lt;YOUR-REGION&gt;\" # This shows using an STS role, but you can also use your system's default permissions. sts_role_arn: \"arn:aws:iam::&lt;123456789012&gt;:role/&lt;DATA-PREPPER-ROLE&gt;\" processor: # You can configure a grok pattern to enrich your documents in OpenSearch. #- grok: # match: # message: [ \"%{COMMONAPACHELOG}\" ] sink: - opensearch: hosts: [ \"https://localhost:9200\" ] # Change to your credentials username: \"admin\" password: \"admin\" index: s3_logs . ",
    "url": "https://vagimeli.github.io/data-prepper/common-use-cases/s3-logs/#pipeline-design",
    "relUrl": "/data-prepper/common-use-cases/s3-logs/#pipeline-design"
  },"1705": {
    "doc": "S3 logs",
    "title": "Multiple Data Prepper pipelines",
    "content": "We recommend that you have one SQS queue per Data Prepper pipeline. In addition, you can have multiple nodes in the same cluster reading from the same SQS queue, which doesn’t require additional configuration with Data Prepper. If you have multiple pipelines, you must create multiple SQS queues for each pipeline, even if both pipelines use the same S3 bucket. ",
    "url": "https://vagimeli.github.io/data-prepper/common-use-cases/s3-logs/#multiple-data-prepper-pipelines",
    "relUrl": "/data-prepper/common-use-cases/s3-logs/#multiple-data-prepper-pipelines"
  },"1706": {
    "doc": "S3 logs",
    "title": "Amazon SNS fanout pattern",
    "content": "To meet the scale of logs produced by S3, some users require multiple SQS queues for their logs. You can use Amazon Simple Notification Service (Amazon SNS) to route event notifications from S3 to an SQS fanout pattern. Using SNS, all S3 event notifications are sent directly to a single SNS topic, where you can subscribe to multiple SQS queues. To make sure that Data Prepper can directly parse the event from the SNS topic, configure raw message delivery on the SNS to SQS subscription. Setting this option will not affect other SQS queues that are subscribed to that SNS topic. ",
    "url": "https://vagimeli.github.io/data-prepper/common-use-cases/s3-logs/#amazon-sns-fanout-pattern",
    "relUrl": "/data-prepper/common-use-cases/s3-logs/#amazon-sns-fanout-pattern"
  },"1707": {
    "doc": "Trace analytics",
    "title": "Trace analytics",
    "content": "Trace analytics allows you to collect trace data and customize a pipeline that ingests and transforms the data for use in OpenSearch. The following provides an overview of the trace analytics workflow in Data Prepper, how to configure it, and how to visualize trace data. ",
    "url": "https://vagimeli.github.io/data-prepper/common-use-cases/trace-analytics/",
    "relUrl": "/data-prepper/common-use-cases/trace-analytics/"
  },"1708": {
    "doc": "Trace analytics",
    "title": "Introduction",
    "content": "When using Data Prepper as a server-side component to collect trace data, you can customize a Data Prepper pipeline to ingest and transform the data for use in OpenSearch. Upon transformation, you can visualize the transformed trace data for use with the Observability plugin inside of OpenSearch Dashboards. Trace data provides visibility into your application’s performance, and helps you gain more information about individual traces. The following flowchart illustrates the trace analytics workflow, from running OpenTelemetry Collector to using OpenSearch Dashboards for visualization. To monitor trace analytics, you need to set up the following components in your service environment: . | Add instrumentation to your application so it can generate telemetry data and send it to an OpenTelemetry collector. | Run an OpenTelemetry collector as a sidecar or daemonset for Amazon Elastic Kubernetes Service (Amazon EKS), a sidecar for Amazon Elastic Container Service (Amazon ECS), or an agent on Amazon Elastic Compute Cloud (Amazon EC2). You should configure the collector to export trace data to Data Prepper. | Deploy Data Prepper as the ingestion collector for OpenSearch. Configure it to send the enriched trace data to your OpenSearch cluster or to the Amazon OpenSearch Service domain. | Use OpenSearch Dashboards to visualize and detect problems in your distributed applications. | . ",
    "url": "https://vagimeli.github.io/data-prepper/common-use-cases/trace-analytics/#introduction",
    "relUrl": "/data-prepper/common-use-cases/trace-analytics/#introduction"
  },"1709": {
    "doc": "Trace analytics",
    "title": "Trace analytics pipeline",
    "content": "To monitor trace analytics in Data Prepper, we provide three pipelines: entry-pipeline, raw-trace-pipeline, and service-map-pipeline. The following image provides an overview of how the pipelines work together to monitor trace analytics. OpenTelemetry trace source . The OpenTelemetry source accepts trace data from the OpenTelemetry Collector. The source follows the OpenTelemetry Protocol and officially supports transport over gRPC and the use of industry-standard encryption (TLS/HTTPS). Processor . There are three processors for the trace analytics feature: . | otel_trace_raw - The otel_trace_raw processor receives a collection of span records from otel-trace-source, and performs stateful processing, extraction, and completion of trace-group-related fields. | otel_trace_group - The otel_trace_group processor fills in the missing trace-group-related fields in the collection of span records by looking up the OpenSearch backend. | service_map_stateful – The service_map_stateful processor performs the required preprocessing for trace data and builds metadata to display the service-map dashboards. | . OpenSearch sink . OpenSearch provides a generic sink that writes data to OpenSearch as the destination. The OpenSearch sink has configuration options related to the OpenSearch cluster, such as endpoint, SSL, username/password, index name, index template, and index state management. The sink provides specific configurations for the trace analytics feature. These configurations allow the sink to use indexes and index templates specific to trace analytics. The following OpenSearch indexes are specific to trace analytics: . | otel-v1-apm-span – The otel-v1-apm-span index stores the output from the otel_trace_raw processor. | otel-v1-apm-service-map – The otel-v1-apm-service-map index stores the output from the service_map_stateful processor. | . ",
    "url": "https://vagimeli.github.io/data-prepper/common-use-cases/trace-analytics/#trace-analytics-pipeline",
    "relUrl": "/data-prepper/common-use-cases/trace-analytics/#trace-analytics-pipeline"
  },"1710": {
    "doc": "Trace analytics",
    "title": "Trace tuning",
    "content": "Starting with version 0.8.x, Data Prepper supports both vertical and horizontal scaling for trace analytics. You can adjust the size of a single Data Prepper instance to meet your workload’s demands and scale vertically. You can scale horizontally by using the core peer forwarder to deploy multiple Data Prepper instances to form a cluster. This enables Data Prepper instances to communicate with instances in the cluster and is required for horizontally scaling deployments. Scaling recommendations . Use the following recommended configurations to scale Data Prepper. We recommend that you modify parameters based on the requirements. We also recommend that you monitor the Data Prepper host metrics and OpenSearch metrics to ensure that the configuration works as expected. Buffer . The total number of trace requests processed by Data Prepper is equal to the sum of the buffer_size values in otel-trace-pipeline and raw-pipeline. The total number of trace requests sent to OpenSearch is equal to the product of batch_size and workers in raw-trace-pipeline. For more information about raw-pipeline, see Trace analytics pipeline. We recommend the following when making changes to buffer settings: . | The buffer_size value in otel-trace-pipeline and raw-pipeline should be the same. | The buffer_size should be greater than or equal to workers * batch_size in the raw-pipeline. | . Workers . The workers setting determines the number of threads that are used by Data Prepper to process requests from the buffer. We recommend that you set workers based on the CPU utilization. This value can be higher than the number of available processors because Data Prepper uses significant input/output time when sending data to OpenSearch. Heap . Configure the Data Prepper heap by setting the JVM_OPTS environment variable. We recommend that you set the heap value to a minimum value of 4 * batch_size * otel_send_batch_size * maximum size of indvidual span. As mentioned in the OpenTelemetry Collector section, set otel_send_batch_size to a value of 50 in your OpenTelemetry Collector configuration. Local disk . Data Prepper uses the local disk to store metadata required for service map processing, so we recommend storing only the following key fields: traceId, spanId, parentSpanId, spanKind, spanName, and serviceName. The service-map plugin stores only two files, each of which stores window_duration seconds of data. As an example, testing with a throughput of 3000 spans/second resulted in the total disk usage of 4 MB. Data Prepper also uses the local disk to write logs. In the most recent version of Data Prepper, you can redirect the logs to your preferred path. AWS CloudFormation template and Kubernetes/Amazon EKS configuration files . The AWS CloudFormation template provides a user-friendly mechanism for configuring the scaling attributes described in the Trace tuning section. The Kubernetes configuration files and Amazon EKS configuration files are available for configuring these attributes in a cluster deployment. Benchmark tests . The benchmark tests were performed on an r5.xlarge EC2 instance with the following configuration: . | buffer_size: 4096 | batch_size: 256 | workers: 8 | Heap: 10 GB | . This setup was able to handle a throughput of 2100 spans/second at 20 percent CPU utilization. ",
    "url": "https://vagimeli.github.io/data-prepper/common-use-cases/trace-analytics/#trace-tuning",
    "relUrl": "/data-prepper/common-use-cases/trace-analytics/#trace-tuning"
  },"1711": {
    "doc": "Trace analytics",
    "title": "Pipeline configuration",
    "content": "The following sections provide examples of different types of pipelines and how to configure each type. Example: Trace analytics pipeline . The following example demonstrates how to build a pipeline that supports the OpenSearch Dashboards Observability plugin. This pipeline takes data from the OpenTelemetry Collector and uses two other pipelines as sinks. These two separate pipelines serve two different purposes and write to different OpenSearch indexes. The first pipeline prepares trace data for OpenSearch and enriches and ingests the span documents into a span index within OpenSearch. The second pipeline aggregates traces into a service map and writes service map documents into a service map index within OpenSearch. Starting with Data Prepper version 2.0, Data Prepper no longer supports the otel_trace_raw_prepper processor. The otel_trace_raw processor replaces the otel_trace_raw_prepper processor and supports some of Data Prepper’s recent data model changes. Instead, you should use the otel_trace_raw processor. See the following YAML file example: . entry-pipeline: delay: \"100\" source: otel_trace_source: ssl: false buffer: bounded_blocking: buffer_size: 10240 batch_size: 160 sink: - pipeline: name: \"raw-trace-pipeline\" - pipeline: name: \"service-map-pipeline\" raw-pipeline: source: pipeline: name: \"entry-pipeline\" buffer: bounded_blocking: buffer_size: 10240 batch_size: 160 processor: - otel_trace_raw: sink: - opensearch: hosts: [\"https://localhost:9200\"] insecure: true username: admin password: admin index_type: trace-analytics-raw service-map-pipeline: delay: \"100\" source: pipeline: name: \"entry-pipeline\" buffer: bounded_blocking: buffer_size: 10240 batch_size: 160 processor: - service_map_stateful: sink: - opensearch: hosts: [\"https://localhost:9200\"] insecure: true username: admin password: admin index_type: trace-analytics-service-map . To maintain similar ingestion throughput and latency, scale the buffer_size and batch_size by the estimated maximum batch size in the client request payload. {: .tip} . Example: otel trace . The following is an example otel-trace-source .yaml file with SSL and basic authentication enabled. Note that you will need to modify your otel-collector-config.yaml file so that it uses your own credentials. source: otel_trace_source: #record_type: event # Add this when using Data Prepper 1.x. This option is removed in 2.0 ssl: true sslKeyCertChainFile: \"/full/path/to/certfile.crt\" sslKeyFile: \"/full/path/to/keyfile.key\" authentication: http_basic: username: \"my-user\" password: \"my_s3cr3t\" . Example: pipeline.yaml . The following is an example pipeline.yaml file without SSL and basic authentication enabled for the otel-trace-pipeline pipeline: . otel-trace-pipeline: # workers is the number of threads processing data in each pipeline. # We recommend same value for all pipelines. # default value is 1, set a value based on the machine you are running Data Prepper workers: 8 # delay in milliseconds is how often the worker threads should process data. # Recommend not to change this config as we want the entry-pipeline to process as quick as possible # default value is 3_000 ms delay: \"100\" source: otel_trace_source: #record_type: event # Add this when using Data Prepper 1.x. This option is removed in 2.0 ssl: false # Change this to enable encryption in transit authentication: unauthenticated: buffer: bounded_blocking: # buffer_size is the number of ExportTraceRequest from otel-collector the data prepper should hold in memeory. # We recommend to keep the same buffer_size for all pipelines. # Make sure you configure sufficient heap # default value is 512 buffer_size: 512 # This is the maximum number of request each worker thread will process within the delay. # Default is 8. # Make sure buffer_size &gt;= workers * batch_size batch_size: 8 sink: - pipeline: name: \"raw-trace-pipeline\" - pipeline: name: \"entry-pipeline\" raw-pipeline: # Configure same as the otel-trace-pipeline workers: 8 # We recommend using the default value for the raw-pipeline. delay: \"3000\" source: pipeline: name: \"entry-pipeline\" buffer: bounded_blocking: # Configure the same value as in entry-pipeline # Make sure you configure sufficient heap # The default value is 512 buffer_size: 512 # The raw processor does bulk request to your OpenSearch sink, so configure the batch_size higher. # If you use the recommended otel-collector setup each ExportTraceRequest could contain max 50 spans. https://github.com/opensearch-project/data-prepper/tree/v0.7.x/deployment/aws # With 64 as batch size each worker thread could process upto 3200 spans (64 * 50) batch_size: 64 processor: - otel_trace_raw: - otel_trace_group: hosts: [ \"https://localhost:9200\" ] # Change to your credentials username: \"admin\" password: \"admin\" # Add a certificate file if you are accessing an OpenSearch cluster with a self-signed certificate #cert: /path/to/cert # If you are connecting to an Amazon OpenSearch Service domain without # Fine-Grained Access Control, enable these settings. Comment out the # username and password above. #aws_sigv4: true #aws_region: us-east-1 sink: - opensearch: hosts: [ \"https://localhost:9200\" ] index_type: trace-analytics-raw # Change to your credentials username: \"admin\" password: \"admin\" # Add a certificate file if you are accessing an OpenSearch cluster with a self-signed certificate #cert: /path/to/cert # If you are connecting to an Amazon OpenSearch Service domain without # Fine-Grained Access Control, enable these settings. Comment out the # username and password above. #aws_sigv4: true #aws_region: us-east-1 service-map-pipeline: workers: 8 delay: \"100\" source: pipeline: name: \"entry-pipeline\" processor: - service_map_stateful: # The window duration is the maximum length of time the data prepper stores the most recent trace data to evaluvate service-map relationships. # The default is 3 minutes, this means we can detect relationships between services from spans reported in last 3 minutes. # Set higher value if your applications have higher latency. window_duration: 180 buffer: bounded_blocking: # buffer_size is the number of ExportTraceRequest from otel-collector the data prepper should hold in memeory. # We recommend to keep the same buffer_size for all pipelines. # Make sure you configure sufficient heap # default value is 512 buffer_size: 512 # This is the maximum number of request each worker thread will process within the delay. # Default is 8. # Make sure buffer_size &gt;= workers * batch_size batch_size: 8 sink: - opensearch: hosts: [ \"https://localhost:9200\" ] index_type: trace-analytics-service-map # Change to your credentials username: \"admin\" password: \"admin\" # Add a certificate file if you are accessing an OpenSearch cluster with a self-signed certificate #cert: /path/to/cert # If you are connecting to an Amazon OpenSearch Service domain without # Fine-Grained Access Control, enable these settings. Comment out the # username and password above. #aws_sigv4: true #aws_region: us-east-1 . You need to modify the preceding configuration for your OpenSearch cluster so that the configuration matches your environment. Note that it has two opensearch sinks that need to be modified. You must make the following changes: . | hosts – Set to your hosts. | username – Provide your OpenSearch username. | password – Provide your OpenSearch password. | aws_sigv4 – If you are using Amazon OpenSearch Service with AWS signing, set this value to true. It will sign requests with the default AWS credentials provider. | aws_region – If you are using Amazon OpenSearch Service with AWS signing, set this value to your AWS Region. | . For other configurations available for OpenSearch sinks, see Data Prepper OpenSearch sink. ",
    "url": "https://vagimeli.github.io/data-prepper/common-use-cases/trace-analytics/#pipeline-configuration",
    "relUrl": "/data-prepper/common-use-cases/trace-analytics/#pipeline-configuration"
  },"1712": {
    "doc": "Trace analytics",
    "title": "OpenTelemetry Collector",
    "content": "You need to run OpenTelemetry Collector in your service environment. Follow Getting Started to install an OpenTelemetry collector. Ensure that you configure the collector with an exporter configured for your Data Prepper instance. The following example otel-collector-config.yaml file receives data from various instrumentations and exports it to Data Prepper. Example otel-collector-config.yaml file . The following is an example otel-collector-config.yaml file: . receivers: jaeger: protocols: grpc: otlp: protocols: grpc: zipkin: processors: batch/traces: timeout: 1s send_batch_size: 50 exporters: otlp/data-prepper: endpoint: localhost:21890 tls: insecure: true service: pipelines: traces: receivers: [jaeger, otlp, zipkin] processors: [batch/traces] exporters: [otlp/data-prepper] . After you run OpenTelemetry in your service environment, you must configure your application to use the OpenTelemetry Collector. The OpenTelemetry Collector typically runs alongside your application. ",
    "url": "https://vagimeli.github.io/data-prepper/common-use-cases/trace-analytics/#opentelemetry-collector",
    "relUrl": "/data-prepper/common-use-cases/trace-analytics/#opentelemetry-collector"
  },"1713": {
    "doc": "Trace analytics",
    "title": "Next steps and more information",
    "content": "The OpenSearch Dashboards Observability plugin documentation provides additional information about configuring OpenSearch to view trace analytics in OpenSearch Dashboards. For more information about how to tune and scale Data Prepper for trace analytics, see Trace tuning. ",
    "url": "https://vagimeli.github.io/data-prepper/common-use-cases/trace-analytics/#next-steps-and-more-information",
    "relUrl": "/data-prepper/common-use-cases/trace-analytics/#next-steps-and-more-information"
  },"1714": {
    "doc": "Trace analytics",
    "title": "Migrating to Data Prepper 2.0",
    "content": "Starting with Data Prepper version 1.4, trace processing uses Data Prepper’s event model. This allows pipeline authors to configure other processors to modify spans or traces. To provide a migration path, Data Prepper version 1.4 introduced the following changes: . | otel_trace_source has an optional record_type parameter that can be set to event. When configured, it will output event objects. | otel_trace_raw replaces otel_trace_raw_prepper for event-based spans. | otel_trace_group replaces otel_trace_group_prepper for event-based spans. | . In Data Prepper version 2.0, otel_trace_source will only output events. Data Prepper version 2.0 also removes otel_trace_raw_prepper and otel_trace_group_prepper entirely. To migrate to Data Prepper version 2.0, you can configure your trace pipeline using the event model. ",
    "url": "https://vagimeli.github.io/data-prepper/common-use-cases/trace-analytics/#migrating-to-data-prepper-20",
    "relUrl": "/data-prepper/common-use-cases/trace-analytics/#migrating-to-data-prepper-20"
  },"1715": {
    "doc": "Getting started",
    "title": "Getting started with Data Prepper",
    "content": "Data Prepper is an independent component, not an OpenSearch plugin, that converts data for use with OpenSearch. It’s not bundled with the all-in-one OpenSearch installation packages. If you are migrating from Open Distro Data Prepper, see Migrating from Open Distro. ",
    "url": "https://vagimeli.github.io/data-prepper/getting-started/#getting-started-with-data-prepper",
    "relUrl": "/data-prepper/getting-started/#getting-started-with-data-prepper"
  },"1716": {
    "doc": "Getting started",
    "title": "1. Installing Data Prepper",
    "content": "There are two ways to install Data Prepper: you can run the Docker image or build from source. The easiest way to use Data Prepper is by running the Docker image. We suggest that you use this approach if you have Docker available. Run the following command: . docker pull opensearchproject/data-prepper:latest . copy . If you have special requirements that require you to build from source, or if you want to contribute, see the Developer Guide. ",
    "url": "https://vagimeli.github.io/data-prepper/getting-started/#1-installing-data-prepper",
    "relUrl": "/data-prepper/getting-started/#1-installing-data-prepper"
  },"1717": {
    "doc": "Getting started",
    "title": "2. Configuring Data Prepper",
    "content": "Two configuration files are required to run a Data Prepper instance. Optionally, you can configure a Log4j 2 configuration file. See Configuring Log4j for more information. The following list describes the purpose of each configuration file: . | pipelines.yaml: This file describes which data pipelines to run, including sources, processors, and sinks. | data-prepper-config.yaml: This file contains Data Prepper server settings that allow you to interact with exposed Data Prepper server APIs. | log4j2-rolling.properties (optional): This file contains Log4j 2 configuration options and can be a JSON, YAML, XML, or .properties file type. | . For Data Prepper versions earlier than 2.0, the .jar file expects the pipeline configuration file path to be followed by the server configuration file path. See the following configuration path example: . java -jar data-prepper-core-$VERSION.jar pipelines.yaml data-prepper-config.yaml . Optionally, you can add \"-Dlog4j.configurationFile=config/log4j2.properties\" to the command to pass a custom Log4j 2 configuration file. If you don’t provide a properties file, Data Prepper defaults to the log4j2.properties file in the shared-config directory. Starting with Data Prepper 2.0, you can launch Data Prepper by using the following data-prepper script that does not require any additional command line arguments: . bin/data-prepper . Configuration files are read from specific subdirectories in the application’s home directory: . | pipelines/: Used for pipeline configurations. Pipeline configurations can be written in one or more YAML files. | config/data-prepper-config.yaml: Used for the Data Prepper server configuration. | . You can supply your own pipeline configuration file path followed by the server configuration file path. However, this method will not be supported in a future release. See the following example: . bin/data-prepper pipelines.yaml data-prepper-config.yaml . The Log4j 2 configuration file is read from the config/log4j2.properties file located in the application’s home directory. To configure Data Prepper, see the following information for each use case: . | Trace analytics: Learn how to collect trace data and customize a pipeline that ingests and transforms that data. | Log analytics: Learn how to set up Data Prepper for log observability. | . ",
    "url": "https://vagimeli.github.io/data-prepper/getting-started/#2-configuring-data-prepper",
    "relUrl": "/data-prepper/getting-started/#2-configuring-data-prepper"
  },"1718": {
    "doc": "Getting started",
    "title": "3. Defining a pipeline",
    "content": "Create a Data Prepper pipeline file named pipelines.yaml using the following configuration: . simple-sample-pipeline: workers: 2 delay: \"5000\" source: random: sink: - stdout: . copy . ",
    "url": "https://vagimeli.github.io/data-prepper/getting-started/#3-defining-a-pipeline",
    "relUrl": "/data-prepper/getting-started/#3-defining-a-pipeline"
  },"1719": {
    "doc": "Getting started",
    "title": "4. Running Data Prepper",
    "content": "Run the following command with your pipeline configuration YAML. docker run --name data-prepper \\ -v /${PWD}/pipelines.yaml:/usr/share/data-prepper/pipelines/pipelines.yaml \\ opensearchproject/data-prepper:latest . copy . The example pipeline configuration above demonstrates a simple pipeline with a source (random) sending data to a sink (stdout). For examples of more advanced pipeline configurations, see Pipelines. After starting Data Prepper, you should see log output and some UUIDs after a few seconds: . 2021-09-30T20:19:44,147 [main] INFO com.amazon.dataprepper.pipeline.server.DataPrepperServer - Data Prepper server running at :4900 2021-09-30T20:19:44,681 [random-source-pool-0] INFO com.amazon.dataprepper.plugins.source.RandomStringSource - Writing to buffer 2021-09-30T20:19:45,183 [random-source-pool-0] INFO com.amazon.dataprepper.plugins.source.RandomStringSource - Writing to buffer 2021-09-30T20:19:45,687 [random-source-pool-0] INFO com.amazon.dataprepper.plugins.source.RandomStringSource - Writing to buffer 2021-09-30T20:19:46,191 [random-source-pool-0] INFO com.amazon.dataprepper.plugins.source.RandomStringSource - Writing to buffer 2021-09-30T20:19:46,694 [random-source-pool-0] INFO com.amazon.dataprepper.plugins.source.RandomStringSource - Writing to buffer 2021-09-30T20:19:47,200 [random-source-pool-0] INFO com.amazon.dataprepper.plugins.source.RandomStringSource - Writing to buffer 2021-09-30T20:19:49,181 [simple-test-pipeline-processor-worker-1-thread-1] INFO com.amazon.dataprepper.pipeline.ProcessWorker - simple-test-pipeline Worker: Processing 6 records from buffer 07dc0d37-da2c-447e-a8df-64792095fb72 5ac9b10a-1d21-4306-851a-6fb12f797010 99040c79-e97b-4f1d-a70b-409286f2a671 5319a842-c028-4c17-a613-3ef101bd2bdd e51e700e-5cab-4f6d-879a-1c3235a77d18 b4ed2d7e-cf9c-4e9d-967c-b18e8af35c90 . The remainder of this page provides examples for running Data Prepper from the Docker image. If you built it from source, refer to the Developer Guide for more information. However you configure your pipeline, you’ll run Data Prepper the same way. You run the Docker image and modify both the pipelines.yaml and data-prepper-config.yaml files. For Data Prepper 2.0 or later, use this command: . docker run --name data-prepper -p 4900:4900 -v ${PWD}/pipelines.yaml:/usr/share/data-prepper/pipelines/pipelines.yaml -v ${PWD}/data-prepper-config.yaml:/usr/share/data-prepper/config/data-prepper-config.yaml opensearchproject/data-prepper:latest . copy . For Data Prepper versions earlier than 2.0, use this command: . docker run --name data-prepper -p 4900:4900 -v ${PWD}/pipelines.yaml:/usr/share/data-prepper/pipelines.yaml -v ${PWD}/data-prepper-config.yaml:/usr/share/data-prepper/data-prepper-config.yaml opensearchproject/data-prepper:1.x . copy . Once Data Prepper is running, it processes data until it is shut down. Once you are done, shut it down with the following command: . POST /shutdown . copy . Additional configurations . For Data Prepper 2.0 or later, the Log4j 2 configuration file is read from config/log4j2.properties in the application’s home directory. By default, it uses log4j2-rolling.properties in the shared-config directory. For Data Prepper 1.5 or earlier, optionally add \"-Dlog4j.configurationFile=config/log4j2.properties\" to the command if you want to pass a custom log4j2 properties file. If no properties file is provided, Data Prepper defaults to the log4j2.properties file in the shared-config directory. ",
    "url": "https://vagimeli.github.io/data-prepper/getting-started/#4-running-data-prepper",
    "relUrl": "/data-prepper/getting-started/#4-running-data-prepper"
  },"1720": {
    "doc": "Getting started",
    "title": "Next steps",
    "content": "Trace analytics is an important Data Prepper use case. If you haven’t yet configured it, see Trace analytics. Log ingestion is also an important Data Prepper use case. To learn more, see Log analytics. To learn how to run Data Prepper with a Logstash configuration, see Migrating from Logstash. For information on how to monitor Data Prepper, see Monitoring. ",
    "url": "https://vagimeli.github.io/data-prepper/getting-started/#next-steps",
    "relUrl": "/data-prepper/getting-started/#next-steps"
  },"1721": {
    "doc": "Getting started",
    "title": "More examples",
    "content": "For more examples of Data Prepper, see examples in the Data Prepper repo. ",
    "url": "https://vagimeli.github.io/data-prepper/getting-started/#more-examples",
    "relUrl": "/data-prepper/getting-started/#more-examples"
  },"1722": {
    "doc": "Getting started",
    "title": "Getting started",
    "content": " ",
    "url": "https://vagimeli.github.io/data-prepper/getting-started/",
    "relUrl": "/data-prepper/getting-started/"
  },"1723": {
    "doc": "Data Prepper",
    "title": "Data Prepper",
    "content": "Data Prepper is a server-side data collector capable of filtering, enriching, transforming, normalizing, and aggregating data for downstream analytics and visualization. Data Prepper lets users build custom pipelines to improve the operational view of applications. Two common uses for Data Prepper are trace and log analytics. Trace analytics can help you visualize the flow of events and identify performance problems, and log analytics can improve searching, analyzing and provide insights into your application. ",
    "url": "https://vagimeli.github.io/data-prepper/index/",
    "relUrl": "/data-prepper/index/"
  },"1724": {
    "doc": "Data Prepper",
    "title": "Concepts",
    "content": "Data Prepper includes one or more pipelines that collect and filter data based on the components set within the pipeline. Each component is pluggable, enabling you to use your own custom implementation of each component. These components include the following: . | One source | One or more sinks | (Optional) One buffer | (Optional) One or more processors | . A single instance of Data Prepper can have one or more pipelines. Each pipeline definition contains two required components: source and sink. If buffers and processors are missing from the Data Prepper pipeline, Data Prepper uses the default buffer and a no-op processor. Source . Source is the input component that defines the mechanism through which a Data Prepper pipeline will consume events. A pipeline can have only one source. The source can consume events either by receiving the events over HTTP or HTTPS or by reading from external endpoints like OTeL Collector for traces and metrics and Amazon Simple Storage Service (Amazon S3). Sources have their own configuration options based on the format of the events (such as string, JSON, Amazon CloudWatch logs, or open telemetry trace). The source component consumes events and writes them to the buffer component. Buffer . The buffer component acts as the layer between the source and the sink. Buffer can be either in-memory or disk based. The default buffer uses an in-memory queue called bounded_blocking that is bounded by the number of events. If the buffer component is not explicitly mentioned in the pipeline configuration, Data Prepper uses the default bounded_blocking. Sink . Sink is the output component that defines the destination(s) to which a Data Prepper pipeline publishes events. A sink destination could be a service, such as OpenSearch or Amazon S3, or another Data Prepper pipeline. When using another Data Prepper pipeline as the sink, you can chain multiple pipelines together based on the needs of the data. Sink contains its own configuration options based on the destination type. Processor . Processors are units within the Data Prepper pipeline that can filter, transform, and enrich events using your desired format before publishing the record to the sink component. The processor is not defined in the pipeline configuration; the events publish in the format defined in the source component. You can have more than one processor within a pipeline. When using multiple processors, the processors are run in the order they are defined inside the pipeline specification. ",
    "url": "https://vagimeli.github.io/data-prepper/index/#concepts",
    "relUrl": "/data-prepper/index/#concepts"
  },"1725": {
    "doc": "Data Prepper",
    "title": "Sample pipeline configurations",
    "content": "To understand how all pipeline components function within a Data Prepper configuration, see the following examples. Each pipeline configuration uses a yaml file format. Minimal component . This pipeline configuration reads from the file source and writes to another file in the same path. It uses the default options for the buffer and processor. sample-pipeline: source: file: path: &lt;path/to/input-file&gt; sink: - file: path: &lt;path/to/output-file&gt; . All components . The following pipeline uses a source that reads string events from the input-file. The source then pushes the data to the buffer, bounded by a max size of 1024. The pipeline is configured to have 4 workers, each of them reading a maximum of 256 events from the buffer for every 100 milliseconds. Each worker runs the string_converter processor and writes the output of the processor to the output-file. sample-pipeline: workers: 4 #Number of workers delay: 100 # in milliseconds, how often the workers should run source: file: path: &lt;path/to/input-file&gt; buffer: bounded_blocking: buffer_size: 1024 # max number of events the buffer will accept batch_size: 256 # max number of events the buffer will drain for each read processor: - string_converter: upper_case: true sink: - file: path: &lt;path/to/output-file&gt; . ",
    "url": "https://vagimeli.github.io/data-prepper/index/#sample-pipeline-configurations",
    "relUrl": "/data-prepper/index/#sample-pipeline-configurations"
  },"1726": {
    "doc": "Data Prepper",
    "title": "Next steps",
    "content": "To get started building your own custom pipelines with Data Prepper, see Getting started. ",
    "url": "https://vagimeli.github.io/data-prepper/index/#next-steps",
    "relUrl": "/data-prepper/index/#next-steps"
  },"1727": {
    "doc": "Configuring Data Prepper",
    "title": "Configuring Data Prepper",
    "content": "You can customize your Data Prepper confiuration by editing the data-prepper-config.yaml file in your Data Prepper installation. The following configuration options are independent from pipeline configuration options. ",
    "url": "https://vagimeli.github.io/data-prepper/managing-data-prepper/configuring-data-prepper/",
    "relUrl": "/data-prepper/managing-data-prepper/configuring-data-prepper/"
  },"1728": {
    "doc": "Configuring Data Prepper",
    "title": "Data Prepper configuration",
    "content": "Use the following options to customize your Data Prepper configuration. | Option | Required | Type | Description | . | ssl | No | Boolean | Indicates whether TLS should be used for server APIs. Defaults to true. | . | keyStoreFilePath | No | String | The path to a .jks or .p12 keystore file. Required if ssl is true. | . | keyStorePassword | No | String | The password for keystore. Optional, defaults to empty string. | . | privateKeyPassword | No | String | The password for a private key within keystore. Optional, defaults to empty string. | . | serverPort | No | Integer | The port number to use for server APIs. Defaults to 4900. | . | metricRegistries | No | List | The metrics registries for publishing the generated metrics. Currently supports Prometheus and Amazon CloudWatch. Defaults to Prometheus. | . | metricTags | No | Map | A map of key-value pairs as common metric tags to metric registries. The maximum number of pairs is three. Note that serviceName is a reserved tag key with DataPrepper as the default tag value. Alternatively, administrators can set this value through the environment variable DATAPREPPER_SERVICE_NAME. If serviceName is defined in metricTags, that value overwrites those set through the above methods. | . | authentication | No | Object | The authentication configuration. Valid option is http_basic with username and password properties. If not defined, the server does not perform authentication. | . | processorShutdownTimeout | No | Duration | The time given to processors to clear any in-flight data and gracefully shut down. Default is 30s. | . | sinkShutdownTimeout | No | Duration | The time given to sinks to clear any in-flight data and gracefully shut down. Default is 30s. | . | peer_forwarder | No | Object | Peer forwarder configurations. See Peer forwarder options for more details. | . | circuit_breakers | No | circuit_breakers | Configures a circuit breaker on incoming data. | . Peer forwarder options . The following section details various configuration options for peer forwarder. General options for peer forwarding . | Option | Required | Type | Description | . | port | No | Integer | The peer forwarding server port. Valid options are between 0 and 65535. Defaults is 4994. | . | request_timeout | No | Integer | The request timeout for the peer forwarder HTTP server in milliseconds. Default is 10000. | . | server_thread_count | No | Integer | The number of threads used by the peer forwarder server. Default is 200. | . | client_thread_count | No | Integer | The number of threads used by the peer forwarder client. Default is 200. | . | max_connection_count | No | Integer | The maximum number of open connections for the peer forwarder server. Default is 500. | . | max_pending_requests | No | Integer | The maximum number of allowed tasks in ScheduledThreadPool work queue. Default is 1024. | . | discovery_mode | No | String | The peer discovery mode to use. Valid options are local_node, static, dns, or aws_cloud_map. Defaults to local_node, which processes events locally. | . | static_endpoints | Conditionally | List | A list containing endpoints of all Data Prepper instances. Required if discovery_mode is set to static. | . | domain_name | Conditionally | String | A single domain name to query DNS against. Typically, used by creating multiple DNS A Records for the same domain. Required if discovery_mode is set to dns. | . | aws_cloud_map_namespace_name | Conditionally | String | Cloud Map namespace when using AWS Cloud Map service discovery. Required if discovery_mode is set to aws_cloud_map. | . | aws_cloud_map_service_name | Conditionally | String | The Cloud Map service name when using AWS Cloud Map service discovery. Required if discovery_mode is set to aws_cloud_map. | . | aws_cloud_map_query_parameters | No | Map | A map of key-value pairs to filter the results based on the custom attributes attached to an instance. Only instances that match all the specified key-value pairs are returned. | . | buffer_size | No | Integer | The maximum number of unchecked records the buffer accepts. Number of unchecked records is the sum of the number of records written into the buffer and the num of in-flight records not yet checked by the Checkpointing API. Default is 512. | . | batch_size | No | Integer | The maximum number of records the buffer returns on read. Default is 48. | . | aws_region | Conditionally | String | The AWS region to use with ACM, S3 or AWS Cloud Map. Required if use_acm_certificate_for_ssl is set to true or ssl_certificate_file and ssl_key_file is AWS S3 path or discovery_mode is set to aws_cloud_map. | . | drain_timeout | No | Duration | The wait time for the peer forwarder to complete processing data before shutdown. Default is 10s. | . TLS/SSL options for peer forwarder . | Option | Required | Type | Description | . | ssl | No | Boolean | Enables TLS/SSL. Default is true. | . | ssl_certificate_file | Conditionally | String | The SSL certificate chain file path or AWS S3 path. S3 path example s3://&lt;bucketName&gt;/&lt;path&gt;. Required if ssl is true and use_acm_certificate_for_ssl is false. Defaults to config/default_certificate.pem which is the default certificate file. Read more about how the certificate file is generated here. | . | ssl_key_file | Conditionally | String | The SSL key file path or AWS S3 path. S3 path example s3://&lt;bucketName&gt;/&lt;path&gt;. Required if ssl is true and use_acm_certificate_for_ssl is false. Defaults to config/default_private_key.pem which is the default private key file. Read more about how the default private key file is generated here. | . | ssl_insecure_disable_verification | No | Boolean | Disables the verification of server’s TLS certificate chain. Default is false. | . | ssl_fingerprint_verification_only | No | Boolean | Disables the verification of server’s TLS certificate chain and instead verifies only the certificate fingerprint. Default is false. | . | use_acm_certificate_for_ssl | No | Boolean | Enables TLS/SSL using certificate and private key from AWS Certificate Manager (ACM). Default is false. | . | acm_certificate_arn | Conditionally | String | The ACM certificate ARN. The ACM certificate takes preference over S3 or a local file system certificate. Required if use_acm_certificate_for_ssl is set to true. | . | acm_private_key_password | No | String | The ACM private key password that decrypts the private key. If not provided, Data Prepper generates a random password. | . | acm_certificate_timeout_millis | No | Integer | The timeout in milliseconds for ACM to get certificates. Default is 120000. | . | aws_region | Conditionally | String | The AWS region to use ACM, S3 or AWS Cloud Map. Required if use_acm_certificate_for_ssl is set to true or ssl_certificate_file and ssl_key_file is AWS S3 path or discovery_mode is set to aws_cloud_map. | . Authentication options for peer forwarder . | Option | Required | Type | Description | . | authentication | No | Map | The authentication method to use. Valid options are mutual_tls (use mTLS) or unauthenticated (no authentication). Default is unauthenticated. | . Circuit breakers . Data Prepper provides a circuit breaker to help prevent exhausting Java memory. And is useful when pipelines have stateful processors as these can retain memory usage outside of the buffers. When a circuit breaker is tripped, Data Prepper rejects incoming data routing into buffers. | Option | Required | Type | Description | . | heap | No | heap | Enables a heap circuit breaker. By default, this is not enabled. | . Heap circuit breaker . Configures Data Prepper to trip a circuit breaker when JVM heap reaches a specified usage threshold. | Option | Required | Type | Description | . | usage | Yes | Bytes | Specifies the JVM heap usage at which to trip a circuit breaker. If the current Java heap usage exceeds this value then the circuit breaker will be open. This can be a value such as 6.5gb. | . | reset | No | Duration | After tripping the circuit breaker, no new checks are made until after this time has passed. This effectively sets the minimum time for a breaker to remain open to allow for clearing memory. Defaults to 1s. | . | check_interval | No | Duration | Specifies the time between checks of the heap size. Defaults to 500ms. | . ",
    "url": "https://vagimeli.github.io/data-prepper/managing-data-prepper/configuring-data-prepper/#data-prepper-configuration",
    "relUrl": "/data-prepper/managing-data-prepper/configuring-data-prepper/#data-prepper-configuration"
  },"1729": {
    "doc": "Configuring Log4j",
    "title": "Configuring Log4j",
    "content": "You can configure logging using Log4j in Data Prepper. ",
    "url": "https://vagimeli.github.io/data-prepper/managing-data-prepper/configuring-log4j/",
    "relUrl": "/data-prepper/managing-data-prepper/configuring-log4j/"
  },"1730": {
    "doc": "Configuring Log4j",
    "title": "Logging",
    "content": "Data Prepper uses SLF4J with a Log4j 2 binding. For Data Prepper versions 2.0 and later, the Log4j 2 configuration file can be found and edited in config/log4j2.properties in the application’s home directory. The default properties for Log4j 2 can be found in log4j2-rolling.properties in the shared-config directory. For Data Prepper versions before 2.0, the Log4j 2 configuration file can be overridden by setting the log4j.configurationFile system property when running Data Prepper. The default properties for Log4j 2 can be found in log4j2.properties in the shared-config directory. Example . When running Data Prepper, the following command can be overridden by setting the system property -Dlog4j.configurationFile={property_value}, where {property_value} is a path to the Log4j 2 configuration file: . java \"-Dlog4j.configurationFile=config/custom-log4j2.properties\" -jar data-prepper-core-$VERSION.jar pipelines.yaml data-prepper-config.yaml . See the Log4j 2 configuration documentation for more information about Log4j 2 configuration. ",
    "url": "https://vagimeli.github.io/data-prepper/managing-data-prepper/configuring-log4j/#logging",
    "relUrl": "/data-prepper/managing-data-prepper/configuring-log4j/#logging"
  },"1731": {
    "doc": "Core APIs",
    "title": "Core APIs",
    "content": "All Data Prepper instances expose a server with some control APIs. By default, this server runs on port 4900. Some plugins, especially source plugins, may expose other servers that run on different ports. Configurations for these plugins are independent of the core API. For example, to shut down Data Prepper, you can run the following curl request: . curl -X POST http://localhost:4900/shutdown . ",
    "url": "https://vagimeli.github.io/data-prepper/managing-data-prepper/core-apis/",
    "relUrl": "/data-prepper/managing-data-prepper/core-apis/"
  },"1732": {
    "doc": "Core APIs",
    "title": "APIs",
    "content": "The following table lists the available APIs. | Name | Description | . | GET /listPOST /list | Returns a list of running pipelines. | . | POST /shutdown | Starts a graceful shutdown of Data Prepper. | . | GET /metrics/prometheusPOST /metrics/prometheus | Returns a scrape of Data Prepper metrics in Prometheus text format. This API is available as a metricsRegistries parameter in the Data Prepper configuration file data-prepper-config.yaml and contains Prometheus as part of the registry. | . | GET /metrics/sysPOST /metrics/sys | Returns JVM metrics in Prometheus text format. This API is available as a metricsRegistries parameter in the Data Prepper configuration file data-prepper-config.yaml and contains Prometheus as part of the registry. | . ",
    "url": "https://vagimeli.github.io/data-prepper/managing-data-prepper/core-apis/#apis",
    "relUrl": "/data-prepper/managing-data-prepper/core-apis/#apis"
  },"1733": {
    "doc": "Core APIs",
    "title": "Configuring the server",
    "content": "You can configure your Data Prepper core APIs through the data-prepper-config.yaml file. SSL/TLS connection . Many of the getting started guides for this project disable SSL on the endpoint: . ssl: false . To enable SSL on your Data Prepper endpoint, configure your data-prepper-config.yaml file with the following options: . ssl: true keyStoreFilePath: \"/usr/share/data-prepper/keystore.p12\" keyStorePassword: \"secret\" privateKeyPassword: \"secret\" . For more information about configuring your Data Prepper server with SSL, see Server Configuration. If you are using a self-signed certificate, you can add the -k flag to the request to quickly test core APIs with SSL. Use the following shutdown request to test core APIs with SSL: . curl -k -X POST https://localhost:4900/shutdown . Authentication . The Data Prepper core APIs support HTTP basic authentication. You can set the username and password with the following configuration in the data-prepper-config.yaml file: . authentication: http_basic: username: \"myuser\" password: \"mys3cr3t\" . You can disable authentication of core endpoints using the following configuration. Use this with caution because the shutdown API and others will be accessible to anybody with network access to your Data Prepper instance. authentication: unauthenticated: . Peer Forwarder . Peer Forwarder can be configured to enable stateful aggregation across multiple Data Prepper nodes. For more information about configuring Peer Forwarder, see Peer forwarder. It is supported by the service_map_stateful, otel_trace_raw, and aggregate processors. Shutdown timeouts . When you run the Data Prepper shutdown API, the process gracefully shuts down and clears any remaining data for both the ExecutorService sink and ExecutorService processor. The default timeout for shutdown of both processes is 10 seconds. You can configure the timeout with the following optional data-prepper-config.yaml file parameters: . processorShutdownTimeout: \"PT15M\" sinkShutdownTimeout: 30s . The values for these parameters are parsed into a Duration object through the Data Prepper Duration Deserializer. ",
    "url": "https://vagimeli.github.io/data-prepper/managing-data-prepper/core-apis/#configuring-the-server",
    "relUrl": "/data-prepper/managing-data-prepper/core-apis/#configuring-the-server"
  },"1734": {
    "doc": "Managing Data Prepper",
    "title": "Managing Data Prepper",
    "content": "You can perform administrator functions for Data Prepper, including system configuration, interacting with core APIs, Log4j configuration, and monitoring. You can set up peer forwarding to coordinate multiple Data Prepper nodes when using stateful aggregation. ",
    "url": "https://vagimeli.github.io/data-prepper/managing-data-prepper/managing-data-prepper/",
    "relUrl": "/data-prepper/managing-data-prepper/managing-data-prepper/"
  },"1735": {
    "doc": "Monitoring",
    "title": "Monitoring Data Prepper with metrics",
    "content": "You can monitor Data Prepper with metrics using Micrometer. There are two types of metrics: JVM/system metrics and plugin metrics. Prometheus is used as the default metrics backend. ",
    "url": "https://vagimeli.github.io/data-prepper/managing-data-prepper/monitoring/#monitoring-data-prepper-with-metrics",
    "relUrl": "/data-prepper/managing-data-prepper/monitoring/#monitoring-data-prepper-with-metrics"
  },"1736": {
    "doc": "Monitoring",
    "title": "JVM and system metrics",
    "content": "JVM and system metrics are runtime metrics that are used to monitor Data Prepper instances. They include metrics for classloaders, memory, garbage collection, threads, and others. For more information, see JVM and system metrics. Naming . JVM and system metrics follow predefined names in Micrometer. For example, the Micrometer metrics name for memory usage is jvm.memory.used. Micrometer changes the name to match the metrics system. Following the same example, jvm.memory.used is reported to Prometheus as jvm_memory_used, and is reported to Amazon CloudWatch as jvm.memory.used.value. Serving . By default, metrics are served from the /metrics/sys endpoint on the Data Prepper server in Prometheus scrape format. You can configure Prometheus to scrape from the Data Prepper URL. Prometheus then polls Data Prepper for metrics and stores them in its database. To visualize the data, you can set up any frontend that accepts Prometheus metrics, such as Grafana. You can update the configuration to serve metrics to other registries like Amazon CloudWatch, which does not require or host the endpoint but publishes the metrics directly to CloudWatch. ",
    "url": "https://vagimeli.github.io/data-prepper/managing-data-prepper/monitoring/#jvm-and-system-metrics",
    "relUrl": "/data-prepper/managing-data-prepper/monitoring/#jvm-and-system-metrics"
  },"1737": {
    "doc": "Monitoring",
    "title": "Plugin metrics",
    "content": "Plugins report their own metrics. Data Prepper uses a naming convention to help with consistency in the metrics. Plugin metrics do not use dimensions. | AbstractBuffer . | Counter . | recordsWritten: The number of records written into a buffer | recordsRead: The number of records read from a buffer | recordsProcessed: The number of records read from a buffer and marked as processed | writeTimeouts: The count of write timeouts in a buffer | . | Gaugefir . | recordsInBuffer: The number of records in a buffer | recordsInFlight: The number of records read from a buffer and being processed by data-prepper downstreams (for example, processor, sink) | . | Timer . | readTimeElapsed: The time elapsed while reading from a buffer | checkpointTimeElapsed: The time elapsed while checkpointing | . | . | AbstractProcessor . | Counter . | recordsIn: The number of records ingressed into a processor | recordsOut: The number of records egressed from a processor | . | Timer . | timeElapsed: The time elapsed during initiation of a processor | . | . | AbstractSink . | Counter . | recordsIn: The number of records ingressed into a sink | . | Timer . | timeElapsed: The time elapsed during execution of a sink | . | . | . Naming . Metrics follow a naming convention of PIPELINE_NAME_PLUGIN_NAME_METRIC_NAME. For example, a recordsIn metric for the opensearch-sink plugin in a pipeline named output-pipeline has a qualified name of output-pipeline_opensearch_sink_recordsIn. Serving . By default, metrics are served from the /metrics/sys endpoint on the Data Prepper server in a Prometheus scrape format. You can configure Prometheus to scrape from the Data Prepper URL. The Data Prepper server port has a default value of 4900 that you can modify, and this port can be used for any frontend that accepts Prometheus metrics, such as Grafana. You can update the configuration to serve metrics to other registries like CloudWatch, that does not require or host the endpoint, but publishes the metrics directly to CloudWatch. ",
    "url": "https://vagimeli.github.io/data-prepper/managing-data-prepper/monitoring/#plugin-metrics",
    "relUrl": "/data-prepper/managing-data-prepper/monitoring/#plugin-metrics"
  },"1738": {
    "doc": "Monitoring",
    "title": "Monitoring",
    "content": " ",
    "url": "https://vagimeli.github.io/data-prepper/managing-data-prepper/monitoring/",
    "relUrl": "/data-prepper/managing-data-prepper/monitoring/"
  },"1739": {
    "doc": "Peer forwarder",
    "title": "Peer forwarder",
    "content": "Peer forwarder is an HTTP service that performs peer forwarding of an event between Data Prepper nodes for aggregation. This HTTP service uses a hash-ring approach to aggregate events and determine which Data Prepper node it should handle on a given trace before rerouting it to that node. Currently, peer forwarder is supported by the aggregate, service_map_stateful, and otel_trace_raw processors. Peer forwarder groups events based on the identification keys provided by the supported processors. For service_map_stateful and otel_trace_raw, the identification key is traceId by default and cannot be configured. The aggregate processor is configured using the identification_keys configuration option. From here, you can specify which keys to use for peer forwarder. See Aggregate Processor page for more information about identification keys. Peer discovery allows Data Prepper to find other nodes that it will communicate with. Currently, peer discovery is provided by a static list, a DNS record lookup, or AWS Cloud Map. ",
    "url": "https://vagimeli.github.io/data-prepper/managing-data-prepper/peer-forwarder/",
    "relUrl": "/data-prepper/managing-data-prepper/peer-forwarder/"
  },"1740": {
    "doc": "Peer forwarder",
    "title": "Discovery modes",
    "content": "The following sections provide information about discovery modes. Static . Static discovery mode allows a Data Prepper node to discover nodes using a list of IP addresses or domain names. See the following YAML file for an example of static discovery mode: . peer_forwarder:4 discovery_mode: static static_endpoints: [\"data-prepper1\", \"data-prepper2\"] . DNS lookup . DNS discovery is preferred over static discovery when scaling out a Data Prepper cluster. DNS discovery configures a DNS provider to return a list of Data Prepper hosts when given a single domain name. This list consists of a DNS A record, and a list of IP addresses of a given domain. See the following YAML file for an example of DNS lookup: . peer_forwarder: discovery_mode: dns domain_name: \"data-prepper-cluster.my-domain.net\" . AWS Cloud Map . AWS Cloud Map provides API-based service discovery as well as DNS-based service discovery. Peer forwarder can use the API-based service discovery in AWS Cloud Map. To support this, you must have an existing namespace configured for API instance discovery. You can create a new one by following the instructions provided by the AWS Cloud Map documentation. Your Data Prepper configuration needs to include the following: . | aws_cloud_map_namespace_name – Set to your AWS Cloud Map namespace name. | aws_cloud_map_service_name – Set to the service name within your specified namespace. | aws_region – Set to the AWS Region in which your namespace exists. | discovery_mode – Set to aws_cloud_map. | . Your Data Prepper configuration can optionally include the following: . | aws_cloud_map_query_parameters – Key-value pairs are used to filter the results based on the custom attributes attached to an instance. Results include only those instances that match all of the specified key-value pairs. | . Example configuration . See the following YAML file example of AWS Cloud Map configuration: . peer_forwarder: discovery_mode: aws_cloud_map aws_cloud_map_namespace_name: \"my-namespace\" aws_cloud_map_service_name: \"data-prepper-cluster\" aws_cloud_map_query_parameters: instance_type: \"r5.xlarge\" aws_region: \"us-east-1\" . IAM policy with necessary permissions . Data Prepper must also be running with the necessary permissions. The following AWS Identity and Access Management (IAM) policy shows the necessary permissions: . { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"CloudMapPeerForwarder\", \"Effect\": \"Allow\", \"Action\": \"servicediscovery:DiscoverInstances\", \"Resource\": \"*\" } ] } . ",
    "url": "https://vagimeli.github.io/data-prepper/managing-data-prepper/peer-forwarder/#discovery-modes",
    "relUrl": "/data-prepper/managing-data-prepper/peer-forwarder/#discovery-modes"
  },"1741": {
    "doc": "Peer forwarder",
    "title": "Configuration",
    "content": "The following table provides optional configuration values. | Value | Type | Description | . | port | Integer | A value between 0 and 65535 that represents the port that the peer forwarder server is running on. Default value is 4994. | . | request_timeout | Integer | Represents the request timeout duration in milliseconds for the peer forwarder HTTP server. Default value is 10000. | . | server_thread_count | Integer | Represents the number of threads used by the peer forwarder server. Default value is 200. | . | client_thread_count | Integer | Represents the number of threads used by the peer forwarder client. Default value is 200. | . | maxConnectionCount | Integer | Represents the maximum number of open connections for the peer forwarder server. Default value is 500. | . | discovery_mode | String | Represents the peer discovery mode to be used. Allowable values are local_node, static, dns, and aws_cloud_map. Defaults to local_node, which processes events locally. | . | static_endpoints | List | Contains the endpoints of all Data Prepper instances. Required if discovery_mode is set to static. | . | domain_name | String | Represents the single domain name to query DNS against. Typically used by creating multiple DNS A records for the same domain. Required if discovery_mode is set to dns. | . | aws_cloud_map_namespace_name | String | Represents the AWS Cloud Map namespace when using AWS Cloud Map service discovery. Required if discovery_mode is set to aws_cloud_map. | . | aws_cloud_map_service_name | String | Represents the AWS Cloud Map service when using AWS Cloud Map service discovery. Required if discovery_mode is set to aws_cloud_map. | . | aws_cloud_map_query_parameters | Map | Key-value pairs used to filter the results based on the custom attributes attached to an instance. Only instances that match all the specified key-value pairs are returned. | . | buffer_size | Integer | Represents the maximum number of unchecked records the buffer accepts (the number of unchecked records equals the number of records written into the buffer plus the number of records that are still processing and not yet checked by the Checkpointing API). Default is 512. | . | batch_size | Integer | Represents the maximum number of records that the buffer returns on read. Default is 48. | . | aws_region | String | Represents the AWS Region that uses ACM, Amazon S3, or AWS Cloud Map and is required when any of the following conditions are met: - The use_acm_certificate_for_ssl setting is set to true. - Either ssl_certificate_file or ssl_key_file specifies an Amazon Simple Storage Service (Amazon S3) URI (for example, s3://mybucket/path/to/public.cert). - The discovery_mode is set to aws_cloud_map. | . | drain_timeout | Duration | Represents the amount of time that peer forwarder will wait to complete data processing before shutdown. | . ",
    "url": "https://vagimeli.github.io/data-prepper/managing-data-prepper/peer-forwarder/#configuration",
    "relUrl": "/data-prepper/managing-data-prepper/peer-forwarder/#configuration"
  },"1742": {
    "doc": "Peer forwarder",
    "title": "SSL configuration",
    "content": "The following table provides optional SSL configuration values that allow you to set up a trust manager for the peer forwarder client in order to connect to other Data Prepper instances. | Value | Type | Description | . | ssl | Boolean | Enables TLS/SSL. Default value is true. | . | ssl_certificate_file | String | Represents the SSL certificate chain file path or Amazon S3 path. The following is an example of an Amazon S3 path: s3://&lt;bucketName&gt;/&lt;path&gt;. Defaults to the default certificate file,config/default_certificate.pem. See Default Certificates for more information about how the certificate is generated. | . | ssl_key_file | String | Represents the SSL key file path or Amazon S3 path. Amazon S3 path example: s3://&lt;bucketName&gt;/&lt;path&gt;. Defaults to config/default_private_key.pem which is the default private key file. See Default Certificates for more information about how the private key file is generated. | . | ssl_insecure_disable_verification | Boolean | Disables the verification of the server’s TLS certificate chain. Default value is false. | . | ssl_fingerprint_verification_only | Boolean | Disables the verification of the server’s TLS certificate chain and instead verifies only the certificate fingerprint. Default value is false. | . | use_acm_certificate_for_ssl | Boolean | Enables TLS/SSL using the certificate and private key from AWS Certificate Manager (ACM). Default value is false. | . | acm_certificate_arn | String | Represents the ACM certificate Amazon Resource Name (ARN). The ACM certificate takes precedence over Amazon S3 or the local file system certificate. Required if use_acm_certificate_for_ssl is set to true. | . | acm_private_key_password | String | Represents the ACM private key password that will be used to decrypt the private key. If it’s not provided, a random password will be generated. | . | acm_certificate_timeout_millis | Integer | Represents the timeout in milliseconds required for ACM to get certificates. Default value is 120000. | . | aws_region | String | Represents the AWS Region that uses ACM, Amazon S3, or AWS Cloud Map. Required if use_acm_certificate_for_ssl is set to true or ssl_certificate_file. Also required when the ssl_key_file is set to use the Amazon S3 path or if discovery_mode is set to aws_cloud_map. | . Example configuration . The following YAML file provides an example configuration: . peer_forwarder: ssl: true ssl_certificate_file: \"&lt;cert-file-path&gt;\" ssl_key_file: \"&lt;private-key-file-path&gt;\" . ",
    "url": "https://vagimeli.github.io/data-prepper/managing-data-prepper/peer-forwarder/#ssl-configuration",
    "relUrl": "/data-prepper/managing-data-prepper/peer-forwarder/#ssl-configuration"
  },"1743": {
    "doc": "Peer forwarder",
    "title": "Authentication",
    "content": "Authentication is optional and is a Map that enables mutual TLS (mTLS). It can either be mutual_tls or unauthenticated. The default value is unauthenticated. The following YAML file provides an example of authentication: . peer_forwarder: authentication: mutual_tls: . ",
    "url": "https://vagimeli.github.io/data-prepper/managing-data-prepper/peer-forwarder/#authentication",
    "relUrl": "/data-prepper/managing-data-prepper/peer-forwarder/#authentication"
  },"1744": {
    "doc": "Peer forwarder",
    "title": "Metrics",
    "content": "Core peer forwarder introduces the following custom metrics. All the metrics are prefixed by core.peerForwarder. Timer . Peer forwarder’s timer capability provides the following information: . | requestForwardingLatency: Measures latency of requests forwarded by the peer forwarder client. | requestProcessingLatency: Measures latency of requests processed by the peer forwarder server. | . Counter . The following table provides counter metric options. | Value | Description | . | requests | Measures the total number of forwarded requests. | . | requestsFailed | Measures the total number of failed requests. Applies to requests with an HTTP response code other than 200. | . | requestsSuccessful | Measures the total number of successful requests. Applies to requests with HTTP response code 200. | . | requestsTooLarge | Measures the total number of requests that are too large to be written to the peer forwarder buffer. Applies to requests with HTTP response code 413. | . | requestTimeouts | Measures the total number of requests that time out while writing content to the peer forwarder buffer. Applies to requests with HTTP response code 408. | . | requestsUnprocessable | Measures the total number of requests that fail due to an unprocessable entity. Applies to requests with HTTP response code 422. | . | badRequests | Measures the total number of requests with a bad request format. Applies to requests with HTTP response code 400. | . | recordsSuccessfullyForwarded | Measures the total number of successfully forwarded records. | . | recordsFailedForwarding | Measures the total number of records that fail to be forwarded. | . | recordsToBeForwarded | Measures the total number of records to be forwarded. | . | recordsToBeProcessedLocally | Measures the total number of records to be processed locally. | . | recordsActuallyProcessedLocally | Measures the total number of records actually processed locally. This value is the sum of recordsToBeProcessedLocally and recordsFailedForwarding. | . | recordsReceivedFromPeers | Measures the total number of records received from remote peers. | . Gauge . peerEndpoints Measures the number of dynamically discovered peer Data Prepper endpoints. For static mode, the size is fixed. ",
    "url": "https://vagimeli.github.io/data-prepper/managing-data-prepper/peer-forwarder/#metrics",
    "relUrl": "/data-prepper/managing-data-prepper/peer-forwarder/#metrics"
  },"1745": {
    "doc": "Migrating from Open Distro",
    "title": "Migrating from Open Distro",
    "content": "Existing users can migrate from the Open Distro Data Prepper to OpenSearch Data Prepper. Beginning with Data Prepper version 1.1, there is only one distribution of OpenSearch Data Prepper. ",
    "url": "https://vagimeli.github.io/data-prepper/migrate-open-distro/",
    "relUrl": "/data-prepper/migrate-open-distro/"
  },"1746": {
    "doc": "Migrating from Open Distro",
    "title": "Change your pipeline configuration",
    "content": "The elasticsearch sink has changed to opensearch. Therefore, change your existing pipeline to use the opensearch plugin instead of elasticsearch. While the Data Prepper plugin is titled opensearch, it remains compatible with Open Distro and ElasticSearch 7.x. ",
    "url": "https://vagimeli.github.io/data-prepper/migrate-open-distro/#change-your-pipeline-configuration",
    "relUrl": "/data-prepper/migrate-open-distro/#change-your-pipeline-configuration"
  },"1747": {
    "doc": "Migrating from Open Distro",
    "title": "Update Docker image",
    "content": "In your Data Prepper Docker configuration, adjust amazon/opendistro-for-elasticsearch-data-prepper to opensearchproject/data-prepper. This change will download the latest Data Prepper Docker image. ",
    "url": "https://vagimeli.github.io/data-prepper/migrate-open-distro/#update-docker-image",
    "relUrl": "/data-prepper/migrate-open-distro/#update-docker-image"
  },"1748": {
    "doc": "Migrating from Open Distro",
    "title": "Next steps",
    "content": "For more information about Data Prepper configurations, see Getting Started with Data Prepper. ",
    "url": "https://vagimeli.github.io/data-prepper/migrate-open-distro/#next-steps",
    "relUrl": "/data-prepper/migrate-open-distro/#next-steps"
  },"1749": {
    "doc": "Migrating from Logstash",
    "title": "Migrating from Logstash",
    "content": "You can run Data Prepper with a Logstash configuration. As mentioned in Getting started with Data Prepper, you’ll need to configure Data Prepper with a pipeline using a pipelines.yaml file. Alternatively, if you have a Logstash configuration logstash.conf to configure Data Prepper instead of pipelines.yaml. ",
    "url": "https://vagimeli.github.io/data-prepper/migrating-from-logstash-data-prepper/",
    "relUrl": "/data-prepper/migrating-from-logstash-data-prepper/"
  },"1750": {
    "doc": "Migrating from Logstash",
    "title": "Supported plugins",
    "content": "As of the Data Prepper 1.2 release, the following plugins from the Logstash configuration are supported: . | HTTP Input plugin | Grok Filter plugin | Elasticsearch Output plugin | Amazon Elasticsearch Output plugin | . ",
    "url": "https://vagimeli.github.io/data-prepper/migrating-from-logstash-data-prepper/#supported-plugins",
    "relUrl": "/data-prepper/migrating-from-logstash-data-prepper/#supported-plugins"
  },"1751": {
    "doc": "Migrating from Logstash",
    "title": "Limitations",
    "content": ". | Apart from the supported plugins, all other plugins from the Logstash configuration will throw an Exception and fail to run. | Conditionals in the Logstash configuration are not supported as of the Data Prepper 1.2 release. | . ",
    "url": "https://vagimeli.github.io/data-prepper/migrating-from-logstash-data-prepper/#limitations",
    "relUrl": "/data-prepper/migrating-from-logstash-data-prepper/#limitations"
  },"1752": {
    "doc": "Migrating from Logstash",
    "title": "Running Data Prepper with a Logstash configuration",
    "content": ". | To install Data Prepper’s Docker image, see Installing Data Prepper in Getting Started with Data Prepper. | Run the Docker image installed in Step 1 by supplying your logstash.conf configuration. | . docker run --name data-prepper -p 4900:4900 -v ${PWD}/logstash.conf:/usr/share/data-prepper/pipelines.conf opensearchproject/data-prepper:latest pipelines.conf . The logstash.conf file is converted to logstash.yaml by mapping the plugins and attributes in the Logstash configuration to the corresponding plugins and attributes in Data Prepper. You can find the converted logstash.yaml file in the same directory where you stored logstash.conf. The following output in your terminal indicates that Data Prepper is running correctly: . INFO org.opensearch.dataprepper.pipeline.ProcessWorker - log-pipeline Worker: No records received from buffer . ",
    "url": "https://vagimeli.github.io/data-prepper/migrating-from-logstash-data-prepper/#running-data-prepper-with-a-logstash-configuration",
    "relUrl": "/data-prepper/migrating-from-logstash-data-prepper/#running-data-prepper-with-a-logstash-configuration"
  },"1753": {
    "doc": "Bounded blocking",
    "title": "Bounded blocking",
    "content": " ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/buffers/bounded-blocking/",
    "relUrl": "/data-prepper/pipelines/configuration/buffers/bounded-blocking/"
  },"1754": {
    "doc": "Bounded blocking",
    "title": "Overview",
    "content": "Bounded blocking is the default buffer and is memory based. The following table describes the Bounded blocking parameters. | Option | Required | Type | Description | . | buffer_size | No | Integer | The maximum number of records the buffer accepts. Default value is 12800. | . | batch_size | No | Integer | The maximum number of records the buffer drains after each read. Default value is 200. | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/buffers/bounded-blocking/#overview",
    "relUrl": "/data-prepper/pipelines/configuration/buffers/bounded-blocking/#overview"
  },"1755": {
    "doc": "Buffers",
    "title": "Buffers",
    "content": "Buffers store data as it passes through the pipeline. If you implement a custom buffer, it can be memory based, which provides better performance, or disk based, which is larger in size. ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/buffers/buffers/",
    "relUrl": "/data-prepper/pipelines/configuration/buffers/buffers/"
  },"1756": {
    "doc": "add_entries",
    "title": "add_entries",
    "content": "The add_entries processor adds entries to an event. Configuration . You can configure the add_entries processor with the following options. | Option | Required | Description | . | entries | Yes | A list of entries to add to an event. | . | key | Yes | The key of the new entry to be added. Some examples of keys include my_key, myKey, and object/sub_Key. | . | value | Yes | The value of the new entry to be added. You can use the following data types: strings, Booleans, numbers, null, nested objects, and arrays. | . | overwrite_if_key_exists | No | When set to true, the existing value is overwritten if key already exists in the event. The default value is false. | . Usage . To get started, create the following pipeline.yaml file: . pipeline: source: ...... processor: - add_entries: entries: - key: \"newMessage\" value: 3 overwrite_if_key_exists: true sink: . copy . For example, when your source contains the following event record: . {\"message\": \"hello\"} . And then you run the add_entries processor using the example pipeline, it adds a new entry, {\"newMessage\": 3}, to the existing event, {\"message\": \"hello\"}, so that the new event contains two entries in the final output: . {\"message\": \"hello\", \"newMessage\": 3} . If newMessage already exists, its existing value is overwritten with a value of 3. ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/add-entries/",
    "relUrl": "/data-prepper/pipelines/configuration/processors/add-entries/"
  },"1757": {
    "doc": "aggregate",
    "title": "aggregate",
    "content": "The aggregate processor groups events based on the values of identification_keys. Then, the processor performs an action on each group, helping reduce unnecessary log volume and creating aggregated logs over time. You can use existing actions or create your own custom aggregations using Java code. ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/aggregate/",
    "relUrl": "/data-prepper/pipelines/configuration/processors/aggregate/"
  },"1758": {
    "doc": "aggregate",
    "title": "Configuration",
    "content": "The following table describes the options you can use to configure the aggregate processor. | Option | Required | Type | Description | . | identification_keys | Yes | List | An unordered list by which to group events. Events with the same values as these keys are put into the same group. If an event does not contain one of the identification_keys, then the value of that key is considered to be equal to null. At least one identification_key is required (for example, [\"sourceIp\", \"destinationIp\", \"port\"]). | . | action | Yes | AggregateAction | The action to be performed on each group. One of the available aggregate actions must be provided, or you can create custom aggregate actions. remove_duplicates and put_all are the available actions. For more information, see Creating New Aggregate Actions. | . | group_duration | No | String | The amount of time that a group should exist before it is concluded automatically. Supports ISO_8601 notation strings (“PT20.345S”, “PT15M”, etc.) as well as simple notation for seconds (\"60s\") and milliseconds (\"1500ms\"). Default value is 180s. | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/aggregate/#configuration",
    "relUrl": "/data-prepper/pipelines/configuration/processors/aggregate/#configuration"
  },"1759": {
    "doc": "aggregate",
    "title": "Available aggregate actions",
    "content": "Use the following aggregate actions to determine how the aggregate processor processes events in each group. remove_duplicates . The remove_duplicates action processes the first event for a group immediately and drops any events that duplicate the first event from the source. For example, when using identification_keys: [\"sourceIp\", \"destination_ip\"]: . | The remove_duplicates action processes { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"status\": 200 }, the first event in the source. | Data Prepper drops the { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"bytes\": 1000 } event because the sourceIp and destinationIp match the first event in the source. | The remove_duplicates action processes the next event, { \"sourceIp\": \"127.0.0.2\", \"destinationIp\": \"192.168.0.1\", \"bytes\": 1000 }. Because the sourceIp is different from the first event of the group, Data Prepper creates a new group based on the event. | . put_all . The put_all action combines events belonging to the same group by overwriting existing keys and adding new keys, similarly to the Java Map.putAll. The action drops all events that make up the combined event. For example, when using identification_keys: [\"sourceIp\", \"destination_ip\"], the put_all action processes the following three events: . { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"status\": 200 } { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"bytes\": 1000 } { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"http_verb\": \"GET\" } . Then the action combines the events into one. The pipeline then uses the following combined event: . { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"status\": 200, \"bytes\": 1000, \"http_verb\": \"GET\" } . count . The count event counts events that belong to the same group and generates a new event with values of the identification_keys and the count, which indicates the number of new events. You can customize the processor with the following configuration options: . | count_key: Key used for storing the count. Default name is aggr._count. | start_time_key: Key used for storing the start time. Default name is aggr._start_time. | output_format: Format of the aggregated event. | otel_metrics: Default output format. Outputs in OTel metrics SUM type with count as value. | raw - Generates a JSON object with the count_key field as a count value and the start_time_key field with aggregation start time as value. | . | . For an example, when using identification_keys: [\"sourceIp\", \"destination_ip\"], the count action counts and processes the following events: . { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"status\": 200 } { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"status\": 503 } { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"status\": 400 } . The processor creates the following event: . {\"isMonotonic\":true,\"unit\":\"1\",\"aggregationTemporality\":\"AGGREGATION_TEMPORALITY_DELTA\",\"kind\":\"SUM\",\"name\":\"count\",\"description\":\"Number of events\",\"startTime\":\"2022-12-02T19:29:51.245358486Z\",\"time\":\"2022-12-02T19:30:15.247799684Z\",\"value\":3.0,\"sourceIp\":\"127.0.0.1\",\"destinationIp\":\"192.168.0.1\"} . histogram . The histogram action aggregates events belonging to the same group and generates a new event with values of the identification_keys and histogram of the aggregated events based on a configured key. The histogram contains the number of events, sum, buckets, bucket counts, and optionally min and max of the values corresponding to the key. The action drops all events that make up the combined event. You can customize the processor with the following configuration options: . | key: Name of the field in the events the histogram generates. | generated_key_prefix: key_prefix used by all the fields created in the aggregated event. Having a prefix ensures that the names of the histogram event do not conflict with the field names in the event. | units: The units for the values in the key. | record_minmax: A Boolean value indicating whether the histogram should include the min and max of the values in the aggregation. | buckets: A list of buckets (values of type double) indicating the buckets in the histogram. | output_format: Format of the aggregated event. | otel_metrics: Default output format. Outputs in OTel metrics SUM type with count as value. | raw: Generates a JSON object with count_key field with count as value and start_time_key field with aggregation start time as value. | . | . For example, when using identification_keys: [\"sourceIp\", \"destination_ip\", \"request\"], key: latency, and buckets: [0.0, 0.25, 0.5], the histogram action processes the following events: . { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"request\" : \"/index.html\", \"latency\": 0.2 } { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"request\" : \"/index.html\", \"latency\": 0.55} { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"request\" : \"/index.html\", \"latency\": 0.25 } { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"request\" : \"/index.html\", \"latency\": 0.15 } . Then the processor creates the following event: . {\"max\":0.55,\"kind\":\"HISTOGRAM\",\"buckets\":[{\"min\":-3.4028234663852886E38,\"max\":0.0,\"count\":0},{\"min\":0.0,\"max\":0.25,\"count\":2},{\"min\":0.25,\"max\":0.50,\"count\":1},{\"min\":0.50,\"max\":3.4028234663852886E38,\"count\":1}],\"count\":4,\"bucketCountsList\":[0,2,1,1],\"description\":\"Histogram of latency in the events\",\"sum\":1.15,\"unit\":\"seconds\",\"aggregationTemporality\":\"AGGREGATION_TEMPORALITY_DELTA\",\"min\":0.15,\"bucketCounts\":4,\"name\":\"histogram\",\"startTime\":\"2022-12-14T06:43:40.848762215Z\",\"explicitBoundsCount\":3,\"time\":\"2022-12-14T06:44:04.852564623Z\",\"explicitBounds\":[0.0,0.25,0.5],\"request\":\"/index.html\",\"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"key\": \"latency\"} . rate_limiter . The rate_limiter action controls the number of events aggregated per second. By default, rate_limiter blocks the aggregate processor from running if it receives more events than the configured number allowed. You can overwrite the number events that triggers the rate_limited by using the when_exceeds configuration option. You can customize the processor with the following configuration options: . | events_per_second: The number of events allowed per second. | when_exceeds: Indicates what action the rate_limiter takes when the number of events received is greater than the number of events allowed per second. Default value is block, which blocks the processor from running after the maximum number of events allowed per second is reached until the next second. Alternatively, the drop option drops the excess events received in that second. | . For example, if events_per_second is set to 1 and when_exceeds is set to drop, the action tries to process the following events when received during the one second time interval: . { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"status\": 200 } { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"bytes\": 1000 } { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"http_verb\": \"GET\" } . The following event is processed, but all other events are ignored because the rate_limiter blocks them: . { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"status\": 200 } . If when_exceeds is set to drop, all three events are processed. percent_sampler . The percent_sampler action controls the number of events aggregated based on a percentage of events. The action drops any events not included in the percentage. You can set the percentage of events using the percent configuration, which indicates the percentage of events processed during a one second interval (0%–100%). For example, if percent is set to 50, the action tries to process the following events in the one-second interval: . { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"bytes\": 2500 } { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"bytes\": 500 } { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"bytes\": 1000 } { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"bytes\": 3100 } . The pipeline processes 50% of the events, drops the other events, and does not generate a new event: . { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"bytes\": 500 } { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"bytes\": 3100 } . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/aggregate/#available-aggregate-actions",
    "relUrl": "/data-prepper/pipelines/configuration/processors/aggregate/#available-aggregate-actions"
  },"1760": {
    "doc": "aggregate",
    "title": "Metrics",
    "content": "The following table describes common Abstract processor metrics. | Metric name | Type | Description | . | recordsIn | Counter | Metric representing the ingress of records to a pipeline component. | . | recordsOut | Counter | Metric representing the egress of records from a pipeline component. | . | timeElapsed | Timer | Metric representing the time elapsed during execution of a pipeline component. | . The aggregate processor includes the following custom metrics. Counter . | actionHandleEventsOut: The number of events that have been returned from the handleEvent call to the configured action. | actionHandleEventsDropped: The number of events that have not been returned from the handleEvent call to the configured action. | actionHandleEventsProcessingErrors: The number of calls made to handleEvent for the configured action that resulted in an error. | actionConcludeGroupEventsOut: The number of events that have been returned from the concludeGroup call to the configured action. | actionConcludeGroupEventsDropped: The number of events that have not been returned from the condludeGroup call to the configured action. | actionConcludeGroupEventsProcessingErrors: The number of calls made to concludeGroup for the configured action that resulted in an error. | . Gauge . | currentAggregateGroups: The current number of groups. This gauge decreases when a group concludes and increases when an event initiates the creation of a new group. | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/aggregate/#metrics",
    "relUrl": "/data-prepper/pipelines/configuration/processors/aggregate/#metrics"
  },"1761": {
    "doc": "anomaly_detector",
    "title": "anomaly_detector",
    "content": "The anomaly detector processor takes structured data and runs anomaly detection algorithms on fields that you can configure in that data. The data must be either an integer or a real number for the anomaly detection algorithm to detect anomalies. Deploying the aggregate processor in a pipeline before the anomaly detector processor can help you achieve the best results, as the aggregate processor automatically aggregates events by key and keeps them on the same host. For example, if you are searching for an anomaly in latencies from a specific IP address and if all the events go to the same host, then the host has more data for these events. This additional data results in better training of the machine learning (ML) algorithm, which results in better anomaly detection. ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/anomaly-detector/",
    "relUrl": "/data-prepper/pipelines/configuration/processors/anomaly-detector/"
  },"1762": {
    "doc": "anomaly_detector",
    "title": "Configuration",
    "content": "You can configure the anomaly detector processor by specifying a key and the options for the selected mode. You can use the following options to configure the anomaly detector processor. | Name | Required | Description | . | keys | Yes | A non-ordered List&lt;String&gt; that is used as input to the ML algorithm to detect anomalies in the values of the keys in the list. At least one key is required. | . | mode | Yes | The ML algorithm (or model) used to detect anomalies. You must provide a mode. See random_cut_forest mode. | . Keys . Keys that are used in the anomaly detector processor are present in the input event. For example, if the input event is {\"key1\":value1, \"key2\":value2, \"key3\":value3}, then any of the keys (such as key1, key2, key3) in that input event can be used as anomaly detector keys as long as their value (such as value1, value2, value3) is an integer or real number. random_cut_forest mode . The random cut forest (RCF) ML algorithm is an unsupervised algorithm for detecting anomalous data points within a dataset. To detect anomalies, the anomaly detector processor uses the random_cut_forest mode. | Name | Description | . | random_cut_forest | Processes events using the RCF ML algorithm to detect anomalies. | . RCF is an unsupervised ML algorithm for detecting anomalous data points within a dataset. Data Prepper uses RCF to detect anomalies in data by passing the values of the configured key to RCF. For example, when an event with a latency value of 11.5 is sent, the following anomaly event is generated: . { \"latency\": 11.5, \"deviation_from_expected\":[10.469302736820003],\"grade\":1.0} . In this example, deviation_from_expected is a list of deviations for each of the keys from their corresponding expected values, and grade is the anomaly grade that indicates the anomaly severity. You can configure random_cut_forest mode with the following options. | Name | Default value | Range | Description | . | shingle_size | 4 | 1–60 | The shingle size used in the ML algorithm. | . | sample_size | 256 | 100–2500 | The sample size used in the ML algorithm. | . | time_decay | 0.1 | 0–1.0 | The time decay value used in the ML algorithm. Used as the mathematical expression timeDecay divided by SampleSize in the ML algorithm. | . | type | metrics | N/A | The type of data sent to the algorithm. | . | version | 1.0 | N/A | The algorithm version number. | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/anomaly-detector/#configuration",
    "relUrl": "/data-prepper/pipelines/configuration/processors/anomaly-detector/#configuration"
  },"1763": {
    "doc": "anomaly_detector",
    "title": "Usage",
    "content": "To get started, create the following pipeline.yaml file. You can use the following pipeline configuration to look for anomalies in the latency field in events that are passed to the processor. Then you can use the following YAML configuration file random_cut_forest mode to detect anomalies: . ad-pipeline: source: ...... processor: - anomaly_detector: keys: [\"latency\"] mode: random_cut_forest: . When you run the anomaly detector processor, the processor extracts the value for the latency key, and then passes the value through the RCF ML algorithm. You can configure any key that comprises integers or real numbers as values. In the following example, you can configure bytes or latency as the key for an anomaly detector. {\"ip\":\"1.2.3.4\", \"bytes\":234234, \"latency\":0.2} . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/anomaly-detector/#usage",
    "relUrl": "/data-prepper/pipelines/configuration/processors/anomaly-detector/#usage"
  },"1764": {
    "doc": "convert_entry_type",
    "title": "convert_entry_type",
    "content": "The convert_entry_type processor converts a value type associated with the specified key in a event to the specified type. It is a casting processor that changes the types of some fields in events. Some data must be converted to a different type, such as an integer to a double, or a string to an integer, so that it will pass the events through condition-based processors or perform conditional routing. ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/convert_entry_type/",
    "relUrl": "/data-prepper/pipelines/configuration/processors/convert_entry_type/"
  },"1765": {
    "doc": "convert_entry_type",
    "title": "Configuration",
    "content": "You can configure the convert_entry_type processor with the following options. | Option | Required | Description | . | key | Yes | Keys whose value needs to be converted to a different type. | . | type | No | Target type for the key-value pair. Possible values are integer, double, string, and Boolean. Default value is integer. | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/convert_entry_type/#configuration",
    "relUrl": "/data-prepper/pipelines/configuration/processors/convert_entry_type/#configuration"
  },"1766": {
    "doc": "convert_entry_type",
    "title": "Usage",
    "content": "To get started, create the following pipeline.yaml file: . type-conv-pipeline: source: ...... processor: - convert_entry_type_type: key: \"response_status\" type: \"integer\" . copy . Next, create a log file named logs_json.log and replace the path in the file source of your pipeline.yaml file with that filepath. For more information, see Configuring Data Prepper. For example, before you run the convert_entry_type processor, if the logs_json.log file contains the following event record: . {\"message\": \"value\", \"response_status\":\"200\"} . The convert_entry_type processor converts the output received to the following output, where the type of response_status value changes from a string to an integer: . {\"message\":\"value\",\"response_status\":200} . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/convert_entry_type/#usage",
    "relUrl": "/data-prepper/pipelines/configuration/processors/convert_entry_type/#usage"
  },"1767": {
    "doc": "copy_values",
    "title": "copy_values",
    "content": "The copy_values processor copies values within an event and is a mutate event processor. ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/copy-values/",
    "relUrl": "/data-prepper/pipelines/configuration/processors/copy-values/"
  },"1768": {
    "doc": "copy_values",
    "title": "Configuration",
    "content": "You can configure the copy_values processor with the following options. | Option | Required | Description | . | entries | Yes | A list of entries to be copied in an event. | . | from_key | Yes | The key of the entry to be copied. | . | to_key | Yes | The key of the new entry to be added. | . | overwrite_if_key_exists | No | When set to true, the existing value is overwritten if key already exists in the event. The default value is false. | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/copy-values/#configuration",
    "relUrl": "/data-prepper/pipelines/configuration/processors/copy-values/#configuration"
  },"1769": {
    "doc": "copy_values",
    "title": "Usage",
    "content": "To get started, create the following pipeline.yaml file: . pipeline: source: ...... processor: - copy_values: entries: - from_key: \"message\" to_key: \"newMessage\" overwrite_if_to_key_exists: true sink: . copy . Next, create a log file named logs_json.log and replace the path in the file source of your pipeline.yaml file with that filepath. For more information, see Configuring Data Prepper. For example, before you run the copy_values processor, if the logs_json.log file contains the following event record: . {\"message\": \"hello\"} . When you run this processor, it parses the message into the following output: . {\"message\": \"hello\", \"newMessage\": \"hello\"} . If newMessage already exists, its existing value is overwritten with value. ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/copy-values/#usage",
    "relUrl": "/data-prepper/pipelines/configuration/processors/copy-values/#usage"
  },"1770": {
    "doc": "csv",
    "title": "csv",
    "content": "The csv processor parses comma-separated values (CSVs) from the event into columns. ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/csv/",
    "relUrl": "/data-prepper/pipelines/configuration/processors/csv/"
  },"1771": {
    "doc": "csv",
    "title": "Configuration",
    "content": "The following table describes the options you can use to configure the csv processor. | Option | Required | Type | Description | . | source | No | String | The field in the event that will be parsed. Default value is message. | . | quote_character | No | String | The character used as a text qualifier for a single column of data. Default value is \". | . | delimiter | No | String | The character separating each column. Default value is ,. | . | delete_header | No | Boolean | If specified, the event header (column_names_source_key) is deleted after the event is parsed. If there is no event header, no action is taken. Default value is true. | . | column_names_source_key | No | String | The field in the event that specifies the CSV column names, which will be automatically detected. If there need to be extra column names, the column names are automatically generated according to their index. If column_names is also defined, the header in column_names_source_key can also be used to generate the event fields. If too few columns are specified in this field, the remaining column names are automatically generated. If too many column names are specified in this field, the CSV processor omits the extra column names. | . | column_names | No | List | User-specified names for the CSV columns. Default value is [column1, column2, ..., columnN] if there are no columns of data in the CSV record and column_names_source_key is not defined. If column_names_source_key is defined, the header in column_names_source_key generates the event fields. If too few columns are specified in this field, the remaining column names are automatically generated. If too many column names are specified in this field, the CSV processor omits the extra column names. | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/csv/#configuration",
    "relUrl": "/data-prepper/pipelines/configuration/processors/csv/#configuration"
  },"1772": {
    "doc": "csv",
    "title": "Metrics",
    "content": "The following table describes common Abstract processor metrics. | Metric name | Type | Description | . | recordsIn | Counter | Metric representing the ingress of records to a pipeline component. | . | recordsOut | Counter | Metric representing the egress of records from a pipeline component. | . | timeElapsed | Timer | Metric representing the time elapsed during execution of a pipeline component. | . The csv processor includes the following custom metrics. Counter . | csvInvalidEvents: The number of invalid events. An exception is thrown when an invalid event is parsed. An unclosed quote usually causes this exception. | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/csv/#metrics",
    "relUrl": "/data-prepper/pipelines/configuration/processors/csv/#metrics"
  },"1773": {
    "doc": "date",
    "title": "date",
    "content": "The date processor adds a default timestamp to an event, parses timestamp fields, and converts timestamp information to the International Organization for Standardization (ISO) 8601 format. This timestamp information can be used as an event timestamp. ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/date/",
    "relUrl": "/data-prepper/pipelines/configuration/processors/date/"
  },"1774": {
    "doc": "date",
    "title": "Configuration",
    "content": "The following table describes the options you can use to configure the date processor. | Option | Required | Type | Description | . | match | Conditionally | List | List of key and patterns where patterns is a list. The list of match can have exactly one key and patterns. There is no default value. This option cannot be defined at the same time as from_time_received. Include multiple date processors in your pipeline if both options should be used. | . | from_time_received | Conditionally | Boolean | A boolean that is used for adding default timestamp to event data from event metadata which is the time when source receives the event. Default value is false. This option cannot be defined at the same time as match. Include multiple date processors in your pipeline if both options should be used. | . | destination | No | String | Field to store the timestamp parsed by date processor. It can be used with both match and from_time_received. Default value is @timestamp. | . | source_timezone | No | String | Time zone used to parse dates. It is used in case the zone or offset cannot be extracted from the value. If the zone or offset are part of the value, then timezone is ignored. Find all the available timezones the list of database time zones in the TZ database name column. | . | destination_timezone | No | String | Timezone used for storing timestamp in destination field. The available timezone values are the same as source_timestamp. | . | locale | No | String | Locale is used for parsing dates. It’s commonly used for parsing month names(MMM). It can have language, country and variant fields using IETF BCP 47 or String representation of Locale object. For example en-US for IETF BCP 47 and en_US for string representation of Locale. Full list of locale fields which includes language, country and variant can be found the language subtag registry. Default value is Locale.ROOT. | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/date/#configuration",
    "relUrl": "/data-prepper/pipelines/configuration/processors/date/#configuration"
  },"1775": {
    "doc": "date",
    "title": "Metrics",
    "content": "The following table describes common Abstract processor metrics. | Metric name | Type | Description | . | recordsIn | Counter | Metric representing the ingress of records to a pipeline component. | . | recordsOut | Counter | Metric representing the egress of records from a pipeline component. | . | timeElapsed | Timer | Metric representing the time elapsed during execution of a pipeline component. | . The date processor includes the following custom metrics. | dateProcessingMatchSuccessCounter: Returns the number of records that match with at least one pattern specified by the match configuration option. | dateProcessingMatchFailureCounter: Returns the number of records that did not match any of the patterns specified by the patterns match configuration option. | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/date/#metrics",
    "relUrl": "/data-prepper/pipelines/configuration/processors/date/#metrics"
  },"1776": {
    "doc": "delete_entries",
    "title": "delete_entries",
    "content": "The delete_entries processor deletes entries, such as key-value pairs, from an event. You can define the keys you want to delete in the with-keys field following delete_entries in the YAML configuration file. Those keys and their values are deleted. ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/delete-entries/",
    "relUrl": "/data-prepper/pipelines/configuration/processors/delete-entries/"
  },"1777": {
    "doc": "delete_entries",
    "title": "Configuration",
    "content": "You can configure the delete_entries processor with the following options. | Option | Required | Description | . | with_keys | Yes | An array of keys for the entries to be deleted. | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/delete-entries/#configuration",
    "relUrl": "/data-prepper/pipelines/configuration/processors/delete-entries/#configuration"
  },"1778": {
    "doc": "delete_entries",
    "title": "Usage",
    "content": "To get started, create the following pipeline.yaml file: . pipeline: source: ...... processor: - delete_entries: with_keys: [\"message\"] sink: . copy . Next, create a log file named logs_json.log and replace the path in the file source of your pipeline.yaml file with that filepath. For more information, see Configuring Data Prepper. For example, before you run the delete_entries processor, if the logs_json.log file contains the following event record: . {\"message\": \"hello\", \"message2\": \"goodbye\"} . When you run the delete_entries processor, it parses the message into the following output: . {\"message2\": \"goodbye\"} . If message does not exist in the event, then no action occurs. ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/delete-entries/#usage",
    "relUrl": "/data-prepper/pipelines/configuration/processors/delete-entries/#usage"
  },"1779": {
    "doc": "drop_events",
    "title": "drop_events",
    "content": "The drop_events processor drops all the events that are passed into it. The following table describes when events are dropped and how exceptions for dropping events are handled. | Option | Required | Type | Description | . | drop_when | Yes | String | Accepts a Data Prepper expression string following the Data Prepper Expression Syntax. Configuring drop_events with drop_when: true drops all the events received. | . | handle_failed_events | No | Enum | Specifies how exceptions are handled when an exception occurs while evaluating an event. Default value is drop, which drops the event so that it is not sent to OpenSearch. Available options are drop, drop_silently, skip, and skip_silently. For more information, see handle_failed_events. | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/drop-events/",
    "relUrl": "/data-prepper/pipelines/configuration/processors/drop-events/"
  },"1780": {
    "doc": "grok",
    "title": "grok",
    "content": "The Grok processor takes unstructured data and utilizes pattern matching to structure and extract important keys. ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/grok/",
    "relUrl": "/data-prepper/pipelines/configuration/processors/grok/"
  },"1781": {
    "doc": "grok",
    "title": "Configuration",
    "content": "The following table describes options you can use with the Grok processor to structure your data and make your data easier to query. | Option | Required | Type | Description | . | match | No | Map | Specifies which keys to match specific patterns against. Default value is an empty body. | . | keep_empty_captures | No | Boolean | Enables preserving null captures. Default value is false. | . | named_captures_only | No | Boolean | Specifies whether to keep only named captures. Default value is true. | . | break_on_match | No | Boolean | Specifies whether to match all patterns or stop once the first successful match is found. Default value is true. | . | keys_to_overwrite | No | List | Specifies which existing keys will be overwritten if there is a capture with the same key value. Default value is []. | . | pattern_definitions | No | Map | Allows for custom pattern use inline. Default value is an empty body. | . | patterns_directories | No | List | Specifies the path of directories that contain customer pattern files. Default value is an empty list. | . | pattern_files_glob | No | String | Specifies which pattern files to use from the directories specified for pattern_directories. Default value is *. | . | target_key | No | String | Specifies a parent-level key used to store all captures. Default value is null. | . | timeout_millis | No | Integer | The maximum amount of time during which matching occurs. Setting to 0 disables the timeout. Default value is 30,000. | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/grok/#configuration",
    "relUrl": "/data-prepper/pipelines/configuration/processors/grok/#configuration"
  },"1782": {
    "doc": "grok",
    "title": "Metrics",
    "content": "The following table describes common Abstract processor metrics. | Metric name | Type | Description | . | recordsIn | Counter | Metric representing the ingress of records to a pipeline component. | . | recordsOut | Counter | Metric representing the egress of records from a pipeline component. | . | timeElapsed | Timer | Metric representing the time elapsed during execution of a pipeline component. | . The Grok processor includes the following custom metrics. Counter . | grokProcessingMismatch: Records the number of records that did not match any of the patterns specified in the match field. | grokProcessingMatch: Records the number of records that matched at least one pattern from the match field. | grokProcessingErrors: Records the total number of record processing errors. | grokProcessingTimeouts: Records the total number of records that timed out while matching. | . Timer . | grokProcessingTime: The time taken by individual records to match against patterns from match. The avg metric is the most useful metric for this timer because it provides you with an average value of the time it takes records to match. | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/grok/#metrics",
    "relUrl": "/data-prepper/pipelines/configuration/processors/grok/#metrics"
  },"1783": {
    "doc": "key_value",
    "title": "key_value",
    "content": "You can use the key_value processor to parse the specified field into key-value pairs. You can customize the key_value processor to parse field information with the following options. The type for each of the following options is string. | Option | Description | Example | . | source | The message field to be parsed. Optional. Default value is message. | If source is \"message1\", {\"message1\": {\"key1=value1\"}, \"message2\": {\"key2=value2\"}} parses into {\"message1\": {\"key1=value1\"}, \"message2\": {\"key2=value2\"}, \"parsed_message\": {\"key1\": \"value1\"}}. | . | destination | The destination field for the parsed source. The parsed source overwrites the preexisting data for that key. Optional. Default value is parsed_message. | If destination is \"parsed_data\", {\"message\": {\"key1=value1\"}} parses into {\"message\": {\"key1=value1\"}, \"parsed_data\": {\"key1\": \"value1\"}}. | . | field_delimiter_regex | A regular expression specifying the delimiter that separates key-value pairs. Special regular expression characters such as [ and ] must be escaped with \\\\. Cannot be defined at the same time as field_split_characters. Optional. If this option is not defined, field_split_characters is used. | If field_delimiter_regex is \"&amp;\\\\{2\\\\}\", {\"key1=value1&amp;&amp;key2=value2\"} parses into {\"key1\": \"value1\", \"key2\": \"value2\"}. | . | field_split_characters | A string of characters specifying the delimeter that separates key-value pairs. Special regular expression characters such as [ and ] must be escaped with \\\\. Cannot be defined at the same time as field_delimiter_regex. Optional. Default value is &amp;. | If field_split_characters is \"&amp;&amp;\", {\"key1=value1&amp;&amp;key2=value2\"} parses into {\"key1\": \"value1\", \"key2\": \"value2\"}. | . | key_value_delimiter_regex | A regular expression specifying the delimiter that separates the key and value within a key-value pair. Special regular expression characters such as [ and ] must be escaped with \\\\. This option cannot be defined at the same time as value_split_characters. Optional. If this option is not defined, value_split_characters is used. | If key_value_delimiter_regex is \"=\\\\{2\\\\}\", {\"key1==value1\"} parses into {\"key1\": \"value1\"}. | . | value_split_characters | A string of characters specifying the delimiter that separates the key and value within a key-value pair. Special regular expression characters such as [ and ] must be escaped with \\\\. Cannot be defined at the same time as key_value_delimiter_regex. Optional. Default value is =. | If value_split_characters is \"==\", {\"key1==value1\"} parses into {\"key1\": \"value1\"}. | . | non_match_value | When a key-value pair cannot be successfully split, the key-value pair is placed in the key field, and the specified value is placed in the value field. Optional. Default value is null. | key1value1&amp;key2=value2 parses into {\"key1value1\": null, \"key2\": \"value2\"}. | . | prefix | A prefix to append before all keys. Optional. Default value is an empty string. | If prefix is \"custom\", {\"key1=value1\"} parses into {\"customkey1\": \"value1\"}. | . | delete_key_regex | A regular expression specifying the characters to delete from the key. Special regular expression characters such as [ and ] must be escaped with \\\\. Cannot be an empty string. Optional. No default value. | If delete_key_regex is \"\\s\", {\"key1 =value1\"} parses into {\"key1\": \"value1\"}. | . | delete_value_regex | A regular expression specifying the characters to delete from the value. Special regular expression characters such as [ and ] must be escaped with \\\\. Cannot be an empty string. Optional. No default value. | If delete_value_regex is \"\\s\", {\"key1=value1 \"} parses into {\"key1\": \"value1\"}. | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/key-value/",
    "relUrl": "/data-prepper/pipelines/configuration/processors/key-value/"
  },"1784": {
    "doc": "list_to_map",
    "title": "list_to_map",
    "content": "The list_to_map processor converts a list of objects from an event, where each object contains a key field, into a map of target keys. ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/list-to-map/",
    "relUrl": "/data-prepper/pipelines/configuration/processors/list-to-map/"
  },"1785": {
    "doc": "list_to_map",
    "title": "Configuration",
    "content": "The following table describes the configuration options used to generate target keys for the mappings. | Option | Required | Type | Description | . | key | Yes | String | The key of the fields to be extracted as keys in the generated mappings. | . | source | Yes | String | The list of objects with key fields to be converted into keys for the generated map. | . | target | No | String | The target for the generated map. When not specified, the generated map will be placed in the root node. | . | value_key | No | String | When specified, values given a value_key in objects contained in the source list will be extracted and converted into the value specified by this option based on the generated map. When not specified, objects contained in the source list retain their original value when mapped. | . | flatten | No | Boolean | When true, values in the generated map output flatten into single items based on the flattened_element. Otherwise, objects mapped to values from the generated map appear as lists. | . | flattened_element | Conditionally | String | The element to keep, either first or last, when flatten is set to true. | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/list-to-map/#configuration",
    "relUrl": "/data-prepper/pipelines/configuration/processors/list-to-map/#configuration"
  },"1786": {
    "doc": "list_to_map",
    "title": "Usage",
    "content": "The following example shows how to test the usage of the list_to_map processor before using the processor on your own source. Create a source file named logs_json.log. Because the file source reads each line in the .log file as an event, the object list appears as one line even though it contains multiple objects: . {\"mylist\":[{\"name\":\"a\",\"value\":\"val-a\"},{\"name\":\"b\",\"value\":\"val-b1\"},{\"name\":\"b\", \"value\":\"val-b2\"},{\"name\":\"c\",\"value\":\"val-c\"}]} . copy . Next, create a pipeline.yaml file that uses the logs_json.log file as the source by pointing to the .log file’s correct path: . pipeline: source: file: path: \"/full/path/to/logs_json.log\" record_type: \"event\" format: \"json\" processor: - list_to_map: key: \"name\" source: \"mylist\" value_key: \"value\" flatten: true sink: - stdout: . copy . Run the pipeline. If successful, the processor returns the generated map with objects mapped according to their value_key. Similar to the original source, which contains one line and therefore one event, the processor returns the following JSON as one line. For readability, the following example and all subsequent JSON examples have been adjusted to span multiple lines: . { \"mylist\": [ { \"name\": \"a\", \"value\": \"val-a\" }, { \"name\": \"b\", \"value\": \"val-b1\" }, { \"name\": \"b\", \"value\": \"val-b2\" }, { \"name\": \"c\", \"value\": \"val-c\" } ], \"a\": \"val-a\", \"b\": \"val-b1\", \"c\": \"val-c\" } . Example: Maps set to target . The following example pipeline.yaml file shows the list_to_map processor when set to a specified target, mymap: . pipeline: source: file: path: \"/full/path/to/logs_json.log\" record_type: \"event\" format: \"json\" processor: - list_to_map: key: \"name\" source: \"mylist\" target: \"mymap\" value_key: \"value\" flatten: true sink: - stdout: . copy . The generated map appears under the target key: . { \"mylist\": [ { \"name\": \"a\", \"value\": \"val-a\" }, { \"name\": \"b\", \"value\": \"val-b1\" }, { \"name\": \"b\", \"value\": \"val-b2\" }, { \"name\": \"c\", \"value\": \"val-c\" } ], \"mymap\": { \"a\": \"val-a\", \"b\": \"val-b1\", \"c\": \"val-c\" } } . Example: No value_key specified . The follow example pipeline.yaml file shows the list_to_map processor with no value_key specified. Because key is set to name, the processor extracts the object names to use as keys in the map. pipeline: source: file: path: \"/full/path/to/logs_json.log\" record_type: \"event\" format: \"json\" processor: - list_to_map: key: \"name\" source: \"mylist\" flatten: true sink: - stdout: . copy . The values from the generated map appear as original objects from the .log source, as shown in the following example response: . { \"mylist\": [ { \"name\": \"a\", \"value\": \"val-a\" }, { \"name\": \"b\", \"value\": \"val-b1\" }, { \"name\": \"b\", \"value\": \"val-b2\" }, { \"name\": \"c\", \"value\": \"val-c\" } ], \"a\": { \"name\": \"a\", \"value\": \"val-a\" }, \"b\": { \"name\": \"b\", \"value\": \"val-b1\" }, \"c\": { \"name\": \"c\", \"value\": \"val-c\" } } . Example: flattened_element set to last . The following example pipeline.yaml file sets the flattened_element to last, therefore flattening the processor output based on each value’s last element: . pipeline: source: file: path: \"/full/path/to/logs_json.log\" record_type: \"event\" format: \"json\" processor: - list_to_map: key: \"name\" source: \"mylist\" target: \"mymap\" value_key: \"value\" flatten: true flattened_element: \"last\" sink: - stdout: . copy . The processor maps object b to value val-b2 because val-b2 is the last element in object b, as shown in the following output: . { \"mylist\": [ { \"name\": \"a\", \"value\": \"val-a\" }, { \"name\": \"b\", \"value\": \"val-b1\" }, { \"name\": \"b\", \"value\": \"val-b2\" }, { \"name\": \"c\", \"value\": \"val-c\" } ], \"a\": \"val-a\", \"b\": \"val-b2\", \"c\": \"val-c\" } . Example: flatten set to false . The following example pipeline.yaml file sets flatten to false, causing the processor to output values from the generated map as a list: . pipeline: source: file: path: \"/full/path/to/logs_json.log\" record_type: \"event\" format: \"json\" processor: - list_to_map: key: \"name\" source: \"mylist\" target: \"mymap\" value_key: \"value\" flatten: false sink: - stdout: . copy . Some objects in the response may have more than one element in their values, as shown in the following response: . { \"mylist\": [ { \"name\": \"a\", \"value\": \"val-a\" }, { \"name\": \"b\", \"value\": \"val-b1\" }, { \"name\": \"b\", \"value\": \"val-b2\" }, { \"name\": \"c\", \"value\": \"val-c\" } ], \"a\": [ \"val-a\" ], \"b\": [ \"val-b1\", \"val-b2\" ], \"c\": [ \"val-c\" ] } . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/list-to-map/#usage",
    "relUrl": "/data-prepper/pipelines/configuration/processors/list-to-map/#usage"
  },"1787": {
    "doc": "lowercase_string",
    "title": "lowercase_string",
    "content": "The lowercase_string processor converts a string to its lowercase counterpart and is a mutate string processor. The following table describes options for configuring the lowercase_string processor to convert strings to a lowercase format. | Option | Required | Type | Description | . | with_keys | Yes | List | A list of keys to convert to lowercase. | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/lowercase-string/",
    "relUrl": "/data-prepper/pipelines/configuration/processors/lowercase-string/"
  },"1788": {
    "doc": "Mutate event",
    "title": "Mutate event processors",
    "content": "Mutate event processors allow you to modify events in Data Prepper. The following processors are available: . | add_entries allows you to add entries to an event. | copy_values allows you to copy values within an event. | delete_entries allows you to delete entries from an event. | rename_keys allows you to rename keys in an event. | convert_entry_type allows you to convert value types in an event. | list_to_map allows you to convert list of objects from an event where each object contains a key field into a map of target keys. | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/mutate-event/#mutate-event-processors",
    "relUrl": "/data-prepper/pipelines/configuration/processors/mutate-event/#mutate-event-processors"
  },"1789": {
    "doc": "Mutate event",
    "title": "Mutate event",
    "content": " ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/mutate-event/",
    "relUrl": "/data-prepper/pipelines/configuration/processors/mutate-event/"
  },"1790": {
    "doc": "Mutate string",
    "title": "Mutate string processors",
    "content": "You can change the way that a string appears by using a mutate string processesor. For example, you can use the uppercase_string processor to convert a string to uppercase, and you can use the lowercase_string processor to convert a string to lowercase. The following is a list of processors that allow you to mutate a string: . | substitute_string | split_string | uppercase_string | lowercase_string | trim_string | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/mutate-string/#mutate-string-processors",
    "relUrl": "/data-prepper/pipelines/configuration/processors/mutate-string/#mutate-string-processors"
  },"1791": {
    "doc": "Mutate string",
    "title": "substitute_string",
    "content": "The substitute_string processor matches a key’s value against a regular expression (regex) and replaces all returned matches with a replacement string. Configuration . You can configure the substitute_string processor with the following options. | Option | Required | Description | . | entries | Yes | A list of entries to add to an event. | . | source | Yes | The key to be modified. | . | from | Yes | The regex string to be replaced. Special regex characters such as [ and ] must be escaped using \\\\ when using double quotes and \\ when using single quotes. For more information, see Class Pattern in the Java documentation. | . | to | Yes | The string that replaces each match of from. | . Usage . To get started, create the following pipeline.yaml file: . pipeline: source: file: path: \"/full/path/to/logs_json.log\" record_type: \"event\" format: \"json\" processor: - substitute_string: entries: - source: \"message\" from: \":\" to: \"-\" sink: - stdout: . copy . Next, create a log file named logs_json.log. After that, replace the path of the file source in your pipeline.yaml file with your file path. For more detailed information, see Configuring Data Prepper. Before you run Data Prepper, the source appears in the following format: . {\"message\": \"ab:cd:ab:cd\"} . After you run Data Prepper, the source is converted to the following format: . {\"message\": \"ab-cd-ab-cd\"} . from defines which string is replaced, and to defines the string that replaces the from string. In the preceding example, string ab:cd:ab:cd becomes ab-cd-ab-cd. If the from regex string does not return a match, the key is returned without any changes. ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/mutate-string/#substitute_string",
    "relUrl": "/data-prepper/pipelines/configuration/processors/mutate-string/#substitute_string"
  },"1792": {
    "doc": "Mutate string",
    "title": "split_string",
    "content": "The split_string processor splits a field into an array using a delimiter character. Configuration . You can configure the split_string processor with the following options. | Option | Required | Description | . | entries | Yes | A list of entries to add to an event. | . | source | Yes | The key to be split. | . | delimiter | No | The separator character responsible for the split. Cannot be defined at the same time as delimiter_regex. At least delimiter or delimiter_regex must be defined. | . | delimiter_regex | No | A regex string responsible for the split. Cannot be defined at the same time as delimiter. Either delimiter or delimiter_regex must be defined. | . Usage . To get started, create the following pipeline.yaml file: . pipeline: source: file: path: \"/full/path/to/logs_json.log\" record_type: \"event\" format: \"json\" processor: - split_string: entries: - source: \"message\" delimiter: \",\" sink: - stdout: . copy . Next, create a log file named logs_json.log. After that, replace the path in the file source of your pipeline.yaml file with your file path. For more detailed information, see Configuring Data Prepper. Before you run Data Prepper, the source appears in the following format: . {\"message\": \"hello,world\"} . After you run Data Prepper, the source is converted to the following format: . {\"message\":[\"hello\",\"world\"]} . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/mutate-string/#split_string",
    "relUrl": "/data-prepper/pipelines/configuration/processors/mutate-string/#split_string"
  },"1793": {
    "doc": "Mutate string",
    "title": "uppercase_string",
    "content": "The uppercase_string processor converts the value (a string) of a key from its current case to uppercase. Configuration . You can configure the uppercase_string processor with the following options. | Option | Required | Description | . | with_keys | Yes | A list of keys to convert to uppercase. | . Usage . To get started, create the following pipeline.yaml file: . pipeline: source: file: path: \"/full/path/to/logs_json.log\" record_type: \"event\" format: \"json\" processor: - uppercase_string: with_keys: - \"uppercaseField\" sink: - stdout: . copy . Next, create a log file named logs_json.log. After that, replace the path in the file source of your pipeline.yaml file with the correct file path. For more detailed information, see Configuring Data Prepper. Before you run Data Prepper, the source appears in the following format: . {\"uppercaseField\": \"hello\"} . After you run Data Prepper, the source is converted to the following format: . {\"uppercaseField\": \"HELLO\"} . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/mutate-string/#uppercase_string",
    "relUrl": "/data-prepper/pipelines/configuration/processors/mutate-string/#uppercase_string"
  },"1794": {
    "doc": "Mutate string",
    "title": "lowercase_string",
    "content": "The lowercase string processor converts a string to lowercase. Configuration . You can configure the lowercase string processor with the following options. | Option | Required | Description | . | with_keys | Yes | A list of keys to convert to lowercase. | . Usage . To get started, create the following pipeline.yaml file: . pipeline: source: file: path: \"/full/path/to/logs_json.log\" record_type: \"event\" format: \"json\" processor: - lowercase_string: with_keys: - \"lowercaseField\" sink: - stdout: . copy . Next, create a log file named logs_json.log. After that, replace the path in the file source of your pipeline.yaml file with the correct file path. For more detailed information, see Configuring Data Prepper. Before you run Data Prepper, the source appears in the following format: . {\"lowercaseField\": \"TESTmeSSage\"} . After you run Data Prepper, the source is converted to the following format: . {\"lowercaseField\": \"testmessage\"} . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/mutate-string/#lowercase_string",
    "relUrl": "/data-prepper/pipelines/configuration/processors/mutate-string/#lowercase_string"
  },"1795": {
    "doc": "Mutate string",
    "title": "trim_string",
    "content": "The trim_string processor removes whitespace from the beginning and end of a key. Configuration . You can configure the trim_string processor with the following options. | Option | Required | Description | . | with_keys | Yes | A list of keys from which to trim the whitespace. | . Usage . To get started, create the following pipeline.yaml file: . pipeline: source: file: path: \"/full/path/to/logs_json.log\" record_type: \"event\" format: \"json\" processor: - trim_string: with_keys: - \"trimField\" sink: - stdout: . copy . Next, create a log file named logs_json.log. After that, replace the path in the file source of your pipeline.yaml file with the correct file path. For more detailed information, see Configuring Data Prepper. Before you run Data Prepper, the source appears in the following format: . {\"trimField\": \" Space Ship \"} . After you run Data Prepper, the source is converted to the following format: . {\"trimField\": \"Space Ship\"} . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/mutate-string/#trim_string",
    "relUrl": "/data-prepper/pipelines/configuration/processors/mutate-string/#trim_string"
  },"1796": {
    "doc": "Mutate string",
    "title": "Mutate string",
    "content": " ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/mutate-string/",
    "relUrl": "/data-prepper/pipelines/configuration/processors/mutate-string/"
  },"1797": {
    "doc": "otel_metrics",
    "title": "otel_metrics",
    "content": "The otel_metrics processor serializes a collection of ExportMetricsServiceRequest records sent from the OTel metrics source into a collection of string records. ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/otel-metrics/",
    "relUrl": "/data-prepper/pipelines/configuration/processors/otel-metrics/"
  },"1798": {
    "doc": "otel_metrics",
    "title": "Usage",
    "content": "To get started, add the following processor to your pipeline.yaml configuration file: . processor: - otel_metrics_raw_processor: . copy . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/otel-metrics/#usage",
    "relUrl": "/data-prepper/pipelines/configuration/processors/otel-metrics/#usage"
  },"1799": {
    "doc": "otel_metrics",
    "title": "Configuration",
    "content": "You can use the following optional parameters to configure histogram buckets and their default values. A histogram displays numerical data by grouping data into buckets. You can use histogram buckets to view sets of events that are organized by the total event count and aggregate sum for all events. For more detailed information, see OpenTelemetry Histograms. | Parameter | Default value | Description | . | calculate_histogram_buckets | True | Whether or not to calculate histogram buckets. | . | calculate_exponential_histogram_buckets | True | Whether or not to calculate exponential histogram buckets. | . | exponential_histogram_max_allowed_scale | 10 | Maximum allowed scale in exponential histogram calculation. | . | flatten_attributes | False | Whether or not to flatten the attributes field in the JSON data. | . calculate_histogram_buckets . If calculate_histogram_buckets is not set to false, then the following JSON file will be added to every histogram JSON. If flatten_attributes is set to false, the JSON string format of the metrics does not change the attributes field. If flatten_attributes is set to true, the values in the attributes field are placed in the parent JSON object. The default value is true. See the following JSON example: . \"buckets\": [ { \"min\": 0.0, \"max\": 5.0, \"count\": 2 }, { \"min\": 5.0, \"max\": 10.0, \"count\": 5 } ] . You can create detailed representations of histogram buckets and their boundaries. You can control this feature by using the following parameters in your pipeline.yaml file: . processor: - otel_metrics_raw_processor: calculate_histogram_buckets: true calculate_exponential_histogram_buckets: true exponential_histogram_max_allowed_scale: 10 flatten_attributes: false . copy . Each array element describes one bucket. Each bucket contains the lower boundary, upper boundary, and its value count. This is a specific form of more detailed OpenTelemetry representation that is a part of the JSON output created by the otel_metrics processor. See the following JSON file, which is added to each JSON histogram by the otel_metrics processor: . \"explicitBounds\": [ 5.0, 10.0 ], \"bucketCountsList\": [ 2, 5 ] . calculate_exponential_histogram_buckets . If calculate_exponential_histogram_buckets is set to true (the default setting), the following JSON values are added to each JSON histogram: . \"negativeBuckets\": [ { \"min\": 0.0, \"max\": 5.0, \"count\": 2 }, { \"min\": 5.0, \"max\": 10.0, \"count\": 5 } ], ... \"positiveBuckets\": [ { \"min\": 0.0, \"max\": 5.0, \"count\": 2 }, { \"min\": 5.0, \"max\": 10.0, \"count\": 5 } ], . The following JSON file is a more detailed form of OpenTelemetry representation that consists of negative and positive buckets, a scale parameter, an offset, and a list of bucket counts: . \"negative\": [ 1, 2, 3 ], \"positive\": [ 1, 2, 3 ], \"scale\" : -3, \"negativeOffset\" : 0, \"positiveOffset\" : 1 . exponential_histogram_max_allowed_scale . The exponential_histogram_max_allowed_scale parameter defines the maximum allowed scale for an exponential histogram. If you increase this parameter, you will increase potential memory consumption. See the OpenTelemetry specifications for more information on exponential histograms and their computational complexity. All exponential histograms that have a scale that is above the configured parameter (by default, a value of 10) are discarded and logged with an error level. You can check the log that Data Prepper creates to see the ERROR log message. The absolute scale value is used for comparison, so a scale of -11 that is treated equally to 11 exceeds the configured value of 10 and can be discarded. ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/otel-metrics/#configuration",
    "relUrl": "/data-prepper/pipelines/configuration/processors/otel-metrics/#configuration"
  },"1800": {
    "doc": "otel_metrics",
    "title": "Metrics",
    "content": "The following table describes metrics that are common to all processors. | Metric name | Type | Description | . | recordsIn | Counter | Metric representing the number of ingress records. | . | recordsOut | Counter | Metric representing the number of egress records. | . | timeElapsed | Timer | Metric representing the time elapsed during execution of records. | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/otel-metrics/#metrics",
    "relUrl": "/data-prepper/pipelines/configuration/processors/otel-metrics/#metrics"
  },"1801": {
    "doc": "otel_trace_group",
    "title": "otel_trace_group",
    "content": "The otel_trace_group processor completes missing trace-group-related fields in the collection of span records by looking up the OpenSearch backend. The otel_trace_group processor identifies the missing trace group information for a spanId by looking up the relevant fields in its root span stored in OpenSearch. ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/otel-trace-group/",
    "relUrl": "/data-prepper/pipelines/configuration/processors/otel-trace-group/"
  },"1802": {
    "doc": "otel_trace_group",
    "title": "OpenSearch",
    "content": "When you connect to an OpenSearch cluster using your username and password, use the following example pipeline.yaml file to configure the otel_trace_group processor: . pipeline: ... processor: - otel_trace_group: hosts: [\"https://localhost:9200\"] cert: path/to/cert username: YOUR_USERNAME_HERE password: YOUR_PASSWORD_HERE . See OpenSearch security for a more detailed explanation of which OpenSearch credentials and permissions are required and how to configure those credentials for the OTel trace group processor. Amazon OpenSearch Service . When you use Amazon OpenSearch Service, use the following example pipeline.yaml file to configure the otel_trace_group processor: . pipeline: ... processor: - otel_trace_group: hosts: [\"https://your-amazon-opensearch-service-endpoint\"] aws_sigv4: true cert: path/to/cert insecure: false . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/otel-trace-group/#opensearch",
    "relUrl": "/data-prepper/pipelines/configuration/processors/otel-trace-group/#opensearch"
  },"1803": {
    "doc": "otel_trace_group",
    "title": "Configuration",
    "content": "You can configure the otel_trace_group processor with the following options. | Name | Description | Default value | . | hosts | A list of IP addresses of OpenSearch nodes. Required. | No default value. | . | cert | A certificate authority (CA) certificate that is PEM encoded. Accepts both .pem or .crt. This enables the client to trust the CA that has signed the certificate that OpenSearch is using. | null | . | aws_sigv4 | A Boolean flag used to sign the HTTP request with AWS credentials. Only applies to Amazon OpenSearch Service. See OpenSearch security for details. | false. | . | aws_region | A string that represents the AWS Region of the Amazon OpenSearch Service domain, for example, us-west-2. Only applies to Amazon OpenSearch Service. | us-east-1 | . | aws_sts_role_arn | An AWS Identity and Access Management (IAM) role that the sink plugin assumes to sign the request to Amazon OpenSearch Service. If not provided, the plugin uses the default credentials. | null | . | aws_sts_header_overrides | A map of header overrides that the IAM role assumes for the sink plugin. | null | . | insecure | A Boolean flag used to turn off SSL certificate verification. If set to true, CA certificate verification is turned off and insecure HTTP requests are sent. | false | . | username | A string that contains the username and is used in the internal users YAML configuration file of your OpenSearch cluster. | null | . | password | A string that contains the password and is used in the internal users YAML configuration file of your OpenSearch cluster. | null | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/otel-trace-group/#configuration",
    "relUrl": "/data-prepper/pipelines/configuration/processors/otel-trace-group/#configuration"
  },"1804": {
    "doc": "otel_trace_group",
    "title": "Configuration option examples",
    "content": "You can define the configuration option values in the aws_sts_header_overrides option. See the following example: . aws_sts_header_overrides: x-my-custom-header-1: my-custom-value-1 x-my-custom-header-2: my-custom-value-2 . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/otel-trace-group/#configuration-option-examples",
    "relUrl": "/data-prepper/pipelines/configuration/processors/otel-trace-group/#configuration-option-examples"
  },"1805": {
    "doc": "otel_trace_group",
    "title": "Metrics",
    "content": "The following table describes custom metrics specific to the otel_trace_group processor. | Metric name | Type | Description | . | recordsInMissingTraceGroup | Counter | The number of ingress records missing trace group fields. | . | recordsOutFixedTraceGroup | Counter | The number of egress records with successfully completed trace group fields. | . | recordsOutMissingTraceGroup | Counter | The number of egress records missing trace group fields. | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/otel-trace-group/#metrics",
    "relUrl": "/data-prepper/pipelines/configuration/processors/otel-trace-group/#metrics"
  },"1806": {
    "doc": "otel_trace",
    "title": "otel_trace",
    "content": "The otel_trace processor completes trace-group-related fields in all incoming Data Prepper span records by state caching the root span information for each tradeId. ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/otel-trace-raw/",
    "relUrl": "/data-prepper/pipelines/configuration/processors/otel-trace-raw/"
  },"1807": {
    "doc": "otel_trace",
    "title": "Parameters",
    "content": "This processor includes the following parameters. | traceGroup: Root span name | endTime: End time of the entire trace in International Organization for Standardization (ISO) 8601 format | durationInNanos: Duration of the entire trace in nanoseconds | statusCode: Status code for the entire trace in nanoseconds | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/otel-trace-raw/#parameters",
    "relUrl": "/data-prepper/pipelines/configuration/processors/otel-trace-raw/#parameters"
  },"1808": {
    "doc": "otel_trace",
    "title": "Configuration",
    "content": "The following table describes the options you can use to configure the otel_trace processor. | Option | Required | Type | Description | . | trace_flush_interval | No | Integer | Represents the time interval in seconds to flush all the descendant spans without any root span. Default is 180. | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/otel-trace-raw/#configuration",
    "relUrl": "/data-prepper/pipelines/configuration/processors/otel-trace-raw/#configuration"
  },"1809": {
    "doc": "otel_trace",
    "title": "Metrics",
    "content": "The following table describes common Abstract processor metrics. | Metric name | Type | Description | . | recordsIn | Counter | Metric representing the ingress of records to a pipeline component. | . | recordsOut | Counter | Metric representing the egress of records from a pipeline component. | . | timeElapsed | Timer | Metric representing the time elapsed during execution of a pipeline component. | . The otel_trace processor includes the following custom metrics: . | traceGroupCacheCount: The number of trace groups in the trace group cache. | spanSetCount: The number of span sets in the span set collection. | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/otel-trace-raw/#metrics",
    "relUrl": "/data-prepper/pipelines/configuration/processors/otel-trace-raw/#metrics"
  },"1810": {
    "doc": "parse_json",
    "title": "parse_json",
    "content": "The parse_json processor parses JSON data for an event, including any nested fields. The processor extracts the JSON pointer data and adds the input event to the extracted fields. ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/parse-json/",
    "relUrl": "/data-prepper/pipelines/configuration/processors/parse-json/"
  },"1811": {
    "doc": "parse_json",
    "title": "Configuration",
    "content": "You can configure the parse_json processor with the following options. | Option | Required | Type | Description | . | source | No | String | The field in the event that will be parsed. Default value is message. | . | destination | No | String | The destination field of the parsed JSON. Defaults to the root of the event. Cannot be \"\", /, or any whitespace-only string because these are not valid event fields. | . | pointer | No | String | A JSON pointer to the field to be parsed. There is no pointer by default, meaning the entire source is parsed. The pointer can access JSON array indexes as well. If the JSON pointer is invalid then the entire source data is parsed into the outgoing event. If the key that is pointed to already exists in the event and the destination is the root, then the pointer uses the entire path of the key. | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/parse-json/#configuration",
    "relUrl": "/data-prepper/pipelines/configuration/processors/parse-json/#configuration"
  },"1812": {
    "doc": "parse_json",
    "title": "Usage",
    "content": "To get started, create the following pipeline.yaml file: . parse-json-pipeline: source: ...... processor: - parse_json: . Basic example . To test the parse_json processor with the previous configuration, run the pipeline and paste the following line into your console, then enter exit on a new line: . {\"outer_key\": {\"inner_key\": \"inner_value\"}} . copy . The parse_json processor parses the message into the following format: . {\"message\": {\"outer_key\": {\"inner_key\": \"inner_value\"}}\", \"outer_key\":{\"inner_key\":\"inner_value\"}}} . Example with a JSON pointer . You can use a JSON pointer to parse a selection of the JSON data by specifying the pointer option in the configuration. To get started, create the following pipeline.yaml file: . parse-json-pipeline: source: ...... processor: - parse_json: pointer: \"outer_key/inner_key\" . To test the parse_json processor with the pointer option, run the pipeline, paste the following line into your console, and then enter exit on a new line: . {\"outer_key\": {\"inner_key\": \"inner_value\"}} . copy . The processor parses the message into the following format: . {\"message\": {\"outer_key\": {\"inner_key\": \"inner_value\"}}\", \"inner_key\": \"inner_value\"} . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/parse-json/#usage",
    "relUrl": "/data-prepper/pipelines/configuration/processors/parse-json/#usage"
  },"1813": {
    "doc": "Processors",
    "title": "Processors",
    "content": "Processors perform an action on your data, such as filtering, transforming, or enriching. Prior to Data Prepper 1.3, processors were named preppers. Starting in Data Prepper 1.3, the term prepper is deprecated in favor of the term processor. Data Prepper will continue to support the term prepper until 2.0, where it will be removed. ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/processors/",
    "relUrl": "/data-prepper/pipelines/configuration/processors/processors/"
  },"1814": {
    "doc": "rename_keys",
    "title": "rename_keys",
    "content": "The rename_keys processor renames keys in an event. ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/rename-keys/",
    "relUrl": "/data-prepper/pipelines/configuration/processors/rename-keys/"
  },"1815": {
    "doc": "rename_keys",
    "title": "Configuration",
    "content": "You can configure the rename_keys processor with the following options. | Option | Required | Description | . | entries | Yes | A list of event entries to rename. | . | from_key | Yes | The key of the entry to be renamed. | . | to_key | Yes | The new key of the entry. | . | overwrite_if_to_key_exists | No | When set to true, the existing value is overwritten if key already exists in the event. The default value is false. | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/rename-keys/#configuration",
    "relUrl": "/data-prepper/pipelines/configuration/processors/rename-keys/#configuration"
  },"1816": {
    "doc": "rename_keys",
    "title": "Usage",
    "content": "To get started, create the following pipeline.yaml file: . pipeline: source: file: path: \"/full/path/to/logs_json.log\" record_type: \"event\" format: \"json\" processor: - rename_keys: entries: - from_key: \"message\" to_key: \"newMessage\" overwrite_if_to_key_exists: true sink: - stdout: . copy . Next, create a log file named logs_json.log and replace the path in the file source of your pipeline.yaml file with that filepath. For more information, see Configuring Data Prepper. For example, before you run the rename_keys processor, if the logs_json.log file contains the following event record: . {\"message\": \"hello\"} . When you run the rename_keys processor, it parses the message into the following “newMessage” output: . {\"newMessage\": \"hello\"} . If newMessage already exists, its existing value is overwritten with value. ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/rename-keys/#usage",
    "relUrl": "/data-prepper/pipelines/configuration/processors/rename-keys/#usage"
  },"1817": {
    "doc": "rename_keys",
    "title": "Special considerations",
    "content": "Renaming operations occur in the order that the key-value pair entries are listed in the pipeline.yaml file. This means that chaining (where key-value pairs are renamed in sequence) is implicit in the rename_keys processor. See the following example pipline.yaml file: . pipeline: source: file: path: \"/full/path/to/logs_json.log\" record_type: \"event\" format: \"json\" processor: - rename_keys: entries: - from_key: \"message\" to_key: \"message2\" - from_key: \"message2\" to_key: \"message3\" sink: - stdout: . Add the following contents to the logs_json.log file: . {\"message\": \"hello\"} . copy . After the rename_keys processor runs, the following output appears: . {\"message3\": \"hello\"} . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/rename-keys/#special-considerations",
    "relUrl": "/data-prepper/pipelines/configuration/processors/rename-keys/#special-considerations"
  },"1818": {
    "doc": "routes",
    "title": "Routes",
    "content": "Routes define conditions that can be used in sinks for conditional routing. Routes are specified at the same level as processors and sinks under the name route and consist of a list of key-value pairs, where the key is the name of a route and the value is a Data Prepper expression representing the routing condition. ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/routes/#routes",
    "relUrl": "/data-prepper/pipelines/configuration/processors/routes/#routes"
  },"1819": {
    "doc": "routes",
    "title": "routes",
    "content": " ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/routes/",
    "relUrl": "/data-prepper/pipelines/configuration/processors/routes/"
  },"1820": {
    "doc": "service_map",
    "title": "service_map",
    "content": "The service_map processor uses OpenTelemetry data to create a distributed service map for visualization in OpenSearch Dashboards. ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/service-map-stateful/",
    "relUrl": "/data-prepper/pipelines/configuration/processors/service-map-stateful/"
  },"1821": {
    "doc": "service_map",
    "title": "Configuration",
    "content": "The following table describes the option you can use to configure the service_map processor. | Option | Required | Type | Description | . | window_duration | No | Integer | Represents the fixed time window, in seconds, during which service map relationships are evaluated. Default value is 180. | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/service-map-stateful/#configuration",
    "relUrl": "/data-prepper/pipelines/configuration/processors/service-map-stateful/#configuration"
  },"1822": {
    "doc": "service_map",
    "title": "Metrics",
    "content": "The following table describes common Abstract processor metrics. | Metric name | Type | Description | . | recordsIn | Counter | Metric representing the ingress of records to a pipeline component. | . | recordsOut | Counter | Metric representing the egress of records from a pipeline component. | . | timeElapsed | Timer | Metric representing the time elapsed during execution of a pipeline component. | . The service-map-stateful processor includes following custom metrics: . | traceGroupCacheCount: The number of trace groups in the trace group cache. | spanSetCount: The number of span sets in the span set collection. | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/service-map-stateful/#metrics",
    "relUrl": "/data-prepper/pipelines/configuration/processors/service-map-stateful/#metrics"
  },"1823": {
    "doc": "split_string",
    "title": "split_string",
    "content": "The split_string processor splits a field into an array using a delimiting character and is a mutate string processor. The following table describes the options you can use to configure the split_string processor. | Option | Required | Type | Description | . | entries | Yes | List | List of entries. Valid values are source, delimiter, and delimiter_regex. | . | source | N/A | N/A | The key to split. | . | delimiter | No | N/A | The separator character responsible for the split. Cannot be defined at the same time as delimiter_regex. At least delimiter or delimiter_regex must be defined. | . | delimiter_regex | No | N/A | The regex string responsible for the split. Cannot be defined at the same time as delimiter. At least delimiter or delimiter_regex must be defined. | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/split-string/",
    "relUrl": "/data-prepper/pipelines/configuration/processors/split-string/"
  },"1824": {
    "doc": "string_converter",
    "title": "string_converter",
    "content": "The string_converter processor converts a string to uppercase or lowercase. You can use it as an example for developing your own processor. The following table describes the option you can use to configure the string_converter processor. | Option | Required | Type | Description | . | upper_case | No | Boolean | Whether to convert to uppercase (true) or lowercase (false). | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/string-converter/",
    "relUrl": "/data-prepper/pipelines/configuration/processors/string-converter/"
  },"1825": {
    "doc": "substitute_string",
    "title": "substitute_string",
    "content": "The substitute_string processor matches a key’s value against a regular expression and replaces all matches with a replacement string. substitute_string is a mutate string processor. ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/substitute-string/",
    "relUrl": "/data-prepper/pipelines/configuration/processors/substitute-string/"
  },"1826": {
    "doc": "substitute_string",
    "title": "Configuration",
    "content": "The following table describes the options you can use to configure the substitue_string processor. | Option | Required | Type | Description | . | entries | Yes | List | List of entries. Valid values are source, from, and to. | . | source | N/A | N/A | The key to modify. | . | from | N/A | N/A | The Regex String to be replaced. Special regex characters such as [ and ] must be escaped using \\\\ when using double quotes and \\ when using single quotes. See Java Patterns for more information. | . | to | N/A | N/A | The String to be substituted for each match of from. | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/substitute-string/#configuration",
    "relUrl": "/data-prepper/pipelines/configuration/processors/substitute-string/#configuration"
  },"1827": {
    "doc": "trace_peer_forwarder",
    "title": "trace peer forwarder",
    "content": "The trace_peer_forwarder processor is used with peer forwarder to reduce by half the number of events forwarded in a Trace Analytics pipeline. In Trace Analytics, each event is typically duplicated when it is sent from otel-trace-pipeline to raw-pipeline and service-map-pipeline. When pipelines forward events, this causes the core peer forwarder to send multiple HTTP requests for the same event. You can use trace peer forwarder to forward an event once through the otel-trace-pipeline instead of raw-pipeline and service-map-pipeline, which prevents unnecessary HTTP requests. You should use trace_peer_forwarder for Trace Analytics pipelines when you have multiple nodes. ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/trace-peer-forwarder/#trace-peer-forwarder",
    "relUrl": "/data-prepper/pipelines/configuration/processors/trace-peer-forwarder/#trace-peer-forwarder"
  },"1828": {
    "doc": "trace_peer_forwarder",
    "title": "Usage",
    "content": "To get started with trace_peer_forwarder, first configure peer forwarder. Then create a pipeline.yaml file and specify trace peer forwarder as the processor. You can configure peer forwarder in your data-prepper-config.yaml file. For more detailed information, see Configuring Data Prepper. See the following example pipeline.yaml file: . otel-trace-pipeline: delay: \"100\" source: otel_trace_source: processor: - trace_peer_forwarder: sink: - pipeline: name: \"raw-pipeline\" - pipeline: name: \"service-map-pipeline\" raw-pipeline: source: pipeline: name: \"entry-pipeline\" processor: - otel_trace_raw: sink: - opensearch: service-map-pipeline: delay: \"100\" source: pipeline: name: \"entry-pipeline\" processor: - service_map_stateful: sink: - opensearch: . In the preceding pipeline.yaml file, events are forwarded in the otel-trace-pipeline to the target peer, and no forwarding is performed in raw-pipeline or service-map-pipeline. This process helps improve network performance by forwarding events (as HTTP requests) once instead of twice. ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/trace-peer-forwarder/#usage",
    "relUrl": "/data-prepper/pipelines/configuration/processors/trace-peer-forwarder/#usage"
  },"1829": {
    "doc": "trace_peer_forwarder",
    "title": "trace_peer_forwarder",
    "content": " ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/trace-peer-forwarder/",
    "relUrl": "/data-prepper/pipelines/configuration/processors/trace-peer-forwarder/"
  },"1830": {
    "doc": "trim_string",
    "title": "trim_string",
    "content": "The trim_string processor removes whitespace from the beginning and end of a key and is a mutate string processor. The following table describes the option you can use to configure the trim_string processor. | Option | Required | Type | Description | . | with_keys | Yes | List | A list of keys to trim the whitespace from. | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/trim-string/",
    "relUrl": "/data-prepper/pipelines/configuration/processors/trim-string/"
  },"1831": {
    "doc": "uppercase_string",
    "title": "uppercase_string",
    "content": "The uppercase_string processor converts an entire string to uppercase and is a mutate string processor. The following table describes the option you can use to configure the uppercase_string processor. | Option | Required | Type | Description | . | with_keys | Yes | List | A list of keys to convert to uppercase. | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/processors/uppercase-string/",
    "relUrl": "/data-prepper/pipelines/configuration/processors/uppercase-string/"
  },"1832": {
    "doc": "file sink",
    "title": "file sink",
    "content": " ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/sinks/file/",
    "relUrl": "/data-prepper/pipelines/configuration/sinks/file/"
  },"1833": {
    "doc": "file sink",
    "title": "Overview",
    "content": "You can use the file sink to create a flat file output. The following table describes options you can configure for the file sink. | Option | Required | Type | Description | . | path | Yes | String | Path for the output file (e.g. logs/my-transformed-log.log). | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/sinks/file/#overview",
    "relUrl": "/data-prepper/pipelines/configuration/sinks/file/#overview"
  },"1834": {
    "doc": "OpenSearch sink",
    "title": "OpenSearch sink",
    "content": "You can use the opensearch sink plugin to send data to an OpenSearch cluster, a legacy Elasticsearch cluster, or an Amazon OpenSearch Service domain. The plugin supports OpenSearch 1.0 and later and Elasticsearch 7.3 and later. ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/sinks/opensearch/",
    "relUrl": "/data-prepper/pipelines/configuration/sinks/opensearch/"
  },"1835": {
    "doc": "OpenSearch sink",
    "title": "Usage",
    "content": "To configure an opensearch sink, specify the opensearch option within the pipeline configuration: . pipeline: ... sink: opensearch: hosts: [\"https://localhost:9200\"] cert: path/to/cert username: YOUR_USERNAME password: YOUR_PASSWORD index_type: trace-analytics-raw dlq_file: /your/local/dlq-file max_retries: 20 bulk_size: 4 . To configure an Amazon OpenSearch Service sink, specify the domain endpoint as the hosts option: . pipeline: ... sink: opensearch: hosts: [\"https://your-amazon-opensearch-service-endpoint\"] aws_sigv4: true cert: path/to/cert insecure: false index_type: trace-analytics-service-map bulk_size: 4 . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/sinks/opensearch/#usage",
    "relUrl": "/data-prepper/pipelines/configuration/sinks/opensearch/#usage"
  },"1836": {
    "doc": "OpenSearch sink",
    "title": "Configuration options",
    "content": "The following table describes options you can configure for the opensearch sink. | Option | Required | Type | Description | . | hosts | Yes | List | List of OpenSearch hosts to write to (for example, [\"https://localhost:9200\", \"https://remote-cluster:9200\"]). | . | cert | No | String | Path to the security certificate (for example, \"config/root-ca.pem\") if the cluster uses the OpenSearch Security plugin. | . | username | No | String | Username for HTTP basic authentication. | . | password | No | String | Password for HTTP basic authentication. | . | aws_sigv4 | No | Boolean | Default value is false. Whether to use AWS Identity and Access Management (IAM) signing to connect to an Amazon OpenSearch Service domain. For your access key, secret key, and optional session token, Data Prepper uses the default credential chain (environment variables, Java system properties, ~/.aws/credential, etc.). | . | aws_region | No | String | The AWS region (for example, \"us-east-1\") for the domain if you are connecting to Amazon OpenSearch Service. | . | aws_sts_role_arn | No | String | IAM role that the plugin uses to sign requests sent to Amazon OpenSearch Service. If this information is not provided, the plugin uses the default credentials. | . | max_retries | No | Integer | The maximum number of times the OpenSearch sink should try to push data to the OpenSearch server before considering it to be a failure. Defaults to Integer.MAX_VALUE. If not provided, the sink will try to push data to the OpenSearch server indefinitely because the default value is high and exponential backoff would increase the waiting time before retry. | . | socket_timeout | No | Integer | The timeout, in milliseconds, waiting for data to return (or the maximum period of inactivity between two consecutive data packets). A timeout value of zero is interpreted as an infinite timeout. If this timeout value is negative or not set, the underlying Apache HttpClient would rely on operating system settings for managing socket timeouts. | . | connect_timeout | No | Integer | The timeout in milliseconds used when requesting a connection from the connection manager. A timeout value of zero is interpreted as an infinite timeout. If this timeout value is negative or not set, the underlying Apache HttpClient would rely on operating system settings for managing connection timeouts. | . | insecure | No | Boolean | Whether or not to verify SSL certificates. If set to true, certificate authority (CA) certificate verification is disabled and insecure HTTP requests are sent instead. Default value is false. | . | proxy | No | String | The address of a forward HTTP proxy server. The format is “&lt;host name or IP&gt;:&lt;port&gt;”. Examples: “example.com:8100”, “http://example.com:8100”, “112.112.112.112:8100”. Port number cannot be omitted. | . | index | Conditionally | String | Name of the export index. Applicable and required only when the index_type is custom. | . | index_type | No | String | This index type tells the Sink plugin what type of data it is handling. Valid values: custom, trace-analytics-raw, trace-analytics-service-map, management-disabled. Default value is custom. | . | template_file | No | String | Path to a JSON index template file (for example, /your/local/template-file.json) if index_type is custom. See otel-v1-apm-span-index-template.json for an example. | . | document_id_field | No | String | The field from the source data to use for the OpenSearch document ID (for example, \"my-field\") if index_type is custom. | . | dlq_file | No | String | The path to your preferred dead letter queue file (for example, /your/local/dlq-file). Data Prepper writes to this file when it fails to index a document on the OpenSearch cluster. | . | dlq | No | N/A | DLQ configurations. See Dead Letter Queues for details. If the dlq_file option is also available, the sink will fail. | . | bulk_size | No | Integer (long) | The maximum size (in MiB) of bulk requests sent to the OpenSearch cluster. Values below 0 indicate an unlimited size. If a single document exceeds the maximum bulk request size, Data Prepper sends it individually. Default value is 5. | . | ism_policy_file | No | String | The absolute file path for an ISM (Index State Management) policy JSON file. This policy file is effective only when there is no built-in policy file for the index type. For example, custom index type is currently the only one without a built-in policy file, thus it would use the policy file here if it’s provided through this parameter. For more information, see ISM policies. | . | number_of_shards | No | Integer | The number of primary shards that an index should have on the destination OpenSearch server. This parameter is effective only when template_file is either explicitly provided in Sink configuration or built-in. If this parameter is set, it would override the value in index template file. For more information, see Create index. | . | number_of_replicas | No | Integer | The number of replica shards each primary shard should have on the destination OpenSearch server. For example, if you have 4 primary shards and set number_of_replicas to 3, the index has 12 replica shards. This parameter is effective only when template_file is either explicitly provided in Sink configuration or built-in. If this parameter is set, it would override the value in index template file. For more information, see Create index. | . Configure max_retries . You can include the max_retries option in your pipeline configuration to control the number of times the source tries to write to sinks with exponential backoff. If you don’t include this option, pipelines keep retrying forever. If you specify max_retries and a pipeline has a dead-letter queue (DLQ) configured, the pipeline will keep trying to write to sinks until it reaches the maximum number of retries, at which point it starts to send failed data to the DLQ. If you don’t specify max_retries, only data that is rejected by sinks is written to the DLQ. Pipelines continue to try to write all other data to the sinks. ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/sinks/opensearch/#configuration-options",
    "relUrl": "/data-prepper/pipelines/configuration/sinks/opensearch/#configuration-options"
  },"1837": {
    "doc": "OpenSearch sink",
    "title": "OpenSearch cluster security",
    "content": "In order to send data to an OpenSearch cluster using the opensearch sink plugin, you must specify your username and password within the pipeline configuration. The following example pipelines.yaml file demonstrates how to specify admin security credentials: . sink: - opensearch: username: \"admin\" password: \"admin\" ... Alternately, rather than admin credentials, you can specify the credentials of a user mapped to a role with the minimum permissions listed in the following sections. Cluster permissions . | cluster_all | indices:admin/template/get | indices:admin/template/put | . Index permissions . | Index: otel-v1*; Index permission: indices_all | Index: .opendistro-ism-config; Index permission: indices_all | Index: *; Index permission: manage_aliases | . For instructions on how to map users to roles, see Map users to roles. ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/sinks/opensearch/#opensearch-cluster-security",
    "relUrl": "/data-prepper/pipelines/configuration/sinks/opensearch/#opensearch-cluster-security"
  },"1838": {
    "doc": "OpenSearch sink",
    "title": "Amazon OpenSearch Service domain security",
    "content": "The opensearch sink plugin can send data to an Amazon OpenSearch Service domain, which uses IAM for security. The plugin uses the default credential chain. Run aws configure using the AWS Command Line Interface (AWS CLI) to set your credentials. Make sure the credentials that you configure have the required IAM permissions. The following domain access policy demonstrates the minimum required permissions: . { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::&lt;AccountId&gt;:user/data-prepper-user\" }, \"Action\": \"es:ESHttp*\", \"Resource\": [ \"arn:aws:es:us-east-1:&lt;AccountId&gt;:domain/&lt;domain-name&gt;/otel-v1*\", \"arn:aws:es:us-east-1:&lt;AccountId&gt;:domain/&lt;domain-name&gt;/_template/otel-v1*\", \"arn:aws:es:us-east-1:&lt;AccountId&gt;:domain/&lt;domain-name&gt;/_plugins/_ism/policies/raw-span-policy\", \"arn:aws:es:us-east-1:&lt;AccountId&gt;:domain/&lt;domain-name&gt;/_alias/otel-v1*\", \"arn:aws:es:us-east-1:&lt;AccountId&gt;:domain/&lt;domain-name&gt;/_alias/_bulk\" ] }, { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::&lt;AccountId&gt;:user/data-prepper-user\" }, \"Action\": \"es:ESHttpGet\", \"Resource\": \"arn:aws:es:us-east-1:&lt;AccountId&gt;:domain/&lt;domain-name&gt;/_cluster/settings\" } ] } . For instructions on how to configure the domain access policy, see Resource-based policies in the Amazon OpenSearch Service documentation. Fine-grained access control . If your OpenSearch Service domain uses fine-grained access control, the opensearch sink plugin requires some additional configuration. IAM ARN as master user . If you’re using an IAM Amazon Resource Name (ARN) as the master user, include the aws_sigv4 option in your sink configuration: ... sink: opensearch: hosts: [\"https://your-fgac-amazon-opensearch-service-endpoint\"] aws_sigv4: true . Run aws configure using the AWS CLI to use the master IAM user credentials. If you don’t want to use the master user, you can specify a different IAM role using the aws_sts_role_arn option. The plugin will then use this role to sign requests sent to the domain sink. The ARN that you specify must be included in the domain access policy. Master user in the internal user database . If your domain uses a master user in the internal user database, specify the master username and password as well as the aws_sigv4 option: . sink: opensearch: hosts: [\"https://your-fgac-amazon-opensearch-service-endpoint\"] aws_sigv4: false username: \"master-username\" password: \"master-password\" . For more information, see Recommended configurations in the Amazon OpenSearch Service documentation. Note: You can create a new IAM role or internal user database user with the all_access permission and use it instead of the master user. ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/sinks/opensearch/#amazon-opensearch-service-domain-security",
    "relUrl": "/data-prepper/pipelines/configuration/sinks/opensearch/#amazon-opensearch-service-domain-security"
  },"1839": {
    "doc": "OpenSearch sink",
    "title": "OpenSearch Serverless collection security",
    "content": "The opensearch sink plugin can send data to an Amazon OpenSearch Serverless collection. OpenSearch Serverless collection sinks have the following limitations: . | You can’t write to a collection that uses virtual private cloud (VPC) access. The collection must be accessible from public networks. | The OTel trace group processor doesn’t currently support collection sinks. | . Creating a pipeline role . First, create an IAM role that the pipeline will assume in order to write to the collection. The role must have the following minimum permissions: . { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"aoss:BatchGetCollection\" ], \"Resource\": \"*\" } ] } . The role must have the following trust relationship, which allows the pipeline to assume it: . { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::&lt;AccountId&gt;:root\" }, \"Action\": \"sts:AssumeRole\" } ] } . Creating a collection . Next, create a collection with the following settings: . | Public network access to both the OpenSearch endpoint and OpenSearch Dashboards. | The following data access policy, which grants the required permissions to the pipeline role: . [ { \"Rules\":[ { \"Resource\":[ \"index/collection-name/*\" ], \"Permission\":[ \"aoss:CreateIndex\", \"aoss:UpdateIndex\", \"aoss:DescribeIndex\", \"aoss:WriteDocument\" ], \"ResourceType\":\"index\" } ], \"Principal\":[ \"arn:aws:iam::&lt;AccountId&gt;:role/PipelineRole\" ], \"Description\":\"Pipeline role access\" } ] . Important: Make sure to replace the ARN in the Principal element with the ARN of the pipeline role that you created in the preceding step. For instructions on how to create collections, see Creating collections in the Amazon OpenSearch Service documentation. | . Creating a pipeline . Within your pipelines.yaml file, specify the OpenSearch Serverless collection endpoint as the hosts option. In addition, you must set the serverless option to true. Specify the pipeline role in the sts_role_arn option: . log-pipeline: source: http: processor: - date: from_time_received: true destination: \"@timestamp\" sink: - opensearch: hosts: [ \"https://&lt;serverless-public-collection-endpoint&gt;\" ] index: \"my-serverless-index\" aws: serverless: true sts_role_arn: \"arn:aws:iam::&lt;AccountId&gt;:role/PipelineRole\" region: \"us-east-1\" . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/sinks/opensearch/#opensearch-serverless-collection-security",
    "relUrl": "/data-prepper/pipelines/configuration/sinks/opensearch/#opensearch-serverless-collection-security"
  },"1840": {
    "doc": "Pipeline sink",
    "title": "Pipeline sink",
    "content": " ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/sinks/pipeline/",
    "relUrl": "/data-prepper/pipelines/configuration/sinks/pipeline/"
  },"1841": {
    "doc": "Pipeline sink",
    "title": "Overview",
    "content": "You can use the pipeline sink to write to another pipeline. | Option | Required | Type | Description | . | name | Yes | String | Name of the pipeline to write to. | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/sinks/pipeline/#overview",
    "relUrl": "/data-prepper/pipelines/configuration/sinks/pipeline/#overview"
  },"1842": {
    "doc": "Sinks",
    "title": "Sinks",
    "content": "Sinks define where Data Prepper writes your data to. ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/sinks/sinks/",
    "relUrl": "/data-prepper/pipelines/configuration/sinks/sinks/"
  },"1843": {
    "doc": "Sinks",
    "title": "General options for all sink types",
    "content": "The following table describes options you can use to configure the sinks sink. | Option | Required | Type | Description | . | routes | No | List | List of routes that the sink accepts. If not specified, the sink accepts all upstream events. | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/sinks/sinks/#general-options-for-all-sink-types",
    "relUrl": "/data-prepper/pipelines/configuration/sinks/sinks/#general-options-for-all-sink-types"
  },"1844": {
    "doc": "stdout sink",
    "title": "stdout sink",
    "content": " ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/sinks/stdout/",
    "relUrl": "/data-prepper/pipelines/configuration/sinks/stdout/"
  },"1845": {
    "doc": "stdout sink",
    "title": "Overview",
    "content": "You can use the stdout sink for console output and testing. It has no configurable options. ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/sinks/stdout/#overview",
    "relUrl": "/data-prepper/pipelines/configuration/sinks/stdout/#overview"
  },"1846": {
    "doc": "http_source",
    "title": "http_source",
    "content": "http_source is a source plugin that supports HTTP. Currently, http_source only supports the JSON UTF-8 codec for incoming requests, such as [{\"key1\": \"value1\"}, {\"key2\": \"value2\"}]. The following table describes options you can use to configure the http_source source. | Option | Required | Type | Description | . | port | No | Integer | The port that the source is running on. Default value is 2021. Valid options are between 0 and 65535. | . | health_check_service | No | Boolean | Enables the health check service on the /health endpoint on the defined port. Default value is false. | . | unauthenticated_health_check | No | Boolean | Determines whether or not authentication is required on the health check endpoint. Data Prepper ignores this option if no authentication is defined. Default value is false. | . | request_timeout | No | Integer | The request timeout, in milliseconds. Default value is 10000. | . | thread_count | No | Integer | The number of threads to keep in the ScheduledThreadPool. Default value is 200. | . | max_connection_count | No | Integer | The maximum allowed number of open connections. Default value is 500. | . | max_pending_requests | No | Integer | The maximum allowed number of tasks in the ScheduledThreadPool work queue. Default value is 1024. | . | authentication | No | Object | An authentication configuration. By default, this creates an unauthenticated server for the pipeline. This uses pluggable authentication for HTTPS. To use basic authentication define the http_basic plugin with a username and password. To provide customer authentication, use or create a plugin that implements ArmeriaHttpAuthenticationProvider. | . | ssl | No | Boolean | Enables TLS/SSL. Default value is false. | . | ssl_certificate_file | Conditionally | String | SSL certificate chain file path or Amazon Simple Storage Service (Amazon S3) path. Amazon S3 path example s3://&lt;bucketName&gt;/&lt;path&gt;. Required if ssl is set to true and use_acm_certificate_for_ssl is set to false. | . | ssl_key_file | Conditionally | String | SSL key file path or Amazon S3 path. Amazon S3 path example s3://&lt;bucketName&gt;/&lt;path&gt;. Required if ssl is set to true and use_acm_certificate_for_ssl is set to false. | . | use_acm_certificate_for_ssl | No | Boolean | Enables a TLS/SSL using certificate and private key from AWS Certificate Manager (ACM). Default value is false. | . | acm_certificate_arn | Conditionally | String | The ACM certificate Amazon Resource Name (ARN). The ACM certificate takes preference over Amazon S3 or a local file system certificate. Required if use_acm_certificate_for_ssl is set to true. | . | acm_private_key_password | No | String | ACM private key password that decrypts the private key. If not provided, Data Prepper generates a random password. | . | acm_certificate_timeout_millis | No | Integer | Timeout, in milliseconds, that ACM takes to get certificates. Default value is 120000. | . | aws_region | Conditionally | String | AWS region used by ACM or Amazon S3. Required if use_acm_certificate_for_ssl is set to true or ssl_certificate_file and ssl_key_file is the Amazon S3 path. | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/sources/http-source/",
    "relUrl": "/data-prepper/pipelines/configuration/sources/http-source/"
  },"1847": {
    "doc": "http_source",
    "title": "Metrics",
    "content": "The http_source source includes the following metrics. Counters . | requestsReceived: Measures the total number of requests received by the /log/ingest endpoint. | requestsRejected: Measures the total number of requests rejected (429 response status code) by the HTTP Source plugin. | successRequests: Measures the total number of requests successfully processed (200 response status code) the by HTTP Source plugin. | badRequests: Measures the total number of requests with either an invalid content type or format processed by the HTTP Source plugin (400 response status code). | requestTimeouts: Measures the total number of requests that time out in the HTTP source server (415 response status code). | requestsTooLarge: Measures the total number of requests where the size of the event is larger than the buffer capacity (413 response status code). | internalServerError: Measures the total number of requests processed by the HTTP Source with a custom exception type (500 response status code). | . Timers . | requestProcessDuration: Measures the latency of requests processed by the HTTP Source plugin in seconds. | . Distribution summaries . | payloadSize: Measures the incoming request payload size in bytes. | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/sources/http-source/#metrics",
    "relUrl": "/data-prepper/pipelines/configuration/sources/http-source/#metrics"
  },"1848": {
    "doc": "otel_logs_source",
    "title": "otel_logs_source",
    "content": "The otel_logs_source source is an OpenTelemetry source that follows the OpenTelemetry Protocol Specification and receives logs from the OTel Collector in the form of ExportLogsServiceRequest records. This source supports the OTLP/gRPC protocol. ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/sources/otel-logs-source/",
    "relUrl": "/data-prepper/pipelines/configuration/sources/otel-logs-source/"
  },"1849": {
    "doc": "otel_logs_source",
    "title": "Configuration",
    "content": "You can configure the otel_logs_source source with the following options. | Option | Type | Description | . | port | int | Represents the port that the otel_logs_source source is running on. Default value is 21892. | . | path | string | Represents the path for sending unframed HTTP requests. You can use this option to support an unframed gRPC request with an HTTP idiomatic path to a configurable path. The path should start with /, and its length should be at least 1. The /opentelemetry.proto.collector.logs.v1.LogsService/Export endpoint is disabled for both gRPC and HTTP requests if the path is configured. The path can contain a ${pipelineName} placeholder, which is replaced with the pipeline name. If the value is empty and unframed_requests is true, then the path that the source provides is /opentelemetry.proto.collector.logs.v1.LogsService/Export. | . | request_timeout | int | Represents the request timeout duration in milliseconds. Default value is 10000. | . | health_check_service | Boolean | Enables the gRPC health check service under grpc.health.v1/Health/Check. Default value is false. | . | proto_reflection_service | Boolean | Enables a reflection service for Protobuf services (see ProtoReflectionService and gRPC reflection). Default value is false. | . | unframed_requests | Boolean | Enables requests that are not framed using the gRPC wire protocol. Default value is false. | . | thread_count | int | The number of threads to keep in the ScheduledThreadPool. Default value is 500. | . | max_connection_count | int | The maximum number of open connections allowed. Default value is 500. | . SSL . You can configure SSL in the otel_logs_source source with the following options. | Option | Type | Description | . | ssl | Boolean | Enables TLS/SSL. Default value is true. | . | sslKeyCertChainFile | string | Represents the SSL certificate chain file path or Amazon Simple Storage Service (Amazon S3) path. For example, see the Amazon S3 path s3://&lt;bucketName&gt;/&lt;path&gt;. Required if ssl is set to true. | . | sslKeyFile | string | Represents the SSL key file path or Amazon S3 path. For example, see the Amazon S3 path s3://&lt;bucketName&gt;/&lt;path&gt;. Required if ssl is set to true. | . | useAcmCertForSSL | Boolean | Enables TLS/SSL using a certificate and private key from AWS Certificate Manager (ACM). Default value is false. | . | acmCertificateArn | string | Represents the ACM certificate Amazon Resource Name (ARN). ACM certificates take precedence over Amazon S3 or local file system certificates. Required if useAcmCertForSSL is set to true. | . | awsRegion | string | Represents the AWS Region used by ACM or Amazon S3. Required if useAcmCertForSSL is set to true or sslKeyCertChainFile or sslKeyFile is the Amazon S3 path. | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/sources/otel-logs-source/#configuration",
    "relUrl": "/data-prepper/pipelines/configuration/sources/otel-logs-source/#configuration"
  },"1850": {
    "doc": "otel_logs_source",
    "title": "Usage",
    "content": "To get started, create a pipeline.yaml file and add otel_logs_source as the source: . source: - otel_logs_source: . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/sources/otel-logs-source/#usage",
    "relUrl": "/data-prepper/pipelines/configuration/sources/otel-logs-source/#usage"
  },"1851": {
    "doc": "otel_logs_source",
    "title": "Metrics",
    "content": "You can use the following metrics with the otel_logs_source source. | Option | Type | Description | . | requestTimeouts | Counter | Measures the total number of requests that time out. | . | requestsReceived | Counter | Measures the total number of requests received by the otel_logs_source source. | . | badRequests | Counter | Measures the total number of requests that could not be parsed. | . | requestsTooLarge | Counter | Measures the total number of requests that exceed the maximum allowed size. Indicates that the size of the data being written into the buffer is beyond the buffer’s maximum capacity. | . | internalServerError | Counter | Measures the total number of requests that are erroneous due to errors other than requestTimeouts or requestsTooLarge. | . | successRequests | Counter | Measures the total number of requests successfully written to the buffer. | . | payloadSize | Distribution summary | Measures the distribution of all incoming payload sizes. | . | requestProcessDuration | Timer | Measures the duration of request processing. | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/sources/otel-logs-source/#metrics",
    "relUrl": "/data-prepper/pipelines/configuration/sources/otel-logs-source/#metrics"
  },"1852": {
    "doc": "otel_metrics_source",
    "title": "otel_metrics_source",
    "content": "otel_metrics_source is an OpenTelemetry Collector source that collects metric data. The following table describes options you can use to configure the otel_metrics_source source. | Option | Required | Type | Description | . | port | No | Integer | The port that the OpenTelemtry metrics source runs on. Default value is 21891. | . | request_timeout | No | Integer | The request timeout, in milliseconds. Default value is 10000. | . | health_check_service | No | Boolean | Enables a gRPC health check service under grpc.health.v1/Health/Check. Default value is false. | . | proto_reflection_service | No | Boolean | Enables a reflection service for Protobuf services (see gRPC reflection and gRPC Server Reflection Tutorial docs). Default value is false. | . | unframed_requests | No | Boolean | Enables requests not framed using the gRPC wire protocol. | . | thread_count | No | Integer | The number of threads to keep in the ScheduledThreadPool. Default value is 200. | . | max_connection_count | No | Integer | The maximum allowed number of open connections. Default value is 500. | . | ssl | No | Boolean | Enables connections to the OpenTelemetry source port over TLS/SSL. Default value is true. | . | sslKeyCertChainFile | Conditionally | String | File-system path or Amazon Simple Storage Service (Amazon S3) path to the security certificate (for example, \"config/demo-data-prepper.crt\" or \"s3://my-secrets-bucket/demo-data-prepper.crt\"). Required if ssl is set to true. | . | sslKeyFile | Conditionally | String | File-system path or Amazon S3 path to the security key (for example, \"config/demo-data-prepper.key\" or \"s3://my-secrets-bucket/demo-data-prepper.key\"). Required if ssl is set to true. | . | useAcmCertForSSL | No | Boolean | Whether to enable TLS/SSL using a certificate and private key from AWS Certificate Manager (ACM). Default value is false. | . | acmCertificateArn | Conditionally | String | Represents the ACM certificate ARN. ACM certificate take preference over S3 or local file system certificates. Required if useAcmCertForSSL is set to true. | . | awsRegion | Conditionally | String | Represents the AWS Region used by ACM or Amazon S3. Required if useAcmCertForSSL is set to true or sslKeyCertChainFile and sslKeyFile is the Amazon S3 path. | . | authentication | No | Object | An authentication configuration. By default, an unauthenticated server is created for the pipeline. This uses pluggable authentication for HTTPS. To use basic authentication, define the http_basic plugin with a username and password. To provide customer authentication, use or create a plugin that implements GrpcAuthenticationProvider. | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/sources/otel-metrics-source/",
    "relUrl": "/data-prepper/pipelines/configuration/sources/otel-metrics-source/"
  },"1853": {
    "doc": "otel_metrics_source",
    "title": "Metrics",
    "content": "The otel_metrics_source source includes the following metrics. Counters . | requestTimeouts: Measures the total number of requests that time out. | requestsReceived: Measures the total number of requests received by the OpenTelemetry metrics source. | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/sources/otel-metrics-source/#metrics",
    "relUrl": "/data-prepper/pipelines/configuration/sources/otel-metrics-source/#metrics"
  },"1854": {
    "doc": "otel_trace_source source",
    "title": "otel_trace source",
    "content": " ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/sources/otel-trace/#otel_trace-source",
    "relUrl": "/data-prepper/pipelines/configuration/sources/otel-trace/#otel_trace-source"
  },"1855": {
    "doc": "otel_trace_source source",
    "title": "Overview",
    "content": "The otel_trace source is a source for the OpenTelemetry Collector. The following table describes options you can use to configure the otel_trace source. | Option | Required | Type | Description | . | port | No | Integer | The port that the otel_trace source runs on. Default value is 21890. | . | request_timeout | No | Integer | The request timeout, in milliseconds. Default value is 10000. | . | health_check_service | No | Boolean | Enables a gRPC health check service under grpc.health.v1/Health/Check. Default value is false. | . | unauthenticated_health_check | No | Boolean | Determines whether or not authentication is required on the health check endpoint. Data Prepper ignores this option if no authentication is defined. Default value is false. | . | proto_reflection_service | No | Boolean | Enables a reflection service for Protobuf services (see gRPC reflection and gRPC Server Reflection Tutorial docs). Default value is false. | . | unframed_requests | No | Boolean | Enable requests not framed using the gRPC wire protocol. | . | thread_count | No | Integer | The number of threads to keep in the ScheduledThreadPool. Default value is 200. | . | max_connection_count | No | Integer | The maximum allowed number of open connections. Default value is 500. | . | ssl | No | Boolean | Enables connections to the OTel source port over TLS/SSL. Defaults to true. | . | sslKeyCertChainFile | Conditionally | String | File system path or Amazon Simple Storage Service (Amazon S3) path to the security certificate (for example, \"config/demo-data-prepper.crt\" or \"s3://my-secrets-bucket/demo-data-prepper.crt\"). Required if ssl is set to true. | . | sslKeyFile | Conditionally | String | File system path or Amazon S3 path to the security key (for example, \"config/demo-data-prepper.key\" or \"s3://my-secrets-bucket/demo-data-prepper.key\"). Required if ssl is set to true. | . | useAcmCertForSSL | No | Boolean | Whether to enable TLS/SSL using a certificate and private key from AWS Certificate Manager (ACM). Default value is false. | . | acmCertificateArn | Conditionally | String | Represents the ACM certificate ARN. ACM certificate take preference over S3 or local file system certificate. Required if useAcmCertForSSL is set to true. | . | awsRegion | Conditionally | String | Represents the AWS region used by ACM or Amazon S3. Required if useAcmCertForSSL is set to true or sslKeyCertChainFile and sslKeyFile are Amazon S3 paths. | . | authentication | No | Object | An authentication configuration. By default, an unauthenticated server is created for the pipeline. This parameter uses pluggable authentication for HTTPS. To use basic authentication, define the http_basic plugin with a username and password. To provide customer authentication, use or create a plugin that implements GrpcAuthenticationProvider. | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/sources/otel-trace/#overview",
    "relUrl": "/data-prepper/pipelines/configuration/sources/otel-trace/#overview"
  },"1856": {
    "doc": "otel_trace_source source",
    "title": "Metrics",
    "content": "Counters . | requestTimeouts: Measures the total number of requests that time out. | requestsReceived: Measures the total number of requests received by the otel_trace source. | successRequests: Measures the total number of requests successfully processed by the otel_trace source plugin. | badRequests: Measures the total number of requests with an invalid format processed by the otel_trace source plugin. | requestsTooLarge: Measures the total number of requests whose number of spans exceeds the buffer capacity. | internalServerError: Measures the total number of requests processed by the otel_trace source with a custom exception type. | . Timers . | requestProcessDuration: Measures the latency of requests processed by the otel_trace source plugin in seconds. | . Distribution summaries . | payloadSize: Measures the incoming request payload size distribution in bytes. | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/sources/otel-trace/#metrics",
    "relUrl": "/data-prepper/pipelines/configuration/sources/otel-trace/#metrics"
  },"1857": {
    "doc": "otel_trace_source source",
    "title": "otel_trace_source source",
    "content": " ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/sources/otel-trace/",
    "relUrl": "/data-prepper/pipelines/configuration/sources/otel-trace/"
  },"1858": {
    "doc": "s3 source",
    "title": "s3 source",
    "content": "s3 is a source plugin that reads events from Amazon Simple Storage Service (Amazon S3) objects. It requires an Amazon Simple Queue Service (Amazon SQS) queue that receives S3 Event Notifications. After Amazon SQS is configured, the s3 source receives messages from Amazon SQS. When the SQS message indicates that an S3 object was created, the s3 source loads the S3 objects and then parses them using the configured codec. You can also configure the s3 source to use Amazon S3 Select instead of Data Prepper to parse S3 objects. ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/sources/s3/",
    "relUrl": "/data-prepper/pipelines/configuration/sources/s3/"
  },"1859": {
    "doc": "s3 source",
    "title": "IAM permissions",
    "content": "In order to use the s3 source, configure your AWS Identity and Access Management (IAM) permissions to grant Data Prepper access to Amazon S3. You can use a configuration similar to the following JSON configuration: . { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"s3-access\", \"Effect\": \"Allow\", \"Action\": \"s3:GetObject\", \"Resource\": \"arn:aws:s3:::&lt;YOUR-BUCKET&gt;/*\" }, { \"Sid\": \"sqs-access\", \"Effect\": \"Allow\", \"Action\": [ \"sqs:DeleteMessage\", \"sqs:ReceiveMessage\" ], \"Resource\": \"arn:aws:sqs:&lt;YOUR-REGION&gt;:&lt;123456789012&gt;:&lt;YOUR-SQS-QUEUE&gt;\" }, { \"Sid\": \"kms-access\", \"Effect\": \"Allow\", \"Action\": \"kms:Decrypt\", \"Resource\": \"arn:aws:kms:&lt;YOUR-REGION&gt;:&lt;123456789012&gt;:key/&lt;YOUR-KMS-KEY&gt;\" } ] } . If your S3 objects or Amazon SQS queues do not use AWS Key Management Service (AWS KMS), remove the kms:Decrypt permission. ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/sources/s3/#iam-permissions",
    "relUrl": "/data-prepper/pipelines/configuration/sources/s3/#iam-permissions"
  },"1860": {
    "doc": "s3 source",
    "title": "Configuration",
    "content": "You can use the following options to configure the s3 source. | Option | Required | Type | Description | . | notification_type | Yes | String | Must be sqs. | . | compression | No | String | The compression algorithm to apply: none, gzip, or automatic. Default value is none. | . | codec | Yes | Codec | The codec to apply. | . | sqs | Yes | sqs | The SQS configuration. See sqs for details. | . | aws | Yes | aws | The AWS configuration. See aws for details. | . | on_error | No | String | Determines how to handle errors in Amazon SQS. Can be either retain_messages or delete_messages. If retain_messages, then Data Prepper will leave the message in the Amazon SQS queue and try again. This is recommended for dead-letter queues. If delete_messages, then Data Prepper will delete failed messages. Default value is retain_messages. | . | buffer_timeout | No | Duration | The amount of time allowed for for writing events to the Data Prepper buffer before timeout occurs. Any events that the Amazon S3 source cannot write to the buffer in this time will be discarded. Default value is 10 seconds. | . | records_to_accumulate | No | Integer | The number of messages that accumulate before writing to the buffer. Default value is 100. | . | metadata_root_key | No | String | Base key for adding S3 metadata to each Event. The metadata includes the key and bucket for each S3 object. Defaults to s3/. | . | disable_bucket_ownership_validation | No | Boolean | If true, the S3Source will not attempt to validate that the bucket is owned by the expected account. The expected account is the same account that owns the Amazon SQS queue. Defaults to false. | . | acknowledgments | No | Boolean | If true, enables s3 sources to receive end-to-end acknowledgments when events are received by OpenSearch sinks. | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/sources/s3/#configuration",
    "relUrl": "/data-prepper/pipelines/configuration/sources/s3/#configuration"
  },"1861": {
    "doc": "s3 source",
    "title": "sqs",
    "content": "The following parameters allow you to configure usage for Amazon SQS in the s3 source plugin. | Option | Required | Type | Description | . | queue_url | Yes | String | The URL of the Amazon SQS queue from which messages are received. | . | maximum_messages | No | Integer | The maximum number of messages to receive from the Amazon SQS queue in any single request. Default value is 10. | . | visibility_timeout | No | Duration | The visibility timeout to apply to messages read from the Amazon SQS queue. This should be set to the amount of time that Data Prepper may take to read all the S3 objects in a batch. Default value is 30s. | . | wait_time | No | Duration | The amount of time to wait for long polling on the Amazon SQS API. Default value is 20s. | . | poll_delay | No | Duration | A delay to place between reading/processing a batch of Amazon SQS messages and making a subsequent request. Default value is 0s. | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/sources/s3/#sqs",
    "relUrl": "/data-prepper/pipelines/configuration/sources/s3/#sqs"
  },"1862": {
    "doc": "s3 source",
    "title": "aws",
    "content": "| Option | Required | Type | Description | . | region | No | String | The AWS Region to use for credentials. Defaults to standard SDK behavior to determine the Region. | . | sts_role_arn | No | String | The AWS Security Token Service (AWS STS) role to assume for requests to Amazon SQS and Amazon S3. Defaults to null, which will use the standard SDK behavior for credentials. | . | aws_sts_header_overrides | No | Map | A map of header overrides that the IAM role assumes for the sink plugin. | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/sources/s3/#aws",
    "relUrl": "/data-prepper/pipelines/configuration/sources/s3/#aws"
  },"1863": {
    "doc": "s3 source",
    "title": "codec",
    "content": "The codec determines how the s3 source parses each S3 object. newline codec . The newline codec parses each single line as a single log event. This is ideal for most application logs because each event parses per single line. It can also be suitable for S3 objects that have individual JSON objects on each line, which matches well when used with the parse_json processor to parse each line. Use the following options to configure the newline codec. | Option | Required | Type | Description | . | skip_lines | No | Integer | The number of lines to skip before creating events. You can use this configuration to skip common header rows. Default is 0. | . | header_destination | No | String | A key value to assign to the header line of the S3 object. If this option is specified, then each event will contain a header_destination field. | . json codec . The json codec parses each S3 object as a single JSON object from a JSON array and then creates a Data Prepper log event for each object in the array. csv codec . The csv codec parses objects in comma-separated value (CSV) format, with each row producing a Data Prepper log event. Use the following options to configure the csv codec. | Option | Required | Type | Description | . | delimiter | Yes | Integer | The delimiter separating columns. Default is ,. | . | quote_character | Yes | String | The character used as a text qualifier for CSV data. Default is \". | . | header | No | String list | The header containing the column names used to parse CSV data. | . | detect_header | No | Boolean | Whether the first line of the S3 object should be interpreted as a header. Default is true. | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/sources/s3/#codec",
    "relUrl": "/data-prepper/pipelines/configuration/sources/s3/#codec"
  },"1864": {
    "doc": "s3 source",
    "title": "Using s3_select with the s3 source",
    "content": "When configuring s3_select to parse S3 objects, use the following options. | Option | Required | Type | Description | . | expression | Yes, when using s3_select | String | The expression used to query the object. Maps directly to the expression property. | . | expression_type | No | String | The type of the provided expression. Default value is SQL. Maps directly to the ExpressionType. | . | input_serialization | Yes, when using s3_select | String | Provides the S3 Select file format. Amazon S3 uses this format to parse object data into records and returns only records that match the specified SQL expression. May be csv, json, or parquet. | . | compression_type | No | String | Specifies an object’s compression format. Maps directly to the CompressionType. | . | csv | No | csv | Provides the CSV configuration for processing CSV data. | . | json | No | json | Provides the JSON configuration for processing JSON data. | . csv . Use the following options in conjunction with the csv configuration for s3_select to determine how your parsed CSV file should be formatted. These options map directly to options available in the S3 Select CSVInput data type. | Option | Required | Type | Description | . | file_header_info | No | String | Describes the first line of input. Maps directly to the FileHeaderInfo property. | . | quote_escape | No | String | A single character used for escaping the quotation mark character inside an already escaped value. Maps directly to the QuoteEscapeCharacter property. | . | comments | No | String | A single character used to indicate that a row should be ignored when the character is present at the start of that row. Maps directly to the Comments property. | . json . Use the following option in conjunction with json for s3_select to determine how S3 Select processes the JSON file. | Option | Required | Type | Description | . | type | No | String | The type of JSON array. May be either DOCUMENT or LINES. Maps directly to the Type property. | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/sources/s3/#using-s3_select-with-the-s3-source",
    "relUrl": "/data-prepper/pipelines/configuration/sources/s3/#using-s3_select-with-the-s3-source"
  },"1865": {
    "doc": "s3 source",
    "title": "Metrics",
    "content": "The s3 source includes the following metrics. Counters . | s3ObjectsFailed: The number of S3 objects that the s3 source failed to read. | s3ObjectsNotFound: The number of S3 objects that the s3 source failed to read due to an S3 “Not Found” error. These are also counted toward s3ObjectsFailed. | s3ObjectsAccessDenied: The number of S3 objects that the s3 source failed to read due to an “Access Denied” or “Forbidden” error. These are also counted toward s3ObjectsFailed. | s3ObjectsSucceeded: The number of S3 objects that the s3 source successfully read. | sqsMessagesReceived: The number of Amazon SQS messages received from the queue by the s3 source. | sqsMessagesDeleted: The number of Amazon SQS messages deleted from the queue by the s3 source. | sqsMessagesFailed: The number of Amazon SQS messages that the s3 source failed to parse. | . Timers . | s3ObjectReadTimeElapsed: Measures the amount of time the s3 source takes to perform a request to GET an S3 object, parse it, and write events to the buffer. | sqsMessageDelay: Measures the time elapsed from when S3 creates an object to when it is fully parsed. | . Distribution summaries . | s3ObjectSizeBytes: Measures the size of S3 objects as reported by the S3 Content-Length. For compressed objects, this is the compressed size. | s3ObjectProcessedBytes: Measures the bytes processed by the s3 source for a given object. For compressed objects, this is the uncompressed size. | s3ObjectsEvents: Measures the number of events (sometimes called records) produced by an S3 object. | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/sources/s3/#metrics",
    "relUrl": "/data-prepper/pipelines/configuration/sources/s3/#metrics"
  },"1866": {
    "doc": "s3 source",
    "title": "Example: Uncompressed logs",
    "content": "The following pipeline.yaml file shows the minimum configuration for reading uncompressed newline-delimited logs: . source: s3: notification_type: sqs codec: newline: compression: none sqs: queue_url: \"https://sqs.us-east-1.amazonaws.com/123456789012/MyQueue\" aws: region: \"us-east-1\" sts_role_arn: \"arn:aws:iam::123456789012:role/Data-Prepper\" . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/sources/s3/#example-uncompressed-logs",
    "relUrl": "/data-prepper/pipelines/configuration/sources/s3/#example-uncompressed-logs"
  },"1867": {
    "doc": "Sources",
    "title": "Sources",
    "content": "Sources define where your data comes from within a Data Prepper pipeline. ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/configuration/sources/sources/",
    "relUrl": "/data-prepper/pipelines/configuration/sources/sources/"
  },"1868": {
    "doc": "Dead-letter queues",
    "title": "Dead-letter queues",
    "content": "Data Prepper pipelines support dead-letter queues (DLQs) for offloading failed events and making them accessible for analysis. As of Data Prepper 2.3, only the s3 source supports DLQs. ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/dlq/",
    "relUrl": "/data-prepper/pipelines/dlq/"
  },"1869": {
    "doc": "Dead-letter queues",
    "title": "Configure a DLQ writer",
    "content": "To configure a DLQ writer for the s3 source, add the following to your pipeline.yaml file: . sink: opensearch: dlq: s3: bucket: \"my-dlq-bucket\" key_path_prefix: \"dlq-files/\" region: \"us-west-2\" sts_role_arn: \"arn:aws:iam::123456789012:role/dlq-role\" . The resulting DLQ file outputs as a JSON array of DLQ objects. Any file written to the S3 DLQ contains the following name pattern: . dlq-v${version}-${pipelineName}-${pluginId}-${timestampIso8601}-${uniqueId} . The following information is replaced in the name pattern: . | version: The Data Prepper version. | pipelineName: The pipeline name indicated in pipeline.yaml. | pluginId: The ID of the plugin associated with the DLQ event. | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/dlq/#configure-a-dlq-writer",
    "relUrl": "/data-prepper/pipelines/dlq/#configure-a-dlq-writer"
  },"1870": {
    "doc": "Dead-letter queues",
    "title": "Configuration",
    "content": "DLQ supports the following configuration options. | Option | Required | Type | Description | . | bucket | Yes | String | The name of the bucket into which the DLQ outputs failed records. | . | key_path_prefix | No | String | The key_prefix used in the S3 bucket. Defaults to \"\". Supports time value pattern variables, such as /%{yyyy}/%{MM}/%{dd}, including any variables listed in the Java DateTimeFormatter. For example, when using the /%{yyyy}/%{MM}/%{dd} pattern, you can set key_prefix as /2023/01/24. | . | region | No | String | The AWS Region of the S3 bucket. Defaults to us-east-1. | . | sts_role_arn | No | String | The STS role the DLQ assumes in order to write to an AWS S3 bucket. Default is null, which uses the standard SDK behavior for credentials. To use this option, the S3 bucket must have the S3:PutObject permission configured. | . When using DLQ with an OpenSearch sink, you can configure the max_retries option to send failed data to the DLQ when the sink reaches the maximum number of retries. ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/dlq/#configuration",
    "relUrl": "/data-prepper/pipelines/dlq/#configuration"
  },"1871": {
    "doc": "Dead-letter queues",
    "title": "Metrics",
    "content": "DLQ supports the following metrics. Counter . | dlqS3RecordsSuccess: Measures the number of successful records sent to S3. | dlqS3RecordsFailed: Measures the number of records that failed to be sent to S3. | dlqS3RequestSuccess: Measures the number of successful S3 requests. | dlqS3RequestFailed: Measures the number of failed S3 requests. | . Distribution summary . | dlqS3RequestSizeBytes: Measures the distribution of the S3 request’s payload size in bytes. | . Timer . | dlqS3RequestLatency: Measures latency when sending each S3 request, including retries. | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/dlq/#metrics",
    "relUrl": "/data-prepper/pipelines/dlq/#metrics"
  },"1872": {
    "doc": "Dead-letter queues",
    "title": "DLQ objects",
    "content": "DLQ supports the following DLQ objects: . | pluginId: The ID of the plugin that originated the event sent to the DLQ. | pluginName: The name of the plugin. | failedData : An object that contains the failed object and its options. This object is unique to each plugin. | pipelineName: The name of the Data Prepper pipeline in which the event failed. | timestamp: The timestamp of the failures in an ISO8601 format. | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/dlq/#dlq-objects",
    "relUrl": "/data-prepper/pipelines/dlq/#dlq-objects"
  },"1873": {
    "doc": "Expression syntax",
    "title": "Expression syntax",
    "content": "The following sections provide information about expression syntax in Data Prepper. ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/expression-syntax/",
    "relUrl": "/data-prepper/pipelines/expression-syntax/"
  },"1874": {
    "doc": "Expression syntax",
    "title": "Supported operators",
    "content": "Operators are listed in order of precedence (top to bottom, left to right). | Operator | Description | Associativity | . | () | Priority Expression | left-to-right | . | not + - | Unary Logical NOTUnary PositiveUnary negative | right-to-left | . | &lt;, &lt;=, &gt;, &gt;= | Relational Operators | left-to-right | . | ==, != | Equality Operators | left-to-right | . | and, or | Conditional Expression | left-to-right | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/expression-syntax/#supported-operators",
    "relUrl": "/data-prepper/pipelines/expression-syntax/#supported-operators"
  },"1875": {
    "doc": "Expression syntax",
    "title": "Reserved for possible future functionality",
    "content": "Reserved symbol set: ^, *, /, %, +, -, xor, =, +=, -=, *=, /=, %=, ++, --, ${&lt;text&gt;} . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/expression-syntax/#reserved-for-possible-future-functionality",
    "relUrl": "/data-prepper/pipelines/expression-syntax/#reserved-for-possible-future-functionality"
  },"1876": {
    "doc": "Expression syntax",
    "title": "Set initializer",
    "content": "The set initializer defines a set or term and/or expressions. Examples . The following are examples of set initializer syntax. HTTP status codes . {200, 201, 202} . HTTP response payloads . {\"Created\", \"Accepted\"} . Handle multiple event types with different keys . {/request_payload, /request_message} . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/expression-syntax/#set-initializer",
    "relUrl": "/data-prepper/pipelines/expression-syntax/#set-initializer"
  },"1877": {
    "doc": "Expression syntax",
    "title": "Priority expression",
    "content": "A priority expression identifies an expression that will be evaluated at the highest priority level. A priority expression must contain an expression or value; empty parentheses are not supported. Example . /is_cool == (/name == \"Steven\") . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/expression-syntax/#priority-expression",
    "relUrl": "/data-prepper/pipelines/expression-syntax/#priority-expression"
  },"1878": {
    "doc": "Expression syntax",
    "title": "Relational operators",
    "content": "Relational operators are used to test the relationship of two numeric values. The operands must be numbers or JSON Pointers that resolve to numbers. Syntax . &lt;Number | JSON Pointer&gt; &lt; &lt;Number | JSON Pointer&gt; &lt;Number | JSON Pointer&gt; &lt;= &lt;Number | JSON Pointer&gt; &lt;Number | JSON Pointer&gt; &gt; &lt;Number | JSON Pointer&gt; &lt;Number | JSON Pointer&gt; &gt;= &lt;Number | JSON Pointer&gt; . Example . /status_code &gt;= 200 and /status_code &lt; 300 . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/expression-syntax/#relational-operators",
    "relUrl": "/data-prepper/pipelines/expression-syntax/#relational-operators"
  },"1879": {
    "doc": "Expression syntax",
    "title": "Equality operators",
    "content": "Equality operators are used to test whether two values are equivalent. Syntax . &lt;Any&gt; == &lt;Any&gt; &lt;Any&gt; != &lt;Any&gt; . Examples . /is_cool == true 3.14 != /status_code {1, 2} == /event/set_property . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/expression-syntax/#equality-operators",
    "relUrl": "/data-prepper/pipelines/expression-syntax/#equality-operators"
  },"1880": {
    "doc": "Expression syntax",
    "title": "Using equality operators to check for a JSON Pointer",
    "content": "Equality operators can also be used to check whether a JSON Pointer exists by comparing the value with null. Syntax . &lt;JSON Pointer&gt; == null &lt;JSON Pointer&gt; != null null == &lt;JSON Pointer&gt; null != &lt;JSON Pointer&gt; . Example . /response == null null != /response . Conditional expression . A conditional expression is used to chain together multiple expressions and/or values. Syntax . &lt;Any&gt; and &lt;Any&gt; &lt;Any&gt; or &lt;Any&gt; not &lt;Any&gt; . Example . /status_code == 200 and /message == \"Hello world\" /status_code == 200 or /status_code == 202 not /status_code in {200, 202} /response == null /response != null . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/expression-syntax/#using-equality-operators-to-check-for-a-json-pointer",
    "relUrl": "/data-prepper/pipelines/expression-syntax/#using-equality-operators-to-check-for-a-json-pointer"
  },"1881": {
    "doc": "Expression syntax",
    "title": "Definitions",
    "content": "This section provides expression definitions. Literal . A literal is a fundamental value that has no children: . | Float: Supports values from 3.40282347 × 1038 to 1.40239846 × 10−45. | Integer: Supports values from −2,147,483,648 to 2,147,483,647. | Boolean: Supports true or false. | JSON Pointer: See the JSON Pointer section for details. | String: Supports valid Java strings. | Null: Supports null check to see whether a JSON Pointer exists. | . Expression string . An expression string takes the highest priority in a Data Prepper expression and only supports one expression string resulting in a return value. An expression string is not the same as an expression. Statement . A statement is the highest-priority component of an expression string. Expression . An expression is a generic component that contains a Primary or an Operator. Expressions may contain expressions. An expression’s imminent children can contain 0–1 Operators. Primary . | Set | Priority Expression | Literal | . Operator . An operator is a hardcoded token that identifies the operation used in an expression. JSON Pointer . A JSON Pointer is a literal used to reference a value within an event and provided as context for an expression string. JSON Pointers are identified by a leading / containing alphanumeric characters or underscores, delimited by /. JSON Pointers can use an extended character set if wrapped in double quotes (\") using the escape character \\. Note that JSON Pointers require ~ and / characters, which should be used as part of the path and not as a delimiter that needs to be escaped. The following are examples of JSON Pointers: . | ~0 representing ~ | ~1 representing / | . Shorthand syntax (Regex, \\w = [A-Za-z_]) . /\\w+(/\\w+)* . Example of shorthand . The following is an example of shorthand: . /Hello/World/0 . Example of escaped syntax . The following is an example of escaped syntax: . \"/&lt;Valid String Characters | Escaped Character&gt;(/&lt;Valid String Characters | Escaped Character&gt;)*\" . Example of an escaped JSON Pointer . The following is an example of an escaped JSON Pointer: . # Path # { \"Hello - 'world/\" : [{ \"\\\"JsonPointer\\\"\": true }] } \"/Hello - 'world\\//0/\\\"JsonPointer\\\"\" . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/expression-syntax/#definitions",
    "relUrl": "/data-prepper/pipelines/expression-syntax/#definitions"
  },"1882": {
    "doc": "Expression syntax",
    "title": "White space",
    "content": "White space is optional surrounding relational operators, regex equality operators, equality operators, and commas. White space is required surrounding set initializers, priority expressions, set operators, and conditional expressions. | Operator | Description | White space required | ✅ Valid examples | ❌ Invalid examples | . | {} | Set initializer | Yes | /status in {200} | /status in{200} | . | () | Priority expression | Yes | /a==(/b==200)/a in ({200}) | /status in({200}) | . | in, not in | Set operators | Yes | /a in {200}/a not in {400} | /a in{200, 202}/a not in{400} | . | &lt;, &lt;=, &gt;, &gt;= | Relational operators | No | /status &lt; 300/status&gt;=300 |   | . | =~, !~ | Regex equality pperators | No | /msg =~ \"^\\w*$\"/msg=~\"^\\w*$\" |   | . | ==, != | Equality operators | No | /status == 200/status_code==200 |   | . | and, or, not | Conditional operators | Yes | /a&lt;300 and /b&gt;200 | /b&lt;300and/b&gt;200 | . | , | Set value delimiter | No | /a in {200, 202}/a in {200,202}/a in {200 , 202} | /a in {200,} | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/expression-syntax/#white-space",
    "relUrl": "/data-prepper/pipelines/expression-syntax/#white-space"
  },"1883": {
    "doc": "Pipeline options",
    "title": "Pipeline options",
    "content": "This page provides information about pipeline configuration options in Data Prepper. ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/pipelines-configuration-options/",
    "relUrl": "/data-prepper/pipelines/pipelines-configuration-options/"
  },"1884": {
    "doc": "Pipeline options",
    "title": "General pipeline options",
    "content": "| Option | Required | Type | Description | . | workers | No | Integer | Essentially the number of application threads. As a starting point for your use case, try setting this value to the number of CPU cores on the machine. Default is 1. | . | delay | No | Integer | Amount of time in milliseconds workers wait between buffer read attempts. Default is 3,000. | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/pipelines-configuration-options/#general-pipeline-options",
    "relUrl": "/data-prepper/pipelines/pipelines-configuration-options/#general-pipeline-options"
  },"1885": {
    "doc": "Pipelines",
    "title": "Pipelines",
    "content": "The following image illustrates how a pipeline works. To use Data Prepper, you define pipelines in a configuration YAML file. Each pipeline is a combination of a source, a buffer, zero or more processors, and one or more sinks. For example: . simple-sample-pipeline: workers: 2 # the number of workers delay: 5000 # in milliseconds, how long workers wait between read attempts source: random: buffer: bounded_blocking: buffer_size: 1024 # max number of records the buffer accepts batch_size: 256 # max number of records the buffer drains after each read processor: - string_converter: upper_case: true sink: - stdout: . | Sources define where your data comes from. In this case, the source is a random UUID generator (random). | Buffers store data as it passes through the pipeline. By default, Data Prepper uses its one and only buffer, the bounded_blocking buffer, so you can omit this section unless you developed a custom buffer or need to tune the buffer settings. | Processors perform some action on your data: filter, transform, enrich, etc. You can have multiple processors, which run sequentially from top to bottom, not in parallel. The string_converter processor transform the strings by making them uppercase. | Sinks define where your data goes. In this case, the sink is stdout. | . Starting from Data Prepper 2.0, you can define pipelines across multiple configuration YAML files, where each file contains the configuration for one or more pipelines. This gives you more freedom to organize and chain complex pipeline configurations. For Data Prepper to load your pipeline configuration properly, place your configuration YAML files in the pipelines folder under your application’s home directory (e.g. /usr/share/data-prepper). ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/pipelines/",
    "relUrl": "/data-prepper/pipelines/pipelines/"
  },"1886": {
    "doc": "Pipelines",
    "title": "End-to-end acknowledgments",
    "content": "Data Prepper ensures the durability and reliability of data written from sources and delivered to sinks through end-to-end (E2E) acknowledgments. An E2E acknowledgment begins at the source, which monitors a batch of events set inside pipelines and waits for a positive acknowledgment when those events are successfully pushed to sinks. When a pipeline contains multiple sinks, including sinks set as additional Data Prepper pipelines, the E2E acknowledgment sends when events are received by the final sink in a pipeline chain. Alternatively, the source sends a negative acknowledgment when an event cannot be delivered to a sink for any reason. When any component of a pipeline fails and is unable to send an event, the source receives no acknowledgment. In the case of a failure, the pipeline’s source times out. This gives you the ability to take any necessary actions to address the source failure, including rerunning the pipeline or logging the failure. As of Data Prepper 2.2, only the s3 source and opensearch sink support E2E acknowledgments. ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/pipelines/#end-to-end-acknowledgments",
    "relUrl": "/data-prepper/pipelines/pipelines/#end-to-end-acknowledgments"
  },"1887": {
    "doc": "Pipelines",
    "title": "Conditional routing",
    "content": "Pipelines also support conditional routing which allows you to route Events to different sinks based on specific conditions. To add conditional routing to a pipeline, specify a list of named routes under the route component and add specific routes to sinks under the routes property. Any sink with the routes property will only accept Events that match at least one of the routing conditions. In the following example, application-logs is a named route with a condition set to /log_type == \"application\". The route uses Data Prepper expressions to define the conditions. Data Prepper only routes events that satisfy the condition to the first OpenSearch sink. By default, Data Prepper routes all Events to a sink which does not define a route. In the example, all Events route into the third OpenSearch sink. conditional-routing-sample-pipeline: source: http: processor: route: - application-logs: '/log_type == \"application\"' - http-logs: '/log_type == \"apache\"' sink: - opensearch: hosts: [ \"https://opensearch:9200\" ] index: application_logs routes: [application-logs] - opensearch: hosts: [ \"https://opensearch:9200\" ] index: http_logs routes: [http-logs] - opensearch: hosts: [ \"https://opensearch:9200\" ] index: all_logs . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/pipelines/#conditional-routing",
    "relUrl": "/data-prepper/pipelines/pipelines/#conditional-routing"
  },"1888": {
    "doc": "Pipelines",
    "title": "Examples",
    "content": "This section provides some pipeline examples that you can use to start creating your own pipelines. For more pipeline configurations, select from the following options for each component: . | Buffers | Processors | Sinks | Sources | . The Data Prepper repository has several sample applications to help you get started. Log ingestion pipeline . The following example pipeline.yaml file with SSL and basic authentication enabled for the http-source demonstrates how to use the HTTP Source and Grok Prepper plugins to process unstructured log data: . log-pipeline: source: http: ssl_certificate_file: \"/full/path/to/certfile.crt\" ssl_key_file: \"/full/path/to/keyfile.key\" authentication: http_basic: username: \"myuser\" password: \"mys3cret\" processor: - grok: match: # This will match logs with a \"log\" key against the COMMONAPACHELOG pattern (ex: { \"log\": \"actual apache log...\" } ) # You should change this to match what your logs look like. See the grok documenation to get started. log: [ \"%{COMMONAPACHELOG}\" ] sink: - opensearch: hosts: [ \"https://localhost:9200\" ] # Change to your credentials username: \"admin\" password: \"admin\" # Add a certificate file if you are accessing an OpenSearch cluster with a self-signed certificate #cert: /path/to/cert # If you are connecting to an Amazon OpenSearch Service domain without # Fine-Grained Access Control, enable these settings. Comment out the # username and password above. #aws_sigv4: true #aws_region: us-east-1 # Since we are grok matching for apache logs, it makes sense to send them to an OpenSearch index named apache_logs. # You should change this to correspond with how your OpenSearch indices are set up. index: apache_logs . This example uses weak security. We strongly recommend securing all plugins which open external ports in production environments. Trace analytics pipeline . The following example demonstrates how to build a pipeline that supports the Trace Analytics OpenSearch Dashboards plugin. This pipeline takes data from the OpenTelemetry Collector and uses two other pipelines as sinks. These two separate pipelines index trace and the service map documents for the dashboard plugin. Starting from Data Prepper 2.0, Data Prepper no longer supports otel_trace_raw_prepper processor due to the Data Prepper internal data model evolution. Instead, users should use otel_trace_raw. entry-pipeline: delay: \"100\" source: otel_trace_source: ssl: false buffer: bounded_blocking: buffer_size: 10240 batch_size: 160 sink: - pipeline: name: \"raw-pipeline\" - pipeline: name: \"service-map-pipeline\" raw-pipeline: source: pipeline: name: \"entry-pipeline\" buffer: bounded_blocking: buffer_size: 10240 batch_size: 160 processor: - otel_trace_raw: sink: - opensearch: hosts: [\"https://localhost:9200\"] insecure: true username: admin password: admin index_type: trace-analytics-raw service-map-pipeline: delay: \"100\" source: pipeline: name: \"entry-pipeline\" buffer: bounded_blocking: buffer_size: 10240 batch_size: 160 processor: - service_map_stateful: sink: - opensearch: hosts: [\"https://localhost:9200\"] insecure: true username: admin password: admin index_type: trace-analytics-service-map . To maintain similar ingestion throughput and latency, scale the buffer_size and batch_size by the estimated maximum batch size in the client request payload. Metrics pipeline . Data Prepper supports metrics ingestion using OTel. It currently supports the following metric types: . | Gauge | Sum | Summary | Histogram | . Other types are not supported. Data Prepper drops all other types, including Exponential Histogram and Summary. Additionally, Data Prepper does not support Scope instrumentation. To set up a metrics pipeline: . metrics-pipeline: source: otel_metrics_source: processor: - otel_metrics_raw_processor: sink: - opensearch: hosts: [\"https://localhost:9200\"] username: admin password: admin . S3 log ingestion pipeline . The following example demonstrates how to use the S3Source and Grok Processor plugins to process unstructured log data from Amazon Simple Storage Service (Amazon S3). This example uses application load balancer logs. As the application load balancer writes logs to S3, S3 creates notifications in Amazon SQS. Data Prepper monitors those notifications and reads the S3 objects to get the log data and process it. log-pipeline: source: s3: notification_type: \"sqs\" compression: \"gzip\" codec: newline: sqs: queue_url: \"https://sqs.us-east-1.amazonaws.com/12345678910/ApplicationLoadBalancer\" aws: region: \"us-east-1\" sts_role_arn: \"arn:aws:iam::12345678910:role/Data-Prepper\" processor: - grok: match: message: [\"%{DATA:type} %{TIMESTAMP_ISO8601:time} %{DATA:elb} %{DATA:client} %{DATA:target} %{BASE10NUM:request_processing_time} %{DATA:target_processing_time} %{BASE10NUM:response_processing_time} %{BASE10NUM:elb_status_code} %{DATA:target_status_code} %{BASE10NUM:received_bytes} %{BASE10NUM:sent_bytes} \\\"%{DATA:request}\\\" \\\"%{DATA:user_agent}\\\" %{DATA:ssl_cipher} %{DATA:ssl_protocol} %{DATA:target_group_arn} \\\"%{DATA:trace_id}\\\" \\\"%{DATA:domain_name}\\\" \\\"%{DATA:chosen_cert_arn}\\\" %{DATA:matched_rule_priority} %{TIMESTAMP_ISO8601:request_creation_time} \\\"%{DATA:actions_executed}\\\" \\\"%{DATA:redirect_url}\\\" \\\"%{DATA:error_reason}\\\" \\\"%{DATA:target_list}\\\" \\\"%{DATA:target_status_code_list}\\\" \\\"%{DATA:classification}\\\" \\\"%{DATA:classification_reason}\"] - grok: match: request: [\"(%{NOTSPACE:http_method})? (%{NOTSPACE:http_uri})? (%{NOTSPACE:http_version})?\"] - grok: match: http_uri: [\"(%{WORD:protocol})?(://)?(%{IPORHOST:domain})?(:)?(%{INT:http_port})?(%{GREEDYDATA:request_uri})?\"] - date: from_time_received: true destination: \"@timestamp\" sink: - opensearch: hosts: [ \"https://localhost:9200\" ] username: \"admin\" password: \"admin\" index: alb_logs . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/pipelines/#examples",
    "relUrl": "/data-prepper/pipelines/pipelines/#examples"
  },"1889": {
    "doc": "Pipelines",
    "title": "Migrating from Logstash",
    "content": "Data Prepper supports Logstash configuration files for a limited set of plugins. Simply use the logstash config to run Data Prepper. docker run --name data-prepper \\ -v /full/path/to/logstash.conf:/usr/share/data-prepper/pipelines/pipelines.conf \\ opensearchproject/opensearch-data-prepper:latest . This feature is limited by feature parity of Data Prepper. As of Data Prepper 1.2 release, the following plugins from the Logstash configuration are supported: . | HTTP Input plugin | Grok Filter plugin | Elasticsearch Output plugin | Amazon Elasticsearch Output plugin | . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/pipelines/#migrating-from-logstash",
    "relUrl": "/data-prepper/pipelines/pipelines/#migrating-from-logstash"
  },"1890": {
    "doc": "Pipelines",
    "title": "Configure the Data Prepper server",
    "content": "Data Prepper itself provides administrative HTTP endpoints such as /list to list pipelines and /metrics/prometheus to provide Prometheus-compatible metrics data. The port that has these endpoints has a TLS configuration and is specified by a separate YAML file. By default, these endpoints are secured by Data Prepper docker images. We strongly recommend providing your own configuration file for securing production environments. Here is an example data-prepper-config.yaml: . ssl: true keyStoreFilePath: \"/usr/share/data-prepper/keystore.jks\" keyStorePassword: \"password\" privateKeyPassword: \"other_password\" serverPort: 1234 . To configure the Data Prepper server, run Data Prepper with the additional yaml file. docker run --name data-prepper \\ -v /full/path/to/my-pipelines.yaml:/usr/share/data-prepper/pipelines/my-pipelines.yaml \\ -v /full/path/to/data-prepper-config.yaml:/usr/share/data-prepper/data-prepper-config.yaml \\ opensearchproject/data-prepper:latest . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/pipelines/#configure-the-data-prepper-server",
    "relUrl": "/data-prepper/pipelines/pipelines/#configure-the-data-prepper-server"
  },"1891": {
    "doc": "Pipelines",
    "title": "Configure peer forwarder",
    "content": "Data Prepper provides an HTTP service to forward Events between Data Prepper nodes for aggregation. This is required for operating Data Prepper in a clustered deployment. Currently, peer forwarding is supported in aggregate, service_map_stateful, and otel_trace_raw processors. Peer forwarder groups events based on the identification keys provided by the processors. For service_map_stateful and otel_trace_raw it’s traceId by default and can not be configured. For aggregate processor, it is configurable using identification_keys option. Peer forwarder supports peer discovery through one of three options: a static list, a DNS record lookup , or AWS Cloud Map. Peer discovery can be configured using discovery_mode option. Peer forwarder also supports SSL for verification and encryption, and mTLS for mutual authentication in a peer forwarding service. To configure peer forwarder, add configuration options to data-prepper-config.yaml mentioned in the Configure the Data Prepper server section: . peer_forwarder: discovery_mode: dns domain_name: \"data-prepper-cluster.my-domain.net\" ssl: true ssl_certificate_file: \"&lt;cert-file-path&gt;\" ssl_key_file: \"&lt;private-key-file-path&gt;\" authentication: mutual_tls: . ",
    "url": "https://vagimeli.github.io/data-prepper/pipelines/pipelines/#configure-peer-forwarder",
    "relUrl": "/data-prepper/pipelines/pipelines/#configure-peer-forwarder"
  },"1892": {
    "doc": "OpenSearch CLI",
    "title": "OpenSearch CLI",
    "content": "The OpenSearch CLI command line interface (opensearch-cli) lets you manage your OpenSearch cluster from the command line and automate tasks. Currently, opensearch-cli supports the Anomaly Detection and k-NN plugins, along with arbitrary REST API paths. Among other things, you can use opensearch-cli to create and delete detectors, start and stop them, and check k-NN statistics. Profiles let you easily access different clusters or sign requests with different credentials. opensearch-cli supports unauthenticated requests, HTTP basic signing, and IAM signing for Amazon Web Services. This example moves a detector (ecommerce-count-quantity) from a staging cluster to a production cluster: . opensearch-cli ad get ecommerce-count-quantity --profile staging &gt; ecommerce-count-quantity.json opensearch-cli ad create ecommerce-count-quantity.json --profile production opensearch-cli ad start ecommerce-count-quantity.json --profile production opensearch-cli ad stop ecommerce-count-quantity --profile staging opensearch-cli ad delete ecommerce-count-quantity --profile staging . ",
    "url": "https://vagimeli.github.io/tools/cli/",
    "relUrl": "/tools/cli/"
  },"1893": {
    "doc": "OpenSearch CLI",
    "title": "Install",
    "content": ". | Download and extract the appropriate installation package for your computer. | Make the opensearch-cli file executable: . chmod +x ./opensearch-cli . | Add the command to your path: . export PATH=$PATH:$(pwd) . | Confirm the CLI is working properly: . opensearch-cli --version . | . ",
    "url": "https://vagimeli.github.io/tools/cli/#install",
    "relUrl": "/tools/cli/#install"
  },"1894": {
    "doc": "OpenSearch CLI",
    "title": "Profiles",
    "content": "Profiles let you easily switch between different clusters and user credentials. To get started, run opensearch-cli profile create with the --auth-type, --endpoint, and --name options: . opensearch-cli profile create --auth-type basic --endpoint https://localhost:9200 --name docker-local . Alternatively, save a configuration file to ~/.opensearch-cli/config.yaml: . profiles: - name: docker-local endpoint: https://localhost:9200 user: admin password: foobar - name: aws endpoint: https://some-cluster.us-east-1.es.amazonaws.com aws_iam: profile: \"\" service: es . ",
    "url": "https://vagimeli.github.io/tools/cli/#profiles",
    "relUrl": "/tools/cli/#profiles"
  },"1895": {
    "doc": "OpenSearch CLI",
    "title": "Usage",
    "content": "opensearch-cli commands use the following syntax: . opensearch-cli &lt;command&gt; &lt;subcommand&gt; &lt;flags&gt; . For example, the following command retrieves information about a detector: . opensearch-cli ad get my-detector --profile docker-local . For a request to the OpenSearch CAT API, try the following command: . opensearch-cli curl get --path _cat/plugins --profile aws . Use the -h or --help flag to see all supported commands, subcommands, or usage for a specific command: . opensearch-cli -h opensearch-cli ad -h opensearch-cli ad get -h . ",
    "url": "https://vagimeli.github.io/tools/cli/#usage",
    "relUrl": "/tools/cli/#usage"
  },"1896": {
    "doc": "Grafana",
    "title": "Grafana support",
    "content": "Grafana has a data source plugin that lets you explore and visualize your OpenSearch data. For information on getting started with the plugin, see the Grafana overview page. ",
    "url": "https://vagimeli.github.io/tools/grafana/#grafana-support",
    "relUrl": "/tools/grafana/#grafana-support"
  },"1897": {
    "doc": "Grafana",
    "title": "Grafana",
    "content": " ",
    "url": "https://vagimeli.github.io/tools/grafana/",
    "relUrl": "/tools/grafana/"
  },"1898": {
    "doc": "Tools",
    "title": "OpenSearch tools",
    "content": "This section provides documentation for OpenSearch-supported tools, including: . | Agents and ingestion tools | OpenSearch CLI | OpenSearch Kubernetes operator | . For information about Data Prepper, the server-side data collector for filtering, enriching, transforming, normalizing, and aggregating data for downstream analytics and visualization, see Data Prepper. ",
    "url": "https://vagimeli.github.io/tools/index/#opensearch-tools",
    "relUrl": "/tools/index/#opensearch-tools"
  },"1899": {
    "doc": "Tools",
    "title": "Agents and ingestion tools",
    "content": "Historically, many multiple popular agents and ingestion tools have worked with Elasticsearch OSS, such as Beats, Logstash, Fluentd, FluentBit, and OpenTelemetry. OpenSearch aims to continue to support a broad set of agents and ingestion tools, but not all have been tested or have explicitly added OpenSearch compatibility. As an intermediate compatibility solution, OpenSearch has a setting that instructs the cluster to return version 7.10.2 rather than its actual version. If you use clients that include a version check, such as versions of Logstash OSS or Filebeat OSS between 7.x - 7.12.x, enable the setting: . PUT _cluster/settings { \"persistent\": { \"compatibility\": { \"override_main_response_version\": true } } } . Just like any other setting, the alternative is to add the following line to opensearch.yml on each node and then restart the node: . compatibility.override_main_response_version: true . Logstash OSS 8.0 introduces a breaking change where all plugins run in ECS compatibility mode by default. If you use a compatible OSS client you must override the default value to maintain legacy behavior: . ecs_compatibility =&gt; disabled . Downloads . You can download the OpenSearch output plugin for Logstash from OpenSearch downloads. The Logstash output plugin is compatible with OpenSearch and Elasticsearch OSS (7.10.2 or lower). These are the latest versions of Beats OSS with OpenSearch compatibility. For more information, see the Compatibility matrices section, below. | Filebeat OSS 7.12.1 | Metricbeat OSS 7.12.1 | Packetbeat OSS 7.12.1 | Heartbeat OSS 7.12.1 | Winlogbeat OSS 7.12.1 | Auditbeat OSS 7.12.1 | . Some users report compatibility issues with ingest pipelines on these versions of Beats. If you use ingest pipelines with OpenSearch, consider using the 7.10.2 versions of Beats instead. ",
    "url": "https://vagimeli.github.io/tools/index/#agents-and-ingestion-tools",
    "relUrl": "/tools/index/#agents-and-ingestion-tools"
  },"1900": {
    "doc": "Tools",
    "title": "Compatibility matrices",
    "content": "Italicized cells are untested, but indicate what a value theoretically should be based on existing information. Compatibility matrix for Logstash . |   | Logstash OSS 7.0.0 to 7.11.x | Logstash OSS 7.12.x* | Logstash 7.13.x-7.16.x without OpenSearch output plugin | Logstash 7.13.x-7.16.x with OpenSearch output plugin | Logstash 8.x+ with OpenSearch output plugin | . | Elasticsearch OSS 7.0.0 to 7.9.x | Yes | Yes | No | Yes | Yes | . | Elasticsearch OSS 7.10.2 | Yes | Yes | No | Yes | Yes | . | ODFE 1.0 to 1.12 | Yes | Yes | No | Yes | Yes | . | ODFE 1.13 | Yes | Yes | No | Yes | Yes | . | OpenSearch 1.x to 2.x | Yes via version setting | Yes via version setting | No | Yes | Yes, with Elastic Common Schema Setting | . * Most current compatible version with Elasticsearch OSS. Compatibility matrix for Beats . |   | Beats OSS 7.0.0 to 7.11.x** | Beats OSS 7.12.x* | Beats 7.13.x | . | Elasticsearch OSS 7.0.0 to 7.9.x | Yes | Yes | No | . | Elasticsearch OSS 7.10.2 | Yes | Yes | No | . | ODFE 1.0 to 1.12 | Yes | Yes | No | . | ODFE 1.13 | Yes | Yes | No | . | OpenSearch 1.x to 2.x | Yes via version setting | Yes via version setting | No | . | Logstash OSS 7.0.0 to 7.11.x | Yes | Yes | Yes | . | Logstash OSS 7.12.x* | Yes | Yes | Yes | . | Logstash 7.13.x with OpenSearch output plugin | Yes | Yes | Yes | . * Most current compatible version with Elasticsearch OSS. ** Beats OSS includes all Apache 2.0 Beats agents (that is, Filebeat, Metricbeat, Auditbeat, Heartbeat, Winlogbeat, and Packetbeat). Beats versions newer than 7.12.x are not supported by OpenSearch. If you must update the Beats agent(s) in your environment to a newer version, you can work around the incompatibility by directing traffic from Beats to Logstash and using the Logstash Output plugin to ingest the data to OpenSearch. ",
    "url": "https://vagimeli.github.io/tools/index/#compatibility-matrices",
    "relUrl": "/tools/index/#compatibility-matrices"
  },"1901": {
    "doc": "Tools",
    "title": "OpenSearch CLI",
    "content": "The OpenSearch CLI command line interface (opensearch-cli) lets you manage your OpenSearch cluster from the command line and automate tasks. For more information about OpenSearch CLI, see OpenSearch CLI. ",
    "url": "https://vagimeli.github.io/tools/index/#opensearch-cli",
    "relUrl": "/tools/index/#opensearch-cli"
  },"1902": {
    "doc": "Tools",
    "title": "OpenSearch Kubernetes operator",
    "content": "The OpenSearch Kubernetes (K8s) Operator is an open-source kubernetes operator that helps automate the deployment and provisioning of OpenSearch and OpenSearch Dashboards in a containerized environment. For information about how to use the K8s operator, see OpenSearch Kubernetes operator . ",
    "url": "https://vagimeli.github.io/tools/index/#opensearch-kubernetes-operator",
    "relUrl": "/tools/index/#opensearch-kubernetes-operator"
  },"1903": {
    "doc": "Tools",
    "title": "Tools",
    "content": " ",
    "url": "https://vagimeli.github.io/tools/index/",
    "relUrl": "/tools/index/"
  },"1904": {
    "doc": "OpenSearch Kubernetes Operator",
    "title": "Installation",
    "content": "There are two ways to get started with the operator: . | Use a Helm chart. | Use a local installation. | . Use a Helm chart . If you use Helm to manage your Kubernetes cluster, you can use the OpenSearch Kubernetes Operator’s Cloud Native Computing Foundation (CNCF) project stored in Artifact Hub, a web-based application for finding, installing, and publishing CNCF packages. To begin, log in to your Kubernetes cluster and add the Helm repository (repo) from Artifact Hub. helm repo add opensearch-operator https://opster.github.io/opensearch-k8s-operator/ . Make sure that the repo is included in your Kubernetes cluster. helm repo list | grep opensearch . Both the opensearch and opensearch-operator repos appear in the list of repos. Install the manager that operates all of the OpenSearch Kubernetes Operator’s actions. helm install opensearch-operator opensearch-operator/opensearch-operator . After the installation completes, the operator returns information on the deployment with STATUS: deployed. Then you can configure and start your OpenSearch cluster. Use a local installation . If you want to create a new Kubernetes cluster on your existing machine, use a local installation. If this is your first time running Kubernetes and you intend to run through these instructions on your laptop, make sure that you have the following installed: . | Kubernetes | Docker | minikube | . Before running through the installation steps, make sure that you have a Kubernetes environment running locally. When using minikube, open a new terminal window and enter minikube start. Kubernetes will now use a containerized minikube cluster with a namespace called default. Then install the OpenSearch Kubernetes Operator using the following steps: . | In your preferred directory, clone the OpenSearch Kubernetes Operator repo. Navigate into repo’s directory using cd. | Go to the opensearch-operator folder. | Enter make build manifests. | Start a Kubernetes cluster. When using minikube, open a new terminal window and enter minikube start. Kubernetes will now use a containerized minikube cluster with a namespace called default. Make sure that ~/.kube/config points to the cluster. | . apiVersion: v1 clusters: - cluster: certificate-authority: /Users/naarcha/.minikube/ca.crt extensions: - extension: last-update: Mon, 29 Aug 2022 10:11:47 CDT provider: minikube.sigs.k8s.io version: v1.26.1 name: cluster_info server: https://127.0.0.1:61661 name: minikube contexts: - context: cluster: minikube extensions: - extension: last-update: Mon, 29 Aug 2022 10:11:47 CDT provider: minikube.sigs.k8s.io version: v1.26.1 name: context_info namespace: default user: minikube name: minikube current-context: minikube kind: Config preferences: {} users: - name: minikube user: client-certificate: /Users/naarcha/.minikube/profiles/minikube/client.crt client-key: /Users/naarcha/.minikube/profiles/minikube/client.key . | Enter make install to create the CustomResourceDefinition that runs in your Kubernetes cluster. | Start the OpenSearch Kubernetes Operator. Enter make run. | . ",
    "url": "https://vagimeli.github.io/tools/k8s-operator/#installation",
    "relUrl": "/tools/k8s-operator/#installation"
  },"1905": {
    "doc": "OpenSearch Kubernetes Operator",
    "title": "Verify Kubernetes deployment",
    "content": "To ensure that Kubernetes recognizes the OpenSearch Kubernetes Operator as a namespace, enter k get ns | grep opensearch. Both opensearch and opensearch-operator-system should appear as Active. With the operator active, use k get pod -n opensearch-operator-system to make sure that the operator’s pods are running. NAME READY STATUS RESTARTS AGE opensearch-operator-controller-manager-&lt;pod-id&gt; 2/2 Running 0 25m . With the Kubernetes cluster running, you can now run OpenSearch inside the cluster. ",
    "url": "https://vagimeli.github.io/tools/k8s-operator/#verify-kubernetes-deployment",
    "relUrl": "/tools/k8s-operator/#verify-kubernetes-deployment"
  },"1906": {
    "doc": "OpenSearch Kubernetes Operator",
    "title": "Deploy a new OpenSearch cluster",
    "content": "From your cloned OpenSearch Kubernetes Operator repo, navigate to the opensearch-operator/examples directory. There you’ll find the opensearch-cluster.yaml file, which can be customized to the needs of your cluster, including the clusterName that acts as the namespace in which your new OpenSearch cluster will reside. With your cluster configured, run the kubectl apply command. kubectl apply -f opensearch-cluster.yaml . The operator creates several pods, including a bootstrap pod, three OpenSearch cluster pods, and one Dashboards pod. To connect to your cluster, use the port-forward command. kubectl port-forward svc/my-cluster-dashboards 5601 . Open http://localhost:5601 in your preferred browser and log in with the default demo credentials admin / admin. You can also run curl commands against the OpenSearch REST API by forwarding to port 9200. kubectl port-forward svc/my-cluster 9200 . In order to delete the OpenSearch cluster, delete the cluster resources. The following command deletes the cluster namespace and all its resources. kubectl delete -f opensearch-cluster.yaml . ",
    "url": "https://vagimeli.github.io/tools/k8s-operator/#deploy-a-new-opensearch-cluster",
    "relUrl": "/tools/k8s-operator/#deploy-a-new-opensearch-cluster"
  },"1907": {
    "doc": "OpenSearch Kubernetes Operator",
    "title": "Next steps",
    "content": "To learn more about how to customize your Kubernetes OpenSearch cluster, including data persistence, authentication methods, and scaling, see the OpenSearch Kubernetes Operator User Guide. If you want to contribute to the development of the OpenSearch Kubernetes Operator, see the repo design documents. ",
    "url": "https://vagimeli.github.io/tools/k8s-operator/#next-steps",
    "relUrl": "/tools/k8s-operator/#next-steps"
  },"1908": {
    "doc": "OpenSearch Kubernetes Operator",
    "title": "OpenSearch Kubernetes Operator",
    "content": "The OpenSearch Kubernetes Operator is an open-source kubernetes operator that helps automate the deployment and provisioning of OpenSearch and OpenSearch Dashboards in a containerized environment. The operator can manage multiple OpenSearch clusters that can be scaled up and down depending on your needs. ",
    "url": "https://vagimeli.github.io/tools/k8s-operator/",
    "relUrl": "/tools/k8s-operator/"
  },"1909": {
    "doc": "Advanced configurations",
    "title": "Advanced configurations",
    "content": "This section describes how to set up advanced configuration options, like referencing field values and conditional statements, for Logstash. ",
    "url": "https://vagimeli.github.io/tools/logstash/advanced-config/",
    "relUrl": "/tools/logstash/advanced-config/"
  },"1910": {
    "doc": "Advanced configurations",
    "title": "Referencing field values",
    "content": "To get access to a field, use the - field syntax. You can also surround the field name by square brackets - [field] which makes it more explicit that you’re referring to a field. For example, if you have the following event: . { \"request\": \"/products/view/123\", \"verb\": \"GET\", \"response\": 200, \"headers\": { \"request_path\" =&gt; \"/\" } } . To access the request field, use - request or - [request]. If you want to reference nested fields, use the square brackets syntax and specify the path to the field. With each level being enclosed within square brackets: - [headers][request_path]. You can reference fields using the sprintf format. This is also called string expansion. You need to add a % sign and then wrap the field reference within curly brackets. You need to reference field values when using conditional statements. For example, you can make the file name dynamic and contain the type of the processed events - either access or error. The type option is mainly used for conditionally applying filter plugins based on the type of events being processed. Let’s add a type option and specify a value of access. input { file { path =&gt; \"\" start_position =&gt; \"beginning\" type =&gt; \"access\" } http { type =&gt; \"access\" } } filter { mutate { remove_field =&gt; {\"host\"} } } output { stdout { codec =&gt; rubydebug } file { path =&gt; \"%{[type]}.log\" } } . Start Logstash and send an HTTP request. The processed event is output in the terminal. The event now includes a field named type. You’ll see the access.log file created within the Logstash directory. ",
    "url": "https://vagimeli.github.io/tools/logstash/advanced-config/#referencing-field-values",
    "relUrl": "/tools/logstash/advanced-config/#referencing-field-values"
  },"1911": {
    "doc": "Advanced configurations",
    "title": "Conditional statements",
    "content": "You can use conditional statements to control the flow of code execution based on some conditions. Syntax: . if EXPR { ... } else if EXPR { ... } else { ... } . EXPR is any valid Logstash syntax that evaluates to a boolean value. For example, you can check if an event type is set to access or error and perform some action based on that: . if [type] == \"access\" { ... } else if [type] == \"error\" { file { .. } } else { ... } . You can compare a field value to some arbitrary value: . if [headers][content_length] &gt;= 1000 { ... } . You can regex: . if [some_field =~ /[0-9]+/ { //some field only contains digits } . You can use arrays: . if [some_field] in [\"one\", \"two\", \"three\"] { some field is either \"one\", \"two\", or \"three\" } . You can use boolean operators: . if [type] == \"access\" or [type] == \"error\" { ... } . ",
    "url": "https://vagimeli.github.io/tools/logstash/advanced-config/#conditional-statements",
    "relUrl": "/tools/logstash/advanced-config/#conditional-statements"
  },"1912": {
    "doc": "Advanced configurations",
    "title": "Formatting dates",
    "content": "You can use the sprintf format or string expansion to format dates. For example, you might want the current date to be part of the filename. To format the date, add a plus sign in curly brackets followed by the date format - %{+yyyy-MM-dd}. file { path =&gt; \"%{[type]}_%{+yyyy_MM_dd}.log\" } . This is the date stored within the @timestamp fields, which is the time and date of the event. Send a request to the pipeline and verify that a filename is outputted that contains the events date. You can embed the date in other outputs as well, for example into the index name in OpenSearch. ",
    "url": "https://vagimeli.github.io/tools/logstash/advanced-config/#formatting-dates",
    "relUrl": "/tools/logstash/advanced-config/#formatting-dates"
  },"1913": {
    "doc": "Advanced configurations",
    "title": "Sending time information",
    "content": "You can set the time of events. Logstash already sets the time when the event is received by the input plugin within the @timestamp field. In some scenarios, you might need to use a different timestamp. For example, if you have an eCommerce store and you process the orders daily at midnight. When Logstash receives the events at midnight, it sets the timestamp to the current time. But you want it to be the time when the order is placed and not when Logstash received the event. Let’s change the event timestamp to the date the request is received by the web server. You can do this using a filter plugin named dates. The dates filter passes a date or datetime value from a field and uses the results as the event timestamp. Add the date plugin at the bottom of the filter block: . date { match =&gt; [ \"timestamp\", \"dd/MMM/yyyy:HH:mm:ss Z\" ] } . timestamp is the field that the grok pattern creates. Z is the timezone. i.e., UTC offsets. Start Logstash and send an HTTP request. You can see that the filename contains the date of the request instead of the present date. If the passing of the date fails, the filter plugin adds a tag named _datepassfailure to the text field. After you have set the @timestamp field to a new value, you don’t really need the other timestamp field anymore. You can remove it with the remove_field option. date { match =&gt; [ \"timestamp\", \"dd/MMM/yyyy:HH:mm:ss Z\" ] remove_field =&gt; [ \"timestamp\" ] } . ",
    "url": "https://vagimeli.github.io/tools/logstash/advanced-config/#sending-time-information",
    "relUrl": "/tools/logstash/advanced-config/#sending-time-information"
  },"1914": {
    "doc": "Advanced configurations",
    "title": "Parsing user agents",
    "content": "The user agent is the last part of a log entry that consists of the name of the browser, the browser version, and the OS of the device. Users might be using a wide range of browsers, devices, and OS’s. Doing this manually is hard. You can’t use grok patterns because the grok pattern only matches the usage in the string as whole and doesn’t figure out which browser the visitor used, for instance. Logstash ships with a file containing regular expressions for this purpose. This makes it really easy to extract user agent information, which you could send to OpenSearch and run aggregations on. To do this, add a source option that contains the name of the field. In this case, that’s the agent field. By default the user agent plugin, adds a number of fields at the top-level of the event. Since that can get pretty confusing, we can add an option named target with a value of ua, short for user agent. What this does is that it nests the fields within an object named ua, making things more organized. useragent { source =&gt; \"agent\" target =&gt; \"ua\" } . Start Logstash and send an HTTP request. You can see a field named ua with a number of keys including the browser name and version, the OS, and the device. You can use OpenSearch Dashboards to create a pie chart that shows how many visitors are using mobile devices and how many are desktop users. Or, you could get statistics on which browser versions are popular. ",
    "url": "https://vagimeli.github.io/tools/logstash/advanced-config/#parsing-user-agents",
    "relUrl": "/tools/logstash/advanced-config/#parsing-user-agents"
  },"1915": {
    "doc": "Advanced configurations",
    "title": "Enriching geographical data",
    "content": "You can take an IP address and perform geographical lookup to resolve the geographical location of the user using the geoip filter. The geoip filter plugin ships with a database called geolite 2, which is provided by a company named MaxMind. geolite 2 is a popular source of geographical data and it’s available for free. Add the geoip plugin at the bottom of the else block. The value of the source option is the name of the field containing the IP address, in this case that’s clientip. You can make this field available using the grok pattern. geoip { source =&gt; \"clientip\" } . Start Logstash and send an HTTP request. Within the terminal, you see a new field named geoip that contains information such as the time zone, country, continent, city, postal code, and the latitude / longitude pair. If you only need the country name for instance, include an option named fields with an array of the field names that you want the geoip plugin to return. Some of the fields, such as city name and region, are not always available because translating IP addresses into geographical locations is generally not that accurate. If the geoip plugin fails to look up the geographical location, it adds a tag named geoip_lookup_failure. You can use the geoip plugin with the OpenSearch output because location object within the geoip object, is a standard format for representing geospatial data in JSON. This is the same format as OpenSearch uses for its geo_point data type. You can use the powerful geospatial queries of OpenSearch for working with geographical data. ",
    "url": "https://vagimeli.github.io/tools/logstash/advanced-config/#enriching-geographical-data",
    "relUrl": "/tools/logstash/advanced-config/#enriching-geographical-data"
  },"1916": {
    "doc": "Common filter plugins",
    "title": "Common filter plugins",
    "content": "This page contains a list of common filter plugins. ",
    "url": "https://vagimeli.github.io/tools/logstash/common-filters/",
    "relUrl": "/tools/logstash/common-filters/"
  },"1917": {
    "doc": "Common filter plugins",
    "title": "mutate",
    "content": "You can use the mutate filter to change the data type of a field. For example, you can use the mutate filter if you’re sending events to OpenSearch and you need to change the data type of a field to match any existing mappings. To convert the quantity field from a string type to an integer type: . input { http { host =&gt; \"127.0.0.1\" port =&gt; 8080 } } filter { mutate { convert =&gt; {\"quantity\" =&gt; \"integer\"} } } output { file { path =&gt; \"output.txt\" } } . Sample output . You can see that the type of the quantity field is changed from a string to an integer. { \"quantity\" =&gt; 3, \"host\" =&gt; \"127.0.0.1\", \"@timestamp\" =&gt; 2021-05-23T19:02:08.026Z, \"amount\" =&gt; 10, \"@version\" =&gt; \"1\", \"headers\" =&gt; { \"request_path\" =&gt; \"/\", \"connection\" =&gt; \"keep-alive\", \"content_length\" =&gt; \"41\", \"http_user_agent\" =&gt; \"PostmanRuntime/7.26.8\", \"request_method\" =&gt; \"PUT\", \"cache_control\" =&gt; \"no-cache\", \"http_accept\" =&gt; \"*/*\", \"content_type\" =&gt; \"application/json\", \"http_version\" =&gt; \"HTTP/1.1\", \"http_host\" =&gt; \"127.0.0.1:8080\", \"accept_encoding\" =&gt; \"gzip, deflate, br\", \"postman_token\" =&gt; \"ffd1cdcb-7a1d-4d63-90f8-0f2773069205\" } } . Other data types you can convert to are float, string, and boolean values. If you pass in an array, the mutate filter converts all the elements in the array. If you pass a string like “world” to cast to an integer type, the result is 0 and Logstash continues processing events. Logstash supports a few common options for all filter plugins: . | Option | Description | . | add_field | Adds one or more fields to the event. | . | remove_field | Removes one or more events from the field. | . | add_tag | Adds one or more tags to the event. You can use tags to perform conditional processing on events depending on which tags they contain. | . | remove_tag | Removes one or more tags from the event. | . For example, you can remove the host field from the event: . input { http { host =&gt; \"127.0.0.1\" port =&gt; 8080 } } filter { mutate { remove_field =&gt; {\"host\"} } } output { file { path =&gt; \"output.txt\" } } . ",
    "url": "https://vagimeli.github.io/tools/logstash/common-filters/#mutate",
    "relUrl": "/tools/logstash/common-filters/#mutate"
  },"1918": {
    "doc": "Common filter plugins",
    "title": "grok",
    "content": "With the grok filter, you can parse unstructured data and and structure it into fields. The grok filter uses text patterns to match text in your logs. You can think of text patterns as variables containing regular expressions. The format of a text pattern is as follows: . %{SYNTAX:SEMANTIC} . SYNTAX is the format a piece of text should be in for the pattern to match. You can enter any of grok’s predefined patterns. For example, you can use the email identifier to match an email address from a given piece of text. SEMANTIC is an arbitrary name for the matched text. For example, if you’re using the email identifier syntax, you can name it “email.” . The following request consists of the IP address of the visitor, name of the visitor, the timestamp of the request, the HTTP verb and URL, the HTTP status code, and the number of bytes: . 184.252.108.229 - joe [20/Sep/2017:13:22:22 +0200] GET /products/view/123 200 12798 . To split this request into different fields: . filter { grok { match =&gt; { \"message\" =&gt; \" %{IP: ip_address} %{USER:identity} %{USER:auth} \\[%{HTTPDATE:reg_ts}\\] \\\"%{WORD:http_verb} %{URIPATHPARAM: req_path} \\\" %{INT:http_status:int} %{INT:num_bytes:int}\"} } } . where: . | IP: matches the IP address field. | USER: matches the user name. | WORD: matches the HTTP verb. | URIPATHPARAM: matches the URI path. | INT: matches the HTTP status field. | INT: matches the number of bytes. | . This is what the event looks like after the grok filter breaks it down into individual fields: . ip_address: 184.252.108.229 identity: joe reg_ts: 20/Sep/2017:13:22:22 +0200 http_verb:GET req_path: /products/view/123 http_status: 200 num_bytes: 12798 . For common log formats, you use the predefined patterns defined here⁠—Logstash patterns. You can make any adjustments to the results with the mutate filter. ",
    "url": "https://vagimeli.github.io/tools/logstash/common-filters/#grok",
    "relUrl": "/tools/logstash/common-filters/#grok"
  },"1919": {
    "doc": "Logstash execution model",
    "title": "Logstash execution model",
    "content": "Here’s a brief introduction to how Logstash processes events internally. ",
    "url": "https://vagimeli.github.io/tools/logstash/execution-model/",
    "relUrl": "/tools/logstash/execution-model/"
  },"1920": {
    "doc": "Logstash execution model",
    "title": "Handling events concurrently",
    "content": "You can configure Logstash to have a number of inputs listening for events. Each input runs in its own thread to avoid inputs blocking each other. If you have two incoming events at the same time, Logstash handles both events concurrently. After receiving an event and possibly applying an input codec, Logstash sends the event to a work queue. Pipeline workers or batchers perform the rest of the work involving filters and outputs along with any codec used at the output. Each pipeline worker also runs within its own thread meaning that Logstash processes multiple events simultaneously. ",
    "url": "https://vagimeli.github.io/tools/logstash/execution-model/#handling-events-concurrently",
    "relUrl": "/tools/logstash/execution-model/#handling-events-concurrently"
  },"1921": {
    "doc": "Logstash execution model",
    "title": "Processing events in batches",
    "content": "A pipeline worker consumes events from the work queue in batches to optimize the throughput of the pipeline as a whole. One reason why Logstash works in batches is that some code needs to be executed regardless of how many events are processed at a time within the pipeline worker. Instead of executing that code 100 times for 100 events, it’s more efficient to execute it once for a batch of 100 events. Another reason is that a few output plugins group together events as batches. For example, if you send 100 requests to OpenSearch, the OpenSearch output plugin uses the bulk API to send a single request that groups together the 100 requests. Logstash determines the batch size by two configuration options⁠—a number representing the maximum batch size and the batch delay. The batch delay is how long Logstash waits before processing the unprocessed batch of events. If you set the maximum batch size to 50 and the batch delay to 100 ms, Logstash processes a batch if they’re either 50 unprocessed events in the work queue or if one hundred milliseconds have elapsed. The reason that a batch is processed, even if the maximum batch size isn’t reached, is to reduce the delay in processing and to continue to process events in a timely manner. This works well for pipelines that process a low volume of events. Imagine that you’ve a pipeline that processes error logs from web servers and pushes them to OpenSearch. You’re using OpenSearch Dashboards to analyze the error logs. Because you’re possibly dealing with a fairly low number of events, it might take a long time to reach 50 events. Logstash processes the events before reaching this threshold because otherwise there would be a long delay before we see the errors appear in OpenSearch Dashboards. The default batch size and batch delay work for most cases. You don’t need to change the default values unless you need to minutely optimize the performance. ",
    "url": "https://vagimeli.github.io/tools/logstash/execution-model/#processing-events-in-batches",
    "relUrl": "/tools/logstash/execution-model/#processing-events-in-batches"
  },"1922": {
    "doc": "Logstash execution model",
    "title": "Optimizing based on CPU cores",
    "content": "The number of pipeline workers are proportional to the number of CPU cores on the nodes. If you have 5 workers running on a server with 2 CPU cores, the 5 workers won’t be able to process events concurrently. On the other hand, running 5 workers on a server running 10 CPU cores limits the throughput of a Logstash instance. Instead of running a fixed number of workers, which results in poor performance in some cases, Logstash examines the number of CPU cores of the instance and selects the number of pipeline workers to optimize its performance for the platform on which its running. For instance, your local development machine might not have the same processing power as a production server. So you don’t need to manually configure Logstash for different machines. ",
    "url": "https://vagimeli.github.io/tools/logstash/execution-model/#optimizing-based-on-cpu-cores",
    "relUrl": "/tools/logstash/execution-model/#optimizing-based-on-cpu-cores"
  },"1923": {
    "doc": "Logstash",
    "title": "Logstash",
    "content": "Logstash is a real-time event processing engine. It’s part of the OpenSearch stack which includes OpenSearch, Beats, and OpenSearch Dashboards. You can send events to Logstash from many different sources. Logstash processes the events and sends it one or more destinations. For example, you can send access logs from a web server to Logstash. Logstash extracts useful information from each log and sends it to a destination like OpenSearch. Sending events to Logstash lets you decouple event processing from your app. Your app only needs to send events to Logstash and doesn’t need to know anything about what happens to the events afterwards. The open-source community originally built Logstash for processing log data but now you can process any type of events, including events in XML or JSON format. ",
    "url": "https://vagimeli.github.io/tools/logstash/index/",
    "relUrl": "/tools/logstash/index/"
  },"1924": {
    "doc": "Logstash",
    "title": "Structure of a pipeline",
    "content": "The way that Logstash works is that you configure a pipeline that has three phases⁠—inputs, filters, and outputs. Each phase uses one or more plugins. Logstash has over 200 built-in plugins so chances are that you’ll find what you need. Apart from the built-in plugins, you can use plugins from the community or even write your own. The structure of a pipeline is as follows: . input { input_plugin =&gt; {} } filter { filter_plugin =&gt; {} } output { output_plugin =&gt; {} } . where: . | input receives events like logs from multiple sources simultaneously. Logstash supports a number of input plugins for TCP/UDP, files, syslog, Microsoft Windows EventLogs, stdin, HTTP, and so on. You can also use an open source collection of input tools called Beats to gather events. The input plugin sends the events to a filter. | filter parses and enriches the events in one way or the other. Logstash has a large collection of filter plugins that modify events and pass them on to an output. For example, a grok filter parses unstructured events into fields and a mutate filter changes fields. Filters are executed sequentially. | output ships the filtered events to one or more destinations. Logstash supports a wide range of output plugins for destinations like OpenSearch, TCP/UDP, emails, files, stdout, HTTP, Nagios, and so on. | . Both the input and output phases support codecs to process events as they enter or exit the pipeline. Some of the popular codecs are json and multiline. The json codec processes data that’s in JSON format and the multiline codec merges multiple line events into a single line. You can also write conditional statements within pipeline configurations to perform certain actions, if a certain criteria is met. ",
    "url": "https://vagimeli.github.io/tools/logstash/index/#structure-of-a-pipeline",
    "relUrl": "/tools/logstash/index/#structure-of-a-pipeline"
  },"1925": {
    "doc": "Logstash",
    "title": "Install Logstash",
    "content": "The OpenSearch Logstash plugin has two installation options at this time: Linux (ARM64/X64) and Docker (ARM64/X64). Make sure you have Java Development Kit (JDK) version 8 or 11 installed. If you’re migrating from an existing Logstash installation, you can install the OpenSearch output plugin manually and update pipeline.conf. We include this plugin by default in our tarball and Docker downloads. Tarball . | Download the Logstash tarball from OpenSearch downloads. | Navigate to the downloaded folder in the terminal and extract the files: . tar -zxvf logstash-oss-with-opensearch-output-plugin-7.16.2-linux-x64.tar.gz . | Navigate to the logstash-7.16.2 directory. | You can add your pipeline configurations to the config directory. Logstash saves any data from the plugins in the data directory. The bin directory contains the binaries for starting Logstash and managing plugins. | . | . Docker . | Pull the Logstash oss package with the OpenSearch output plugin image: . docker pull opensearchproject/logstash-oss-with-opensearch-output-plugin:7.16.2 . | Create a Docker network: . docker network create test . | Start OpenSearch with this network: . docker run -p 9200:9200 -p 9600:9600 --name opensearch --net test -e \"discovery.type=single-node\" opensearchproject/opensearch:1.2.0 . | Start Logstash: . docker run -it --rm --name logstash --net test opensearchproject/logstash-oss-with-opensearch-output-plugin:7.16.2 -e 'input { stdin { } } output { opensearch { hosts =&gt; [\"https://opensearch:9200\"] index =&gt; \"opensearch-logstash-docker-%{+YYYY.MM.dd}\" user =&gt; \"admin\" password =&gt; \"admin\" ssl =&gt; true ssl_certificate_verification =&gt; false } }' . | . ",
    "url": "https://vagimeli.github.io/tools/logstash/index/#install-logstash",
    "relUrl": "/tools/logstash/index/#install-logstash"
  },"1926": {
    "doc": "Logstash",
    "title": "Process text from the terminal",
    "content": "You can define a pipeline that listens for events on stdin and outputs events on stdout. stdin and stdout refer to the terminal in which you’re running Logstash. To enter some text in the terminal and see the event data in the output: . | Use the -e argument to pass a pipeline configuration directly to the Logstash binary. In this case, stdin is the input plugin and stdout is the output plugin: . bin/logstash -e \"input { stdin { } } output { stdout { } }\" . Add the —debug flag to see a more detailed output. | Enter “hello world” in your terminal. Logstash processes the text and outputs it back to the terminal: . { \"message\" =&gt; \"hello world\", \"host\" =&gt; \"a483e711a548.ant.amazon.com\", \"@timestamp\" =&gt; 2021-05-30T05:15:56.816Z, \"@version\" =&gt; \"1\" } . The message field contains your raw input. The host field is an IP address when you don’t run Logstash locally. @timestamp shows the date and time for when the event is processed. Logstash uses the @version field for internal processing. | Press Ctrl + C to shut down Logstash. | . Troubleshooting . If you already have a Logstash process running, you’ll get an error. To fix this issue: . | Delete the .lock file from the data directory: . cd data rm -rf .lock . | Restart Logstash. | . ",
    "url": "https://vagimeli.github.io/tools/logstash/index/#process-text-from-the-terminal",
    "relUrl": "/tools/logstash/index/#process-text-from-the-terminal"
  },"1927": {
    "doc": "Logstash",
    "title": "Process JSON or HTTP input and output it to a file",
    "content": "To define a pipeline that handles JSON requests: . | Open the config/pipeline.conf file in any text editor you like. You can create a pipeline configuration file with any extension, the .conf extension is a Logstash convention. Add the json codec to accept JSON as the input and the file plugin to output the processed events to a .txt file: . input { stdin { codec =&gt; json } } output { file { path =&gt; \"output.txt\" } } . To process inputs from a file, add an input file to the events-data directory and then pass its path to the file plugin at the input: . input { file { path =&gt; \"events-data/input_data.log\" } } . | Start Logstash: . $ bin/logstash -f config/pipeline.conf . config/pipeline.conf is a relative path to the pipeline.conf file. You can use an absolute path as well. | Add a JSON object in the terminal: . { \"amount\": 10, \"quantity\": 2} . The pipeline only handles a single line of input. If you paste some JSON that spans multiple lines, you’ll get an error. | Check that the fields from the JSON object are added to the output.txt file: . $ cat output.txt { \"@version\": \"1\", \"@timestamp\": \"2021-05-30T05:52:52.421Z\", \"host\": \"a483e711a548.ant.amazon.com\", \"amount\": 10, \"quantity\": 2 } . If you type in some invalid JSON as the input, you’ll see a JSON parsing error. Logstash doesn’t discard the invalid JSON because you still might want to do something with it. For example, you can trigger an email or send a notification to a Slack channel. | . To define a pipeline that handles HTTP requests: . | Use the http plugin to send events to Logstash through HTTP: . input { http { host =&gt; \"127.0.0.1\" port =&gt; 8080 } } output { file { path =&gt; \"output.txt\" } } . If you don’t specify any options, the http plugin binds to localhost and listens on port 8080. | Start Logstash: . $ bin/logstash -f config/pipeline.conf . | Use Postman to send an HTTP request. Set Content-Type to an HTTP header with a value of application/json: . PUT 127.0.0.1:8080 { \"amount\": 10, \"quantity\": 2 } . Or, you can use the curl command: . curl -XPUT -H \"Content-Type: application/json\" -d ' {\"amount\": 7, \"quantity\": 3 }' http://localhost:8080 (http://localhost:8080/) . Even though we haven’t added the json plugin to the input, the pipeline configuration still works because the HTTP plugin automatically applies the appropriate codec based on the Content-Type header. If you specify a value of applications/json, Logstash parses the request body as JSON. The headers field contains the HTTP headers that Logstash receives: . { \"host\": \"127.0.0.1\", \"quantity\": \"3\", \"amount\": 10, \"@timestamp\": \"2021-05-30T06:05:48.135Z\", \"headers\": { \"http_version\": \"HTTP/1.1\", \"request_method\": \"PUT\", \"http_user_agent\": \"PostmanRuntime/7.26.8\", \"connection\": \"keep-alive\", \"postman_token\": \"c6cd29cf-1b37-4420-8db3-9faec66b9e7e\", \"http_host\": \"127.0.0.1:8080\", \"cache_control\": \"no-cache\", \"request_path\": \"/\", \"content_type\": \"application/json\", \"http_accept\": \"*/*\", \"content_length\": \"41\", \"accept_encoding\": \"gzip, deflate, br\" }, \"@version\": \"1\" } . | . ",
    "url": "https://vagimeli.github.io/tools/logstash/index/#process-json-or-http-input-and-output-it-to-a-file",
    "relUrl": "/tools/logstash/index/#process-json-or-http-input-and-output-it-to-a-file"
  },"1928": {
    "doc": "Logstash",
    "title": "Automatically reload the pipeline configuration",
    "content": "You can configure Logstash to detect any changes to the pipeline configuration file or the input log file and automatically reload the configuration. The stdin plugin doesn’t supporting automatic reloading. | Add an option named start_position with a value of beginning to the input plugin: . input { file { path =&gt; \"/Users/&lt;user&gt;/Desktop/logstash7-12.1/events-data/input_file.log\" start_position =&gt; \"beginning\" } } . Logstash only processes any new events added to the input file and ignores the ones that it has already processed to avoid processing the same event more than once on restart. Logstash records its progress in a file that’s referred to as a sinceDB file. Logstash creates a sinceDB file for each file that it watches for changes. | Open the sinceDB file to check how much of the input files are processed: . cd data/plugins/inputs/file/ ls -al -rw-r--r-- 1 user staff 0 Jun 13 10:50 .sincedb_9e484f2a9e6c0d1bdfe6f23ac107ffc5 cat .sincedb_9e484f2a9e6c0d1bdfe6f23ac107ffc5 51575938 1 4 7727 . The last number in the sinceDB file (7727) is the byte offset of the last known event processed. | To process the input file from the beginning, delete the sinceDB file: . rm .sincedb_* . | Start Logstash with a —-config.reload.automatic argument: . bin/logstash -f config/pipeline.conf --config.reload.automatic . The reload option only reloads if you add a new line at the end of the pipeline configuration file. Sample output: . { \"message\" =&gt; \"216.243.171.38 - - [20/Sep/2017:19:11:52 +0200] \\\"GET /products/view/123 HTTP/1.1\\\" 200 12798 \\\"https://codingexplained.com/products\\\" \\\"Mozilla/5.0 (compatible; YandexBot/3.0; +http://yandex.com/bots)\\\"\", \"@version\" =&gt; \"1\", \"host\" =&gt; \"a483e711a548.ant.amazon.com\", \"path\" =&gt; \"/Users/kumarjao/Desktop/odfe1/logstash-7.12.1/events-data/input_file.log\", \"@timestamp\" =&gt; 2021-06-13T18:03:30.423Z } { \"message\" =&gt; \"91.59.108.75 - - [20/Sep/2017:20:11:43 +0200] \\\"GET /js/main.js HTTP/1.1\\\" 200 588 \\\"https://codingexplained.com/products/view/863\\\" \\\"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:45.0) Gecko/20100101 Firefox/45.0\\\"\", \"@version\" =&gt; \"1\", \"host\" =&gt; \"a483e711a548.ant.amazon.com\", \"path\" =&gt; \"/Users/kumarjao/Desktop/odfe1/logstash-7.12.1/events-data/input_file.log\", \"@timestamp\" =&gt; 2021-06-13T18:03:30.424Z } . | Add a new line to the input file. | Logstash immediately detects the change and processes the new line as an event. | . | Make a change to the pipeline.conf file. | Logstash immediately detects the change and reloads the modified pipeline. | . | . ",
    "url": "https://vagimeli.github.io/tools/logstash/index/#automatically-reload-the-pipeline-configuration",
    "relUrl": "/tools/logstash/index/#automatically-reload-the-pipeline-configuration"
  },"1929": {
    "doc": "Read from OpenSearch",
    "title": "Read from OpenSearch",
    "content": "As we ship Logstash events to an OpenSearch cluster using the OpenSearch output plugin, we can also perform read operations on an OpenSearch cluster and load data into Logstash using the OpenSearch input plugin. The OpenSearch input plugin reads the search query results performed on an OpenSearch cluster and loads them into Logstash. This lets you replay test logs, reindex, and perform other operations based on the loaded data. You can schedule ingestions to run periodically by using cron expressions, or manually load data into Logstash by running the query once. ",
    "url": "https://vagimeli.github.io/tools/logstash/read-from-opensearch/",
    "relUrl": "/tools/logstash/read-from-opensearch/"
  },"1930": {
    "doc": "Read from OpenSearch",
    "title": "OpenSearch input plugin",
    "content": "To run the OpenSearch input plugin, add the configuration to the pipeline.conf file within your Logstash’s config folder. The example below runs the match_all query filter and loads in data once. input { opensearch { hosts =&gt; \"https://hostname:port\" user =&gt; \"admin\" password =&gt; \"admin\" index =&gt; \"logstash-logs-%{+YYYY.MM.dd}\" query =&gt; \"{ \"query\": { \"match_all\": {}} }\" } } filter { } output { } . To ingest data according to a schedule, use a cron expression that specifies the schedule you want. For example, to load in data every minute, add schedule =&gt; \"* * * * *\" to the input section of your pipeline.conf file. Like the output plugin, after adding your configuration to the pipeline.conf file, start Logstash by providing the path to this file: . $ bin/logstash -f config/pipeline.conf --config.reload.automatic . config/pipeline.conf is a relative path to the pipeline.conf file. You can use an absolute path as well. Adding stdout{} to the output{} section of your pipeline.conf file prints the query results to the console. To reindex the data into an OpenSearch domain, add the destination domain configuration in the output{} section like shown here. ",
    "url": "https://vagimeli.github.io/tools/logstash/read-from-opensearch/#opensearch-input-plugin",
    "relUrl": "/tools/logstash/read-from-opensearch/#opensearch-input-plugin"
  },"1931": {
    "doc": "Ship events to OpenSearch",
    "title": "Ship events to OpenSearch",
    "content": "You can Ship Logstash events to an OpenSearch cluster and then visualize your events with OpenSearch Dashboards. Make sure you have Logstash, OpenSearch, and OpenSearch Dashboards. ",
    "url": "https://vagimeli.github.io/tools/logstash/ship-to-opensearch/",
    "relUrl": "/tools/logstash/ship-to-opensearch/"
  },"1932": {
    "doc": "Ship events to OpenSearch",
    "title": "OpenSearch output plugin",
    "content": "To run the OpenSearch output plugin, add the following configuration in your pipeline.conf file: . output { opensearch { hosts =&gt; \"https://localhost:9200\" user =&gt; \"admin\" password =&gt; \"admin\" index =&gt; \"logstash-logs-%{+YYYY.MM.dd}\" ssl_certificate_verification =&gt; false } } . ",
    "url": "https://vagimeli.github.io/tools/logstash/ship-to-opensearch/#opensearch-output-plugin",
    "relUrl": "/tools/logstash/ship-to-opensearch/#opensearch-output-plugin"
  },"1933": {
    "doc": "Ship events to OpenSearch",
    "title": "Sample walkthrough",
    "content": ". | Open the config/pipeline.conf file and add in the following configuration: . input { stdin { codec =&gt; json } } output { opensearch { hosts =&gt; \"https://localhost:9200\" user =&gt; \"admin\" password =&gt; \"admin\" index =&gt; \"logstash-logs-%{+YYYY.MM.dd}\" ssl_certificate_verification =&gt; false } } . This Logstash pipeline accepts JSON input through the terminal and ships the events to an OpenSearch cluster running locally. Logstash writes the events to an index with the logstash-logs-%{+YYYY.MM.dd} naming convention. | Start Logstash: . $ bin/logstash -f config/pipeline.conf --config.reload.automatic . config/pipeline.conf is a relative path to the pipeline.conf file. You can use an absolute path as well. | Add a JSON object in the terminal: . { \"amount\": 10, \"quantity\": 2} . | Start OpenSearch Dashboards and choose Dev Tools: . GET _cat/indices?v health | status | index | uuid | pri | rep | docs.count | docs.deleted | store.size | pri.store.size green | open | logstash-logs-2021.07.01 | iuh648LYSnmQrkGf70pplA | 1 | 1 | 1 | 0 | 10.3kb | 5.1kb . | . ",
    "url": "https://vagimeli.github.io/tools/logstash/ship-to-opensearch/#sample-walkthrough",
    "relUrl": "/tools/logstash/ship-to-opensearch/#sample-walkthrough"
  },"1934": {
    "doc": "Ship events to OpenSearch",
    "title": "Adding different Authentication mechanisms in the Output plugin",
    "content": " ",
    "url": "https://vagimeli.github.io/tools/logstash/ship-to-opensearch/#adding-different-authentication-mechanisms-in-the-output-plugin",
    "relUrl": "/tools/logstash/ship-to-opensearch/#adding-different-authentication-mechanisms-in-the-output-plugin"
  },"1935": {
    "doc": "Ship events to OpenSearch",
    "title": "auth_type to support different authentication mechanisms",
    "content": "In addition to the existing authentication mechanisms, if we want to add new authentication then we will be adding them in the configuration by using auth_type . Example Configuration for basic authentication: . output { opensearch { hosts =&gt; [\"https://hostname:port\"] auth_type =&gt; { type =&gt; 'basic' user =&gt; 'admin' password =&gt; 'admin' } index =&gt; \"logstash-logs-%{+YYYY.MM.dd}\" } } . Parameters inside auth_type . | type (string) - We should specify the type of authentication | We should add credentials required for that authentication like ‘user’ and ‘password’ for ‘basic’ authentication | We should also add other parameters required for that authentication mechanism like we added ‘region’ for ‘aws_iam’ authentication | . ",
    "url": "https://vagimeli.github.io/tools/logstash/ship-to-opensearch/#auth_type-to-support-different-authentication-mechanisms",
    "relUrl": "/tools/logstash/ship-to-opensearch/#auth_type-to-support-different-authentication-mechanisms"
  },"1936": {
    "doc": "Ship events to OpenSearch",
    "title": "Configuration for AWS IAM Authentication",
    "content": "To run the Logstash Output Opensearch plugin using aws_iam authentication, simply add a configuration following the below documentation. Example Configuration: . output { opensearch { hosts =&gt; [\"https://hostname:port\"] auth_type =&gt; { type =&gt; 'aws_iam' aws_access_key_id =&gt; 'ACCESS_KEY' aws_secret_access_key =&gt; 'SECRET_KEY' region =&gt; 'us-west-2' service_name =&gt; 'es' } index =&gt; \"logstash-logs-%{+YYYY.MM.dd}\" } } . Required Parameters . | hosts (array of string) - AmazonOpensearchService domain endpoint : port number | auth_type (Json object) - Which holds other parameters required for authentication . | type (string) - “aws_iam” | aws_access_key_id (string) - AWS access key | aws_secret_access_key (string) - AWS secret access key | region (string, :default =&gt; “us-east-1”) - region in which the domain is located | if we want to pass other optional parameters like profile, session_token,etc. They needs to be added in auth_type | . | port (string) - AmazonOpensearchService listens on port 443 for HTTPS | protocol (string) - The protocol used to connect to AmazonOpensearchService is ‘https’ | . Optional Parameters . | The credential resolution logic can be described as follows: . | User passed aws_access_key_id and aws_secret_access_key in configuration | Environment variables - AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY (RECOMMENDED since they are recognized by all the AWS SDKs and CLI except for .NET), or AWS_ACCESS_KEY and AWS_SECRET_KEY (only recognized by Java SDK) | Credential profiles file at the default location (~/.aws/credentials) shared by all AWS SDKs and the AWS CLI | Instance profile credentials delivered through the Amazon EC2 metadata service | . | template (path) - You can set the path to your own template here. If no template is specified, the plugin uses the default template. | template_name (string, default =&gt; “logstash”) - Defines how the template is named inside Opensearch | service_name (string, default =&gt; “es”) - Defines the service name to be used for aws_iam authentication. | legacy_template (boolean, default =&gt; true) - Selects the OpenSearch template API. When true, uses legacy templates via the _template API. When false, uses composable templates via the _index_template API. | default_server_major_version (number) - The OpenSearch server major version to use when it’s not available from the OpenSearch root URL. If not set, the plugin throws an exception when the version can’t be fetched. | . ",
    "url": "https://vagimeli.github.io/tools/logstash/ship-to-opensearch/#configuration-for-aws-iam-authentication",
    "relUrl": "/tools/logstash/ship-to-opensearch/#configuration-for-aws-iam-authentication"
  },"1937": {
    "doc": "Ship events to OpenSearch",
    "title": "Data streams",
    "content": "The OpenSearch output plugin can store both time series datasets (such as logs, events, and metrics) and non-time series data in OpenSearch. The data stream is recommended to index time series datasets (such as logs, metrics, and events) into OpenSearch. To know more about data streams, refer to this documentation. We can ingest data into a data stream through logstash. We need to create the data stream and specify the name of data stream and the op_type of create in the output configuration. The sample configuration is shown below: . output { opensearch { hosts =&gt; [\"https://hostname:port\"] auth_type =&gt; { type =&gt; 'basic' user =&gt; 'admin' password =&gt; 'admin' } index =&gt; \"my-data-stream\" action =&gt; \"create\" } } . ",
    "url": "https://vagimeli.github.io/tools/logstash/ship-to-opensearch/#data-streams",
    "relUrl": "/tools/logstash/ship-to-opensearch/#data-streams"
  },"1938": {
    "doc": "Terraform",
    "title": "Terraform provider",
    "content": "You can use the Terraform OpenSearch provider to provision OpenSearch resources and interact with the OpenSearch API. The Terraform provider supports Amazon OpenSearch Service domains and OpenSearch clusters deployed on Kubernetes or another infrastructure. For more information, see the Terraform provider documentation. ",
    "url": "https://vagimeli.github.io/tools/terraform/#terraform-provider",
    "relUrl": "/tools/terraform/#terraform-provider"
  },"1939": {
    "doc": "Terraform",
    "title": "Terraform",
    "content": " ",
    "url": "https://vagimeli.github.io/tools/terraform/",
    "relUrl": "/tools/terraform/"
  },"1940": {
    "doc": "Alias",
    "title": "Alias",
    "content": "Introduced 1.0 . An alias is a virtual pointer that you can use to reference one or more indexes. Creating and updating aliases are atomic operations, so you can reindex your data and point an alias at it without any downtime. ",
    "url": "https://vagimeli.github.io/api-reference/alias/",
    "relUrl": "/api-reference/alias/"
  },"1941": {
    "doc": "Alias",
    "title": "Example",
    "content": "POST _aliases { \"actions\": [ { \"add\": { \"index\": \"movies\", \"alias\": \"movies-alias1\" } }, { \"remove\": { \"index\": \"old-index\", \"alias\": \"old-index-alias\" } } ] } . copy . ",
    "url": "https://vagimeli.github.io/api-reference/alias/#example",
    "relUrl": "/api-reference/alias/#example"
  },"1942": {
    "doc": "Alias",
    "title": "Path and HTTP methods",
    "content": "POST _aliases . ",
    "url": "https://vagimeli.github.io/api-reference/alias/#path-and-http-methods",
    "relUrl": "/api-reference/alias/#path-and-http-methods"
  },"1943": {
    "doc": "Alias",
    "title": "URL parameters",
    "content": "All alias parameters are optional. | Parameter | Data Type | Description | . | master_timeout | Time | The amount of time to wait for a response from the master node. Default is 30s. | . | timeout | Time | The amount of time to wait for a response from the cluster. Default is 30s. | . ",
    "url": "https://vagimeli.github.io/api-reference/alias/#url-parameters",
    "relUrl": "/api-reference/alias/#url-parameters"
  },"1944": {
    "doc": "Alias",
    "title": "Request body",
    "content": "In your request body, you need to specify what action to take, the alias name, and the index you want to associate with the alias. Other fields are optional. | Field | Data Type | Description | Required | . | actions | Array | Set of actions you want to perform on the index. Valid options are: add, remove, and remove_index. You must have at least one action in the array. | Yes | . | add | N/A | Adds an alias to the specified index. | No | . | remove | N/A | Removes an alias from the specified index. | No | . | remove_index | N/A | Deletes an index. | No | . | index | String | Name of the index you want to associate with the alias. Supports wildcard expressions. | Yes if you don’t supply an indices field in the body. | . | indices | Array | Array of index names you want to associate with the alias. | Yes if you don’t supply an index field in the body. | . | alias | String | The name of the alias. | Yes if you don’t supply an aliases field in the body. | . | aliases | Array | Array of alias names. | Yes if you don’t supply an alias field in the body. | . | filter | Object | A filter to use with the alias, so the alias points to a filtered part of the index. | No | . | is_hidden | Boolean | Specifies whether the alias should be hidden from results that include wildcard expressions | No | . | must_exist | Boolean | Specifies whether the alias to remove must exist. | No | . | is_write_index | Boolean | Specifies whether the index should be a write index. An alias can only have one write index at a time. If a write request is submitted to a alias that links to multiple indexes, OpenSearch executes the request only on the write index. | No | . | routing | String | Used to assign a custom value to a shard for specific operations. | No | . | index_routing | String | Assigns a custom value to a shard only for index operations. | No | . | search_routing | String | Assigns a custom value to a shard only for search operations. | No | . ",
    "url": "https://vagimeli.github.io/api-reference/alias/#request-body",
    "relUrl": "/api-reference/alias/#request-body"
  },"1945": {
    "doc": "Alias",
    "title": "Response",
    "content": "{ \"acknowledged\": true } . For more alias API operations, see Index aliases. ",
    "url": "https://vagimeli.github.io/api-reference/alias/#response",
    "relUrl": "/api-reference/alias/#response"
  },"1946": {
    "doc": "Analyze API",
    "title": "Analyze API",
    "content": "The analyze API allows you to perform text analysis, which is the process of converting unstructured text into individual tokens (usually words) that are optimized for search. ",
    "url": "https://vagimeli.github.io/api-reference/analyze-apis/index/",
    "relUrl": "/api-reference/analyze-apis/index/"
  },"1947": {
    "doc": "Perform text analysis",
    "title": "Perform text analysis",
    "content": "The perform text analysis API analyzes a text string and returns the resulting tokens. If you use the Security plugin, you must have the manage index privilege. If you simply want to analyze text, you must have the manager cluster privilege. ",
    "url": "https://vagimeli.github.io/api-reference/analyze-apis/perform-text-analysis/",
    "relUrl": "/api-reference/analyze-apis/perform-text-analysis/"
  },"1948": {
    "doc": "Perform text analysis",
    "title": "Path and HTTP methods",
    "content": "GET /_analyze GET /{index}/_analyze POST /_analyze POST /{index}/_analyze . Although you can issue an analyzer request via both GET and POST requests, the two have important distinctions. A GET request causes data to be cached in the index so that the next time the data is requested, it is retrieved faster. A POST request sends a string that does not already exist to the analyzer to be compared to data that is already in the index. POST requests are not cached. ",
    "url": "https://vagimeli.github.io/api-reference/analyze-apis/perform-text-analysis/#path-and-http-methods",
    "relUrl": "/api-reference/analyze-apis/perform-text-analysis/#path-and-http-methods"
  },"1949": {
    "doc": "Perform text analysis",
    "title": "Path parameter",
    "content": "You can include the following optional path parameter in your request. | Parameter | Data type | Description | . | index | String | Index that is used to derive the analyzer. | . ",
    "url": "https://vagimeli.github.io/api-reference/analyze-apis/perform-text-analysis/#path-parameter",
    "relUrl": "/api-reference/analyze-apis/perform-text-analysis/#path-parameter"
  },"1950": {
    "doc": "Perform text analysis",
    "title": "Query parameters",
    "content": "You can include the following optional query parameters in your request. | Field | Data type | Description | . | analyzer | String | The name of the analyzer to apply to the text field. The analyzer can be built in or configured in the index.If analyzer is not specified, the analyze API uses the analyzer defined in the mapping of the field field.If the field field is not specified, the analyze API uses the default analyzer for the index. If no index is specified or the index does not have a default analyzer, the analyze API uses the standard analyzer. | . | attributes | Array of Strings | Array of token attributes for filtering the output of the explain field. | . | char_filter | Array of Strings | Array of character filters for preprocessing characters before the tokenizer field. | . | explain | Boolean | If true, causes the response to include token attributes and additional details. Defaults to false. | . | field | String | Field for deriving the analyzer. If you specify field, you must also specify the index path parameter. If you specify the analyzer field, it overrides the value of field. If you do not specify field, the analyze API uses the default analyzer for the index. If you do not specify the index field, or the index does not have a default analyzer, the analyze API uses the standard analyzer. | . | filter | Array of Strings | Array of token filters to apply after the tokenizer field. | . | normalizer | String | Normalizer for converting text into a single token. | . | tokenizer | String | Tokenizer for converting the text field into tokens. | . The following query parameter is required. | Field | Data type | Description | . | text | String or Array of Strings | Text to analyze. If you provide an array of strings, the text is analyzed as a multi-value field. | . Example requests . Analyze array of text strings . Apply a built-in analyzer . Apply a custom analyzer . Apply a custom transient analyzer . Specify an index . Derive the analyzer from an index field . Specify a normalizer . Get token details . Set a token limit . Analyze array of text strings . When you pass an array of strings to the text field, it is analyzed as a multi-value field. GET /_analyze { \"analyzer\" : \"standard\", \"text\" : [\"first array element\", \"second array element\"] } . copy . The previous request returns the following fields: . { \"tokens\" : [ { \"token\" : \"first\", \"start_offset\" : 0, \"end_offset\" : 5, \"type\" : \"&lt;ALPHANUM&gt;\", \"position\" : 0 }, { \"token\" : \"array\", \"start_offset\" : 6, \"end_offset\" : 11, \"type\" : \"&lt;ALPHANUM&gt;\", \"position\" : 1 }, { \"token\" : \"element\", \"start_offset\" : 12, \"end_offset\" : 19, \"type\" : \"&lt;ALPHANUM&gt;\", \"position\" : 2 }, { \"token\" : \"second\", \"start_offset\" : 20, \"end_offset\" : 26, \"type\" : \"&lt;ALPHANUM&gt;\", \"position\" : 3 }, { \"token\" : \"array\", \"start_offset\" : 27, \"end_offset\" : 32, \"type\" : \"&lt;ALPHANUM&gt;\", \"position\" : 4 }, { \"token\" : \"element\", \"start_offset\" : 33, \"end_offset\" : 40, \"type\" : \"&lt;ALPHANUM&gt;\", \"position\" : 5 } ] } . Apply a built-in analyzer . If you omit the index path parameter, you can apply any of the built-in analyzers to the text string. The following request analyzes text using the standard built-in analyzer: . GET /_analyze { \"analyzer\" : \"standard\", \"text\" : \"OpenSearch text analysis\" } . copy . The previous request returns the following fields: . { \"tokens\" : [ { \"token\" : \"opensearch\", \"start_offset\" : 0, \"end_offset\" : 10, \"type\" : \"&lt;ALPHANUM&gt;\", \"position\" : 0 }, { \"token\" : \"text\", \"start_offset\" : 11, \"end_offset\" : 15, \"type\" : \"&lt;ALPHANUM&gt;\", \"position\" : 1 }, { \"token\" : \"analysis\", \"start_offset\" : 16, \"end_offset\" : 24, \"type\" : \"&lt;ALPHANUM&gt;\", \"position\" : 2 } ] } . Apply a custom analyzer . You can create your own analyzer and specify it in an analyze request. In this scenario, a custom analyzer lowercase_ascii_folding has been created and associated with the books2 index. The analyzer converts text to lowercase and converts non-ASCII characters to ASCII. The following request applies the custom analyzer to the provided text: . GET /books2/_analyze { \"analyzer\": \"lowercase_ascii_folding\", \"text\" : \"Le garçon m'a SUIVI.\" } . copy . The previous request returns the following fields: . { \"tokens\" : [ { \"token\" : \"le\", \"start_offset\" : 0, \"end_offset\" : 2, \"type\" : \"&lt;ALPHANUM&gt;\", \"position\" : 0 }, { \"token\" : \"garcon\", \"start_offset\" : 3, \"end_offset\" : 9, \"type\" : \"&lt;ALPHANUM&gt;\", \"position\" : 1 }, { \"token\" : \"m'a\", \"start_offset\" : 10, \"end_offset\" : 13, \"type\" : \"&lt;ALPHANUM&gt;\", \"position\" : 2 }, { \"token\" : \"suivi\", \"start_offset\" : 14, \"end_offset\" : 19, \"type\" : \"&lt;ALPHANUM&gt;\", \"position\" : 3 } ] } . Apply a custom transient analyzer . You can build a custom transient analyzer from tokenizers, token filters, or character filters. Use the filter parameter to specify token filters. The following request uses the uppercase character filter to convert the text to uppercase: . GET /_analyze { \"tokenizer\" : \"keyword\", \"filter\" : [\"uppercase\"], \"text\" : \"OpenSearch filter\" } . copy . The previous request returns the following fields: . { \"tokens\" : [ { \"token\" : \"OPENSEARCH FILTER\", \"start_offset\" : 0, \"end_offset\" : 17, \"type\" : \"word\", \"position\" : 0 } ] } . The following request uses the html_strip filter to remove HTML characters from the text: . GET /_analyze { \"tokenizer\" : \"keyword\", \"filter\" : [\"lowercase\"], \"char_filter\" : [\"html_strip\"], \"text\" : \"&lt;b&gt;Leave&lt;/b&gt; right now!\" } . copy . The previous request returns the following fields: . { \"tokens\" : [ { \"token\" : \"leave right now!\", \"start_offset\" : 3, \"end_offset\" : 23, \"type\" : \"word\", \"position\" : 0 } ] } . You can combine filters using an array. The following request combines a lowercase translation with a stop filter that removes the words in the stopwords array: . GET /_analyze { \"tokenizer\" : \"whitespace\", \"filter\" : [\"lowercase\", {\"type\": \"stop\", \"stopwords\": [ \"to\", \"in\"]}], \"text\" : \"how to train your dog in five steps\" } . copy . The previous request returns the following fields: . { \"tokens\" : [ { \"token\" : \"how\", \"start_offset\" : 0, \"end_offset\" : 3, \"type\" : \"word\", \"position\" : 0 }, { \"token\" : \"train\", \"start_offset\" : 7, \"end_offset\" : 12, \"type\" : \"word\", \"position\" : 2 }, { \"token\" : \"your\", \"start_offset\" : 13, \"end_offset\" : 17, \"type\" : \"word\", \"position\" : 3 }, { \"token\" : \"dog\", \"start_offset\" : 18, \"end_offset\" : 21, \"type\" : \"word\", \"position\" : 4 }, { \"token\" : \"five\", \"start_offset\" : 25, \"end_offset\" : 29, \"type\" : \"word\", \"position\" : 6 }, { \"token\" : \"steps\", \"start_offset\" : 30, \"end_offset\" : 35, \"type\" : \"word\", \"position\" : 7 } ] } . Specify an index . You can analyze text using an index’s default analyzer, or you can specify a different analyzer. The following request analyzes the provided text using the default analyzer associated with the books index: . GET /books/_analyze { \"text\" : \"OpenSearch analyze test\" } . copy . The previous request returns the following fields: . \"tokens\" : [ { \"token\" : \"opensearch\", \"start_offset\" : 0, \"end_offset\" : 10, \"type\" : \"&lt;ALPHANUM&gt;\", \"position\" : 0 }, { \"token\" : \"analyze\", \"start_offset\" : 11, \"end_offset\" : 18, \"type\" : \"&lt;ALPHANUM&gt;\", \"position\" : 1 }, { \"token\" : \"test\", \"start_offset\" : 19, \"end_offset\" : 23, \"type\" : \"&lt;ALPHANUM&gt;\", \"position\" : 2 } ] } . The following request analyzes the provided text using the keyword analyzer, which returns the entire text value as a single token: . GET /books/_analyze { \"analyzer\" : \"keyword\", \"text\" : \"OpenSearch analyze test\" } . copy . The previous request returns the following fields: . { \"tokens\" : [ { \"token\" : \"OpenSearch analyze test\", \"start_offset\" : 0, \"end_offset\" : 23, \"type\" : \"word\", \"position\" : 0 } ] } . Derive the analyzer from an index field . You can pass text and a field in the index. The API looks up the field’s analyzer and uses it to analyze the text. If the mapping does not exist, the API uses the standard analyzer, which converts all text to lowercase and tokenizes based on white space. The following request causes the analysis to be based on the mapping for name: . GET /books2/_analyze { \"field\" : \"name\", \"text\" : \"OpenSearch analyze test\" } . copy . The previous request returns the following fields: . { \"tokens\" : [ { \"token\" : \"opensearch\", \"start_offset\" : 0, \"end_offset\" : 10, \"type\" : \"&lt;ALPHANUM&gt;\", \"position\" : 0 }, { \"token\" : \"analyze\", \"start_offset\" : 11, \"end_offset\" : 18, \"type\" : \"&lt;ALPHANUM&gt;\", \"position\" : 1 }, { \"token\" : \"test\", \"start_offset\" : 19, \"end_offset\" : 23, \"type\" : \"&lt;ALPHANUM&gt;\", \"position\" : 2 } ] } . Specify a normalizer . Instead of using a keyword field, you can use the normalizer associated with the index. A normalizer causes the analysis change to produce a single token. In this example, the books2 index includes a normalizer called to_lower_fold_ascii that converts text to lowercase and translates non-ASCII text to ASCII. The following request applies to_lower_fold_ascii to the text: . GET /books2/_analyze { \"normalizer\" : \"to_lower_fold_ascii\", \"text\" : \"C'est le garçon qui m'a suivi.\" } . copy . The previous request returns the following fields: . { \"tokens\" : [ { \"token\" : \"c'est le garcon qui m'a suivi.\", \"start_offset\" : 0, \"end_offset\" : 30, \"type\" : \"word\", \"position\" : 0 } ] } . You can create a custom transient normalizer with token and character filters. The following request uses the uppercase character filter to convert the given text to all uppercase: . GET /_analyze { \"filter\" : [\"uppercase\"], \"text\" : \"That is the boy who followed me.\" } . copy . The previous request returns the following fields: . { \"tokens\" : [ { \"token\" : \"THAT IS THE BOY WHO FOLLOWED ME.\", \"start_offset\" : 0, \"end_offset\" : 32, \"type\" : \"word\", \"position\" : 0 } ] } . Get token details . You can obtain additional details for all tokens by setting the explain attribute to true. The following request provides detailed token information for the reverse filter used with the standard tokenizer: . GET /_analyze { \"tokenizer\" : \"standard\", \"filter\" : [\"reverse\"], \"text\" : \"OpenSearch analyze test\", \"explain\" : true, \"attributes\" : [\"keyword\"] } . copy . The previous request returns the following fields: . { \"detail\" : { \"custom_analyzer\" : true, \"charfilters\" : [ ], \"tokenizer\" : { \"name\" : \"standard\", \"tokens\" : [ { \"token\" : \"OpenSearch\", \"start_offset\" : 0, \"end_offset\" : 10, \"type\" : \"&lt;ALPHANUM&gt;\", \"position\" : 0 }, { \"token\" : \"analyze\", \"start_offset\" : 11, \"end_offset\" : 18, \"type\" : \"&lt;ALPHANUM&gt;\", \"position\" : 1 }, { \"token\" : \"test\", \"start_offset\" : 19, \"end_offset\" : 23, \"type\" : \"&lt;ALPHANUM&gt;\", \"position\" : 2 } ] }, \"tokenfilters\" : [ { \"name\" : \"reverse\", \"tokens\" : [ { \"token\" : \"hcraeSnepO\", \"start_offset\" : 0, \"end_offset\" : 10, \"type\" : \"&lt;ALPHANUM&gt;\", \"position\" : 0 }, { \"token\" : \"ezylana\", \"start_offset\" : 11, \"end_offset\" : 18, \"type\" : \"&lt;ALPHANUM&gt;\", \"position\" : 1 }, { \"token\" : \"tset\", \"start_offset\" : 19, \"end_offset\" : 23, \"type\" : \"&lt;ALPHANUM&gt;\", \"position\" : 2 } ] } ] } } . Set a token limit . You can set a limit to the number of tokens generated. Setting a lower value reduces a node’s memory usage. The default value is 10000. The following request limits the tokens to four: . PUT /books2 { \"settings\" : { \"index.analyze.max_token_count\" : 4 } } . copy . The preceding request is an index API rather than an analyze API. See DYNAMIC INDEX SETTINGS for additional details. Response fields . The text analysis endpoints return the following response fields. | Field | Data type | Description | . | tokens | Array | Array of tokens derived from the text. See token object. | . | detail | Object | Details about the analysis and each token. Included only when you request token details. See detail object. | . Token object . | Field | Data type | Description | . | token | String | The token’s text. | . | start_offset | Integer | The token’s starting position within the original text string. Offsets are zero-based. | . | end_offset | Integer | The token’s ending position within the original text string. | . | type | String | Classification of the token: &lt;ALPHANUM&gt;, &lt;NUM&gt;, and so on. The tokenizer usually sets the type, but some filters define their own types. For example, the synonym filter defines the &lt;SYNONYM&gt; type. | . | position | Integer | The token’s position within the tokens array. | . Detail object . | Field | Data type | Description | . | custom_analyzer | Boolean | Whether the analyzer applied to the text is custom or built in. | . | charfilters | Array | List of character filters applied to the text. | . | tokenizer | Object | Name of the tokenizer applied to the text and a list of tokens* with content before the token filters were applied. | . | tokenfilters | Array | List of token filters applied to the text. Each token filter includes the filter’s name and a list of tokens* with content after the filters were applied. Token filters are listed in the order they are specified in the request. | . See token object for token field descriptions. ",
    "url": "https://vagimeli.github.io/api-reference/analyze-apis/perform-text-analysis/#query-parameters",
    "relUrl": "/api-reference/analyze-apis/perform-text-analysis/#query-parameters"
  },"1951": {
    "doc": "Analysis API Terminology",
    "title": "Terminology",
    "content": "The following sections provide descriptions of important text analysis terms. ",
    "url": "https://vagimeli.github.io/api-reference/analyze-apis/terminology/#terminology",
    "relUrl": "/api-reference/analyze-apis/terminology/#terminology"
  },"1952": {
    "doc": "Analysis API Terminology",
    "title": "Analyzers",
    "content": "Analyzers tell OpenSearch how to index and search text. An analyzer is composed of three components: a tokenizer, zero or more token filters, and zero or more character filters. OpenSearch provides built-in analyzers. For example, the standard built-in analyzer converts text to lowercase and breaks text into tokens based on word boundaries such as carriage returns and white space. The standard analyzer is also called the default analyzer and is used when no analyzer is specified in the text analysis request. If needed, you can combine tokenizers, token filters, and character filters to create a custom analyzer. Tokenizers . Tokenizers break unstuctured text into tokens and maintain metadata about tokens, such as their start and ending positions in the text. Character filters . Character filters examine text and perform translations, such as changing, removing, and adding characters. Token filters . Token filters modify tokens, performing operations such as converting a token’s characters to uppercase and adding or removing tokens. ",
    "url": "https://vagimeli.github.io/api-reference/analyze-apis/terminology/#analyzers",
    "relUrl": "/api-reference/analyze-apis/terminology/#analyzers"
  },"1953": {
    "doc": "Analysis API Terminology",
    "title": "Normalizers",
    "content": "Similar to analyzers, normalizers tokenize text but return a single token only. Normalizers do not employ tokenizers; they make limited use of character and token filters, such as those that operate on one character at a time. By default, OpenSearch does not apply normalizers. To apply normalizers, you must add them to your data before creating an index. ",
    "url": "https://vagimeli.github.io/api-reference/analyze-apis/terminology/#normalizers",
    "relUrl": "/api-reference/analyze-apis/terminology/#normalizers"
  },"1954": {
    "doc": "Analysis API Terminology",
    "title": "Analysis API Terminology",
    "content": " ",
    "url": "https://vagimeli.github.io/api-reference/analyze-apis/terminology/",
    "relUrl": "/api-reference/analyze-apis/terminology/"
  },"1955": {
    "doc": "CAT aliases",
    "title": "CAT aliases",
    "content": "Introduced 1.0 . The CAT aliases operation lists the mapping of aliases to indexes, plus routing and filtering information. ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-aliases/",
    "relUrl": "/api-reference/cat/cat-aliases/"
  },"1956": {
    "doc": "CAT aliases",
    "title": "Example",
    "content": "GET _cat/aliases?v . copy . To limit the information to a specific alias, add the alias name after your query: . GET _cat/aliases/&lt;alias&gt;?v . copy . If you want to get information for more than one alias, separate the alias names with commas: . GET _cat/aliases/alias1,alias2,alias3 . copy . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-aliases/#example",
    "relUrl": "/api-reference/cat/cat-aliases/#example"
  },"1957": {
    "doc": "CAT aliases",
    "title": "Path and HTTP methods",
    "content": "GET _cat/aliases/&lt;alias&gt; GET _cat/aliases . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-aliases/#path-and-http-methods",
    "relUrl": "/api-reference/cat/cat-aliases/#path-and-http-methods"
  },"1958": {
    "doc": "CAT aliases",
    "title": "URL parameters",
    "content": "All CAT aliases URL parameters are optional. In addition to the common URL parameters, you can specify the following parameters: . | Parameter | Type | Description | . | local | Boolean | Whether to return information from the local node only instead of from the master node. Default is false. | . | expand_wildcards | Enum | Expands wildcard expressions to concrete indices. Combine multiple values with commas. Supported values are all, open, closed, hidden, and none. Default is open. | . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-aliases/#url-parameters",
    "relUrl": "/api-reference/cat/cat-aliases/#url-parameters"
  },"1959": {
    "doc": "CAT aliases",
    "title": "Response",
    "content": "The following response shows that alias1 refers to a movies index and has a configured filter: . alias | index | filter | routing.index | routing.search | is_write_index alias1 | movies | * | - | - | - .opensearch-dashboards | .opensearch-dashboards_1 | - | - | - | - . To learn more about index aliases, see Index aliases. ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-aliases/#response",
    "relUrl": "/api-reference/cat/cat-aliases/#response"
  },"1960": {
    "doc": "CAT allocation",
    "title": "CAT allocation",
    "content": "Introduced 1.0 . The CAT allocation operation lists the allocation of disk space for indexes and the number of shards on each node. ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-allocation/",
    "relUrl": "/api-reference/cat/cat-allocation/"
  },"1961": {
    "doc": "CAT allocation",
    "title": "Example",
    "content": "GET _cat/allocation?v . copy . To limit the information to a specific node, add the node name after your query: . GET _cat/allocation/&lt;node_name&gt; . copy . If you want to get information for more than one node, separate the node names with commas: . GET _cat/allocation/node_name_1,node_name_2,node_name_3 . copy . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-allocation/#example",
    "relUrl": "/api-reference/cat/cat-allocation/#example"
  },"1962": {
    "doc": "CAT allocation",
    "title": "Path and HTTP methods",
    "content": "GET _cat/allocation?v GET _cat/allocation/&lt;node_name&gt; . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-allocation/#path-and-http-methods",
    "relUrl": "/api-reference/cat/cat-allocation/#path-and-http-methods"
  },"1963": {
    "doc": "CAT allocation",
    "title": "URL parameters",
    "content": "All CAT allocation URL parameters are optional. In addition to the common URL parameters, you can specify the following parameters: . | Parameter | Type | Description | . | bytes | Byte size | Specify the units for byte size. For example, 7kb or 6gb. For more information, see Supported units. | . | local | Boolean | Whether to return information from the local node only instead of from the cluster_manager node. Default is false. | . | cluster_manager_timeout | Time | The amount of time to wait for a connection to the cluster_manager node. Default is 30 seconds. | . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-allocation/#url-parameters",
    "relUrl": "/api-reference/cat/cat-allocation/#url-parameters"
  },"1964": {
    "doc": "CAT allocation",
    "title": "Response",
    "content": "The following response shows that eight shards are allocated to each of the two nodes available: . shards | disk.indices | disk.used | disk.avail | disk.total | disk.percent host | ip | node 8 | 989.4kb | 25.9gb | 32.4gb | 58.4gb | 44 172.18.0.4 | 172.18.0.4 | odfe-node1 8 | 962.4kb | 25.9gb | 32.4gb | 58.4gb | 44 172.18.0.3 | 172.18.0.3 | odfe-node2 . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-allocation/#response",
    "relUrl": "/api-reference/cat/cat-allocation/#response"
  },"1965": {
    "doc": "CAT cluster manager",
    "title": "CAT cluster_manager",
    "content": "Introduced 1.0 . The CAT cluster manager operation lists information that helps identify the elected cluster manager node. ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-cluster_manager/#cat-cluster_manager",
    "relUrl": "/api-reference/cat/cat-cluster_manager/#cat-cluster_manager"
  },"1966": {
    "doc": "CAT cluster manager",
    "title": "Example",
    "content": "GET _cat/cluster_manager?v . copy . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-cluster_manager/#example",
    "relUrl": "/api-reference/cat/cat-cluster_manager/#example"
  },"1967": {
    "doc": "CAT cluster manager",
    "title": "Path and HTTP methods",
    "content": "GET _cat/cluster_manager . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-cluster_manager/#path-and-http-methods",
    "relUrl": "/api-reference/cat/cat-cluster_manager/#path-and-http-methods"
  },"1968": {
    "doc": "CAT cluster manager",
    "title": "URL parameters",
    "content": "All CAT cluster manager URL parameters are optional. In addition to the common URL parameters, you can specify the following parameters: . Parameter | Type | Description :— | :— | :— cluster_manager_timeout | Time | The amount of time to wait for a connection to the cluster manager node. Default is 30 seconds. ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-cluster_manager/#url-parameters",
    "relUrl": "/api-reference/cat/cat-cluster_manager/#url-parameters"
  },"1969": {
    "doc": "CAT cluster manager",
    "title": "Response",
    "content": "id | host | ip | node ZaIkkUd4TEiAihqJGkp5CA | 172.18.0.3 | 172.18.0.3 | opensearch-node2 . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-cluster_manager/#response",
    "relUrl": "/api-reference/cat/cat-cluster_manager/#response"
  },"1970": {
    "doc": "CAT cluster manager",
    "title": "CAT cluster manager",
    "content": " ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-cluster_manager/",
    "relUrl": "/api-reference/cat/cat-cluster_manager/"
  },"1971": {
    "doc": "CAT count",
    "title": "CAT count",
    "content": "Introduced 1.0 . The CAT count operation lists the number of documents in your cluster. ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-count/",
    "relUrl": "/api-reference/cat/cat-count/"
  },"1972": {
    "doc": "CAT count",
    "title": "Example",
    "content": "GET _cat/count?v . copy . To see the number of documents in a specific index or alias, add the index or alias name after your query: . GET _cat/count/&lt;index_or_alias&gt;?v . copy . If you want to get information for more than one index or alias, separate the index or alias names with commas: . GET _cat/count/index_or_alias_1,index_or_alias_2,index_or_alias_3 . copy . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-count/#example",
    "relUrl": "/api-reference/cat/cat-count/#example"
  },"1973": {
    "doc": "CAT count",
    "title": "Path and HTTP methods",
    "content": "GET _cat/count?v GET _cat/count/&lt;index&gt;?v . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-count/#path-and-http-methods",
    "relUrl": "/api-reference/cat/cat-count/#path-and-http-methods"
  },"1974": {
    "doc": "CAT count",
    "title": "URL parameters",
    "content": "All CAT count URL parameters are optional. You can specify any of the common URL parameters. ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-count/#url-parameters",
    "relUrl": "/api-reference/cat/cat-count/#url-parameters"
  },"1975": {
    "doc": "CAT count",
    "title": "Response",
    "content": "The following response shows the overall document count as 1625: . epoch | timestamp | count 1624237738 | 01:08:58 | 1625 . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-count/#response",
    "relUrl": "/api-reference/cat/cat-count/#response"
  },"1976": {
    "doc": "CAT field data",
    "title": "CAT fielddata",
    "content": "Introduced 1.0 . The CAT fielddata operation lists the memory size used by each field per node. ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-field-data/#cat-fielddata",
    "relUrl": "/api-reference/cat/cat-field-data/#cat-fielddata"
  },"1977": {
    "doc": "CAT field data",
    "title": "Example",
    "content": "GET _cat/fielddata?v . copy . To limit the information to a specific field, add the field name after your query: . GET _cat/fielddata/&lt;field_name&gt;?v . copy . If you want to get information for more than one field, separate the field names with commas: . GET _cat/fielddata/field_name_1,field_name_2,field_name_3 . copy . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-field-data/#example",
    "relUrl": "/api-reference/cat/cat-field-data/#example"
  },"1978": {
    "doc": "CAT field data",
    "title": "Path and HTTP methods",
    "content": "GET _cat/fielddata?v GET _cat/fielddata/&lt;field_name&gt;?v . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-field-data/#path-and-http-methods",
    "relUrl": "/api-reference/cat/cat-field-data/#path-and-http-methods"
  },"1979": {
    "doc": "CAT field data",
    "title": "URL parameters",
    "content": "All CAT fielddata URL parameters are optional. In addition to the common URL parameters, you can specify the following parameter: . | Parameter | Type | Description | . | bytes | Byte size | Specify the units for byte size. For example, 7kb or 6gb. For more information, see Supported units. | . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-field-data/#url-parameters",
    "relUrl": "/api-reference/cat/cat-field-data/#url-parameters"
  },"1980": {
    "doc": "CAT field data",
    "title": "Response",
    "content": "The following response shows the memory size for all fields as 284 bytes: . id host ip node field size 1vo54NuxSxOrbPEYdkSF0w 172.18.0.4 172.18.0.4 odfe-node1 _id 284b ZaIkkUd4TEiAihqJGkp5CA 172.18.0.3 172.18.0.3 odfe-node2 _id 284b . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-field-data/#response",
    "relUrl": "/api-reference/cat/cat-field-data/#response"
  },"1981": {
    "doc": "CAT field data",
    "title": "CAT field data",
    "content": " ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-field-data/",
    "relUrl": "/api-reference/cat/cat-field-data/"
  },"1982": {
    "doc": "CAT health",
    "title": "CAT health",
    "content": "Introduced 1.0 . The CAT health operation lists the status of the cluster, how long the cluster has been up, the number of nodes, and other useful information that helps you analyze the health of your cluster. ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-health/",
    "relUrl": "/api-reference/cat/cat-health/"
  },"1983": {
    "doc": "CAT health",
    "title": "Example",
    "content": "GET _cat/health?v . copy . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-health/#example",
    "relUrl": "/api-reference/cat/cat-health/#example"
  },"1984": {
    "doc": "CAT health",
    "title": "Path and HTTP methods",
    "content": "GET _cat/health?v . copy . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-health/#path-and-http-methods",
    "relUrl": "/api-reference/cat/cat-health/#path-and-http-methods"
  },"1985": {
    "doc": "CAT health",
    "title": "URL parameters",
    "content": "All CAT health URL parameters are optional. | Parameter | Type | Description | . | time | Time | Specify the units for time. For example, 5d or 7h. For more information, see Supported units. | . | ts | Boolean | If true, returns HH:MM:SS and Unix epoch timestamps. Default is true. | . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-health/#url-parameters",
    "relUrl": "/api-reference/cat/cat-health/#url-parameters"
  },"1986": {
    "doc": "CAT health",
    "title": "Response",
    "content": "GET _cat/health?v&amp;time=5d epoch | timestamp | cluster | status | node.total | node.data | shards | pri | relo | init | unassign | pending_tasks | max_task_wait_time | active_shards_percent 1624248112 | 04:01:52 | odfe-cluster | green | 2 | 2 | 16 | 8 | 0 | 0 | 0 | 0 | - | 100.0% . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-health/#response",
    "relUrl": "/api-reference/cat/cat-health/#response"
  },"1987": {
    "doc": "CAT indices operation",
    "title": "CAT indices",
    "content": "Introduced 1.0 . The CAT indices operation lists information related to indexes, that is, how much disk space they are using, how many shards they have, their health status, and so on. ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-indices/#cat-indices",
    "relUrl": "/api-reference/cat/cat-indices/#cat-indices"
  },"1988": {
    "doc": "CAT indices operation",
    "title": "Example",
    "content": "GET _cat/indices?v . copy . To limit the information to a specific index, add the index name after your query. GET _cat/indices/&lt;index&gt;?v . copy . If you want to get information for more than one index, separate the indexes with commas: . GET _cat/indices/index1,index2,index3 . copy . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-indices/#example",
    "relUrl": "/api-reference/cat/cat-indices/#example"
  },"1989": {
    "doc": "CAT indices operation",
    "title": "Path and HTTP methods",
    "content": "GET _cat/indices/&lt;index&gt; GET _cat/indices . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-indices/#path-and-http-methods",
    "relUrl": "/api-reference/cat/cat-indices/#path-and-http-methods"
  },"1990": {
    "doc": "CAT indices operation",
    "title": "URL parameters",
    "content": "All CAT indices URL parameters are optional. In addition to the common URL parameters, you can specify the following parameters: . | Parameter | Type | Description | . | bytes | Byte size | Specify the units for byte size. For example, 7kb or 6gb. For more information, see Supported units. | . | health | String | Limit indexes based on their health status. Supported values are green, yellow, and red. | . | include_unloaded_segments | Boolean | Whether to include information from segments not loaded into memory. Default is false. | . | cluster_manager_timeout | Time | The amount of time to wait for a connection to the cluster manager node. Default is 30 seconds. | . | pri | Boolean | Whether to return information only from the primary shards. Default is false. | . | time | Time | Specify the units for time. For example, 5d or 7h. For more information, see Supported units. | . | expand_wildcards | Enum | Expands wildcard expressions to concrete indexes. Combine multiple values with commas. Supported values are all, open, closed, hidden, and none. Default is open. | . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-indices/#url-parameters",
    "relUrl": "/api-reference/cat/cat-indices/#url-parameters"
  },"1991": {
    "doc": "CAT indices operation",
    "title": "Response",
    "content": "health | status | index | uuid | pri | rep | docs.count | docs.deleted | store.size | pri.store.size green | open | movies | UZbpfERBQ1-3GSH2bnM3sg | 1 | 1 | 1 | 0 | 7.7kb | 3.8kb . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-indices/#response",
    "relUrl": "/api-reference/cat/cat-indices/#response"
  },"1992": {
    "doc": "CAT indices operation",
    "title": "CAT indices operation",
    "content": " ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-indices/",
    "relUrl": "/api-reference/cat/cat-indices/"
  },"1993": {
    "doc": "CAT nodeattrs",
    "title": "CAT nodeattrs",
    "content": "Introduced 1.0 . The CAT nodeattrs operation lists the attributes of custom nodes. ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-nodeattrs/",
    "relUrl": "/api-reference/cat/cat-nodeattrs/"
  },"1994": {
    "doc": "CAT nodeattrs",
    "title": "Example",
    "content": "GET _cat/nodeattrs?v . copy . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-nodeattrs/#example",
    "relUrl": "/api-reference/cat/cat-nodeattrs/#example"
  },"1995": {
    "doc": "CAT nodeattrs",
    "title": "Path and HTTP methods",
    "content": "GET _cat/nodeattrs . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-nodeattrs/#path-and-http-methods",
    "relUrl": "/api-reference/cat/cat-nodeattrs/#path-and-http-methods"
  },"1996": {
    "doc": "CAT nodeattrs",
    "title": "URL parameters",
    "content": "All CAT nodeattrs URL parameters are optional. In addition to the common URL parameters, you can specify the following parameters: . | Parameter | Type | Description | . | local | Boolean | Whether to return information from the local node only instead of from the cluster_manager node. Default is false. | . | cluster_manager_timeout | Time | The amount of time to wait for a connection to the cluster manager node. Default is 30 seconds. | . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-nodeattrs/#url-parameters",
    "relUrl": "/api-reference/cat/cat-nodeattrs/#url-parameters"
  },"1997": {
    "doc": "CAT nodeattrs",
    "title": "Response",
    "content": "node | host | ip | attr | value odfe-node2 | 172.18.0.3 | 172.18.0.3 | testattr | test . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-nodeattrs/#response",
    "relUrl": "/api-reference/cat/cat-nodeattrs/#response"
  },"1998": {
    "doc": "CAT nodes operation",
    "title": "CAT nodes",
    "content": "Introduced 1.0 . The CAT nodes operation lists node-level information, including node roles and load metrics. A few important node metrics are pid, name, cluster_manager, ip, port, version, build, jdk, along with disk, heap, ram, and file_desc. ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-nodes/#cat-nodes",
    "relUrl": "/api-reference/cat/cat-nodes/#cat-nodes"
  },"1999": {
    "doc": "CAT nodes operation",
    "title": "Example",
    "content": "GET _cat/nodes?v . copy . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-nodes/#example",
    "relUrl": "/api-reference/cat/cat-nodes/#example"
  },"2000": {
    "doc": "CAT nodes operation",
    "title": "Path and HTTP methods",
    "content": "GET _cat/nodes . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-nodes/#path-and-http-methods",
    "relUrl": "/api-reference/cat/cat-nodes/#path-and-http-methods"
  },"2001": {
    "doc": "CAT nodes operation",
    "title": "URL parameters",
    "content": "All CAT nodes URL parameters are optional. In addition to the common URL parameters, you can specify the following parameters: . | Parameter | Type | Description | . | bytes | Byte size | Specify the units for byte size. For example, 7kb or 6gb. For more information, see Supported units. | . | full_id | Boolean | If true, return the full node ID. If false, return the shortened node ID. Defaults to false. | . | local | Boolean | Whether to return information from the local node only instead of from the cluster_manager node. Default is false. | . | cluster_manager_timeout | Time | The amount of time to wait for a connection to the cluster manager node. Default is 30 seconds. | . | time | Time | Specify the units for time. For example, 5d or 7h. For more information, see Supported units. | . | include_unloaded_segments | Boolean | Whether to include information from segments not loaded into memory. Default is false. | . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-nodes/#url-parameters",
    "relUrl": "/api-reference/cat/cat-nodes/#url-parameters"
  },"2002": {
    "doc": "CAT nodes operation",
    "title": "Response",
    "content": "ip | heap.percent | ram.percent | cpu load_1m | load_5m | load_15m | node.role | node.roles | cluster_manager | name 10.11.1.225 | 31 | 32 | 0 | 0.00 | 0.00 | di | data,ingest,ml | - | data-e5b89ad7 . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-nodes/#response",
    "relUrl": "/api-reference/cat/cat-nodes/#response"
  },"2003": {
    "doc": "CAT nodes operation",
    "title": "CAT nodes operation",
    "content": " ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-nodes/",
    "relUrl": "/api-reference/cat/cat-nodes/"
  },"2004": {
    "doc": "CAT pending tasks",
    "title": "CAT pending tasks",
    "content": "Introduced 1.0 . The CAT pending tasks operation lists the progress of all pending tasks, including task priority and time in queue. ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-pending-tasks/",
    "relUrl": "/api-reference/cat/cat-pending-tasks/"
  },"2005": {
    "doc": "CAT pending tasks",
    "title": "Example",
    "content": "GET _cat/pending_tasks?v . copy . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-pending-tasks/#example",
    "relUrl": "/api-reference/cat/cat-pending-tasks/#example"
  },"2006": {
    "doc": "CAT pending tasks",
    "title": "Path and HTTP methods",
    "content": "GET _cat/pending_tasks . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-pending-tasks/#path-and-http-methods",
    "relUrl": "/api-reference/cat/cat-pending-tasks/#path-and-http-methods"
  },"2007": {
    "doc": "CAT pending tasks",
    "title": "URL parameters",
    "content": "All CAT nodes URL parameters are optional. In addition to the common URL parameters, you can specify the following parameters: . | Parameter | Type | Description | . | local | Boolean | Whether to return information from the local node only instead of from the cluster_manager node. Default is false. | . | cluster_manager_timeout | Time | The amount of time to wait for a connection to the cluster manager node. Default is 30 seconds. | . | time | Time | Specify the units for time. For example, 5d or 7h. For more information, see Supported units. | . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-pending-tasks/#url-parameters",
    "relUrl": "/api-reference/cat/cat-pending-tasks/#url-parameters"
  },"2008": {
    "doc": "CAT pending tasks",
    "title": "Response",
    "content": "insertOrder | timeInQueue | priority | source 1786 | 1.8s | URGENT | shard-started . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-pending-tasks/#response",
    "relUrl": "/api-reference/cat/cat-pending-tasks/#response"
  },"2009": {
    "doc": "CAT plugins",
    "title": "CAT plugins",
    "content": "Introduced 1.0 . The CAT plugins operation lists the names, components, and versions of the installed plugins. ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-plugins/",
    "relUrl": "/api-reference/cat/cat-plugins/"
  },"2010": {
    "doc": "CAT plugins",
    "title": "Example",
    "content": "GET _cat/plugins?v . copy . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-plugins/#example",
    "relUrl": "/api-reference/cat/cat-plugins/#example"
  },"2011": {
    "doc": "CAT plugins",
    "title": "Path and HTTP methods",
    "content": "GET _cat/plugins . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-plugins/#path-and-http-methods",
    "relUrl": "/api-reference/cat/cat-plugins/#path-and-http-methods"
  },"2012": {
    "doc": "CAT plugins",
    "title": "URL parameters",
    "content": "All CAT plugins URL parameters are optional. In addition to the common URL parameters, you can specify the following parameters: . | Parameter | Type | Description | . | local | Boolean | Whether to return information from the local node only instead of from the cluster_manager node. Default is false. | . | cluster_manager_timeout | Time | The amount of time to wait for a connection to the cluster_manager node. Default is 30 seconds. | . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-plugins/#url-parameters",
    "relUrl": "/api-reference/cat/cat-plugins/#url-parameters"
  },"2013": {
    "doc": "CAT plugins",
    "title": "Response",
    "content": "name component version odfe-node2 opendistro-alerting 1.13.1.0 odfe-node2 opendistro-anomaly-detection 1.13.0.0 odfe-node2 opendistro-asynchronous-search 1.13.0.1 odfe-node2 opendistro-index-management 1.13.2.0 odfe-node2 opendistro-job-scheduler 1.13.0.0 odfe-node2 opendistro-knn 1.13.0.0 odfe-node2 opendistro-performance-analyzer 1.13.0.0 odfe-node2 opendistro-reports-scheduler 1.13.0.0 odfe-node2 opendistro-sql 1.13.2.0 odfe-node2 opendistro_security 1.13.1.0 odfe-node1 opendistro-alerting 1.13.1.0 odfe-node1 opendistro-anomaly-detection 1.13.0.0 odfe-node1 opendistro-asynchronous-search 1.13.0.1 odfe-node1 opendistro-index-management 1.13.2.0 odfe-node1 opendistro-job-scheduler 1.13.0.0 odfe-node1 opendistro-knn 1.13.0.0 odfe-node1 opendistro-performance-analyzer 1.13.0.0 odfe-node1 opendistro-reports-scheduler 1.13.0.0 odfe-node1 opendistro-sql 1.13.2.0 odfe-node1 opendistro_security 1.13.1.0 . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-plugins/#response",
    "relUrl": "/api-reference/cat/cat-plugins/#response"
  },"2014": {
    "doc": "CAT recovery",
    "title": "CAT recovery",
    "content": "Introduced 1.0 . The CAT recovery operation lists all completed and ongoing index and shard recoveries. ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-recovery/",
    "relUrl": "/api-reference/cat/cat-recovery/"
  },"2015": {
    "doc": "CAT recovery",
    "title": "Example",
    "content": "GET _cat/recovery?v . copy . To see only the recoveries of a specific index, add the index name after your query. GET _cat/recovery/&lt;index&gt;?v . copy . If you want to get information for more than one index, separate the indexes with commas: . GET _cat/recovery/index1,index2,index3 . copy . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-recovery/#example",
    "relUrl": "/api-reference/cat/cat-recovery/#example"
  },"2016": {
    "doc": "CAT recovery",
    "title": "Path and HTTP methods",
    "content": "GET _cat/recovery . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-recovery/#path-and-http-methods",
    "relUrl": "/api-reference/cat/cat-recovery/#path-and-http-methods"
  },"2017": {
    "doc": "CAT recovery",
    "title": "URL parameters",
    "content": "All CAT recovery URL parameters are optional. In addition to the common URL parameters, you can specify the following parameters: . | Parameter | Type | Description | . | active_only | Boolean | Whether to only include ongoing shard recoveries. Default is false. | . | bytes | Byte size | Specify the units for byte size. For example, 7kb or 6gb. For more information, see Supported units. | . | detailed | Boolean | Whether to include detailed information about shard recoveries. Default is false. | . | time | Time | Specify the units for time. For example, 5d or 7h. For more information, see Supported units. | . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-recovery/#url-parameters",
    "relUrl": "/api-reference/cat/cat-recovery/#url-parameters"
  },"2018": {
    "doc": "CAT recovery",
    "title": "Response",
    "content": "index | shard | time | type | stage | source_host | source_node | target_host | target_node | repository | snapshot | files | files_recovered | files_percent | files_total | bytes | bytes_recovered | bytes_percent | bytes_total | translog_ops | translog_ops_recovered | translog_ops_percent movies | 0 | 117ms | empty_store | done | n/a | n/a | 172.18.0.4 | odfe-node1 | n/a | n/a | 0 | 0 | 0.0% | 0 | 0 | 0 | 0.0% | 0 | 0 | 0 | 100.0% movies | 0 | 382ms | peer | done | 172.18.0.4 | odfe-node1 | 172.18.0.3 | odfe-node2 | n/a | n/a | 1 | 1 | 100.0% | 1 | 208 | 208 | 100.0% | 208 | 1 | 1 | 100.0% . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-recovery/#response",
    "relUrl": "/api-reference/cat/cat-recovery/#response"
  },"2019": {
    "doc": "CAT repositories",
    "title": "CAT repositories",
    "content": "Introduced 1.0 . The CAT repositories operation lists all completed and ongoing index and shard recoveries. ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-repositories/",
    "relUrl": "/api-reference/cat/cat-repositories/"
  },"2020": {
    "doc": "CAT repositories",
    "title": "Example",
    "content": "GET _cat/repositories?v . copy . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-repositories/#example",
    "relUrl": "/api-reference/cat/cat-repositories/#example"
  },"2021": {
    "doc": "CAT repositories",
    "title": "Path and HTTP methods",
    "content": "GET _cat/repositories . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-repositories/#path-and-http-methods",
    "relUrl": "/api-reference/cat/cat-repositories/#path-and-http-methods"
  },"2022": {
    "doc": "CAT repositories",
    "title": "URL parameters",
    "content": "All CAT repositories URL parameters are optional. In addition to the common URL parameters, you can specify the following parameters: . | Parameter | Type | Description | . | local | Boolean | Whether to return information from the local node only instead of from the cluster_manager node. Default is false. | . | cluster_manager_timeout | Time | The amount of time to wait for a connection to the cluster_manager node. Default is 30 seconds. | . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-repositories/#url-parameters",
    "relUrl": "/api-reference/cat/cat-repositories/#url-parameters"
  },"2023": {
    "doc": "CAT repositories",
    "title": "Response",
    "content": "id type repo1 fs repo2 s3 . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-repositories/#response",
    "relUrl": "/api-reference/cat/cat-repositories/#response"
  },"2024": {
    "doc": "CAT segment replication",
    "title": "CAT segment replication",
    "content": "Introduced 2.7 . The CAT segment replication operation returns information about active and last completed segment replication events on each replica shard, including related shard-level metrics. These metrics provide information about how far behind the primary shard the replicas are lagging. Call the CAT Segment Replication API only on indexes with segment replication enabled. ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-segment-replication/",
    "relUrl": "/api-reference/cat/cat-segment-replication/"
  },"2025": {
    "doc": "CAT segment replication",
    "title": "Path and HTTP methods",
    "content": "GET /_cat/segment_replication GET /_cat/segment_replication/&lt;index&gt; . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-segment-replication/#path-and-http-methods",
    "relUrl": "/api-reference/cat/cat-segment-replication/#path-and-http-methods"
  },"2026": {
    "doc": "CAT segment replication",
    "title": "Path parameters",
    "content": "The following table lists the available optional path parameter. | Parameter | Type | Description | . | index | String | The name of the index, or a comma-separated list or wildcard expression of index names used to filter results. If this parameter is not provided, the response contains information about all indexes in the cluster. | . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-segment-replication/#path-parameters",
    "relUrl": "/api-reference/cat/cat-segment-replication/#path-parameters"
  },"2027": {
    "doc": "CAT segment replication",
    "title": "Query parameters",
    "content": "The CAT segment replication API operation supports the following optional query parameters. | Parameter | Data type | Description | . | active_only | Boolean | If true, the response only includes active segment replications. Defaults to false. | . | detailed | String | If true, the response includes additional metrics for each stage of a segment replication event. Defaults to false. | . | shards | String | A comma-separated list of shards to display. | . | format | String | A short version of the HTTP accept header. Valid values include JSON and YAML. | . | h | String | A comma-separated list of column names to display. | . | help | Boolean | If true, the response includes help information. Defaults to false. | . | time | Time value | Units used to display time values. Defaults to ms (milliseconds). | . | v | Boolean | If true, the response includes column headings. Defaults to false. | . | s | String | Specifies to sort the results. For example, s=shardId:desc sorts by shardId in descending order. | . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-segment-replication/#query-parameters",
    "relUrl": "/api-reference/cat/cat-segment-replication/#query-parameters"
  },"2028": {
    "doc": "CAT segment replication",
    "title": "Examples",
    "content": "The following examples illustrate various segment replication responses. Example 1: No active segment replication events . The following query requests segment replication metrics with column headings for all indexes: . GET /_cat/segment_replication?v=true . copy . The response contains the metrics for the preceding request: . shardId target_node target_host checkpoints_behind bytes_behind current_lag last_completed_lag rejected_requests [index-1][0] runTask-1 127.0.0.1 0 0b 0s 7ms 0 . Example 2: Shard ID specified . The following query requests segment replication metrics with column headings for shards with the ID 0 from indexes index1 and index2: . GET /_cat/segment_replication/index1,index2?v=true&amp;shards=0 . copy . The response contains the metrics for the preceding request. The column headings correspond to the metric names: . shardId target_node target_host checkpoints_behind bytes_behind current_lag last_completed_lag rejected_requests [index-1][0] runTask-1 127.0.0.1 0 0b 0s 3ms 0 [index-2][0] runTask-1 127.0.0.1 0 0b 0s 5ms 0 . Example 3: Detailed response . The following query requests detailed segment replication metrics with column headings for all indexes: . GET /_cat/segment_replication?v=true&amp;detailed=true . copy . The response contains additional metrics about the files and stages of a segment replication event: . shardId target_node target_host checkpoints_behind bytes_behind current_lag last_completed_lag rejected_requests stage time files_fetched files_percent bytes_fetched bytes_percent start_time stop_time files files_total bytes bytes_total replicating_stage_time_taken get_checkpoint_info_stage_time_taken file_diff_stage_time_taken get_files_stage_time_taken finalize_replication_stage_time_taken [index-1][0] runTask-1 127.0.0.1 0 0b 0s 3ms 0 done 10ms 6 100.0% 4753 100.0% 2023-03-16T13:46:16.802Z 2023-03-16T13:46:16.812Z 6 6 4.6kb 4.6kb 0s 2ms 0s 3ms 3ms [index-2][0] runTask-1 127.0.0.1 0 0b 0s 5ms 0 done 7ms 3 100.0% 3664 100.0% 2023-03-16T13:53:33.466Z 2023-03-16T13:53:33.474Z 3 3 3.5kb 3.5kb 0s 1ms 0s 2ms 2ms . Example 4: Sorting the results . The following query requests segment replication metrics with column headings for all indexes, sorted by shard ID in descending order: . GET /_cat/segment_replication?v&amp;s=shardId:desc . copy . The response contains the sorted results: . shardId target_node target_host checkpoints_behind bytes_behind current_lag last_completed_lag rejected_requests [test6][1] runTask-2 127.0.0.1 0 0b 0s 5ms 0 [test6][0] runTask-2 127.0.0.1 0 0b 0s 4ms 0 . Example 5: Using a metric alias . In a request, you can either use a metric’s full name or one of its aliases. The following query is the same as the preceding query, but it uses the alias s instead of shardID for sorting: . GET /_cat/segment_replication?v&amp;s=s:desc . copy . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-segment-replication/#examples",
    "relUrl": "/api-reference/cat/cat-segment-replication/#examples"
  },"2029": {
    "doc": "CAT segment replication",
    "title": "Response metrics",
    "content": "The following table lists the response metrics that are returned for all requests. When referring to a metric in a query parameter, you can provide either the metric’s full name or any of its aliases, as shown in the previous example. | Metric | Alias | Description | . | shardId | s | The ID of a specific shard. | . | target_host | thost | The target host IP address. | . | target_node | tnode | The target node name. | . | checkpoints_behind | cpb | The number of checkpoints by which the replica shard is behind the primary shard. | . | bytes_behind | bb | The number of bytes by which the replica shard is behind the primary shard. | . | current_lag | clag | The time elapsed while waiting for a replica shard to catch up to the primary shard. | . | last_completed_lag | lcl | The time taken for a replica shard to catch up to the latest primary shard refresh. | . | rejected_requests | rr | The number of rejected requests for the replication group. | . Additional detailed response metrics . The following table lists the additional response fields returned if detailed is set to true. | Metric | Alias | Description | . | stage | st | The current stage of a segment replication event. | . | time | t, ti | The amount of time a segment replication event took to complete, in milliseconds. | . | files_fetched | ff | The number of files fetched so far for a segment replication event. | . | files_percent | fp | The percentage of files fetched so far for a segment replication event. | . | bytes_fetched | bf | The number of bytes fetched so far for a segment replication event. | . | bytes_percent | bp | The number of bytes fetched so far for a segment replication event as a percentage. | . | start_time | start | The segment replication start time. | . | stop_time | stop | The segment replication stop time. | . | files | f | The number of files that needs to be fetched for a segment replication event. | . | files_total | tf | The total number of files that are part of this recovery, including both reused and recovered files. | . | bytes | b | The number of bytes that needs to be fetched for a segment replication event. | . | bytes_total | tb | The total number of bytes in the shard. | . | replicating_stage_time_taken | rstt | The amount of time the replicating stage of a segment replication event took to complete. | . | get_checkpoint_info_stage_time_taken | gcistt | The amount of time the get checkpoint info stage of a segment replication event took to complete. | . | file_diff_stage_time_taken | fdstt | The amount of time the file diff stage of a segment replication event took to complete. | . | get_files_stage_time_taken | gfstt | The amount of time the get files stage of a segment replication event took to complete. | . | finalize_replication_stage_time_taken | frstt | The amount of time the finalize replication stage of a segment replication event took to complete. | . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-segment-replication/#response-metrics",
    "relUrl": "/api-reference/cat/cat-segment-replication/#response-metrics"
  },"2030": {
    "doc": "CAT segments",
    "title": "CAT segments",
    "content": "Introduced 1.0 . The cat segments operation lists Lucene segment-level information for each index. ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-segments/",
    "relUrl": "/api-reference/cat/cat-segments/"
  },"2031": {
    "doc": "CAT segments",
    "title": "Example",
    "content": "GET _cat/segments?v . copy . To see only the information about segments of a specific index, add the index name after your query. GET _cat/segments/&lt;index&gt;?v . copy . If you want to get information for more than one index, separate the indexes with commas: . GET _cat/segments/index1,index2,index3 . copy . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-segments/#example",
    "relUrl": "/api-reference/cat/cat-segments/#example"
  },"2032": {
    "doc": "CAT segments",
    "title": "Path and HTTP methods",
    "content": "GET _cat/segments . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-segments/#path-and-http-methods",
    "relUrl": "/api-reference/cat/cat-segments/#path-and-http-methods"
  },"2033": {
    "doc": "CAT segments",
    "title": "URL parameters",
    "content": "All CAT segments URL parameters are optional. In addition to the common URL parameters, you can specify the following parameters: . | Parameter | Type | Description | . | bytes | Byte size | Specify the units for byte size. For example, 7kb or 6gb. For more information, see Supported units.. | . | cluster_manager_timeout | Time | The amount of time to wait for a connection to the cluster manager node. Default is 30 seconds. | . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-segments/#url-parameters",
    "relUrl": "/api-reference/cat/cat-segments/#url-parameters"
  },"2034": {
    "doc": "CAT segments",
    "title": "Response",
    "content": "index | shard | prirep | ip | segment | generation | docs.count | docs.deleted | size | size.memory | committed | searchable | version | compound movies | 0 | p | 172.18.0.4 | _0 | 0 | 1 | 0 | 3.5kb | 1364 | true | true | 8.7.0 | true movies | 0 | r | 172.18.0.3 | _0 | 0 | 1 | 0 | 3.5kb | 1364 | true | true | 8.7.0 | true . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-segments/#response",
    "relUrl": "/api-reference/cat/cat-segments/#response"
  },"2035": {
    "doc": "CAT shards",
    "title": "CAT shards",
    "content": "Introduced 1.0 . The CAT shards operation lists the state of all primary and replica shards and how they are distributed. ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-shards/",
    "relUrl": "/api-reference/cat/cat-shards/"
  },"2036": {
    "doc": "CAT shards",
    "title": "Example",
    "content": "GET _cat/shards?v . copy . To see only the information about shards of a specific index, add the index name after your query. GET _cat/shards/&lt;index&gt;?v . copy . If you want to get information for more than one index, separate the indexes with commas: . GET _cat/shards/index1,index2,index3 . copy . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-shards/#example",
    "relUrl": "/api-reference/cat/cat-shards/#example"
  },"2037": {
    "doc": "CAT shards",
    "title": "Path and HTTP methods",
    "content": "GET _cat/shards . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-shards/#path-and-http-methods",
    "relUrl": "/api-reference/cat/cat-shards/#path-and-http-methods"
  },"2038": {
    "doc": "CAT shards",
    "title": "URL parameters",
    "content": "All cat shards URL parameters are optional. In addition to the common URL parameters, you can specify the following parameters: . | Parameter | Type | Description | . | bytes | Byte size | Specify the units for byte size. For example, 7kb or 6gb. For more information, see Supported units. | . | local | Boolean | Whether to return information from the local node only instead of from the cluster_manager node. Default is false. | . | cluster_manager_timeout | Time | The amount of time to wait for a connection to the cluster_manager node. Default is 30 seconds. | . | time | Time | Specify the units for time. For example, 5d or 7h. For more information, see Supported units. | . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-shards/#url-parameters",
    "relUrl": "/api-reference/cat/cat-shards/#url-parameters"
  },"2039": {
    "doc": "CAT shards",
    "title": "Response",
    "content": "index | shard | prirep | state | docs | store | ip | node plugins | 0 | p | STARTED | 0 | 208b | 172.18.0.4 | odfe-node1 plugins | 0 | r | STARTED | 0 | 208b | 172.18.0.3 | odfe-node2 . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-shards/#response",
    "relUrl": "/api-reference/cat/cat-shards/#response"
  },"2040": {
    "doc": "CAT snapshots",
    "title": "CAT snapshots",
    "content": "Introduced 1.0 . The CAT snapshots operation lists all snapshots for a repository. ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-snapshots/",
    "relUrl": "/api-reference/cat/cat-snapshots/"
  },"2041": {
    "doc": "CAT snapshots",
    "title": "Example",
    "content": "GET _cat/snapshots?v . copy . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-snapshots/#example",
    "relUrl": "/api-reference/cat/cat-snapshots/#example"
  },"2042": {
    "doc": "CAT snapshots",
    "title": "Path and HTTP methods",
    "content": "GET _cat/snapshots . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-snapshots/#path-and-http-methods",
    "relUrl": "/api-reference/cat/cat-snapshots/#path-and-http-methods"
  },"2043": {
    "doc": "CAT snapshots",
    "title": "URL parameters",
    "content": "All CAT snapshots URL parameters are optional. In addition to the common URL parameters, you can specify the following parameter: . | Parameter | Type | Description | . | cluster_manager_timeout | Time | The amount of time to wait for a connection to the cluster manager node. Default is 30 seconds. | . | time | Time | Specify the units for time. For example, 5d or 7h. For more information, see Supported units. | . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-snapshots/#url-parameters",
    "relUrl": "/api-reference/cat/cat-snapshots/#url-parameters"
  },"2044": {
    "doc": "CAT snapshots",
    "title": "Response",
    "content": "index | shard | prirep | state | docs | store | ip | node plugins | 0 | p | STARTED | 0 | 208b | 172.18.0.4 | odfe-node1 plugins | 0 | r | STARTED | 0 | 208b | 172.18.0.3 | odfe-node2 . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-snapshots/#response",
    "relUrl": "/api-reference/cat/cat-snapshots/#response"
  },"2045": {
    "doc": "CAT tasks",
    "title": "CAT tasks",
    "content": "Introduced 1.0 . The CAT tasks operation lists the progress of all tasks currently running on your cluster. ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-tasks/",
    "relUrl": "/api-reference/cat/cat-tasks/"
  },"2046": {
    "doc": "CAT tasks",
    "title": "Example",
    "content": "GET _cat/tasks?v . copy . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-tasks/#example",
    "relUrl": "/api-reference/cat/cat-tasks/#example"
  },"2047": {
    "doc": "CAT tasks",
    "title": "Path and HTTP methods",
    "content": "GET _cat/tasks . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-tasks/#path-and-http-methods",
    "relUrl": "/api-reference/cat/cat-tasks/#path-and-http-methods"
  },"2048": {
    "doc": "CAT tasks",
    "title": "URL parameters",
    "content": "All CAT tasks URL parameters are optional. In addition to the common URL parameters, you can specify the following parameters: . | Parameter | Type | Description | . | nodes | List | A comma-separated list of node IDs or names to limit the returned information. Use _local to return information from the node you’re connecting to, specify the node name to get information from specific nodes, or keep the parameter empty to get information from all nodes. | . | detailed | Boolean | Returns detailed task information. (Default: false) | . | parent_task_id | String | Returns tasks with a specified parent task ID (node_id:task_number). Keep empty or set to -1 to return all. | . | time | Time | Specify the units for time. For example, 5d or 7h. For more information, see Supported units. | . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-tasks/#url-parameters",
    "relUrl": "/api-reference/cat/cat-tasks/#url-parameters"
  },"2049": {
    "doc": "CAT tasks",
    "title": "Response",
    "content": "action | task_id | parent_task_id | type | start_time | timestamp | running_time | ip | node cluster:monitor/tasks/lists | 1vo54NuxSxOrbPEYdkSF0w:168062 | - | transport | 1624337809471 | 04:56:49 | 489.5ms | 172.18.0.4 | odfe-node1 . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-tasks/#response",
    "relUrl": "/api-reference/cat/cat-tasks/#response"
  },"2050": {
    "doc": "CAT templates",
    "title": "CAT templates",
    "content": "Introduced 1.0 . The CAT templates operation lists the names, patterns, order numbers, and version numbers of index templates. ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-templates/",
    "relUrl": "/api-reference/cat/cat-templates/"
  },"2051": {
    "doc": "CAT templates",
    "title": "Example",
    "content": "GET _cat/templates?v . copy . If you want to get information for a specific template or pattern: . GET _cat/templates/&lt;template_name_or_pattern&gt; . copy . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-templates/#example",
    "relUrl": "/api-reference/cat/cat-templates/#example"
  },"2052": {
    "doc": "CAT templates",
    "title": "Path and HTTP methods",
    "content": "GET _cat/templates . copy . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-templates/#path-and-http-methods",
    "relUrl": "/api-reference/cat/cat-templates/#path-and-http-methods"
  },"2053": {
    "doc": "CAT templates",
    "title": "URL parameters",
    "content": "All CAT templates URL parameters are optional. In addition to the common URL parameters, you can specify the following parameters: . | Parameter | Type | Description | . | local | Boolean | Whether to return information from the local node only instead of from the cluster manager node. Default is false. | . | cluster_manager_timeout | Time | The amount of time to wait for a connection to the cluster manager node. Default is 30 seconds. | . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-templates/#url-parameters",
    "relUrl": "/api-reference/cat/cat-templates/#url-parameters"
  },"2054": {
    "doc": "CAT templates",
    "title": "Response",
    "content": "name | index_patterns order version composed_of tenant_template | [opensearch-dashboards*] | 0 | . To learn more about index templates, see Index templates. ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-templates/#response",
    "relUrl": "/api-reference/cat/cat-templates/#response"
  },"2055": {
    "doc": "CAT thread pool",
    "title": "CAT thread pool",
    "content": "Introduced 1.0 . The CAT thread pool operation lists the active, queued, and rejected threads of different thread pools on each node. ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-thread-pool/",
    "relUrl": "/api-reference/cat/cat-thread-pool/"
  },"2056": {
    "doc": "CAT thread pool",
    "title": "Example",
    "content": "GET _cat/thread_pool?v . copy . If you want to get information for more than one thread pool, separate the thread pool names with commas: . GET _cat/thread_pool/thread_pool_name_1,thread_pool_name_2,thread_pool_name_3 . copy . If you want to limit the information to a specific thread pool, add the thread pool name after your query: . GET _cat/thread_pool/&lt;thread_pool_name&gt;?v . copy . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-thread-pool/#example",
    "relUrl": "/api-reference/cat/cat-thread-pool/#example"
  },"2057": {
    "doc": "CAT thread pool",
    "title": "Path and HTTP methods",
    "content": "GET _cat/thread_pool . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-thread-pool/#path-and-http-methods",
    "relUrl": "/api-reference/cat/cat-thread-pool/#path-and-http-methods"
  },"2058": {
    "doc": "CAT thread pool",
    "title": "URL parameters",
    "content": "All CAT thread pool URL parameters are optional. In addition to the common URL parameters, you can specify the following parameters: . | Parameter | Type | Description | . | local | Boolean | Whether to return information from the local node only instead of from the cluster_manager node. Default is false. | . | cluster_manager_timeout | Time | The amount of time to wait for a connection to the cluster_manager node. Default is 30 seconds. | . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-thread-pool/#url-parameters",
    "relUrl": "/api-reference/cat/cat-thread-pool/#url-parameters"
  },"2059": {
    "doc": "CAT thread pool",
    "title": "Response",
    "content": "node_name name active queue rejected odfe-node2 ad-batch-task-threadpool 0 0 0 odfe-node2 ad-threadpool 0 0 0 odfe-node2 analyze 0 0 0s . ",
    "url": "https://vagimeli.github.io/api-reference/cat/cat-thread-pool/#response",
    "relUrl": "/api-reference/cat/cat-thread-pool/#response"
  },"2060": {
    "doc": "CAT API",
    "title": "CAT API",
    "content": "You can get essential statistics about your cluster in an easy-to-understand, tabular format using the compact and aligned text (CAT) API. The CAT API is a human-readable interface that returns plain text instead of traditional JSON. Using the CAT API, you can answer questions like which node is the elected master, what state is the cluster in, how many documents are in each index, and so on. ",
    "url": "https://vagimeli.github.io/api-reference/cat/index/",
    "relUrl": "/api-reference/cat/index/"
  },"2061": {
    "doc": "CAT API",
    "title": "Example",
    "content": "To see the available operations in the CAT API, use the following command: . GET _cat . copy . ",
    "url": "https://vagimeli.github.io/api-reference/cat/index/#example",
    "relUrl": "/api-reference/cat/index/#example"
  },"2062": {
    "doc": "CAT API",
    "title": "Optional query parameters",
    "content": "You can use the following query parameters with any CAT API to filter your results. | Parameter | Description | . | v | Provides verbose output by adding headers to the columns. It also adds some formatting to help align each of the columns together. All examples in this section include the v parameter. | . | help | Lists the default and other available headers for a given operation. | . | h | Limits the output to specific headers. | . | format | Returns the result in JSON, YAML, or CBOR formats. | . | sort | Sorts the output by the specified columns. | . Query parameter usage examples . You can specify a query parameter to any CAT operation to obtain more specific results. Get verbose output . To query aliases and get verbose output that includes all column headings in the response, use the v query parameter. GET _cat/aliases?v . copy . The response provides more details, such as names of each column in the response. alias index filter routing.index routing.search is_write_index .kibana .kibana_1 - - - - sample-alias1 sample-index-1 - - - - . Without the verbose parameter, v, the response simply returns the alias names: .kibana .kibana_1 - - - - sample-alias1 sample-index-1 - - - - . Get all available headers . To see all the available headers, use the help parameter: . GET _cat/&lt;operation_name&gt;?help . Get a subset of headers . To limit the output to a subset of headers, use the h parameter: . GET _cat/&lt;operation_name&gt;?h=&lt;header_name_1&gt;,&lt;header_name_2&gt;&amp;v . Typically, for any operation you can find out what headers are available using the help parameter, and then use the h parameter to limit the output to only the headers that you care about. If you use the Security plugin, make sure you have the appropriate permissions. ",
    "url": "https://vagimeli.github.io/api-reference/cat/index/#optional-query-parameters",
    "relUrl": "/api-reference/cat/index/#optional-query-parameters"
  },"2063": {
    "doc": "Cluster allocation explain",
    "title": "Cluster allocation explain",
    "content": "Introduced 1.0 . The most basic cluster allocation explain request finds an unassigned shard and explains why it can’t be allocated to a node. If you add some options, you can instead get information on a specific shard, including why OpenSearch assigned it to its current node. ",
    "url": "https://vagimeli.github.io/api-reference/cluster-api/cluster-allocation/",
    "relUrl": "/api-reference/cluster-api/cluster-allocation/"
  },"2064": {
    "doc": "Cluster allocation explain",
    "title": "Example",
    "content": "GET _cluster/allocation/explain?include_yes_decisions=true { \"index\": \"movies\", \"shard\": 0, \"primary\": true } . copy . ",
    "url": "https://vagimeli.github.io/api-reference/cluster-api/cluster-allocation/#example",
    "relUrl": "/api-reference/cluster-api/cluster-allocation/#example"
  },"2065": {
    "doc": "Cluster allocation explain",
    "title": "Path and HTTP methods",
    "content": "GET _cluster/allocation/explain POST _cluster/allocation/explain . ",
    "url": "https://vagimeli.github.io/api-reference/cluster-api/cluster-allocation/#path-and-http-methods",
    "relUrl": "/api-reference/cluster-api/cluster-allocation/#path-and-http-methods"
  },"2066": {
    "doc": "Cluster allocation explain",
    "title": "URL parameters",
    "content": "All cluster allocation explain parameters are optional. | Parameter | Type | Description | . | include_yes_decisions | Boolean | OpenSearch makes a series of yes or no decisions when trying to allocate a shard to a node. If this parameter is true, OpenSearch includes the (generally more numerous) “yes” decisions in its response. Default is false. | . | include_disk_info | Boolean | Whether to include information about disk usage in the response. Default is false. | . ",
    "url": "https://vagimeli.github.io/api-reference/cluster-api/cluster-allocation/#url-parameters",
    "relUrl": "/api-reference/cluster-api/cluster-allocation/#url-parameters"
  },"2067": {
    "doc": "Cluster allocation explain",
    "title": "Request body",
    "content": "All cluster allocation explain fields are optional. | Field | Type | Description | . | current_node | String | If you only want an explanation if the shard happens to be on a particular node, specify that node name here. | . | index | String | The name of the shard’s index. | . | primary | Boolean | Whether to provide an explanation for the primary shard (true) or its first replica (false), which share the same shard ID. | . | shard | Integer | The shard ID that you want an explanation for. | . ",
    "url": "https://vagimeli.github.io/api-reference/cluster-api/cluster-allocation/#request-body",
    "relUrl": "/api-reference/cluster-api/cluster-allocation/#request-body"
  },"2068": {
    "doc": "Cluster allocation explain",
    "title": "Response",
    "content": "{ \"index\": \"movies\", \"shard\": 0, \"primary\": true, \"current_state\": \"started\", \"current_node\": { \"id\": \"d8jRZcW1QmCBeVFlgOJx5A\", \"name\": \"opensearch-node1\", \"transport_address\": \"172.24.0.4:9300\", \"weight_ranking\": 1 }, \"can_remain_on_current_node\": \"yes\", \"can_rebalance_cluster\": \"yes\", \"can_rebalance_to_other_node\": \"no\", \"rebalance_explanation\": \"cannot rebalance as no target node exists that can both allocate this shard and improve the cluster balance\", \"node_allocation_decisions\": [{ \"node_id\": \"vRxi4uPcRt2BtHlFoyCyTQ\", \"node_name\": \"opensearch-node2\", \"transport_address\": \"172.24.0.3:9300\", \"node_decision\": \"no\", \"weight_ranking\": 1, \"deciders\": [{ \"decider\": \"max_retry\", \"decision\": \"YES\", \"explanation\": \"shard has no previous failures\" }, { \"decider\": \"replica_after_primary_active\", \"decision\": \"YES\", \"explanation\": \"shard is primary and can be allocated\" }, { \"decider\": \"enable\", \"decision\": \"YES\", \"explanation\": \"all allocations are allowed\" }, { \"decider\": \"node_version\", \"decision\": \"YES\", \"explanation\": \"can relocate primary shard from a node with version [1.0.0] to a node with equal-or-newer version [1.0.0]\" }, { \"decider\": \"snapshot_in_progress\", \"decision\": \"YES\", \"explanation\": \"no snapshots are currently running\" }, { \"decider\": \"restore_in_progress\", \"decision\": \"YES\", \"explanation\": \"ignored as shard is not being recovered from a snapshot\" }, { \"decider\": \"filter\", \"decision\": \"YES\", \"explanation\": \"node passes include/exclude/require filters\" }, { \"decider\": \"same_shard\", \"decision\": \"NO\", \"explanation\": \"a copy of this shard is already allocated to this node [[movies][0], node[vRxi4uPcRt2BtHlFoyCyTQ], [R], s[STARTED], a[id=x8w7QxWdQQa188HKGn0iMQ]]\" }, { \"decider\": \"disk_threshold\", \"decision\": \"YES\", \"explanation\": \"enough disk for shard on node, free: [35.9gb], shard size: [15.1kb], free after allocating shard: [35.9gb]\" }, { \"decider\": \"throttling\", \"decision\": \"YES\", \"explanation\": \"below shard recovery limit of outgoing: [0 &lt; 2] incoming: [0 &lt; 2]\" }, { \"decider\": \"shards_limit\", \"decision\": \"YES\", \"explanation\": \"total shard limits are disabled: [index: -1, cluster: -1] &lt;= 0\" }, { \"decider\": \"awareness\", \"decision\": \"YES\", \"explanation\": \"allocation awareness is not enabled, set cluster setting [cluster.routing.allocation.awareness.attributes] to enable it\" } ] }] } . ",
    "url": "https://vagimeli.github.io/api-reference/cluster-api/cluster-allocation/#response",
    "relUrl": "/api-reference/cluster-api/cluster-allocation/#response"
  },"2069": {
    "doc": "Cluster routing and awareness",
    "title": "Cluster routing and awareness",
    "content": "To control the distribution of search or HTTP traffic, you can use the weights per awareness attribute to control the distribution of search or HTTP traffic across zones. This is commonly used for zonal deployments, heterogeneous instances, and routing traffic away from zones during zonal failure. ",
    "url": "https://vagimeli.github.io/api-reference/cluster-api/cluster-awareness/",
    "relUrl": "/api-reference/cluster-api/cluster-awareness/"
  },"2070": {
    "doc": "Cluster routing and awareness",
    "title": "Path and HTTP methods",
    "content": "PUT /_cluster/routing/awareness/&lt;attribute&gt;/weights GET /_cluster/routing/awareness/&lt;attribute&gt;/weights?local GET /_cluster/routing/awareness/&lt;attribute&gt;/weights . ",
    "url": "https://vagimeli.github.io/api-reference/cluster-api/cluster-awareness/#path-and-http-methods",
    "relUrl": "/api-reference/cluster-api/cluster-awareness/#path-and-http-methods"
  },"2071": {
    "doc": "Cluster routing and awareness",
    "title": "Path parameters",
    "content": "| Parameter | Type | Description | . | attribute | String | The name of the awareness attribute, usually zone. The attribute name must match the values listed in the request body when assigning weights to zones. | . ",
    "url": "https://vagimeli.github.io/api-reference/cluster-api/cluster-awareness/#path-parameters",
    "relUrl": "/api-reference/cluster-api/cluster-awareness/#path-parameters"
  },"2072": {
    "doc": "Cluster routing and awareness",
    "title": "Request body parameters",
    "content": "| Parameter | Type | Description | . | weights | JSON object | Assigns weights to attributes within the request body of the PUT request. Weights can be set in any ratio, for example, 2:3:5. In a 2:3:5 ratio with 3 zones, for every 100 requests sent to the cluster, each zone would receive either 20, 30, or 50 search requests in a random order. When assigned a weight of 0, the zone does not receive any search traffic. | . | _version | String | Implements optimistic concurrency control (OCC) through versioning. The parameter uses simple versioning, such as 1, and increments upward based on each subsequent modification. This allows any servers from which a request originates to validate whether or not a zone has been modified. | . In the following example request body, zone_1 and zone_2 receive 50 requests each, whereas zone_3 is prevented from receiving requests: . { \"weights\": { \"zone_1\": \"5\", \"zone_2\": \"5\", \"zone_3\": \"0\" } \"_version\" : 1 } . ",
    "url": "https://vagimeli.github.io/api-reference/cluster-api/cluster-awareness/#request-body-parameters",
    "relUrl": "/api-reference/cluster-api/cluster-awareness/#request-body-parameters"
  },"2073": {
    "doc": "Cluster routing and awareness",
    "title": "Example: Weighted round robin search",
    "content": "The following example request creates a round robin shard allocation for search traffic by using an undefined ratio: . Request . PUT /_cluster/routing/awareness/zone/weights { \"weights\": { \"zone_1\": \"1\", \"zone_2\": \"1\", \"zone_3\": \"0\" } \"_version\" : 1 } . copy . Response . { \"acknowledged\": true } . ",
    "url": "https://vagimeli.github.io/api-reference/cluster-api/cluster-awareness/#example-weighted-round-robin-search",
    "relUrl": "/api-reference/cluster-api/cluster-awareness/#example-weighted-round-robin-search"
  },"2074": {
    "doc": "Cluster routing and awareness",
    "title": "Example: Getting weights for all zones",
    "content": "The following example request gets weights for all zones. Request . GET /_cluster/routing/awareness/zone/weights . copy . Response . OpenSearch responds with the weight of each zone: . { \"weights\": { \"zone_1\": \"1.0\", \"zone_2\": \"1.0\", \"zone_3\": \"0.0\" }, \"_version\":1 } . ",
    "url": "https://vagimeli.github.io/api-reference/cluster-api/cluster-awareness/#example-getting-weights-for-all-zones",
    "relUrl": "/api-reference/cluster-api/cluster-awareness/#example-getting-weights-for-all-zones"
  },"2075": {
    "doc": "Cluster routing and awareness",
    "title": "Example: Deleting weights",
    "content": "You can remove your weight ratio for each zone using the DELETE method. Request . DELETE /_cluster/routing/awareness/zone/weights . copy . Response . { \"_version\":1 } . ",
    "url": "https://vagimeli.github.io/api-reference/cluster-api/cluster-awareness/#example-deleting-weights",
    "relUrl": "/api-reference/cluster-api/cluster-awareness/#example-deleting-weights"
  },"2076": {
    "doc": "Cluster routing and awareness",
    "title": "Next steps",
    "content": ". | For more information about zone commissioning, see Cluster decommission. | For more information about allocation awareness, see Cluster formation. | . ",
    "url": "https://vagimeli.github.io/api-reference/cluster-api/cluster-awareness/#next-steps",
    "relUrl": "/api-reference/cluster-api/cluster-awareness/#next-steps"
  },"2077": {
    "doc": "Cluster decommission",
    "title": "Cluster decommission",
    "content": "The cluster decommission operation adds support decommissioning based on awareness. It greatly benefits multi-zone deployments, where awareness attributes, such as zones, can aid in applying new upgrades to a cluster in a controlled fashion. This is especially useful during outages, in which case, you can decommission the unhealthy zone to prevent replication requests from stalling and prevent your request backlog from becoming too large. For more information about allocation awareness, see Shard allocation awareness. ",
    "url": "https://vagimeli.github.io/api-reference/cluster-api/cluster-decommission/",
    "relUrl": "/api-reference/cluster-api/cluster-decommission/"
  },"2078": {
    "doc": "Cluster decommission",
    "title": "HTTP and Path methods",
    "content": "PUT /_cluster/decommission/awareness/{awareness_attribute_name}/{awareness_attribute_value} GET /_cluster/decommission/awareness/{awareness_attribute_name}/_status DELETE /_cluster/decommission/awareness . ",
    "url": "https://vagimeli.github.io/api-reference/cluster-api/cluster-decommission/#http-and-path-methods",
    "relUrl": "/api-reference/cluster-api/cluster-decommission/#http-and-path-methods"
  },"2079": {
    "doc": "Cluster decommission",
    "title": "URL parameters",
    "content": "| Parameter | Type | Description | . | awareness_attribute_name | String | The name of awareness attribute, usually zone. | . | awareness_attribute_value | String | The value of the awareness attribute. For example, if you have shards allocated in two different zones, you can give each zone a value of zone-a or zoneb. The cluster decommission operation decommissions the zone listed in the method. | . ",
    "url": "https://vagimeli.github.io/api-reference/cluster-api/cluster-decommission/#url-parameters",
    "relUrl": "/api-reference/cluster-api/cluster-decommission/#url-parameters"
  },"2080": {
    "doc": "Cluster decommission",
    "title": "Example: Decommissioning and recommissioning a zone",
    "content": "You can use the following example requests to decommission and recommission a zone: . Request . The following example request decommissions zone-a: . PUT /_cluster/decommission/awareness/&lt;zone&gt;/&lt;zone-a&gt; . copy . If you want to recommission a decommissioned zone, you can use the DELETE method: . DELETE /_cluster/decommission/awareness . copy . Response . { \"acknowledged\": true } . ",
    "url": "https://vagimeli.github.io/api-reference/cluster-api/cluster-decommission/#example-decommissioning-and-recommissioning-a-zone",
    "relUrl": "/api-reference/cluster-api/cluster-decommission/#example-decommissioning-and-recommissioning-a-zone"
  },"2081": {
    "doc": "Cluster decommission",
    "title": "Example: Getting zone decommission status",
    "content": "The following example requests returns the decommission status of all zones. Request . GET /_cluster/decommission/awareness/zone/_status . copy . Response . { \"zone-1\": \"INIT | DRAINING | IN_PROGRESS | SUCCESSFUL | FAILED\" } . ",
    "url": "https://vagimeli.github.io/api-reference/cluster-api/cluster-decommission/#example-getting-zone-decommission-status",
    "relUrl": "/api-reference/cluster-api/cluster-decommission/#example-getting-zone-decommission-status"
  },"2082": {
    "doc": "Cluster decommission",
    "title": "Next steps",
    "content": ". | For more information about zone awareness and weight, see Cluster awareness. | For more information about allocation awareness, see Cluster formation. | . ",
    "url": "https://vagimeli.github.io/api-reference/cluster-api/cluster-decommission/#next-steps",
    "relUrl": "/api-reference/cluster-api/cluster-decommission/#next-steps"
  },"2083": {
    "doc": "Cluster health",
    "title": "Cluster health",
    "content": "Introduced 1.0 . The most basic cluster health request returns a simple status of the health of your cluster. OpenSearch expresses cluster health in three colors: green, yellow, and red. A green status means all primary shards and their replicas are allocated to nodes. A yellow status means all primary shards are allocated to nodes, but some replicas aren’t. A red status means at least one primary shard is not allocated to any node. To get the status of a specific index, provide the index name. ",
    "url": "https://vagimeli.github.io/api-reference/cluster-api/cluster-health/",
    "relUrl": "/api-reference/cluster-api/cluster-health/"
  },"2084": {
    "doc": "Cluster health",
    "title": "Example",
    "content": "This request waits 50 seconds for the cluster to reach the yellow status or better: . GET _cluster/health?wait_for_status=yellow&amp;timeout=50s . copy . If the cluster health becomes yellow or green before 50 seconds elapse, it returns a response immediately. Otherwise it returns a response as soon as it exceeds the timeout. ",
    "url": "https://vagimeli.github.io/api-reference/cluster-api/cluster-health/#example",
    "relUrl": "/api-reference/cluster-api/cluster-health/#example"
  },"2085": {
    "doc": "Cluster health",
    "title": "Path and HTTP methods",
    "content": "GET _cluster/health GET _cluster/health/&lt;index&gt; . ",
    "url": "https://vagimeli.github.io/api-reference/cluster-api/cluster-health/#path-and-http-methods",
    "relUrl": "/api-reference/cluster-api/cluster-health/#path-and-http-methods"
  },"2086": {
    "doc": "Cluster health",
    "title": "Query parameters",
    "content": "The following table lists the available query parameters. All query parameters are optional. | Parameter | Type | Description | . | expand_wildcards | Enum | Expands wildcard expressions to concrete indexes. Combine multiple values with commas. Supported values are all, open, closed, hidden, and none. Default is open. | . | level | Enum | The level of detail for returned health information. Supported values are cluster, indices, shards, and awareness_attributes. Default is cluster. | . | awareness_attribute | String | The name of the awareness attribute, for which to return cluster health (for example, zone). Applicable only if level is set to awareness_attributes. | . | local | Boolean | Whether to return information from the local node only instead of from the cluster manager node. Default is false. | . | cluster_manager_timeout | Time | The amount of time to wait for a connection to the cluster manager node. Default is 30 seconds. | . | timeout | Time | The amount of time to wait for a response. If the timeout expires, the request fails. Default is 30 seconds. | . | wait_for_active_shards | String | Wait until the specified number of shards is active before returning a response. all for all shards. Default is 0. | . | wait_for_nodes | String | Wait for N number of nodes. Use 12 for exact match, &gt;12 and &lt;12 for range. | . | wait_for_events | Enum | Wait until all currently queued events with the given priority are processed. Supported values are immediate, urgent, high, normal, low, and languid. | . | wait_for_no_relocating_shards | Boolean | Whether to wait until there are no relocating shards in the cluster. Default is false. | . | wait_for_no_initializing_shards | Boolean | Whether to wait until there are no initializing shards in the cluster. Default is false. | . | wait_for_status | Enum | Wait until the cluster health reaches the specified status or better. Supported values are green, yellow, and red. | . | weights | JSON object | Assigns weights to attributes within the request body of the PUT request. Weights can be set in any ration, for example, 2:3:5. In a 2:3:5 ratio with three zones, for every 100 requests sent to the cluster, each zone would receive either 20, 30, or 50 search requests in a random order. When assigned a weight of 0, the zone does not receive any search traffic. | . Example request . The following example request retrieves cluster health for all indexes in the cluster: . GET _cluster/health . copy . Example response . The response contains cluster health information: . { \"cluster_name\" : \"opensearch-cluster\", \"status\" : \"green\", \"timed_out\" : false, \"number_of_nodes\" : 2, \"number_of_data_nodes\" : 2, \"discovered_master\" : true, \"active_primary_shards\" : 6, \"active_shards\" : 12, \"relocating_shards\" : 0, \"initializing_shards\" : 0, \"unassigned_shards\" : 0, \"delayed_unassigned_shards\" : 0, \"number_of_pending_tasks\" : 0, \"number_of_in_flight_fetch\" : 0, \"task_max_waiting_in_queue_millis\" : 0, \"active_shards_percent_as_number\" : 100.0 } . ",
    "url": "https://vagimeli.github.io/api-reference/cluster-api/cluster-health/#query-parameters",
    "relUrl": "/api-reference/cluster-api/cluster-health/#query-parameters"
  },"2087": {
    "doc": "Cluster health",
    "title": "Response fields",
    "content": "The following table lists all response fields. | Field | Data type | Description | . | cluster_name | String | The name of the cluster. | . | status | String | The cluster health status, which represents the state of shard allocation in the cluster. May be green, yellow, or red. | . | number_of_nodes | Integer | The number of nodes in the cluster. | . | number_of_data_nodes | Integer | The number of data nodes in the cluster. | . | discovered_cluster_manager | Boolean | Specifies whether the cluster manager is discovered. | . | active_primary_shards | Integer | The number of active primary shards. | . | active_shards | Integer | The total number of active shards, including primary and replica shards. | . | relocating_shards | Integer | The number of relocating shards. | . | initializing_shards | Integer | The number of initializing shards. | . | unassigned_shards | Integer | The number of unassigned shards. | . | delayed_unassigned_shards | Integer | The number of delayed unassigned shards. | . | number_of_pending_tasks | Integer | The number of pending tasks in the cluster. | . | number_of_in_flight_fetch | Integer | The number of unfinished fetches. | . | task_max_waiting_in_queue_millis | Integer | The maximum wait time for all tasks waiting to be performed, in milliseconds. | . | active_shards_percent_as_number | Double | The percentage of active shards in the cluster. | . | awareness_attributes | Object | Contains cluster health information for each awareness attribute. | . ",
    "url": "https://vagimeli.github.io/api-reference/cluster-api/cluster-health/#response-fields",
    "relUrl": "/api-reference/cluster-api/cluster-health/#response-fields"
  },"2088": {
    "doc": "Cluster health",
    "title": "Returning cluster health by awareness attribute",
    "content": "To check cluster health by awareness attribute (for example, zone or rack), specify awareness_attributes in the level query parameter: . GET _cluster/health?level=awareness_attributes . copy . The response contains cluster health metrics partitioned by awareness attribute: . { \"cluster_name\": \"runTask\", \"status\": \"green\", \"timed_out\": false, \"number_of_nodes\": 3, \"number_of_data_nodes\": 3, \"discovered_master\": true, \"discovered_cluster_manager\": true, \"active_primary_shards\": 0, \"active_shards\": 0, \"relocating_shards\": 0, \"initializing_shards\": 0, \"unassigned_shards\": 0, \"delayed_unassigned_shards\": 0, \"number_of_pending_tasks\": 0, \"number_of_in_flight_fetch\": 0, \"task_max_waiting_in_queue_millis\": 0, \"active_shards_percent_as_number\": 100, \"awareness_attributes\": { \"zone\": { \"zone-3\": { \"active_shards\": 0, \"initializing_shards\": 0, \"relocating_shards\": 0, \"unassigned_shards\": 0, \"data_nodes\": 1, \"weight\": 1 }, \"zone-1\": { \"active_shards\": 0, \"initializing_shards\": 0, \"relocating_shards\": 0, \"unassigned_shards\": 0, \"data_nodes\": 1, \"weight\": 1 }, \"zone-2\": { \"active_shards\": 0, \"initializing_shards\": 0, \"relocating_shards\": 0, \"unassigned_shards\": 0, \"data_nodes\": 1, \"weight\": 1 } }, \"rack\": { \"rack-3\": { \"active_shards\": 0, \"initializing_shards\": 0, \"relocating_shards\": 0, \"unassigned_shards\": 0, \"data_nodes\": 1, \"weight\": 1 }, \"rack-1\": { \"active_shards\": 0, \"initializing_shards\": 0, \"relocating_shards\": 0, \"unassigned_shards\": 0, \"data_nodes\": 1, \"weight\": 1 }, \"rack-2\": { \"active_shards\": 0, \"initializing_shards\": 0, \"relocating_shards\": 0, \"unassigned_shards\": 0, \"data_nodes\": 1, \"weight\": 1 } } } } . If you’re interested in a particular awareness attribute, you can include the name of the awareness attribute as a query parameter: . GET _cluster/health?level=awareness_attributes&amp;awareness_attribute=zone . copy . In response to the preceding request, OpenSearch returns cluster health information only for the zone awareness attribute. The unassigned shard information will be accurate only if you enable replica count enforcement and configure forced awareness for the awareness attribute either before cluster start or after cluster start but before any indexing requests. If you enable replica enforcement after the cluster receives indexing requests, the unassigned shard information may be inaccurate. If you don’t configure replica count enforcement and forced awareness, the unassigned_shards field will contain -1. ",
    "url": "https://vagimeli.github.io/api-reference/cluster-api/cluster-health/#returning-cluster-health-by-awareness-attribute",
    "relUrl": "/api-reference/cluster-api/cluster-health/#returning-cluster-health-by-awareness-attribute"
  },"2089": {
    "doc": "Cluster health",
    "title": "Required permissions",
    "content": "If you use the Security plugin, make sure you have the appropriate permissions: cluster:monitor/health. ",
    "url": "https://vagimeli.github.io/api-reference/cluster-api/cluster-health/#required-permissions",
    "relUrl": "/api-reference/cluster-api/cluster-health/#required-permissions"
  },"2090": {
    "doc": "Cluster settings",
    "title": "Cluster settings",
    "content": "Introduced 1.0 . The cluster settings operation lets you check the current settings for your cluster, review default settings, and change settings. When you update a setting using the API, OpenSearch applies it to all nodes in the cluster. Path and HTTP methods . GET _cluster/settings PUT _cluster/settings . Path parameters . All cluster setting parameters are optional. | Parameter | Data type | Description | . | flat_settings | Boolean | Whether to return settings in the flat form, which can improve readability, especially for heavily nested settings. For example, the flat form of \"cluster\": { \"max_shards_per_node\": 500 } is \"cluster.max_shards_per_node\": \"500\". | . | include_defaults (GET only) | Boolean | Whether to include default settings as part of the response. This parameter is useful for identifying the names and current values of settings you want to update. | . | cluster_manager_timeout | Time unit | The amount of time to wait for a response from the cluster manager node. Default is 30 seconds. | . | timeout (PUT only) | Time unit | The amount of time to wait for a response from the cluster. Default is 30 seconds. | . Example request . GET _cluster/settings?include_defaults=true . copy . Example response . PUT _cluster/settings { \"persistent\":{ \"action.auto_create_index\": false } } . Request fields . The GET operation has no request body options. All cluster setting field parameters are optional. Not all cluster settings can be updated using the cluster settings API. You will receive the error message \"setting [cluster.some.setting], not dynamically updateable\" when trying to configure these settings via the API. The following request field parameters are compatible with the cluster API. | Field | Data type | Description | . | action.auto_create_index | Boolean | Automatically creates an index if the index doesn’t already exist. Also applies any index templates that are configured. Default is true. | . | action.destructive_requires_name | Boolean | When set to true, you must specify the index name to delete an index. You cannot delete all indexes or use wildcards. Default is true. | . | cluster.indices.close.enable | Boolean | Enables closing of open indexes in OpenSearch. Default is true. | . | indices.recovery.max_bytes_per_sec | String | Limits the total inbound and outbound recovery traffic for each node. This applies to peer recoveries and snapshot recoveries. Default is 40mb. If you set the recovery traffic value to less than or equal to 0mb, rate limiting will be disabled, which causes recovery data to be transferred at the highest possible rate. | . | indices.recovery.max_concurrent_file_chunks | Integer | The number of file chunks sent in parallel for each recovery operation. Default is 2. | . | indices.recovery.max_concurrent_operations | Integer | The number of operations sent in parallel for each recovery. Default is 1. | . | logger.org.opensearch.discovery | String | Loggers accept Log4j2’s built-in log levels: OFF, FATAL, ERROR, WARN, INFO, DEBUG, and TRACE. Default is INFO. | . | breaker.model_inference.limit | String | The limit for the trained model circuit breaker. Default is 50% of the JVM heap. | . | breaker.model_inference.overhead | Integer | The constant that all trained model estimations are multiplied by to determine a final estimation. Default is 1. | . | search.max_buckets | Integer | The maximum number of aggregation buckets allowed in a single response. Default is 65536. | . | snapshot.max_concurrent_operations | Integer | The maximum number of concurrent snapshot operations. Default is 1000. | . | slm.health.failed_snapshot_warn_threshold | String | The number of failed invocations since the last successful snapshot that will indicate a problem as per the health API profile. Default is five repeated failures: 5L. | . | indices.breaker.total.limit | String | The starting limit for the overall parent breaker. Default is 70% of the JVM heap if indices.breaker.total.use_real_memory is set to false. Default is 95% of the JVM heap if indices.breaker.total.use_real_memory is set to true. | . | indices.breaker.fielddata.limit | String | The limit for the fielddata breaker. Default is 40% of the JVM heap. | . | indices.breaker.fielddata.overhead | Floating point | The constant that all fielddata estimations are multiplied by to determine a final estimation. Default is 1.03. | . | indices.breaker.request.limit | String | The limit for the request breaker. Default is 60% of the JVM heap. | . | indices.breaker.request.overhead | Integer | The constant that all request estimations are multiplied by to determine a final estimation. Default is 1. | . | network.breaker.inflight_requests.limit | String | The limit for the in-flight requests breaker. Default is 100% of the JVM heap. | . | network.breaker.inflight_requests.overhead | Integer/Time unit | The constant that all in-flight request estimations are multiplied by to determine a final estimation. Default is 2. | . | script.max_compilations_rate | String | The limit for the number of unique dynamic scripts within a defined interval that are allowed to be compiled. Default is 150 every 5 minutes: 150/5m. | . | cluster.routing.allocation.enable | String | Enables or disables allocation for specific kinds of shards: all – Allows shard allocation for all types of shards. primaries – Allows shard allocation for primary shards only. new_primaries – Allows shard allocation for primary shards for new indexes only. none – No shard allocations are allowed for any indexes. Default is all. | . | cluster.routing.allocation.node_concurrent_incoming_recoveries | Integer | Configures how many concurrent incoming shard recoveries are allowed to happen on a node. Default is 2. | . | cluster.routing.allocation.node_concurrent_outgoing_recoveries | Integer | Configures how many concurrent outgoing shard recoveries are allowed to happen on a node. Default is 2. | . | cluster.routing.allocation.node_concurrent_recoveries | String | Used to set cluster.routing.allocation.node_concurrent_incoming_recoveries and cluster.routing.allocation.node_concurrent_outgoing_recoveries to the same value. | . | cluster.routing.allocation.node_initial_primaries_recoveries | Integer | Sets the number of recoveries for unassigned primaries after a node restart. Default is 4. | . | cluster.routing.allocation.same_shard.host | Boolean | When set to true, multiple copies of a shard are prevented from being allocated to distinct nodes on the same host. Default is false. | . | cluster.routing.rebalance.enable | String | Enables or disables rebalancing for specific kinds of shards: all – Allows shard balancing for all types of shards. primaries – Allows shard balancing for primary shards only. replicas – Allows shard balancing for replica shards only. none – No shard balancing is allowed for any indexes. Default is all. | . | cluster.routing.allocation.allow_rebalance | String | Specifies when shard rebalancing is allowed: always – Always allow rebalancing. indices_primaries_active – Only allow rebalancing when all primaries in the cluster are allocated. indices_all_active – Only allow rebalancing when all shards in the cluster are allocated. Default is indices_all_active. | . | cluster.routing.allocation.cluster_concurrent_rebalance | Integer | Allows you to control how many concurrent shard rebalances are allowed across a cluster. Default is 2. | . | cluster.routing.allocation.balance.shard | Floating point | Defines the weight factor for the total number of shards allocated per node. Default is 0.45. | . | cluster.routing.allocation.balance.index | Floating point | Defines the weight factor for the number of shards per index allocated on a node. Default is 0.55. | . | cluster.routing.allocation.balance.threshold | Floating point | The minimum optimization value of operations that should be performed. Default is 1.0. | . | cluster.routing.allocation.balance.prefer_primary | Boolean | When set to true, OpenSearch attempts to evenly distribute the primary shards between the cluster nodes. Enabling this setting does not always guarantee an equal number of primary shards on each node, especially in the event of failover. Changing this setting to false after it was set to true does not invoke redistribution of primary shards. Default is false. | . | cluster.routing.allocation.disk.threshold_enabled | Boolean | When set to false, disables the disk allocation decider. This will also remove any existing index.blocks.read_only_allow_delete index blocks when disabled. Default is true. | . | cluster.routing.allocation.disk.watermark.low | String | Controls the low watermark for disk usage. When set to a percentage, OpenSearch will not allocate shards to nodes with that percentage of disk used. This can also be entered as ratio value, like 0.85. Finally, this can also be set to a byte value, like 400mb. This setting does not affect the primary shards of newly-created indexes, but will prevent their replicas from being allocated. Default is 85%. | . | cluster.routing.allocation.disk.watermark.high | String | Controls the high watermark. OpenSearch will attempt to relocate shards away from a node whose disk usage is above the percentage defined. This can also be entered as a ratio value, like 0.85. Finally, this can also be set to a byte value, like 400mb. This setting affects the allocation of all shards. Default is 90%. | . | cluster.routing.allocation.disk.watermark.flood_stage | String | Controls the flood stage watermark. This is a last resort to prevent nodes from running out of disk space. OpenSearch enforces a read-only index block (index.blocks.read_only_allow_delete) on every index that has one or more shards allocated on the node, and that has at least one disk exceeding the flood stage. The index block is released once the disk utilization falls below the high watermark. This can also be entered as a ratio value, like 0.85. Finally, this can also be set to a byte value, like 400mb. Default is 95%. | . | cluster.info.update.interval | Time unit | Sets how often OpenSearch should check disk usage for each node in the cluster. Default is 30s. | . | cluster.routing.allocation.include. | Enum | Allocates shards to a node whose attribute has at least one of the included comma-separated values. | . | cluster.routing.allocation.require. | Enum | Only allocates shards to a node whose attribute has all of the included comma-separated values. | . | cluster.routing.allocation.exclude. | Enum | Does not allocate shards to a node whose attribute has any of the included comma-separated values. The cluster allocation settings support the following built-in attributes: _name – Match nodes by node name. _host_ip – Match nodes by host IP address. _publish_ip – Match nodes by publish IP address. _ip – Match either _host_ip or _publish_ip. _host – Match nodes by hostname. _id – Match nodes by node ID. _tier – Match nodes by data tier role. | . | cluster.blocks.read_only | Boolean | Sets the entire cluster to read-only. Default is false. | . | cluster.blocks.read_only_allow_delete | Boolean | Similar to cluster.blocks.read_only but allows you to delete indexes. | . | cluster.max_shards_per_node | Integer | Limits the total number of primary and replica shards for the cluster. The limit is calculated as follows: cluster.max_shards_per_node multiplied by the number of non-frozen data nodes. Shards for closed indexes do not count toward this limit. Default is 1000. | . | cluster.persistent_tasks.allocation.enable | String | Enables or disables allocation for persistent tasks: all – Allows persistent tasks to be assigned to nodes. none – No allocations are allowed for persistent tasks. This does not affect persistent tasks already running. Default is all. | . | cluster.persistent_tasks.allocation.recheck_interval | Time unit | The cluster manager automatically checks whether or not persistent tasks need to be assigned when the cluster state changes in a significant way. There are other factors, such as memory usage, that will affect whether or not persistent tasks are assigned to nodes but do not otherwise cause the cluster state to change. This setting defines how often assignment checks are performed in response to these factors. Default is 30 seconds, with a minimum of 10 seconds being required. | . Example request . For a PUT operation, the request body must contain transient or persistent, along with the setting you want to update: . PUT _cluster/settings { \"persistent\":{ \"cluster.max_shards_per_node\": 500 } } . copy . For more information about transient settings, persistent settings, and precedence, see OpenSearch configuration. Example response . { \"acknowledged\":true, \"persistent\":{ \"cluster\":{ \"max_shards_per_node\":\"500\" } }, \"transient\":{} } . ",
    "url": "https://vagimeli.github.io/api-reference/cluster-api/cluster-settings/",
    "relUrl": "/api-reference/cluster-api/cluster-settings/"
  },"2091": {
    "doc": "Cluster stats",
    "title": "Cluster stats",
    "content": "Introduced 1.0 . The cluster stats API operation returns statistics about your cluster. ",
    "url": "https://vagimeli.github.io/api-reference/cluster-api/cluster-stats/",
    "relUrl": "/api-reference/cluster-api/cluster-stats/"
  },"2092": {
    "doc": "Cluster stats",
    "title": "Examples",
    "content": "GET _cluster/stats/nodes/_cluster_manager . copy . ",
    "url": "https://vagimeli.github.io/api-reference/cluster-api/cluster-stats/#examples",
    "relUrl": "/api-reference/cluster-api/cluster-stats/#examples"
  },"2093": {
    "doc": "Cluster stats",
    "title": "Path and HTTP methods",
    "content": "GET _cluster/stats GET _cluster/stats/nodes/&lt;node-filters&gt; . ",
    "url": "https://vagimeli.github.io/api-reference/cluster-api/cluster-stats/#path-and-http-methods",
    "relUrl": "/api-reference/cluster-api/cluster-stats/#path-and-http-methods"
  },"2094": {
    "doc": "Cluster stats",
    "title": "URL parameters",
    "content": "All cluster stats parameters are optional. | Parameter | Type | Description | . | &lt;node-filters&gt; | List | A comma-separated list of node filters that OpenSearch uses to filter results. | . Although the master node is now called cluster_manager for version 2.0, we retained the master field for backwards compatibility. If you have a node that has either a master role or a cluster_manager role, the count increases for both fields by 1. To see an example node count increase, see the Response sample. ",
    "url": "https://vagimeli.github.io/api-reference/cluster-api/cluster-stats/#url-parameters",
    "relUrl": "/api-reference/cluster-api/cluster-stats/#url-parameters"
  },"2095": {
    "doc": "Cluster stats",
    "title": "Response",
    "content": "{ \"_nodes\": { \"total\": 1, \"successful\": 1, \"failed\": 0 }, \"cluster_name\": \"opensearch-cluster\", \"cluster_uuid\": \"QravFieJS_SlZJyBMcDMqQ\", \"timestamp\": 1644607845054, \"status\": \"yellow\", \"indices\": { \"count\": 114, \"shards\": { \"total\": 121, \"primaries\": 60, \"replication\": 1.0166666666666666, \"index\": { \"shards\": { \"min\": 1, \"max\": 2, \"avg\": 1.0614035087719298 }, \"primaries\": { \"min\": 0, \"max\": 2, \"avg\": 0.5263157894736842 }, \"replication\": { \"min\": 0.0, \"max\": 1.0, \"avg\": 0.008771929824561403 } } }, \"docs\": { \"count\": 134263, \"deleted\": 115 }, \"store\": { \"size_in_bytes\": 70466547, \"reserved_in_bytes\": 0 }, \"fielddata\": { \"memory_size_in_bytes\": 664, \"evictions\": 0 }, \"query_cache\": { \"memory_size_in_bytes\": 0, \"total_count\": 1, \"hit_count\": 0, \"miss_count\": 1, \"cache_size\": 0, \"cache_count\": 0, \"evictions\": 0 }, \"completion\": { \"size_in_bytes\": 0 }, \"segments\": { \"count\": 341, \"memory_in_bytes\": 3137244, \"terms_memory_in_bytes\": 2488992, \"stored_fields_memory_in_bytes\": 167672, \"term_vectors_memory_in_bytes\": 0, \"norms_memory_in_bytes\": 346816, \"points_memory_in_bytes\": 0, \"doc_values_memory_in_bytes\": 133764, \"index_writer_memory_in_bytes\": 0, \"version_map_memory_in_bytes\": 0, \"fixed_bit_set_memory_in_bytes\": 1112, \"max_unsafe_auto_id_timestamp\": 1644269449096, \"file_sizes\": {} }, \"mappings\": { \"field_types\": [ { \"name\": \"alias\", \"count\": 1, \"index_count\": 1 }, { \"name\": \"binary\", \"count\": 1, \"index_count\": 1 }, { \"name\": \"boolean\", \"count\": 87, \"index_count\": 22 }, { \"name\": \"date\", \"count\": 185, \"index_count\": 91 }, { \"name\": \"double\", \"count\": 5, \"index_count\": 2 }, { \"name\": \"float\", \"count\": 4, \"index_count\": 1 }, { \"name\": \"geo_point\", \"count\": 4, \"index_count\": 3 }, { \"name\": \"half_float\", \"count\": 12, \"index_count\": 1 }, { \"name\": \"integer\", \"count\": 144, \"index_count\": 29 }, { \"name\": \"ip\", \"count\": 2, \"index_count\": 1 }, { \"name\": \"keyword\", \"count\": 1939, \"index_count\": 109 }, { \"name\": \"knn_vector\", \"count\": 1, \"index_count\": 1 }, { \"name\": \"long\", \"count\": 158, \"index_count\": 92 }, { \"name\": \"nested\", \"count\": 25, \"index_count\": 10 }, { \"name\": \"object\", \"count\": 420, \"index_count\": 91 }, { \"name\": \"text\", \"count\": 1768, \"index_count\": 102 } ] }, \"analysis\": { \"char_filter_types\": [], \"tokenizer_types\": [], \"filter_types\": [], \"analyzer_types\": [], \"built_in_char_filters\": [], \"built_in_tokenizers\": [], \"built_in_filters\": [], \"built_in_analyzers\": [ { \"name\": \"english\", \"count\": 1, \"index_count\": 1 } ] } }, \"nodes\": { \"count\": { \"total\": 1, \"coordinating_only\": 0, \"data\": 1, \"ingest\": 1, \"master\": 1, \"cluster_manager\": 1, \"remote_cluster_client\": 1 }, \"versions\": [ \"1.2.4\" ], \"os\": { \"available_processors\": 6, \"allocated_processors\": 6, \"names\": [ { \"name\": \"Linux\", \"count\": 1 } ], \"pretty_names\": [ { \"pretty_name\": \"Amazon Linux 2\", \"count\": 1 } ], \"mem\": { \"total_in_bytes\": 6232674304, \"free_in_bytes\": 1452658688, \"used_in_bytes\": 4780015616, \"free_percent\": 23, \"used_percent\": 77 } }, \"process\": { \"cpu\": { \"percent\": 0 }, \"open_file_descriptors\": { \"min\": 970, \"max\": 970, \"avg\": 970 } }, \"jvm\": { \"max_uptime_in_millis\": 108800629, \"versions\": [ { \"version\": \"15.0.1\", \"vm_name\": \"OpenJDK 64-Bit Server VM\", \"vm_version\": \"15.0.1+9\", \"vm_vendor\": \"AdoptOpenJDK\", \"bundled_jdk\": true, \"using_bundled_jdk\": true, \"count\": 1 } ], \"mem\": { \"heap_used_in_bytes\": 178956256, \"heap_max_in_bytes\": 536870912 }, \"threads\": 112 }, \"fs\": { \"total_in_bytes\": 62725623808, \"free_in_bytes\": 28442726400, \"available_in_bytes\": 25226010624 }, \"plugins\": [ { \"name\": \"opensearch-index-management\", \"version\": \"1.2.4.0\", \"opensearch_version\": \"1.2.4\", \"java_version\": \"1.8\", \"description\": \"OpenSearch Index Management Plugin\", \"classname\": \"org.opensearch.indexmanagement.IndexManagementPlugin\", \"custom_foldername\": \"\", \"extended_plugins\": [ \"opensearch-job-scheduler\" ], \"has_native_controller\": false }, { \"name\": \"opensearch-security\", \"version\": \"1.2.4.0\", \"opensearch_version\": \"1.2.4\", \"java_version\": \"1.8\", \"description\": \"Provide access control related features for OpenSearch 1.0.0\", \"classname\": \"org.opensearch.security.OpenSearchSecurityPlugin\", \"custom_foldername\": \"opensearch-security\", \"extended_plugins\": [], \"has_native_controller\": false }, { \"name\": \"opensearch-cross-cluster-replication\", \"version\": \"1.2.4.0\", \"opensearch_version\": \"1.2.4\", \"java_version\": \"1.8\", \"description\": \"OpenSearch Cross Cluster Replication Plugin\", \"classname\": \"org.opensearch.replication.ReplicationPlugin\", \"custom_foldername\": \"\", \"extended_plugins\": [], \"has_native_controller\": false }, { \"name\": \"opensearch-job-scheduler\", \"version\": \"1.2.4.0\", \"opensearch_version\": \"1.2.4\", \"java_version\": \"1.8\", \"description\": \"OpenSearch Job Scheduler plugin\", \"classname\": \"org.opensearch.jobscheduler.JobSchedulerPlugin\", \"custom_foldername\": \"\", \"extended_plugins\": [], \"has_native_controller\": false }, { \"name\": \"opensearch-anomaly-detection\", \"version\": \"1.2.4.0\", \"opensearch_version\": \"1.2.4\", \"java_version\": \"1.8\", \"description\": \"OpenSearch anomaly detector plugin\", \"classname\": \"org.opensearch.ad.AnomalyDetectorPlugin\", \"custom_foldername\": \"\", \"extended_plugins\": [ \"lang-painless\", \"opensearch-job-scheduler\" ], \"has_native_controller\": false }, { \"name\": \"opensearch-performance-analyzer\", \"version\": \"1.2.4.0\", \"opensearch_version\": \"1.2.4\", \"java_version\": \"1.8\", \"description\": \"OpenSearch Performance Analyzer Plugin\", \"classname\": \"org.opensearch.performanceanalyzer.PerformanceAnalyzerPlugin\", \"custom_foldername\": \"\", \"extended_plugins\": [], \"has_native_controller\": false }, { \"name\": \"opensearch-reports-scheduler\", \"version\": \"1.2.4.0\", \"opensearch_version\": \"1.2.4\", \"java_version\": \"1.8\", \"description\": \"Scheduler for Dashboards Reports Plugin\", \"classname\": \"org.opensearch.reportsscheduler.ReportsSchedulerPlugin\", \"custom_foldername\": \"\", \"extended_plugins\": [ \"opensearch-job-scheduler\" ], \"has_native_controller\": false }, { \"name\": \"opensearch-asynchronous-search\", \"version\": \"1.2.4.0\", \"opensearch_version\": \"1.2.4\", \"java_version\": \"1.8\", \"description\": \"Provides support for asynchronous search\", \"classname\": \"org.opensearch.search.asynchronous.plugin.AsynchronousSearchPlugin\", \"custom_foldername\": \"\", \"extended_plugins\": [], \"has_native_controller\": false }, { \"name\": \"opensearch-knn\", \"version\": \"1.2.4.0\", \"opensearch_version\": \"1.2.4\", \"java_version\": \"1.8\", \"description\": \"OpenSearch k-NN plugin\", \"classname\": \"org.opensearch.knn.plugin.KNNPlugin\", \"custom_foldername\": \"\", \"extended_plugins\": [ \"lang-painless\" ], \"has_native_controller\": false }, { \"name\": \"opensearch-alerting\", \"version\": \"1.2.4.0\", \"opensearch_version\": \"1.2.4\", \"java_version\": \"1.8\", \"description\": \"Amazon OpenSearch alerting plugin\", \"classname\": \"org.opensearch.alerting.AlertingPlugin\", \"custom_foldername\": \"\", \"extended_plugins\": [ \"lang-painless\" ], \"has_native_controller\": false }, { \"name\": \"opensearch-observability\", \"version\": \"1.2.4.0\", \"opensearch_version\": \"1.2.4\", \"java_version\": \"1.8\", \"description\": \"OpenSearch Plugin for OpenSearch Dashboards Observability\", \"classname\": \"org.opensearch.observability.ObservabilityPlugin\", \"custom_foldername\": \"\", \"extended_plugins\": [], \"has_native_controller\": false }, { \"name\": \"opensearch-sql\", \"version\": \"1.2.4.0\", \"opensearch_version\": \"1.2.4\", \"java_version\": \"1.8\", \"description\": \"OpenSearch SQL\", \"classname\": \"org.opensearch.sql.plugin.SQLPlugin\", \"custom_foldername\": \"\", \"extended_plugins\": [], \"has_native_controller\": false } ], \"network_types\": { \"transport_types\": { \"org.opensearch.security.ssl.http.netty.SecuritySSLNettyTransport\": 1 }, \"http_types\": { \"org.opensearch.security.http.SecurityHttpServerTransport\": 1 } }, \"discovery_types\": { \"zen\": 1 }, \"packaging_types\": [ { \"type\": \"tar\", \"count\": 1 } ], \"ingest\": { \"number_of_pipelines\": 0, \"processor_stats\": {} } } } . ",
    "url": "https://vagimeli.github.io/api-reference/cluster-api/cluster-stats/#response",
    "relUrl": "/api-reference/cluster-api/cluster-stats/#response"
  },"2096": {
    "doc": "Cluster stats",
    "title": "Response body fields",
    "content": "| Field | Description | . | nodes | How many nodes returned in the response. | . | cluster_name | The cluster’s name. | . | cluster_uuid | The cluster’s uuid. | . | timestamp | The Unix epoch time of when the cluster was last refreshed. | . | status | The cluster’s health status. | . | indices | Statistics about the indexes in the cluster. | . | indices.count | How many indexes are in the cluster. | . | indices.shards | Information about the cluster’s shards. | . | indices.docs | How many documents are still in the cluster and how many documents are deleted. | . | indices.store | Information about the cluster’s storage. | . | indices.fielddata | Information about the cluster’s field data | . | indices.query_cache | Data about the cluster’s query cache. | . | indices.completion | How many bytes in memory are used to complete operations. | . | indices.segments | Information about the cluster’s segments, which are small Lucene indexes. | . | indices.mappings | Mappings within the cluster. | . | indices.analysis | Information about analyzers used in the cluster. | . | nodes | Statistics about the nodes in the cluster. | . | nodes.count | How many nodes were returned from the request. | . | nodes.versions | OpenSearch’s version number. | . | nodes.os | Information about the operating systems used in the nodes. | . | nodes.process | The processes the returned nodes use. | . | nodes.jvm | Statistics about the Java Virtual Machines in use. | . | nodes.fs | The nodes’ file storage. | . | nodes.plugins | The OpenSearch plugins integrated within the nodes. | . | nodes.network_types | The transport and HTTP networks within the nodes. | . | nodes.discovery_type | The method the nodes use to find other nodes within the cluster. | . | nodes.packaging_types | Information about the nodes’ OpenSearch distribution. | . | nodes.ingest | Information about the nodes’ ingest pipelines/nodes, if there are any. | . ",
    "url": "https://vagimeli.github.io/api-reference/cluster-api/cluster-stats/#response-body-fields",
    "relUrl": "/api-reference/cluster-api/cluster-stats/#response-body-fields"
  },"2097": {
    "doc": "Cluster APIs",
    "title": "Cluster APIs",
    "content": "The cluster APIs allow you to manage your cluster. You can use them to check cluster health, modify settings, retrieve statistics, and more. ",
    "url": "https://vagimeli.github.io/api-reference/cluster-api/index/",
    "relUrl": "/api-reference/cluster-api/index/"
  },"2098": {
    "doc": "Common REST Parameters",
    "title": "Common REST parameters",
    "content": "OpenSearch supports the following parameters for all REST operations: . ",
    "url": "https://vagimeli.github.io/api-reference/common-parameters/#common-rest-parameters",
    "relUrl": "/api-reference/common-parameters/#common-rest-parameters"
  },"2099": {
    "doc": "Common REST Parameters",
    "title": "Human-readable output",
    "content": "To convert output units to human-readable values (for example, 1h for 1 hour and 1kb for 1,024 bytes), add ?human=true to the request URL. Example request . The following request requires response values to be in human-readable format: . GET &lt;index_name&gt;/_search?human=true . ",
    "url": "https://vagimeli.github.io/api-reference/common-parameters/#human-readable-output",
    "relUrl": "/api-reference/common-parameters/#human-readable-output"
  },"2100": {
    "doc": "Common REST Parameters",
    "title": "Pretty result",
    "content": "To get back JSON responses in a readable format, add ?pretty=true to the request URL. Example request . The following request requires the response to be displayed in pretty JSON format: . GET &lt;index_name&gt;/_search?pretty=true . ",
    "url": "https://vagimeli.github.io/api-reference/common-parameters/#pretty-result",
    "relUrl": "/api-reference/common-parameters/#pretty-result"
  },"2101": {
    "doc": "Common REST Parameters",
    "title": "Content type",
    "content": "To specify the type of content in the request body, use the Content-Type key name in the request header. Most operations support JSON, YAML, and CBOR formats. Example request . The following request specifies JSON format for the request body: . curl -H \"Content-type: application/json\" -XGET localhost:9200/_scripts/&lt;template_name&gt; . ",
    "url": "https://vagimeli.github.io/api-reference/common-parameters/#content-type",
    "relUrl": "/api-reference/common-parameters/#content-type"
  },"2102": {
    "doc": "Common REST Parameters",
    "title": "Request body in query string",
    "content": "If the client library does not accept a request body for non-POST requests, use the source query string parameter to pass the request body. Also, specify the source_content_type parameter with a supported media type such as application/json. Example request . The following request searches the documents in the shakespeare index for a specific field and value: . GET shakespeare/search?source={\"query\":{\"exists\":{\"field\":\"speaker\"}}}&amp;source_content_type=application/json . ",
    "url": "https://vagimeli.github.io/api-reference/common-parameters/#request-body-in-query-string",
    "relUrl": "/api-reference/common-parameters/#request-body-in-query-string"
  },"2103": {
    "doc": "Common REST Parameters",
    "title": "Stack traces",
    "content": "To include the error stack trace in the response when an exception is raised, add error_trace=true to the request URL. Example request . The following request sets error_trace to true so that the response returns exception-triggered errors: . GET &lt;index_name&gt;/_search?error_trace=true . ",
    "url": "https://vagimeli.github.io/api-reference/common-parameters/#stack-traces",
    "relUrl": "/api-reference/common-parameters/#stack-traces"
  },"2104": {
    "doc": "Common REST Parameters",
    "title": "Filtered responses",
    "content": "To reduce the response size use the filter_path parameter to filter the fields that are returned. This parameter takes a comma-separated list of filters. It supports using wildcards to match any field or part of a field’s name. You can also exclude fields with -. Example request . The following request specifies filters to limit the fields returned in the response: . GET _search?filter_path=&lt;field_name&gt;.*,-&lt;field_name&gt; . ",
    "url": "https://vagimeli.github.io/api-reference/common-parameters/#filtered-responses",
    "relUrl": "/api-reference/common-parameters/#filtered-responses"
  },"2105": {
    "doc": "Common REST Parameters",
    "title": "Common REST Parameters",
    "content": " ",
    "url": "https://vagimeli.github.io/api-reference/common-parameters/",
    "relUrl": "/api-reference/common-parameters/"
  },"2106": {
    "doc": "Count",
    "title": "Count",
    "content": "Introduced 1.0 . The count API gives you quick access to the number of documents that match a query. You can also use it to check the document count of an index, data stream, or cluster. ",
    "url": "https://vagimeli.github.io/api-reference/count/",
    "relUrl": "/api-reference/count/"
  },"2107": {
    "doc": "Count",
    "title": "Example",
    "content": "To see the number of documents that match a query: . GET opensearch_dashboards_sample_data_logs/_count { \"query\": { \"term\": { \"response\": \"200\" } } } . copy . The following call to the search API produces equivalent results: . GET opensearch_dashboards_sample_data_logs/_search { \"query\": { \"term\": { \"response\": \"200\" } }, \"size\": 0, \"track_total_hits\": true } . copy . To see the number of documents in an index: . GET opensearch_dashboards_sample_data_logs/_count . copy . To check for the number of documents in a data stream, replace the index name with the data stream name. To see the number of documents in your cluster: . GET _count . copy . Alternatively, you could use the cat indices and cat count APIs to see the number of documents per index or data stream. ",
    "url": "https://vagimeli.github.io/api-reference/count/#example",
    "relUrl": "/api-reference/count/#example"
  },"2108": {
    "doc": "Count",
    "title": "Path and HTTP methods",
    "content": "GET &lt;target&gt;/_count/&lt;id&gt; POST &lt;target&gt;/_count/&lt;id&gt; . ",
    "url": "https://vagimeli.github.io/api-reference/count/#path-and-http-methods",
    "relUrl": "/api-reference/count/#path-and-http-methods"
  },"2109": {
    "doc": "Count",
    "title": "URL parameters",
    "content": "All count parameters are optional. | Parameter | Type | Description | . | allow_no_indices | Boolean | If false, the request returns an error if any wildcard expression or index alias targets any closed or missing indexes. Default is false. | . | analyzer | String | The analyzer to use in the query string. | . | analyze_wildcard | Boolean | Specifies whether to analyze wildcard and prefix queries. Default is false. | . | default_operator | String | Indicates whether the default operator for a string query should be AND or OR. Default is OR. | . | df | String | The default field in case a field prefix is not provided in the query string. | . | expand_wildcards | String | Specifies the type of index that wildcard expressions can match. Supports comma-separated values. Valid values are all (match any index), open (match open, non-hidden indexes), closed (match closed, non-hidden indexes), hidden (match hidden indexes), and none (deny wildcard expressions). Default is open. | . | ignore_unavailable | Boolean | Specifies whether to include missing or closed indexes in the response. Default is false. | . | lenient | Boolean | Specifies whether OpenSearch should accept requests if queries have format errors (for example, querying a text field for an integer). Default is false. | . | min_score | Float | Include only documents with a minimum _score value in the result. | . | routing | String | Value used to route the operation to a specific shard. | . | preference | String | Specifies which shard or node OpenSearch should perform the count operation on. | . | terminate_after | Integer | The maximum number of documents OpenSearch should process before terminating the request. | . ",
    "url": "https://vagimeli.github.io/api-reference/count/#url-parameters",
    "relUrl": "/api-reference/count/#url-parameters"
  },"2110": {
    "doc": "Count",
    "title": "Response",
    "content": "{ \"count\" : 14074, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 } } . ",
    "url": "https://vagimeli.github.io/api-reference/count/#response",
    "relUrl": "/api-reference/count/#response"
  },"2111": {
    "doc": "Bulk",
    "title": "Bulk",
    "content": "Introduced 1.0 . The bulk operation lets you add, update, or delete multiple documents in a single request. Compared to individual OpenSearch indexing requests, the bulk operation has significant performance benefits. Whenever practical, we recommend batching indexing operations into bulk requests. ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/bulk/",
    "relUrl": "/api-reference/document-apis/bulk/"
  },"2112": {
    "doc": "Bulk",
    "title": "Example",
    "content": "POST _bulk { \"delete\": { \"_index\": \"movies\", \"_id\": \"tt2229499\" } } { \"index\": { \"_index\": \"movies\", \"_id\": \"tt1979320\" } } { \"title\": \"Rush\", \"year\": 2013 } { \"create\": { \"_index\": \"movies\", \"_id\": \"tt1392214\" } } { \"title\": \"Prisoners\", \"year\": 2013 } { \"update\": { \"_index\": \"movies\", \"_id\": \"tt0816711\" } } { \"doc\" : { \"title\": \"World War Z\" } } . copy . ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/bulk/#example",
    "relUrl": "/api-reference/document-apis/bulk/#example"
  },"2113": {
    "doc": "Bulk",
    "title": "Path and HTTP methods",
    "content": "POST _bulk POST &lt;index&gt;/_bulk . Specifying the index in the path means you don’t need to include it in the request body. OpenSearch also accepts PUT requests to the _bulk path, but we highly recommend using POST. The accepted usage of PUT—adding or replacing a single resource at a given path—doesn’t make sense for bulk requests. ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/bulk/#path-and-http-methods",
    "relUrl": "/api-reference/document-apis/bulk/#path-and-http-methods"
  },"2114": {
    "doc": "Bulk",
    "title": "URL parameters",
    "content": "All bulk URL parameters are optional. | Parameter | Type | Description | . | pipeline | String | The pipeline ID for preprocessing documents. | . | refresh | Enum | Whether to refresh the affected shards after performing the indexing operations. Default is false. true makes the changes show up in search results immediately, but hurts cluster performance. wait_for waits for a refresh. Requests take longer to return, but cluster performance doesn’t suffer. | . | require_alias | Boolean | Set to true to require that all actions target an index alias rather than an index. Default is false. | . | routing | String | Routes the request to the specified shard. | . | timeout | Time | How long to wait for the request to return. Default 1m. | . | type | String | (Deprecated) The default document type for documents that don’t specify a type. Default is _doc. We highly recommend ignoring this parameter and using a type of _doc for all indices. | . | wait_for_active_shards | String | Specifies the number of active shards that must be available before OpenSearch processes the bulk request. Default is 1 (only the primary shard). Set to all or a positive integer. Values greater than 1 require replicas. For example, if you specify a value of 3, the index must have two replicas distributed across two additional nodes for the request to succeed. | . ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/bulk/#url-parameters",
    "relUrl": "/api-reference/document-apis/bulk/#url-parameters"
  },"2115": {
    "doc": "Bulk",
    "title": "Request body",
    "content": "The bulk request body follows this pattern: . Action and metadata\\n Optional document\\n Action and metadata\\n Optional document\\n . The optional JSON document doesn’t need to be minified—spaces are fine—but it does need to be on a single line. OpenSearch uses newline characters to parse bulk requests and requires that the request body end with a newline character. All actions support the same metadata: _index, _id, and _require_alias. If you don’t provide an ID, OpenSearch generates one automatically, which can make it challenging to update the document at a later time. | Create . Creates a document if it doesn’t already exist and returns an error otherwise. The next line must include a JSON document. { \"create\": { \"_index\": \"movies\", \"_id\": \"tt1392214\" } } { \"title\": \"Prisoners\", \"year\": 2013 } . | Delete . This action deletes a document if it exists. If the document doesn’t exist, OpenSearch doesn’t return an error, but instead returns not_found under result. Delete actions don’t require documents on the next line. { \"delete\": { \"_index\": \"movies\", \"_id\": \"tt2229499\" } } . | Index . Index actions create a document if it doesn’t yet exist and replace the document if it already exists. The next line must include a JSON document. { \"index\": { \"_index\": \"movies\", \"_id\": \"tt1979320\" } } { \"title\": \"Rush\", \"year\": 2013} . | Update . This action updates existing documents and returns an error if the document doesn’t exist. The next line must include a full or partial JSON document, depending on how much of the document you want to update. { \"update\": { \"_index\": \"movies\", \"_id\": \"tt0816711\" } } { \"doc\" : { \"title\": \"World War Z\" } } . It can also include a script or upsert for more complex document updates. | Script { \"update\": { \"_index\": \"movies\", \"_id\": \"tt0816711\" } } { \"script\" : { \"source\": \"ctx._source.title = \\\"World War Z\\\"\" } } . | Upsert { \"update\": { \"_index\": \"movies\", \"_id\": \"tt0816711\" } } { \"doc\" : { \"title\": \"World War Z\" }, \"doc_as_upsert\": true } . | . | . ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/bulk/#request-body",
    "relUrl": "/api-reference/document-apis/bulk/#request-body"
  },"2116": {
    "doc": "Bulk",
    "title": "Response",
    "content": "In the response, pay particular attention to the top-level errors boolean. If true, you can iterate over the individual actions for more detailed information. { \"took\": 11, \"errors\": true, \"items\": [ { \"index\": { \"_index\": \"movies\", \"_id\": \"tt1979320\", \"_version\": 1, \"result\": \"created\", \"_shards\": { \"total\": 2, \"successful\": 1, \"failed\": 0 }, \"_seq_no\": 1, \"_primary_term\": 1, \"status\": 201 } }, { \"create\": { \"_index\": \"movies\", \"_id\": \"tt1392214\", \"status\": 409, \"error\": { \"type\": \"version_conflict_engine_exception\", \"reason\": \"[tt1392214]: version conflict, document already exists (current version [1])\", \"index\": \"movies\", \"shard\": \"0\", \"index_uuid\": \"yhizhusbSWmP0G7OJnmcLg\" } } }, { \"update\": { \"_index\": \"movies\", \"_id\": \"tt0816711\", \"status\": 404, \"error\": { \"type\": \"document_missing_exception\", \"reason\": \"[_doc][tt0816711]: document missing\", \"index\": \"movies\", \"shard\": \"0\", \"index_uuid\": \"yhizhusbSWmP0G7OJnmcLg\" } } } ] } . ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/bulk/#response",
    "relUrl": "/api-reference/document-apis/bulk/#response"
  },"2117": {
    "doc": "Delete by query",
    "title": "Delete by query",
    "content": "Introduced 1.0 . You can include a query as part of your delete request so OpenSearch deletes all documents that match that query. ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/delete-by-query/",
    "relUrl": "/api-reference/document-apis/delete-by-query/"
  },"2118": {
    "doc": "Delete by query",
    "title": "Example",
    "content": "POST sample-index1/_delete_by_query { \"query\": { \"match\": { \"movie-length\": \"124\" } } } . copy . ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/delete-by-query/#example",
    "relUrl": "/api-reference/document-apis/delete-by-query/#example"
  },"2119": {
    "doc": "Delete by query",
    "title": "Path and HTTP methods",
    "content": "POST &lt;index&gt;/_delete_by_query . ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/delete-by-query/#path-and-http-methods",
    "relUrl": "/api-reference/document-apis/delete-by-query/#path-and-http-methods"
  },"2120": {
    "doc": "Delete by query",
    "title": "URL parameters",
    "content": "All URL parameters are optional. | Parameter | Type | Description | . | &lt;index&gt; | String | Name or list of the data streams, indices, or aliases to delete from. Supports wildcards. If left blank, OpenSearch searches all indices. | . | allow_no_indices | Boolean | Whether to ignore wildcards that don’t match any indices. Default is true. | . | analyzer | String | The analyzer to use in the query string. | . | analyze_wildcard | Boolean | Specifies whether to analyze wildcard and prefix queries. Default is false. | . | conflicts | String | Indicates to OpenSearch what should happen if the delete by query operation runs into a version conflict. Valid options are abort and proceed. Default is abort. | . | default_operator | String | Indicates whether the default operator for a string query should be AND or OR. Default is OR. | . | df | String | The default field in case a field prefix is not provided in the query string. | . | expand_wildcards | String | Specifies the type of index that wildcard expressions can match. Supports comma-separated values. Valid values are all (match any index), open (match open, non-hidden indices), closed (match closed, non-hidden indices), hidden (match hidden indices), and none (deny wildcard expressions). Default is open. | . | from | Integer | The starting index to search from. Default is 0. | . | ignore_unavailable | Boolean | Specifies whether to include missing or closed indices in the response. Default is false. | . | lenient | Boolean | Specifies whether OpenSearch should accept requests if queries have format errors (for example, querying a text field for an integer). Default is false. | . | max_docs | Integer | How many documents the delete by query operation should process at most. Default is all documents. | . | preference | String | Specifies which shard or node OpenSearch should perform the delete by query operation on. | . | q | String | Lucene query string’s query. | . | request_cache | Boolean | Specifies whether OpenSearch should use the request cache. Default is whether it’s enabled in the index’s settings. | . | refresh | Boolean | If true, OpenSearch refreshes shards to make the delete by query operation available to search results. Valid options are true, false, and wait_for, which tells OpenSearch to wait for a refresh before executing the operation. Default is false. | . | requests_per_second | Integer | Specifies the request’s throttling in sub-requests per second. Default is -1, which means no throttling. | . | routing | String | Value used to route the operation to a specific shard. | . | scroll | Time | Amount of time the search context should be open. | . | scroll_size | Integer | Size of the operation’s scroll requests. Default is 1000. | . | search_type | String | Whether OpenSearch should use global term and document frequencies calculating revelance scores. Valid choices are query_then_fetch and dfs_query_then_fetch. query_then_fetch scores documents using local term and document frequencies for the shard. It’s usually faster but less accurate. dfs_query_then_fetch scores documents using global term and document frequencies across all shards. It’s usually slower but more accurate. Default is query_then_fetch. | . | search_timeout | Time | How long to wait until OpenSearch deems the request timed out. Default is no timeout. | . | slices | String or Integer | How many slices to cut the operation into for faster processing. Specify an integer to set how many slices to divide the operation into, or use auto, which tells OpenSearch it should decide how many slices to divide into. If you have a lot of shards in your index, set a lower number for better efficiency. Default is 1, which means the task should not be divided. | . | sort | String | A comma-separated list of &lt;field&gt; : &lt;direction&gt; pairs to sort by. | . | _source | String | Specifies whether to include the _source field in the response. | . | _source_excludes | String | A comma-separated list of source fields to exclude from the response. | . | _source_includes | String | A comma-separated list of source fields to include in the response. | . | stats | String | Value to associate with the request for additional logging. | . | terminate_after | Integer | The maximum number of documents OpenSearch should process before terminating the request. | . | timeout | Time | How long the operation should wait from a response from active shards. Default is 1m. | . | version | Boolean | Whether to include the document version as a match. | . | wait_for_active_shards | String | The number of shards that must be active before OpenSearch executes the operation. Valid values are all or any integer up to the total number of shards in the index. Default is 1, which is the primary shard. | . | wait_for_completion | Boolean | Setting this parameter to false indicates to OpenSearch it should not wait for completion and perform this request asynchronously. Asynchronous requests run in the background, and you can use the Tasks API to monitor progress. | . ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/delete-by-query/#url-parameters",
    "relUrl": "/api-reference/document-apis/delete-by-query/#url-parameters"
  },"2121": {
    "doc": "Delete by query",
    "title": "Request body",
    "content": "To search your index for specific documents, you must include a query in the request body that OpenSearch uses to match documents. If you don’t use a query, OpenSearch treats your delete request as a simple delete document operation. { \"query\": { \"match\": { \"movie-length\": \"124\" } } } . ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/delete-by-query/#request-body",
    "relUrl": "/api-reference/document-apis/delete-by-query/#request-body"
  },"2122": {
    "doc": "Delete by query",
    "title": "Response",
    "content": "{ \"took\": 143, \"timed_out\": false, \"total\": 1, \"deleted\": 1, \"batches\": 1, \"version_conflicts\": 0, \"noops\": 0, \"retries\": { \"bulk\": 0, \"search\": 0 }, \"throttled_millis\": 0, \"requests_per_second\": -1.0, \"throttled_until_millis\": 0, \"failures\": [] } . ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/delete-by-query/#response",
    "relUrl": "/api-reference/document-apis/delete-by-query/#response"
  },"2123": {
    "doc": "Delete by query",
    "title": "Response body fields",
    "content": "| Field | Description | . | took | The amount of time in milliseconds OpenSearch needed to complete the operation. | . | timed_out | Whether any delete requests during the operation timed out. | . | total | Total number of documents processed. | . | deleted | Total number of documents deleted. | . | batches | Number of scroll responses the request processed. | . | version_conflicts | Number of conflicts the request ran into. | . | noops | How many delete requests OpenSearch ignored during the operation. This field always returns 0. | . | retries | The number of bulk and search retry requests. | . | throttled_millis | Number of throttled milliseconds during the request. | . | requests_per_second | Number of requests executed per second during the operation. | . | throttled_until_millis | The amount of time until OpenSearch executes the next throttled request. Always equal to 0 in a delete by query request. | . | failures | Any failures that occur during the request. | . ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/delete-by-query/#response-body-fields",
    "relUrl": "/api-reference/document-apis/delete-by-query/#response-body-fields"
  },"2124": {
    "doc": "Delete document",
    "title": "Delete document",
    "content": "Introduced 1.0 . If you no longer need a document in your index, you can use the delete document API operation to delete it. ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/delete-document/",
    "relUrl": "/api-reference/document-apis/delete-document/"
  },"2125": {
    "doc": "Delete document",
    "title": "Example",
    "content": "DELETE /sample-index1/_doc/1 . copy . ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/delete-document/#example",
    "relUrl": "/api-reference/document-apis/delete-document/#example"
  },"2126": {
    "doc": "Delete document",
    "title": "Path and HTTP methods",
    "content": "DELETE /&lt;index&gt;/_doc/&lt;_id&gt; . ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/delete-document/#path-and-http-methods",
    "relUrl": "/api-reference/document-apis/delete-document/#path-and-http-methods"
  },"2127": {
    "doc": "Delete document",
    "title": "URL parameters",
    "content": "| Parameter | Type | Description | Required | . | &lt;index&gt; | String | The index to delete from. | Yes | . | &lt;_id&gt; | String | The ID of the document to delete. | Yes | . | if_seq_no | Integer | Only perform the delete operation if the document’s version number matches the specified number. | No | . | if_primary_term | Integer | Only perform the delete operation if the document has the specified primary term. | No | . | refresh | Enum | If true, OpenSearch refreshes shards to make the delete operation available to search results. Valid options are true, false, and wait_for, which tells OpenSearch to wait for a refresh before executing the operation. Default is false. | No | . | routing | String | Value used to route the operation to a specific shard. | No | . | timeout | Time | How long to wait for a response from the cluster. Default is 1m. | No | . | version | Integer | The version of the document to delete, which must match the last updated version of the document. | No | . | version_type | Enum | Retrieves a specifically typed document. Available options are external (retrieve the document if the specified version number is greater than the document’s current version) and external_gte (retrieve the document if the specified version number is greater than or equal to the document’s current version). For example, to delete version 3 of a document, use /_doc/1?version=3&amp;version_type=external. | No | . | wait_for_active_shards | String | The number of active shards that must be available before OpenSearch processes the delete request. Default is 1 (only the primary shard). Set to all or a positive integer. Values greater than 1 require replicas. For example, if you specify a value of 3, the index must have two replicas distributed across two additional nodes for the operation to succeed. | No | . ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/delete-document/#url-parameters",
    "relUrl": "/api-reference/document-apis/delete-document/#url-parameters"
  },"2128": {
    "doc": "Delete document",
    "title": "Response",
    "content": "{ \"_index\": \"sample-index1\", \"_id\": \"1\", \"_version\": 2, \"result\": \"deleted\", \"_shards\": { \"total\": 2, \"successful\": 2, \"failed\": 0 }, \"_seq_no\": 1, \"_primary_term\": 15 } . ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/delete-document/#response",
    "relUrl": "/api-reference/document-apis/delete-document/#response"
  },"2129": {
    "doc": "Delete document",
    "title": "Response body fields",
    "content": "| Field | Description | . | _index | The name of the index. | . | _id | The document’s ID. | . | _version | The document’s version. | . | _result | The result of the delete operation. | . | _shards | Detailed information about the cluster’s shards. | . | total | The total number of shards. | . | successful | The number of shards OpenSearch successfully deleted the document from. | . | failed | The number of shards OpenSearch failed to delete the document from. | . | _seq_no | The sequence number assigned when the document was indexed. | . | _primary_term | The primary term assigned when the document was indexed. | . ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/delete-document/#response-body-fields",
    "relUrl": "/api-reference/document-apis/delete-document/#response-body-fields"
  },"2130": {
    "doc": "Get document",
    "title": "Get document",
    "content": "Introduced 1.0 . After adding a JSON document to your index, you can use the get document API operation to retrieve the document’s information and data. ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/get-documents/",
    "relUrl": "/api-reference/document-apis/get-documents/"
  },"2131": {
    "doc": "Get document",
    "title": "Example",
    "content": "GET sample-index1/_doc/1 . copy . ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/get-documents/#example",
    "relUrl": "/api-reference/document-apis/get-documents/#example"
  },"2132": {
    "doc": "Get document",
    "title": "Path and HTTP methods",
    "content": "GET &lt;index&gt;/_doc/&lt;_id&gt; HEAD &lt;index&gt;/_doc/&lt;_id&gt; . GET &lt;index&gt;/_source/&lt;_id&gt; HEAD &lt;index&gt;/_source/&lt;_id&gt; . ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/get-documents/#path-and-http-methods",
    "relUrl": "/api-reference/document-apis/get-documents/#path-and-http-methods"
  },"2133": {
    "doc": "Get document",
    "title": "URL parameters",
    "content": "All get document URL parameters are optional. | Parameter | Type | Description | . | preference | String | Specifies a preference of which shard to retrieve results from. Available options are _local, which tells the operation to retrieve results from a locally allocated shard replica, and a custom string value assigned to a specific shard replica. By default, OpenSearch executes get document operations on random shards. | . | realtime | Boolean | Specifies whether the operation should run in realtime. If false, the operation waits for the index to refresh to analyze the source to retrieve data, which makes the operation near-realtime. Default is true. | . | refresh | Boolean | If true, OpenSearch refreshes shards to make the get operation available to search results. Valid options are true, false, and wait_for, which tells OpenSearch to wait for a refresh before executing the operation. Default is false. | . | routing | String | A value used to route the operation to a specific shard. | . | stored_fields | Boolean | Whether the get operation should retrieve fields stored in the index. Default is false. | . | _source | String | Whether to include the _source field in the response body. Default is true. | . | _source_excludes | String | A comma-separated list of source fields to exclude in the query response. | . | _source_includes | String | A comma-separated list of source fields to include in the query response. | . | version | Integer | The version of the document to return, which must match the current version of the document. | . | version_type | Enum | Retrieves a specifically typed document. Available options are external (retrieve the document if the specified version number is greater than the document’s current version) and external_gte (retrieve the document if the specified version number is greater than or equal to the document’s current version). For example, to retrieve version 3 of a document, use /_doc/1?version=3&amp;version_type=external. | . ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/get-documents/#url-parameters",
    "relUrl": "/api-reference/document-apis/get-documents/#url-parameters"
  },"2134": {
    "doc": "Get document",
    "title": "Response",
    "content": "{ \"_index\": \"sample-index1\", \"_id\": \"1\", \"_version\": 1, \"_seq_no\": 0, \"_primary_term\": 9, \"found\": true, \"_source\": { \"text\": \"This is just some sample text.\" } } . ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/get-documents/#response",
    "relUrl": "/api-reference/document-apis/get-documents/#response"
  },"2135": {
    "doc": "Get document",
    "title": "Response body fields",
    "content": "| Field | Description | . | _index | The name of the index. | . | _id | The document’s ID. | . | _version | The document’s version number. Updated whenever the document changes. | . | _seq_no | The sequence number assigned when the document is indexed. | . | primary_term | The primary term assigned when the document is indexed. | . | found | Whether the document exists. | . | _routing | The shard that the document is routed to. If the document is not routed to a particular shard, this field is omitted. | . | _source | Contains the document’s data if found is true. If _source is set to false or stored_fields is set to true in the URL parameters, this field is omitted. | . | _fields | Contains the document’s data that’s stored in the index. Only returned if both stored_fields and found are true. | . ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/get-documents/#response-body-fields",
    "relUrl": "/api-reference/document-apis/get-documents/#response-body-fields"
  },"2136": {
    "doc": "Index document",
    "title": "Index document",
    "content": "Introduced 1.0 . Before you can search for data, you must first add documents. This operation adds a single document to your index. ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/index-document/",
    "relUrl": "/api-reference/document-apis/index-document/"
  },"2137": {
    "doc": "Index document",
    "title": "Example",
    "content": "PUT sample-index/_doc/1 { \"Description\": \"To be or not to be, that is the question.\" } . copy . ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/index-document/#example",
    "relUrl": "/api-reference/document-apis/index-document/#example"
  },"2138": {
    "doc": "Index document",
    "title": "Path and HTTP methods",
    "content": "PUT &lt;index&gt;/_doc/&lt;_id&gt; POST &lt;index&gt;/_doc PUT &lt;index&gt;/_create/&lt;_id&gt; POST &lt;index&gt;/_create/&lt;_id&gt; . ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/index-document/#path-and-http-methods",
    "relUrl": "/api-reference/document-apis/index-document/#path-and-http-methods"
  },"2139": {
    "doc": "Index document",
    "title": "URL parameters",
    "content": "In your request, you must specify the index you want to add your document to. If the index doesn’t already exist, OpenSearch automatically creates the index and adds in your document. All other URL parameters are optional. | Parameter | Type | Description | Required | . | &lt;index&gt; | String | Name of the index. | Yes | . | &lt;_id&gt; | String | A unique identifier to attach to the document. To automatically generate an ID, use POST &lt;target&gt;/doc in your request instead of PUT. | No | . | if_seq_no | Integer | Only perform the index operation if the document has the specified sequence number. | No | . | if_primary_term | Integer | Only perform the index operation if the document has the specified primary term. | No | . | op_type | Enum | Specifies the type of operation to complete with the document. Valid values are create (create the index if it doesn’t exist) and index. If a document ID is included in the request, then the default is index. Otherwise, the default is create. | No | . | pipeline | String | Route the index operation to a certain pipeline. | No | . | routing | String | value used to assign the index operation to a specific shard. | No | . | refresh | Enum | If true, OpenSearch refreshes shards to make the operation visible to searching. Valid options are true, false, and wait_for, which tells OpenSearch to wait for a refresh before executing the operation. Default is false. | No | . | timeout | Time | How long to wait for a response from the cluster. Default is 1m. | No | . | version | Integer | The document’s version number. | No | . | version_type | Enum | Assigns a specific type to the document. Valid options are external (retrieve the document if the specified version number is greater than the document’s current version) and external_gte (retrieve the document if the specified version number is greater than or equal to the document’s current version). For example, to index version 3 of a document, use /_doc/1?version=3&amp;version_type=external. | No | . | wait_for_active_shards | String | The number of active shards that must be available before OpenSearch processes the request. Default is 1 (only the primary shard). Set to all or a positive integer. Values greater than 1 require replicas. For example, if you specify a value of 3, the index must have two replicas distributed across two additional nodes for the operation to succeed. | No | . | require_alias | Boolean | Specifies whether the target index must be an index alias. Default is false. | No | . ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/index-document/#url-parameters",
    "relUrl": "/api-reference/document-apis/index-document/#url-parameters"
  },"2140": {
    "doc": "Index document",
    "title": "Request body",
    "content": "Your request body must contain the information you want to index. { \"Description\": \"This is just a sample document\" } . ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/index-document/#request-body",
    "relUrl": "/api-reference/document-apis/index-document/#request-body"
  },"2141": {
    "doc": "Index document",
    "title": "Response",
    "content": "{ \"_index\": \"sample-index\", \"_id\": \"1\", \"_version\": 1, \"result\": \"created\", \"_shards\": { \"total\": 2, \"successful\": 1, \"failed\": 0 }, \"_seq_no\": 0, \"_primary_term\": 1 } . ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/index-document/#response",
    "relUrl": "/api-reference/document-apis/index-document/#response"
  },"2142": {
    "doc": "Index document",
    "title": "Response body fields",
    "content": "| Field | Description | . | _index | The name of the index. | . | _id | The document’s ID. | . | _version | The document’s version. | . | result | The result of the index operation. | . | _shards | Detailed information about the cluster’s shards. | . | total | The total number of shards. | . | successful | The number of shards OpenSearch successfully added the document to. | . | failed | The number of shards OpenSearch failed to added the document to. | . | _seq_no | The sequence number assigned when the document was indexed. | . | _primary_term | The primary term assigned when the document was indexed. | . ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/index-document/#response-body-fields",
    "relUrl": "/api-reference/document-apis/index-document/#response-body-fields"
  },"2143": {
    "doc": "Document APIs",
    "title": "Document APIs",
    "content": "The document APIs allow you to handle documents relative to your index, such as adding, updating, and deleting documents. Document APIs are separated into two categories: single document operations and multi-document operations. Multi-document operations offer performance advantages over submitting many individual requests, so whenever practical, we recommend that you use multi-document operations. ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/index/",
    "relUrl": "/api-reference/document-apis/index/"
  },"2144": {
    "doc": "Document APIs",
    "title": "Single document operations",
    "content": ". | Index | Get | Delete | Update | . ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/index/#single-document-operations",
    "relUrl": "/api-reference/document-apis/index/#single-document-operations"
  },"2145": {
    "doc": "Document APIs",
    "title": "Multi-document operations",
    "content": ". | Bulk | Multi get | Delete by query | Update by query | Reindex | . ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/index/#multi-document-operations",
    "relUrl": "/api-reference/document-apis/index/#multi-document-operations"
  },"2146": {
    "doc": "Multi-get document",
    "title": "Multi-get documents",
    "content": "Introduced 1.0 . The multi-get operation allows you to execute multiple GET operations in one request, so you can get back all documents that match your criteria. ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/multi-get/#multi-get-documents",
    "relUrl": "/api-reference/document-apis/multi-get/#multi-get-documents"
  },"2147": {
    "doc": "Multi-get document",
    "title": "Example without specifying index in URL",
    "content": "GET _mget { \"docs\": [ { \"_index\": \"sample-index1\", \"_id\": \"1\" }, { \"_index\": \"sample-index2\", \"_id\": \"1\", \"_source\": { \"include\": [\"Length\"] } } ] } . copy . ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/multi-get/#example-without-specifying-index-in-url",
    "relUrl": "/api-reference/document-apis/multi-get/#example-without-specifying-index-in-url"
  },"2148": {
    "doc": "Multi-get document",
    "title": "Example of specifying index in URL",
    "content": "GET sample-index1/_mget { \"docs\": [ { \"_id\": \"1\", \"_source\": false }, { \"_id\": \"2\", \"_source\": [ \"Director\", \"Title\" ] } ] } . copy . ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/multi-get/#example-of-specifying-index-in-url",
    "relUrl": "/api-reference/document-apis/multi-get/#example-of-specifying-index-in-url"
  },"2149": {
    "doc": "Multi-get document",
    "title": "Path and HTTP methods",
    "content": "GET _mget GET &lt;index&gt;/_mget . ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/multi-get/#path-and-http-methods",
    "relUrl": "/api-reference/document-apis/multi-get/#path-and-http-methods"
  },"2150": {
    "doc": "Multi-get document",
    "title": "URL parameters",
    "content": "All multi-get URL parameters are optional. | Parameter | Type | Description | . | &lt;index&gt; | String | Name of the index to retrieve documents from. | . | preference | String | Specifies the nodes or shards OpenSearch should execute the multi-get operation on. Default is random. | . | realtime | Boolean | Specifies whether the operation should run in realtime. If false, the operation waits for the index to refresh to analyze the source to retrieve data, which makes the operation near-realtime. Default is true. | . | refresh | Boolean | If true, OpenSearch refreshes shards to make the multi-get operation available to search results. Valid options are true, false, and wait_for, which tells OpenSearch to wait for a refresh before executing the operation. Default is false. | . | routing | String | Value used to route the multi-get operation to a specific shard. | . | stored_fields | Boolean | Specifies whether OpenSearch should retrieve documents fields from the index instead of the document’s _source. Default is false. | . | _source | String | Whether to include the _source field in the query response. Default is true. | . | _source_excludes | String | A comma-separated list of source fields to exclude in the query response. | . | _source_includes | String | A comma-separated list of source fields to include in the query response. | . ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/multi-get/#url-parameters",
    "relUrl": "/api-reference/document-apis/multi-get/#url-parameters"
  },"2151": {
    "doc": "Multi-get document",
    "title": "Request body",
    "content": "If you don’t specify an index in your request’s URL, you must specify your target indexes and the relevant document IDs in the request body. Other fields are optional. | Field | Type | Description | Required | . | docs | Array | The documents you want to retrieve data from. Can contain the attributes: _id, _index, _routing, _source, and _stored_fields. If you specify an index in the URL, you can omit this field and add IDs of the documents to retrieve. | Yes if an index is not specified in the URL | . | _id | String | The ID of the document. | Yes if docs is specified in the request body | . | _index | String | Name of the index. | Yes if an index is not specified in the URL | . | _routing | String | The value of the shard that has the document. | Yes if a routing value was used when indexing the document | . | _source | Object | Specifies whether to return the _source field from an index (boolean), whether to return specific fields (array), or whether to include or exclude certain fields. | No | . | _source.includes | Array | Specifies which fields to include in the query response. For example, \"_source\": { \"include\": [\"Title\"] } retrieves Title from the index. | No | . | _source.excludes | Array | Specifies which fields to exclude in the query response. For example, \"_source\": { \"exclude\": [\"Director\"] } excludes Director from the query response. | No | . | ids | Array | IDs of the documents to retrieve. Only allowed when an index is specified in the URL. | No | . ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/multi-get/#request-body",
    "relUrl": "/api-reference/document-apis/multi-get/#request-body"
  },"2152": {
    "doc": "Multi-get document",
    "title": "Response",
    "content": "{ \"docs\": [ { \"_index\": \"sample-index1\", \"_id\": \"1\", \"_version\": 4, \"_seq_no\": 5, \"_primary_term\": 19, \"found\": true, \"_source\": { \"Title\": \"Batman Begins\", \"Director\": \"Christopher Nolan\" } }, { \"_index\": \"sample-index2\", \"_id\": \"1\", \"_version\": 1, \"_seq_no\": 6, \"_primary_term\": 19, \"found\": true, \"_source\": { \"Title\": \"The Dark Knight\", \"Director\": \"Christopher Nolan\" } } ] } . ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/multi-get/#response",
    "relUrl": "/api-reference/document-apis/multi-get/#response"
  },"2153": {
    "doc": "Multi-get document",
    "title": "Response body fields",
    "content": "| Field | Description | . | _index | The name of the index. | . | _id | The document’s ID. | . | _version | The document’s version number. Updated whenever the document changes. | . | _seq_no | The sequence number assigned when the document is indexed. | . | primary_term | The primary term assigned when the document is indexed. | . | found | Whether the document exists. | . | _routing | The shard that the document is routed to. If the document is not routed to a particular shard, this field is omitted. | . | _source | Contains the document’s data if found is true. If _source is set to false or stored_fields is set to true in the URL parameters, this field is omitted. | . | _fields | Contains the document’s data that’s stored in the index. Only returned if both stored_fields and found are true. | . ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/multi-get/#response-body-fields",
    "relUrl": "/api-reference/document-apis/multi-get/#response-body-fields"
  },"2154": {
    "doc": "Multi-get document",
    "title": "Multi-get document",
    "content": " ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/multi-get/",
    "relUrl": "/api-reference/document-apis/multi-get/"
  },"2155": {
    "doc": "Reindex document",
    "title": "Reindex document",
    "content": "Introduced 1.0 . The reindex document API operation lets you copy all or a subset of your data from a source index into a destination index. ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/reindex/",
    "relUrl": "/api-reference/document-apis/reindex/"
  },"2156": {
    "doc": "Reindex document",
    "title": "Example",
    "content": "POST /_reindex { \"source\":{ \"index\":\"my-source-index\" }, \"dest\":{ \"index\":\"my-destination-index\" } } . copy . ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/reindex/#example",
    "relUrl": "/api-reference/document-apis/reindex/#example"
  },"2157": {
    "doc": "Reindex document",
    "title": "Path and HTTP methods",
    "content": "POST /_reindex . ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/reindex/#path-and-http-methods",
    "relUrl": "/api-reference/document-apis/reindex/#path-and-http-methods"
  },"2158": {
    "doc": "Reindex document",
    "title": "URL parameters",
    "content": "All URL parameters are optional. | Parameter | Type | Description | . | refresh | Boolean | If true, OpenSearch refreshes shards to make the reindex operation available to search results. Valid options are true, false, and wait_for, which tells OpenSearch to wait for a refresh before executing the operation. Default is false. | . | timeout | Time | How long to wait for a response from the cluster. Default is 30s. | . | wait_for_active_shards | String | The number of active shards that must be available before OpenSearch processes the reindex request. Default is 1 (only the primary shard). Set to all or a positive integer. Values greater than 1 require replicas. For example, if you specify a value of 3, the index must have two replicas distributed across two additional nodes for the operation to succeed. | . | wait_for_completion | Boolean | Waits for the matching tasks to complete. Default is false. | . | requests_per_second | Integer | Specifies the request’s throttling in sub-requests per second. Default is -1, which means no throttling. | . | require_alias | Boolean | Whether the destination index must be an index alias. Default is false. | . | scroll | Time | How long to keep the search context open. Default is 5m. | . | slices | Integer | Number of sub-tasks OpenSearch should divide this task into. Default is 1, which means OpenSearch should not divide this task. Setting this parameter to auto indicates to OpenSearch that it should automatically decide how many slices to split the task into. | . | max_docs | Integer | How many documents the update by query operation should process at most. Default is all documents. | . ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/reindex/#url-parameters",
    "relUrl": "/api-reference/document-apis/reindex/#url-parameters"
  },"2159": {
    "doc": "Reindex document",
    "title": "Request body",
    "content": "Your request body must contain the names of the source index and destination index. All other fields are optional. | Field | Description | . | conflicts | Indicates to OpenSearch what should happen if the delete by query operation runs into a version conflict. Valid options are abort and proceed. Default is abort. | . | source | Information about the source index to include. Valid fields are index, max_docs, query, remote, size, slice, and _source. | . | index | The name of the source index to copy data from. | . | max_docs | The maximum number of documents to reindex. | . | query | The search query to use for the reindex operation. | . | remote | Information about a remote OpenSearch cluster to copy data from. Valid fields are host, username, password, socket_timeout, and connect_timeout. | . | host | Host URL of the OpenSearch cluster to copy data from. | . | username | Username to authenticate with the remote cluster. | . | password | Password to authenticate with the remote cluster. | . | socket_timeout | The wait time for socket reads. Default is 30s. | . | connect_timeout | The wait time for remote connection timeouts. Default is 30s. | . | size | The number of documents to reindex. | . | slice | Whether to manually or automatically slice the reindex operation so it executes in parallel. Setting this field to auto allows OpenSearch to control the number of slices to use, which is one slice per shard, up to a maximum of 20. If there are multiple sources, the number of slices used are based on the index or backing index with the smallest number of shards. | . | _source | Whether to reindex source fields. Specify a list of fields to reindex or true to reindex all fields. Default is true. | . | id | The ID to associate with manual slicing. | . | max | Maximum number of slices. | . | dest | Information about the destination index. Valid values are index, version_type, and op_type. | . | index | Name of the destination index. | . | version_type | The indexing operation’s version type. Valid values are internal, external, external_gt (retrieve the document if the specified version number is greater than the document’s current version), and external_gte (retrieve the document if the specified version number is greater or equal to than the document’s current version). | . | op_type | Whether to copy over documents that are missing in the destination index. Valid values are create (ignore documents with the same ID from the source index) and index (copy everything from the source index). | . | script | A script that OpenSearch uses to apply transformations to the data during the reindex operation. | . | source | The actual script that OpenSearch runs. | . | lang | The scripting language. Valid options are painless, expression, mustache, and java. | . ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/reindex/#request-body",
    "relUrl": "/api-reference/document-apis/reindex/#request-body"
  },"2160": {
    "doc": "Reindex document",
    "title": "Response",
    "content": "{ \"took\": 28829, \"timed_out\": false, \"total\": 111396, \"updated\": 0, \"created\": 111396, \"deleted\": 0, \"batches\": 112, \"version_conflicts\": 0, \"noops\": 0, \"retries\": { \"bulk\": 0, \"search\": 0 }, \"throttled_millis\": 0, \"requests_per_second\": -1.0, \"throttled_until_millis\": 0, \"failures\": [] } . ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/reindex/#response",
    "relUrl": "/api-reference/document-apis/reindex/#response"
  },"2161": {
    "doc": "Reindex document",
    "title": "Response body fields",
    "content": "| Field | Description | . | took | How long the operation took in milliseconds. | . | timed_out | Whether the operation timed out. | . | total | The total number of documents processed. | . | updated | The number of documents updated in the destination index. | . | created | The number of documents created in the destination index. | . | deleted | The number of documents deleted. | . | batches | Number of scroll responses. | . | version_conflicts | Number of version conflicts. | . | noops | How many documents OpenSearch ignored during the operation. | . | retries | Number of bulk and search retry requests. | . | throttled_millis | Number of throttled milliseconds during the request. | . | requests_per_second | Number of requests executed per second during the operation. | . | throttled_until_millis | The amount of time until OpenSearch executes the next throttled request. | . | failures | Any failures that occurred during the operation. | . ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/reindex/#response-body-fields",
    "relUrl": "/api-reference/document-apis/reindex/#response-body-fields"
  },"2162": {
    "doc": "Update by query",
    "title": "Update by query",
    "content": "Introduced 1.0 . You can include a query and a script as part of your update request so OpenSearch can run the script to update all of the documents that match the query. ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/update-by-query/",
    "relUrl": "/api-reference/document-apis/update-by-query/"
  },"2163": {
    "doc": "Update by query",
    "title": "Example",
    "content": "POST test-index1/_update_by_query { \"query\": { \"term\": { \"oldValue\": 10 } }, \"script\" : { \"source\": \"ctx._source.oldValue += params.newValue\", \"lang\": \"painless\", \"params\" : { \"newValue\" : 20 } } } . copy . ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/update-by-query/#example",
    "relUrl": "/api-reference/document-apis/update-by-query/#example"
  },"2164": {
    "doc": "Update by query",
    "title": "Path and HTTP methods",
    "content": "POST &lt;target-index1&gt;, &lt;target-index2&gt;/_update_by_query . ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/update-by-query/#path-and-http-methods",
    "relUrl": "/api-reference/document-apis/update-by-query/#path-and-http-methods"
  },"2165": {
    "doc": "Update by query",
    "title": "URL parameters",
    "content": "All URL parameters are optional. | Parameter | Type | Description | . | &lt;index&gt; | String | Comma-separated list of indexes to update. To update all indexes, use * or omit this parameter. | . | allow_no_indices | Boolean | Whether to ignore wildcards that don’t match any indexes. Default is true. | . | analyzer | String | Analyzer to use in the query string. | . | analyze_wildcard | Boolean | Whether the update operation should include wildcard and prefix queries in the analysis. Default is false. | . | conflicts | String | Indicates to OpenSearch what should happen if the update by query operation runs into a version conflict. Valid options are abort and proceed. Default is abort. | . | default_operator | String | Indicates whether the default operator for a string query should be AND or OR. Default is OR. | . | df | String | The default field if a field prefix is not provided in the query string. | . | expand_wildcards | String | Specifies the type of index that wildcard expressions can match. Supports comma-separated values. Valid values are all (match any index), open (match open, non-hidden indexes), closed (match closed, non-hidden indexes), hidden (match hidden indexes), and none (deny wildcard expressions). Default is open. | . | from | Integer | The starting index to search from. Default is 0. | . | ignore_unavailable | Boolean | Whether to exclude missing or closed indexes in the response. Default is false. | . | lenient | Boolean | Specifies whether OpenSearch should accept requests if queries have format errors (for example, querying a text field for an integer). Default is false. | . | max_docs | Integer | How many documents the update by query operation should process at most. Default is all documents. | . | pipeline | String | ID of the pipeline to use to process documents. | . | preference | String | Specifies which shard or node OpenSearch should perform the update by query operation on. | . | q | String | Lucene query string’s query. | . | request_cache | Boolean | Specifies whether OpenSearch should use the request cache. Default is whether it’s enabled in the index’s settings. | . | refresh | Boolean | If true, OpenSearch refreshes shards to make the update by query operation available to search results. Valid options are true and false. Default is false. | . | requests_per_second | Integer | Specifies the request’s throttling in sub-requests per second. Default is -1, which means no throttling. | . | routing | String | Value used to route the update by query operation to a specific shard. | . | scroll | Time | How long to keep the search context open. | . | scroll_size | Integer | Size of the operation’s scroll request. Default is 1000. | . | search_type | String | Whether OpenSearch should use global term and document frequencies calculating relevance scores. Valid choices are query_then_fetch and dfs_query_then_fetch. query_then_fetch scores documents using local term and document frequencies for the shard. It’s usually faster but less accurate. dfs_query_then_fetch scores documents using global term and document frequencies across all shards. It’s usually slower but more accurate. Default is query_then_fetch. | . | search_timeout | Time | How long to wait until OpenSearch deems the request timed out. Default is no timeout. | . | slices | Integer | Number of sub-tasks OpenSearch should divide this task into. Default is 1, which means OpenSearch should not divide this task. | . | sort | List | A comma-separated list of &lt;field&gt; : &lt;direction&gt; pairs to sort by. | . | _source | String | Whether to include the _source field in the response. | . | _source_excludes | String | A comma-separated list of source fields to exclude from the response. | . | _source_includes | String | A comma-separated list of source fields to include in the response. | . | stats | String | Value to associate with the request for additional logging. | . | terminate_after | Integer | The maximum number of documents OpenSearch should process before terminating the request. | . | timeout | Time | How long the operation should wait from a response from active shards. Default is 1m. | . | version | Boolean | Whether to include the document version as a match. | . | wait_for_active_shards | String | The number of shards that must be active before OpenSearch executes the operation. Valid values are all or any integer up to the total number of shards in the index. Default is 1, which is the primary shard. | . | wait_for_completion | boolean | When set to false, the response body includes a task ID and OpenSearch executes the operation asynchronously. The task ID can be used to check the status of the task or to cancel the task. Default is set to true. | . ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/update-by-query/#url-parameters",
    "relUrl": "/api-reference/document-apis/update-by-query/#url-parameters"
  },"2166": {
    "doc": "Update by query",
    "title": "Request body",
    "content": "To update your indexes and documents by query, you must include a query and a script in the request body that OpenSearch can run to update your documents. If you don’t specify a query, then every document in the index gets updated. { \"query\": { \"term\": { \"oldValue\": 20 } }, \"script\" : { \"source\": \"ctx._source.oldValue += params.newValue\", \"lang\": \"painless\", \"params\" : { \"newValue\" : 10 } } } . ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/update-by-query/#request-body",
    "relUrl": "/api-reference/document-apis/update-by-query/#request-body"
  },"2167": {
    "doc": "Update by query",
    "title": "Response",
    "content": "{ \"took\": 21, \"timed_out\": false, \"total\": 1, \"updated\": 1, \"deleted\": 0, \"batches\": 1, \"version_conflicts\": 0, \"noops\": 0, \"retries\": { \"bulk\": 0, \"search\": 0 }, \"throttled_millis\": 0, \"requests_per_second\": -1.0, \"throttled_until_millis\": 0, \"failures\": [] } . ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/update-by-query/#response",
    "relUrl": "/api-reference/document-apis/update-by-query/#response"
  },"2168": {
    "doc": "Update by query",
    "title": "Response body fields",
    "content": "| Field | Description | . | took | The amount of time in milliseconds OpenSearch needed to complete the operation. | . | timed_out | Whether any update requests during the operation timed out. | . | total | Total number of documents processed. | . | updated | Total number of documents updated. | . | batches | Number of scroll responses the request processed. | . | version_conflicts | Number of conflicts the request ran into. | . | noops | How many update requests OpenSearch ignored during the operation. This field always returns 0. | . | retries | The number of bulk and search retry requests. | . | throttled_millis | Number of throttled milliseconds during the request. | . | requests_per_second | Number of requests executed per second during the operation. | . | throttled_until_millis | The amount of time until OpenSearch executes the next throttled request. Always equal to 0 in an update by query request. | . | failures | Any failures that occur during the request. | . ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/update-by-query/#response-body-fields",
    "relUrl": "/api-reference/document-apis/update-by-query/#response-body-fields"
  },"2169": {
    "doc": "Update document",
    "title": "Update document",
    "content": "Introduced 1.0 . If you need to update a document’s fields in your index, you can use the update document API operation. You can do so by specifying the new data you want in your index or by including a script in your request body, which OpenSearch runs to update the document. ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/update-document/",
    "relUrl": "/api-reference/document-apis/update-document/"
  },"2170": {
    "doc": "Update document",
    "title": "Example",
    "content": "POST /sample-index1/_update/1 { \"doc\": { \"first_name\" : \"Bruce\", \"last_name\" : \"Wayne\" } } . copy . ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/update-document/#example",
    "relUrl": "/api-reference/document-apis/update-document/#example"
  },"2171": {
    "doc": "Update document",
    "title": "Script example",
    "content": "POST /test-index1/_update/1 { \"script\" : { \"source\": \"ctx._source.secret_identity = \\\"Batman\\\"\" } } . copy . ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/update-document/#script-example",
    "relUrl": "/api-reference/document-apis/update-document/#script-example"
  },"2172": {
    "doc": "Update document",
    "title": "Path and HTTP methods",
    "content": "POST /&lt;index&gt;/_update/&lt;_id&gt; . ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/update-document/#path-and-http-methods",
    "relUrl": "/api-reference/document-apis/update-document/#path-and-http-methods"
  },"2173": {
    "doc": "Update document",
    "title": "URL parameters",
    "content": "| Parameter | Type | Description | Required | . | &lt;index&gt; | String | Name of the index. | Yes | . | &lt;_id&gt; | String | The ID of the document to update. | Yes | . | if_seq_no | Integer | Only perform the delete operation if the document’s version number matches the specified number. | No | . | if_primary_term | Integer | Perform the update operation if the document has the specified primary term. | No | . | lang | String | Language of the script. Default is painless. | No | . | require_alias | Boolean | Specifies whether the destination must be an index alias. Default is false. | No | . | refresh | Enum | If true, OpenSearch refreshes shards to make the operation visible to searching. Valid options are true, false, and wait_for, which tells OpenSearch to wait for a refresh before executing the operation. Default is false. | No | . | retry_on_conflict | Integer | The amount of times OpenSearch should retry the operation if there’s a document conflict. Default is 0. | No | . | routing | String | Value to route the update operation to a specific shard. | No | . | _source | Boolean or List | Whether or not to include the _source field in the response body. Default is false. This parameter also supports a comma-separated list of source fields for including multiple source fields in the query response. | No | . | _source_excludes | List | A comma-separated list of source fields to exclude in the query response. | No | . | _source_includes | List | A comma-separated list of source fields to include in the query response. | No | . | timeout | Time | How long to wait for a response from the cluster. | No | . | wait_for_active_shards | String | The number of active shards that must be available before OpenSearch processes the update request. Default is 1 (only the primary shard). Set to all or a positive integer. Values greater than 1 require replicas. For example, if you specify a value of 3, the index must have two replicas distributed across two additional nodes for the operation to succeed. | No | . ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/update-document/#url-parameters",
    "relUrl": "/api-reference/document-apis/update-document/#url-parameters"
  },"2174": {
    "doc": "Update document",
    "title": "Request body",
    "content": "Your request body must contain the information you want to update your document with. If you just want to replace certain fields in your document, your request body must include a doc object, which has the fields you want to update. { \"doc\": { \"first_name\": \"Thomas\", \"last_name\": \"Wayne\" } } . You can also use a script to tell OpenSearch how to update your document. { \"script\" : { \"source\": \"ctx._source.oldValue += params.newValue\", \"lang\": \"painless\", \"params\" : { \"newValue\" : 10 } } } . Upsert . Upsert is an operation that conditionally either updates an existing document or inserts a new one based on information in the object. In the sample below, the upsert object updates the last name and adds the age field if a document already exists. If a document does not exist, a new one is indexed using content in the upsert object. { \"doc\": { \"first_name\": \"Martha\", \"last_name\": \"Rivera\" }, \"upsert\": { \"last_name\": \"Oliveira\", \"age\": \"31\" } } . You can also add doc_as_upsert to the request and set it to true to use the information in doc for performing the upsert operation. { \"doc\": { \"first_name\": \"Martha\", \"last_name\": \"Oliveira\", \"age\": \"31\" }, \"doc_as_upsert\": true } . ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/update-document/#request-body",
    "relUrl": "/api-reference/document-apis/update-document/#request-body"
  },"2175": {
    "doc": "Update document",
    "title": "Response",
    "content": "{ \"_index\": \"sample-index1\", \"_id\": \"1\", \"_version\": 3, \"result\": \"updated\", \"_shards\": { \"total\": 2, \"successful\": 2, \"failed\": 0 }, \"_seq_no\": 4, \"_primary_term\": 17 } . ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/update-document/#response",
    "relUrl": "/api-reference/document-apis/update-document/#response"
  },"2176": {
    "doc": "Update document",
    "title": "Response body fields",
    "content": "| Field | Description | . | _index | The name of the index. | . | _id | The document’s ID. | . | _version | The document’s version. | . | _result | The result of the delete operation. | . | _shards | Detailed information about the cluster’s shards. | . | total | The total number of shards. | . | successful | The number of shards OpenSearch successfully deleted the document from. | . | failed | The number of shards OpenSearch failed to delete the document from. | . | _seq_no | The sequence number assigned when the document was indexed. | . | _primary_term | The primary term assigned when the document was indexed. | . ",
    "url": "https://vagimeli.github.io/api-reference/document-apis/update-document/#response-body-fields",
    "relUrl": "/api-reference/document-apis/update-document/#response-body-fields"
  },"2177": {
    "doc": "Explain",
    "title": "Explain",
    "content": "Introduced 1.0 . Wondering why a specific document ranks higher (or lower) for a query? You can use the explain API for an explanation of how the relevance score (_score) is calculated for every result. OpenSearch uses a probabilistic ranking framework called Okapi BM25 to calculate relevance scores. Okapi BM25 is based on the original TF/IDF framework used by Apache Lucene. The explain API is an expensive operation in terms of both resources and time. On production clusters, we recommend using it sparingly for the purpose of troubleshooting. ",
    "url": "https://vagimeli.github.io/api-reference/explain/",
    "relUrl": "/api-reference/explain/"
  },"2178": {
    "doc": "Explain",
    "title": "Example",
    "content": "To see the explain output for all results, set the explain flag to true either in the URL or in the body of the request: . POST opensearch_dashboards_sample_data_ecommerce/_search?explain=true { \"query\": { \"match\": { \"customer_first_name\": \"Mary\" } } } . copy . More often, you want the output for a single document. In that case, specify the document ID in the URL: . POST opensearch_dashboards_sample_data_ecommerce/_explain/EVz1Q3sBgg5eWQP6RSte { \"query\": { \"match\": { \"customer_first_name\": \"Mary\" } } } . copy . ",
    "url": "https://vagimeli.github.io/api-reference/explain/#example",
    "relUrl": "/api-reference/explain/#example"
  },"2179": {
    "doc": "Explain",
    "title": "Path and HTTP methods",
    "content": "GET &lt;target&gt;/_explain/&lt;id&gt; POST &lt;target&gt;/_explain/&lt;id&gt; . ",
    "url": "https://vagimeli.github.io/api-reference/explain/#path-and-http-methods",
    "relUrl": "/api-reference/explain/#path-and-http-methods"
  },"2180": {
    "doc": "Explain",
    "title": "URL parameters",
    "content": "You must specify the index and document ID. All other URL parameters are optional. | Parameter | Type | Description | Required | . | &lt;index&gt; | String | Name of the index. You can only specify a single index. | Yes | . | &lt;_id&gt; | String | A unique identifier to attach to the document. | Yes | . | analyzer | String | The analyzer to use in the query string. | No | . | analyze_wildcard | Boolean | Specifies whether to analyze wildcard and prefix queries. Default is false. | No | . | default_operator | String | Indicates whether the default operator for a string query should be AND or OR. Default is OR. | No | . | df | String | The default field in case a field prefix is not provided in the query string. | No | . | lenient | Boolean | Specifies whether OpenSearch should ignore format-based query failures (for example, querying a text field for an integer). Default is false. | No | . | preference | String | Specifies a preference of which shard to retrieve results from. Available options are _local, which tells the operation to retrieve results from a locally allocated shard replica, and a custom string value assigned to a specific shard replica. By default, OpenSearch executes the explain operation on random shards. | No | . | q | String | Query in the Lucene query string syntax. | No | . | stored_fields | Boolean | If true, the operation retrieves document fields stored in the index rather than the document’s _source. Default is false. | No | . | routing | String | Value used to route the operation to a specific shard. | No | . | _source | String | Whether to include the _source field in the response body. Default is true. | No | . | _source_excludes | String | A comma-separated list of source fields to exclude in the query response. | No | . | _source_includes | String | A comma-separated list of source fields to include in the query response. | No | . ",
    "url": "https://vagimeli.github.io/api-reference/explain/#url-parameters",
    "relUrl": "/api-reference/explain/#url-parameters"
  },"2181": {
    "doc": "Explain",
    "title": "Response",
    "content": "{ \"_index\" : \"kibana_sample_data_ecommerce\", \"_id\" : \"EVz1Q3sBgg5eWQP6RSte\", \"matched\" : true, \"explanation\" : { \"value\" : 3.5671005, \"description\" : \"weight(customer_first_name:mary in 1) [PerFieldSimilarity], result of:\", \"details\" : [ { \"value\" : 3.5671005, \"description\" : \"score(freq=1.0), computed as boost * idf * tf from:\", \"details\" : [ { \"value\" : 2.2, \"description\" : \"boost\", \"details\" : [ ] }, { \"value\" : 3.4100041, \"description\" : \"idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:\", \"details\" : [ { \"value\" : 154, \"description\" : \"n, number of documents containing term\", \"details\" : [ ] }, { \"value\" : 4675, \"description\" : \"N, total number of documents with field\", \"details\" : [ ] } ] }, { \"value\" : 0.47548598, \"description\" : \"tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:\", \"details\" : [ { \"value\" : 1.0, \"description\" : \"freq, occurrences of term within document\", \"details\" : [ ] }, { \"value\" : 1.2, \"description\" : \"k1, term saturation parameter\", \"details\" : [ ] }, { \"value\" : 0.75, \"description\" : \"b, length normalization parameter\", \"details\" : [ ] }, { \"value\" : 1.0, \"description\" : \"dl, length of field\", \"details\" : [ ] }, { \"value\" : 1.1206417, \"description\" : \"avgdl, average length of field\", \"details\" : [ ] } ] } ] } ] } } . ",
    "url": "https://vagimeli.github.io/api-reference/explain/#response",
    "relUrl": "/api-reference/explain/#response"
  },"2182": {
    "doc": "Explain",
    "title": "Response body fields",
    "content": "| Field | Description | . | matched | Indicates if the document is a match for the query. | . | explanation | The explanation object has three properties: value, description, and details. The value shows the result of the calculation, the description explains what type of calculation is performed, and the details shows any subcalculations performed. | . | Term frequency (tf) | How many times the term appears in a field for a given document. The more times the term occurs the higher is the relevance score. | . | Inverse document frequency (idf) | How often the term appears within the index (across all the documents). The more often the term appears the lower is the relevance score. | . | Field normalization factor (fieldNorm) | The length of the field. OpenSearch assigns a higher relevance score to a term appearing in a relatively short field. | . The tf, idf, and fieldNorm values are calculated and stored at index time when a document is added or updated. The values might have some (typically small) inaccuracies as it’s based on summing the samples returned from each shard. Individual queries include other factors for calculating the relevance score, such as term proximity, fuzziness, and so on. ",
    "url": "https://vagimeli.github.io/api-reference/explain/#response-body-fields",
    "relUrl": "/api-reference/explain/#response-body-fields"
  },"2183": {
    "doc": "Clear Index or Data Stream Cache",
    "title": "Clear index or data stream cache",
    "content": "The clear cache API operation clears the caches of one or more indexes. For data streams, the API clears the caches of the stream’s backing indexes. If you use the Security plugin, you must have the manage index privileges. Path parameters . | Parameter | Data type | Description | . | target | String | Comma-delimited list of data streams, indexes, and index aliases to which cache clearing will be applied. Wildcard expressions (*) are supported. To target all data streams and indexes in a cluster, omit this parameter or use _all or *. Optional. | . Query parameters . All query parameters are optional. | Parameter | Data type | Description | . | allow_no_indices | Boolean | Whether to ignore wildcards, index aliases, or _all target (target path parameter) values that don’t match any indexes. If false, the request returns an error if any wildcard expression, index alias, or _all target value doesn’t match any indexes. This behavior also applies if the request targets include other open indexes. For example, a request where the target is fig*,app* returns an error if an index starts with fig but no index starts with app. Defaults to true. | . | expand_wildcards | String | Determines the index types that wildcard expressions can expand to. Accepts multiple values separated by a comma, such as open,hidden. Valid values are: all – Expand to open, closed, and hidden indexes.open – Expand only to open indexes.closed – Expand only to closed indexeshidden – Expand to include hidden indexes. Must be combined with open, closed, or both.none – Expansions are not accepted. Defaults to open. | . | fielddata | Boolean | If true, clears the fields cache. Use the fields parameter to clear specific fields’ caches. Defaults to true. | . | fields | String | Used in conjunction with the fielddata parameter. Comma-delimited list of field names that will be cleared out of the cache. Does not support objects or field aliases. Defaults to all fields. | . | index | String | Comma-delimited list of index names that will be cleared out of the cache. | . | ignore_unavailable | Boolean | If true, OpenSearch ignores missing or closed indexes. Defaults to false. | . | query | Boolean | If true, clears the query cache. Defaults to true. | . | request | Boolean | If true, clears the request cache. Defaults to true. | . Example requests . The following example requests show multiple clear cache API uses. Clear a specific cache . The following request clears the fields cache only: . POST /my-index/_cache/clear?fielddata=true . copy . The following request clears the query cache only: . POST /my-index/_cache/clear?query=true . copy . The following request clears the request cache only: . POST /my-index/_cache/clear?request=true . copy . Clear the cache for specific fields . The following request clears the fields caches of fielda and fieldb: . POST /my-index/_cache/clear?fields=fielda,fieldb . copy . Clear caches for specific data streams and indexes . The following request clears the cache for two specific indexes: . POST /my-index,my-index2/_cache/clear . copy . Clear caches for all data streams and indexes . The following request clears the cache for all data streams and indexes: . POST /_cache/clear . copy . Example response . The POST /books,hockey/_cache/clear request returns the following fields: . { \"_shards\" : { \"total\" : 4, \"successful\" : 2, \"failed\" : 0 } } . Response fields . The POST /books,hockey/_cache/clear request returns the following response fields: . | Field | Data type | Description | . | _shards | Object | Shard information. | . | total | Integer | Total number of shards. | . | successful | Integer | Number of index shards with caches successfully cleared. | . | failed | Integer | Number of index shards with caches that failed to clear. | . ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/clear-index-cache/#clear-index-or-data-stream-cache",
    "relUrl": "/api-reference/index-apis/clear-index-cache/#clear-index-or-data-stream-cache"
  },"2184": {
    "doc": "Clear Index or Data Stream Cache",
    "title": "Clear Index or Data Stream Cache",
    "content": " ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/clear-index-cache/",
    "relUrl": "/api-reference/index-apis/clear-index-cache/"
  },"2185": {
    "doc": "Clone index",
    "title": "Clone index",
    "content": "The clone index API operation clones all data in an existing read-only index into a new index. The new index cannot already exist. ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/clone/",
    "relUrl": "/api-reference/index-apis/clone/"
  },"2186": {
    "doc": "Clone index",
    "title": "Example",
    "content": "PUT /sample-index1/_clone/cloned-index1 { \"settings\": { \"index\": { \"number_of_shards\": 2, \"number_of_replicas\": 1 } }, \"aliases\": { \"sample-alias1\": {} } } . copy . ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/clone/#example",
    "relUrl": "/api-reference/index-apis/clone/#example"
  },"2187": {
    "doc": "Clone index",
    "title": "Path and HTTP methods",
    "content": "POST /&lt;source-index&gt;/_clone/&lt;target-index&gt; PUT /&lt;source-index&gt;/_clone/&lt;target-index&gt; . ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/clone/#path-and-http-methods",
    "relUrl": "/api-reference/index-apis/clone/#path-and-http-methods"
  },"2188": {
    "doc": "Clone index",
    "title": "Index naming restrictions",
    "content": "OpenSearch indexes have the following naming restrictions: . | All letters must be lowercase. | Index names can’t begin with underscores (_) or hyphens (-). | Index names can’t contain spaces, commas, or the following characters: . :, \", *, +, /, \\, |, ?, #, &gt;, or &lt; . | . ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/clone/#index-naming-restrictions",
    "relUrl": "/api-reference/index-apis/clone/#index-naming-restrictions"
  },"2189": {
    "doc": "Clone index",
    "title": "URL parameters",
    "content": "Your request must include the source and target indexes. All other clone index parameters are optional. | Parameter | Type | Description | . | &lt;source-index&gt; | String | The source index to clone. | . | &lt;target-index&gt; | String | The index to create and add cloned data to. | . | wait_for_active_shards | String | The number of active shards that must be available before OpenSearch processes the request. Default is 1 (only the primary shard). Set to all or a positive integer. Values greater than 1 require replicas. For example, if you specify a value of 3, the index must have two replicas distributed across two additional nodes for the operation to succeed. | . | master_timeout | Time | How long to wait for a connection to the master node. Default is 30s. | . | timeout | Time | How long to wait for the request to return. Default is 30s. | . | wait_for_completion | Boolean | When set to false, the request returns immediately instead of after the operation is finished. To monitor the operation status, use the Tasks API with the task ID returned by the request. Default is true. | . | task_execution_timeout | Time | The explicit task execution timeout. Only useful when wait_for_completion is set to false. Default is 1h. | . ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/clone/#url-parameters",
    "relUrl": "/api-reference/index-apis/clone/#url-parameters"
  },"2190": {
    "doc": "Clone index",
    "title": "Request body",
    "content": "The clone index API operation creates a new target index, so you can specify any index settings and aliases to apply to the target index. ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/clone/#request-body",
    "relUrl": "/api-reference/index-apis/clone/#request-body"
  },"2191": {
    "doc": "Clone index",
    "title": "Response",
    "content": "{ \"acknowledged\": true, \"shards_acknowledged\": true, \"index\": \"cloned-index1\" } . ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/clone/#response",
    "relUrl": "/api-reference/index-apis/clone/#response"
  },"2192": {
    "doc": "Close index",
    "title": "Close index",
    "content": "Introduced 1.0 . The close index API operation closes an index. Once an index is closed, you cannot add data to it or search for any data within the index. ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/close-index/",
    "relUrl": "/api-reference/index-apis/close-index/"
  },"2193": {
    "doc": "Close index",
    "title": "Example",
    "content": "POST /sample-index/_close . copy . ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/close-index/#example",
    "relUrl": "/api-reference/index-apis/close-index/#example"
  },"2194": {
    "doc": "Close index",
    "title": "Path and HTTP methods",
    "content": "POST /&lt;index-name&gt;/_close . ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/close-index/#path-and-http-methods",
    "relUrl": "/api-reference/index-apis/close-index/#path-and-http-methods"
  },"2195": {
    "doc": "Close index",
    "title": "URL parameters",
    "content": "All parameters are optional. | Parameter | Type | Description | . | &lt;index-name&gt; | String | The index to close. Can be a comma-separated list of multiple index names. Use _all or * to close all indices. | . | allow_no_indices | Boolean | Whether to ignore wildcards that don’t match any indices. Default is true. | . | expand_wildcards | String | Expands wildcard expressions to different indices. Combine multiple values with commas. Available values are all (match all indices), open (match open indices), closed (match closed indices), hidden (match hidden indices), and none (do not accept wildcard expressions). Default is open. | . | ignore_unavailable | Boolean | If true, OpenSearch does not search for missing or closed indices. Default is false. | . | wait_for_active_shards | String | Specifies the number of active shards that must be available before OpenSearch processes the request. Default is 1 (only the primary shard). Set to all or a positive integer. Values greater than 1 require replicas. For example, if you specify a value of 3, the index must have two replicas distributed across two additional nodes for the request to succeed. | . | master_timeout | Time | How long to wait for a connection to the master node. Default is 30s. | . | timeout | Time | How long to wait for a response from the cluster. Default is 30s. | . ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/close-index/#url-parameters",
    "relUrl": "/api-reference/index-apis/close-index/#url-parameters"
  },"2196": {
    "doc": "Close index",
    "title": "Response",
    "content": "{ \"acknowledged\": true, \"shards_acknowledged\": true, \"indices\": { \"sample-index1\": { \"closed\": true } } } . ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/close-index/#response",
    "relUrl": "/api-reference/index-apis/close-index/#response"
  },"2197": {
    "doc": "Create index",
    "title": "Create index",
    "content": "Introduced 1.0 . While you can create an index by using a document as a base, you can also create an empty index for later use. ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/create-index/",
    "relUrl": "/api-reference/index-apis/create-index/"
  },"2198": {
    "doc": "Create index",
    "title": "Example",
    "content": "The following example demonstrates how to create an index with a non-default number of primary and replica shards, specifies that age is of type integer, and assigns a sample-alias1 alias to the index. PUT /sample-index1 { \"settings\": { \"index\": { \"number_of_shards\": 2, \"number_of_replicas\": 1 } }, \"mappings\": { \"properties\": { \"age\": { \"type\": \"integer\" } } }, \"aliases\": { \"sample-alias1\": {} } } . ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/create-index/#example",
    "relUrl": "/api-reference/index-apis/create-index/#example"
  },"2199": {
    "doc": "Create index",
    "title": "Path and HTTP methods",
    "content": "PUT &lt;index-name&gt; . ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/create-index/#path-and-http-methods",
    "relUrl": "/api-reference/index-apis/create-index/#path-and-http-methods"
  },"2200": {
    "doc": "Create index",
    "title": "Index naming restrictions",
    "content": "OpenSearch indexes have the following naming restrictions: . | All letters must be lowercase. | Index names can’t begin with underscores (_) or hyphens (-). | Index names can’t contain spaces, commas, or the following characters: . :, \", *, +, /, \\, |, ?, #, &gt;, or &lt; . | . ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/create-index/#index-naming-restrictions",
    "relUrl": "/api-reference/index-apis/create-index/#index-naming-restrictions"
  },"2201": {
    "doc": "Create index",
    "title": "URL parameters",
    "content": "You can include the following URL parameters in your request. All parameters are optional. | Parameter | Type | Description | . | wait_for_active_shards | String | Specifies the number of active shards that must be available before OpenSearch processes the request. Default is 1 (only the primary shard). Set to all or a positive integer. Values greater than 1 require replicas. For example, if you specify a value of 3, the index must have two replicas distributed across two additional nodes for the request to succeed. | . | master_timeout | Time | How long to wait for a connection to the master node. Default is 30s. | . | timeout | Time | How long to wait for the request to return. Default is 30s. | . ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/create-index/#url-parameters",
    "relUrl": "/api-reference/index-apis/create-index/#url-parameters"
  },"2202": {
    "doc": "Create index",
    "title": "Request body",
    "content": "As part of your request, you can supply parameters in your request’s body that specify index settings, mappings, and aliases for your newly created index. The following sections provide more information about index settings and mappings. Index settings . Index settings are separated into two varieties: static index settings and dynamic index settings. Static index settings are settings that you specify at index creation and can’t change later. You can change dynamic settings at any time, including at index creation. Static index settings . | Setting | Description | . | index.number_of_shards | The number of primary shards in the index. Default is 1. | . | index.number_of_routing_shards | The number of routing shards used to split an index. | . | index.shard.check_on_startup | Whether the index’s shards should be checked for corruption. Available options are false (do not check for corruption), checksum (check for physical corruption), and true (check for both physical and logical corruption). Default is false. | . | index.codec | The compression type to use to compress stored data. Available values are default (optimizes for retrieval speed) and best_compression (optimizes for better compression at the expense of speed, leading to smaller data sizes on disk). | . | index.routing_partition_size | The number of shards a custom routing value can go to. Routing helps an imbalanced cluster by relocating values to a subset of shards rather than just a single shard. To enable, set this value to greater than 1 but less than index.number_of_shards. Default is 1. | . | index.soft_deletes.retention_lease.period | The maximum amount of time to retain a shard’s history of operations. Default is 12h. | . | index.load_fixed_bitset_filters_eagerly | Whether OpenSearch should pre-load cached filters. Available options are true and false. Default is true. | . | index.hidden | Whether the index should be hidden. Hidden indexes are not returned as part of queries that have wildcards. Available options are true and false. Default is false. | . Dynamic index Settings . | Setting | Description | . | index.number_of_replicas | The number of replica shards each primary shard should have. For example, if you have 4 primary shards and set index.number_of_replicas to 3, the index has 12 replica shards. Default is 1. | . | index.auto_expand_replicas | Whether the cluster should automatically add replica shards based on the number of data nodes. Specify a lower bound and upper limit (for example, 0-9), or all for the upper limit. For example, if you have 5 data nodes and set index.auto_expand_replicas to 0-3, then the cluster does not automatically add another replica shard. However, if you set this value to 0-all and add 2 more nodes for a total of 7, the cluster will expand to now have 6 replica shards. Default is disabled. | . | index.search.idle.after | Amount of time a shard should wait for a search or get request until it goes idle. Default is 30s. | . | index.refresh_interval | How often the index should refresh, which publishes its most recent changes and makes them available for searching. Can be set to -1 to disable refreshing. Default is 1s. | . | index.max_result_window | The maximum value of from + size for searches to the index. from is the starting index to search from, and size is the amount of results to return. Default: 10000. | . | index.max_inner_result_window | Maximum value of from + size to return nested search hits and most relevant document aggregated during the query. from is the starting index to search from, and size is the amount of top hits to return. Default is 100. | . | index.max_rescore_window | The maximum value of window_size for rescore requests to the index. Rescore requests reorder the index’s documents and return a new score, which can be more precise. Default is the same as index.max_inner_result_window or 10000 by default. | . | index.max_docvalue_fields_search | Maximum amount of docvalue_fields allowed in a query. Default is 100. | . | index.max_script_fields | Maximum amount of script_fields allowed in a query. Default is 32. | . | index.max_ngram_diff | Maximum difference between min_gram and max_gram values for NGramTokenizer and NGramTokenFilter fields. Default is 1. | . | index.max_shingle_diff | Maximum difference between max_shingle_size and min_shingle_size to feed into the shingle token filter. Default is 3. | . | index.max_refresh_listeners | Maximum amount of refresh listeners each shard is allowed to have. | . | index.analyze.max_token_count | Maximum amount of tokens that can return from the _analyze API operation. Default is 10000. | . | index.highlight.max_analyzed_offset | The amount of characters a highlight request can analyze. Default is 1000000. | . | index.max_terms_count | The maximum amount of terms a terms query can accept. Default is 65536. | . | index.max_regex_length | The maximum character length of regex that can be in a regexp query. Default is 1000. | . | index.query.default_field | A field or list of fields that OpenSearch uses in queries in case a field isn’t specified in the parameters. | . | index.routing.allocation.enable | Specifies options for the index’s shard allocation. Available options are all (allow allocation for all shards), primaries (allow allocation only for primary shards), new_primaries (allow allocation only for new primary shards), and none (do not allow allocation). Default is all. | . | index.routing.rebalance.enable | Enables shard rebalancing for the index. Available options are all (allow rebalancing for all shards), primaries (allow rebalancing only for primary shards), replicas (allow rebalancing only for replicas), and none (do not allow rebalancing). Default is all. | . | index.gc_deletes | Amount of time to retain a deleted document’s version number. Default is 60s. | . | index.default_pipeline | The default ingest node pipeline for the index. If the default pipeline is set and the pipeline does not exist, then index requests fail. The pipeline name _none specifies that the index does not have an ingest pipeline. | . | index.final_pipeline | The final ingest node pipeline for the index. If the final pipeline is set and the pipeline does not exist, then index requests fail. The pipeline name _none specifies that the index does not have an ingest pipeline. | . ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/create-index/#request-body",
    "relUrl": "/api-reference/index-apis/create-index/#request-body"
  },"2203": {
    "doc": "Dangling indexes",
    "title": "Dangling indexes API",
    "content": "After a node joins a cluster, dangling indexes occur if any shards exist in the node’s local directory that do not already exist in the cluster. Dangling indexes can be listed, deleted, or imported. ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/dangling-index/#dangling-indexes-api",
    "relUrl": "/api-reference/index-apis/dangling-index/#dangling-indexes-api"
  },"2204": {
    "doc": "Dangling indexes",
    "title": "Path and HTTP methods",
    "content": "List dangling indexes: . GET /_dangling . Import a dangling index: . POST /_dangling/&lt;index-uuid&gt; . Delete a dangling index: . DELETE /_dangling/&lt;index-uuid&gt; . ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/dangling-index/#path-and-http-methods",
    "relUrl": "/api-reference/index-apis/dangling-index/#path-and-http-methods"
  },"2205": {
    "doc": "Dangling indexes",
    "title": "Path parameters",
    "content": "Path parameters are required. | Path parameter | Description | . | index-uuid | UUID of index. | . ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/dangling-index/#path-parameters",
    "relUrl": "/api-reference/index-apis/dangling-index/#path-parameters"
  },"2206": {
    "doc": "Dangling indexes",
    "title": "Query parameters",
    "content": "Query parameters are optional. | Query parameter | Data type | Description | . | accept_data_loss | Boolean | Must be set to true for an import or delete because OpenSearch is unaware of where the dangling index data came from. | . | timeout | Time units | The amount of time to wait for a response. If no response is received in the defined time period, an error is returned. Default is 30 seconds. | . | master_timeout | Time units | The amount of time to wait for the connection to the cluster manager. If no response is received in the defined time period, an error is returned. Default is 30 seconds. | . ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/dangling-index/#query-parameters",
    "relUrl": "/api-reference/index-apis/dangling-index/#query-parameters"
  },"2207": {
    "doc": "Dangling indexes",
    "title": "Examples",
    "content": "The following are example requests and a example response. Sample list . GET /_dangling . copy . Sample import . POST /_dangling/msdjernajxAT23RT-BupMB?accept_data_loss=true . copy . Sample delete . DELETE /_dangling/msdjernajxAT23RT-BupMB?accept_data_loss=true . Example response body . { \"_nodes\": { \"total\": 1, \"successful\": 1, \"failed\": 0 }, \"cluster_name\": \"opensearch-cluster\", \"dangling_indices\": [msdjernajxAT23RT-BupMB] } . ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/dangling-index/#examples",
    "relUrl": "/api-reference/index-apis/dangling-index/#examples"
  },"2208": {
    "doc": "Dangling indexes",
    "title": "Dangling indexes",
    "content": " ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/dangling-index/",
    "relUrl": "/api-reference/index-apis/dangling-index/"
  },"2209": {
    "doc": "Delete index",
    "title": "Delete index",
    "content": "Introduced 1.0 . If you no longer need an index, you can use the delete index API operation to delete it. ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/delete-index/",
    "relUrl": "/api-reference/index-apis/delete-index/"
  },"2210": {
    "doc": "Delete index",
    "title": "Example",
    "content": "DELETE /sample-index . copy . ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/delete-index/#example",
    "relUrl": "/api-reference/index-apis/delete-index/#example"
  },"2211": {
    "doc": "Delete index",
    "title": "Path and HTTP methods",
    "content": "DELETE /&lt;index-name&gt; . ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/delete-index/#path-and-http-methods",
    "relUrl": "/api-reference/index-apis/delete-index/#path-and-http-methods"
  },"2212": {
    "doc": "Delete index",
    "title": "URL parameters",
    "content": "All parameters are optional. | Parameter | Type | Description | . | allow_no_indices | Boolean | Whether to ignore wildcards that don’t match any indexes. Default is true. | . | expand_wildcards | String | Expands wildcard expressions to different indexes. Combine multiple values with commas. Available values are all (match all indexes), open (match open indexes), closed (match closed indexes), hidden (match hidden indexes), and none (do not accept wildcard expressions), which must be used with open, closed, or both. Default is open. | . | ignore_unavailable | Boolean | If true, OpenSearch does not include missing or closed indexes in the response. | . | master_timeout | Time | How long to wait for a connection to the master node. Default is 30s. | . | timeout | Time | How long to wait for the response to return. Default is 30s. | . ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/delete-index/#url-parameters",
    "relUrl": "/api-reference/index-apis/delete-index/#url-parameters"
  },"2213": {
    "doc": "Delete index",
    "title": "Response",
    "content": "{ \"acknowledged\": true } . ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/delete-index/#response",
    "relUrl": "/api-reference/index-apis/delete-index/#response"
  },"2214": {
    "doc": "Index exists",
    "title": "Index exists",
    "content": "Introduced 1.0 . The index exists API operation returns whether or not an index already exists. ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/exists/",
    "relUrl": "/api-reference/index-apis/exists/"
  },"2215": {
    "doc": "Index exists",
    "title": "Example",
    "content": "HEAD /sample-index . copy . ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/exists/#example",
    "relUrl": "/api-reference/index-apis/exists/#example"
  },"2216": {
    "doc": "Index exists",
    "title": "Path and HTTP methods",
    "content": "HEAD /&lt;index-name&gt; . ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/exists/#path-and-http-methods",
    "relUrl": "/api-reference/index-apis/exists/#path-and-http-methods"
  },"2217": {
    "doc": "Index exists",
    "title": "URL parameters",
    "content": "All parameters are optional. | Parameter | Type | Description | . | allow_no_indices | Boolean | Whether to ignore wildcards that don’t match any indexes. Default is true. | . | expand_wildcards | String | Expands wildcard expressions to different indexes. Combine multiple values with commas. Available values are all (match all indexes), open (match open indexes), closed (match closed indexes), hidden (match hidden indexes), and none (do not accept wildcard expressions). Default is open. | . | flat_settings | Boolean | Whether to return settings in the flat form, which can improve readability, especially for heavily nested settings. For example, the flat form of “index”: { “creation_date”: “123456789” } is “index.creation_date”: “123456789”. | . | include_defaults | Boolean | Whether to include default settings as part of the response. This parameter is useful for identifying the names and current values of settings you want to update. | . | ignore_unavailable | Boolean | If true, OpenSearch does not search for missing or closed indexes. Default is false. | . | local | Boolean | Whether to return information from only the local node instead of from the master node. Default is false. | . ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/exists/#url-parameters",
    "relUrl": "/api-reference/index-apis/exists/#url-parameters"
  },"2218": {
    "doc": "Index exists",
    "title": "Response",
    "content": "The index exists API operation returns only one of two possible response codes: 200 – the index exists, and 404 – the index does not exist. ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/exists/#response",
    "relUrl": "/api-reference/index-apis/exists/#response"
  },"2219": {
    "doc": "Get index",
    "title": "Get index",
    "content": "Introduced 1.0 . You can use the get index API operation to return information about an index. ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/get-index/",
    "relUrl": "/api-reference/index-apis/get-index/"
  },"2220": {
    "doc": "Get index",
    "title": "Example",
    "content": "GET /sample-index . copy . ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/get-index/#example",
    "relUrl": "/api-reference/index-apis/get-index/#example"
  },"2221": {
    "doc": "Get index",
    "title": "Path and HTTP methods",
    "content": "GET /&lt;index-name&gt; . ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/get-index/#path-and-http-methods",
    "relUrl": "/api-reference/index-apis/get-index/#path-and-http-methods"
  },"2222": {
    "doc": "Get index",
    "title": "URL parameters",
    "content": "All parameters are optional. | Parameter | Type | Description | . | allow_no_indices | Boolean | Whether to ignore wildcards that don’t match any indexes. Default is true. | . | expand_wildcards | String | Expands wildcard expressions to different indexes. Combine multiple values with commas. Available values are all (match all indexes), open (match open indexes), closed (match closed indexes), hidden (match hidden indexes), and none (do not accept wildcard expressions), which must be used with open, closed, or both. Default is open. | . | flat_settings | Boolean | Whether to return settings in the flat form, which can improve readability, especially for heavily nested settings. For example, the flat form of “index”: { “creation_date”: “123456789” } is “index.creation_date”: “123456789”. | . | include_defaults | Boolean | Whether to include default settings as part of the response. This parameter is useful for identifying the names and current values of settings you want to update. | . | ignore_unavailable | Boolean | If true, OpenSearch does not include missing or closed indexes in the response. | . | local | Boolean | Whether to return information from only the local node instead of from the master node. Default is false. | . | master_timeout | Time | How long to wait for a connection to the master node. Default is 30s. | . ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/get-index/#url-parameters",
    "relUrl": "/api-reference/index-apis/get-index/#url-parameters"
  },"2223": {
    "doc": "Get index",
    "title": "Response",
    "content": "{ \"sample-index1\": { \"aliases\": {}, \"mappings\": {}, \"settings\": { \"index\": { \"creation_date\": \"1633044652108\", \"number_of_shards\": \"2\", \"number_of_replicas\": \"1\", \"uuid\": \"XcXA0aZ5S0aiqx3i1Ce95w\", \"version\": { \"created\": \"135217827\" }, \"provided_name\": \"sample-index1\" } } } } . ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/get-index/#response",
    "relUrl": "/api-reference/index-apis/get-index/#response"
  },"2224": {
    "doc": "Get index",
    "title": "Response body fields",
    "content": "| Field | Description | . | aliases | Any aliases associated with the index. | . | mappings | Any mappings in the index. | . | settings | The index’s settings | . | creation_date | The Unix epoch time of when the index was created. | . | number_of_shards | How many shards the index has. | . | number_of_replicas | How many replicas the index has. | . | uuid | The index’s uuid. | . | created | The version of OpenSearch when the index was created. | . | provided_name | Name of the index. | . ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/get-index/#response-body-fields",
    "relUrl": "/api-reference/index-apis/get-index/#response-body-fields"
  },"2225": {
    "doc": "Get settings",
    "title": "Get settings",
    "content": "Introduced 1.0 . The get settings API operation returns all the settings in your index. ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/get-settings/",
    "relUrl": "/api-reference/index-apis/get-settings/"
  },"2226": {
    "doc": "Get settings",
    "title": "Example",
    "content": "GET /sample-index1/_settings . copy . ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/get-settings/#example",
    "relUrl": "/api-reference/index-apis/get-settings/#example"
  },"2227": {
    "doc": "Get settings",
    "title": "Path and HTTP methods",
    "content": "GET /_settings GET /&lt;target-index&gt;/_settings GET /&lt;target-index&gt;/_settings/&lt;setting&gt; . ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/get-settings/#path-and-http-methods",
    "relUrl": "/api-reference/index-apis/get-settings/#path-and-http-methods"
  },"2228": {
    "doc": "Get settings",
    "title": "URL parameters",
    "content": "All update settings parameters are optional. | Parameter | Data type | Description | . | &lt;target-index&gt; | String | The index to get settings from. Can be a comma-separated list to get settings from multiple indexes, or use _all to return settings from all indexes within the cluster. | . | &lt;setting&gt; | String | Filter to return specific settings. | . | allow_no_indices | Boolean | Whether to ignore wildcards that don’t match any indexes. Default is true. | . | expand_wildcards | String | Expands wildcard expressions to different indexes. Combine multiple values with commas. Available values are all (match all indexes), open (match open indexes), closed (match closed indexes), hidden (match hidden indexes), and none (do not accept wildcard expressions), which must be used with open, closed, or both. Default is open. | . | flat_settings | Boolean | Whether to return settings in the flat form, which can improve readability, especially for heavily nested settings. For example, the flat form of “index”: { “creation_date”: “123456789” } is “index.creation_date”: “123456789”. | . | include_defaults | String | Whether to include default settings, including settings used within OpenSearch plugins, in the response. Default is false. | . | ignore_unavailable | Boolean | If true, OpenSearch does not include missing or closed indexes in the response. | . | local | Boolean | Whether to return information from the local node only instead of the master node. Default is false. | . | master_timeout | Time | How long to wait for a connection to the master node. Default is 30s. | . ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/get-settings/#url-parameters",
    "relUrl": "/api-reference/index-apis/get-settings/#url-parameters"
  },"2229": {
    "doc": "Get settings",
    "title": "Response",
    "content": "{ \"sample-index1\": { \"settings\": { \"index\": { \"creation_date\": \"1622672553417\", \"number_of_shards\": \"1\", \"number_of_replicas\": \"1\", \"uuid\": \"GMEA0_TkSaamrnJSzNLzwg\", \"version\": { \"created\": \"135217827\", \"upgraded\": \"135238227\" }, \"provided_name\": \"sample-index1\" } } } } . ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/get-settings/#response",
    "relUrl": "/api-reference/index-apis/get-settings/#response"
  },"2230": {
    "doc": "Index APIs",
    "title": "Index APIs",
    "content": "The index API operations let you interact with indexes in your cluster. Using these operations, you can create, delete, close, and complete other index-related operations. If you use the Security plugin, make sure you have the appropriate permissions. ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/index/",
    "relUrl": "/api-reference/index-apis/index/"
  },"2231": {
    "doc": "Open index",
    "title": "Open index",
    "content": "Introduced 1.0 . The open index API operation opens a closed index, letting you add or search for data within the index. ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/open-index/",
    "relUrl": "/api-reference/index-apis/open-index/"
  },"2232": {
    "doc": "Open index",
    "title": "Example",
    "content": "POST /sample-index/_open . copy . ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/open-index/#example",
    "relUrl": "/api-reference/index-apis/open-index/#example"
  },"2233": {
    "doc": "Open index",
    "title": "Path and HTTP methods",
    "content": "POST /&lt;index-name&gt;/_open . ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/open-index/#path-and-http-methods",
    "relUrl": "/api-reference/index-apis/open-index/#path-and-http-methods"
  },"2234": {
    "doc": "Open index",
    "title": "URL parameters",
    "content": "All parameters are optional. | Parameter | Type | Description | . | &lt;index-name&gt; | String | The index to open. Can be a comma-separated list of multiple index names. Use _all or * to open all indexes. | . | allow_no_indices | Boolean | Whether to ignore wildcards that don’t match any indexes. Default is true. | . | expand_wildcards | String | Expands wildcard expressions to different indexes. Combine multiple values with commas. Available values are all (match all indexes), open (match open indexes), closed (match closed indexes), hidden (match hidden indexes), and none (do not accept wildcard expressions). Default is open. | . | ignore_unavailable | Boolean | If true, OpenSearch does not search for missing or closed indexes. Default is false. | . | wait_for_active_shards | String | Specifies the number of active shards that must be available before OpenSearch processes the request. Default is 1 (only the primary shard). Set to all or a positive integer. Values greater than 1 require replicas. For example, if you specify a value of 3, the index must have two replicas distributed across two additional nodes for the request to succeed. | . | master_timeout | Time | How long to wait for a connection to the master node. Default is 30s. | . | timeout | Time | How long to wait for a response from the cluster. Default is 30s. | . | wait_for_completion | Boolean | When set to false, the request returns immediately instead of after the operation is finished. To monitor the operation status, use the Tasks API with the task ID returned by the request. Default is true. | . | task_execution_timeout | Time | The explicit task execution timeout. Only useful when wait_for_completion is set to false. Default is 1h. | . ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/open-index/#url-parameters",
    "relUrl": "/api-reference/index-apis/open-index/#url-parameters"
  },"2235": {
    "doc": "Open index",
    "title": "Response",
    "content": "{ \"acknowledged\": true, \"shards_acknowledged\": true } . ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/open-index/#response",
    "relUrl": "/api-reference/index-apis/open-index/#response"
  },"2236": {
    "doc": "Create or update mappings",
    "title": "Create or update mappings",
    "content": "Introduced 1.0 . If you want to create or add mappings and fields to an index, you can use the put mapping API operation. For an existing mapping, this operation updates the mapping. You can’t use this operation to update mappings that already map to existing data in the index. You must first create a new index with your desired mappings, and then use the reindex API operation to map all the documents from your old index to the new index. If you don’t want any downtime while you re-index your indexes, you can use aliases. ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/put-mapping/",
    "relUrl": "/api-reference/index-apis/put-mapping/"
  },"2237": {
    "doc": "Create or update mappings",
    "title": "Required path parameter",
    "content": "The only required path parameter is the index with which to associate the mapping. If you don’t specify an index, you will get an error. You can specify a single index, or multiple indexes separated by a comma as follows: . PUT /&lt;target-index&gt;/_mapping PUT /&lt;target-index1&gt;,&lt;target-index2&gt;/_mapping . ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/put-mapping/#required-path-parameter",
    "relUrl": "/api-reference/index-apis/put-mapping/#required-path-parameter"
  },"2238": {
    "doc": "Create or update mappings",
    "title": "Required request body field",
    "content": "The request body must contain properties, which has all of the mappings that you want to create or update. { \"properties\":{ \"color\":{ \"type\": \"text\" }, \"year\":{ \"type\": \"integer\" } } } . ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/put-mapping/#required-request-body-field",
    "relUrl": "/api-reference/index-apis/put-mapping/#required-request-body-field"
  },"2239": {
    "doc": "Create or update mappings",
    "title": "Optional request body fields",
    "content": "dynamic . You can make the document structure match the structure of the index mapping by setting the dynamic request body field to strict, as seen in the following example: . { \"properties\":{ \"dynamic\": \"strict\", \"color\":{ \"type\": \"text\" } } } . ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/put-mapping/#optional-request-body-fields",
    "relUrl": "/api-reference/index-apis/put-mapping/#optional-request-body-fields"
  },"2240": {
    "doc": "Create or update mappings",
    "title": "Optional query parameters",
    "content": "Optionally, you can add query parameters to make a more specific request. For example, to skip any missing or closed indexes in the response, you can add the ignore_unavailable query parameter to your request as follows: . PUT /sample-index/_mapping?ignore_unavailable . The following table defines the put mapping query parameters: . | Parameter | Data type | Description | . | allow_no_indices | Boolean | Whether to ignore wildcards that don’t match any indexes. Default is true. | . | expand_wildcards | String | Expands wildcard expressions to different indexes. Combine multiple values with commas. Available values are all (match all indexes), open (match open indexes), closed (match closed indexes), hidden (match hidden indexes), and none (do not accept wildcard expressions), which must be used with open, closed, or both. Default is open. | . | ignore_unavailable | Boolean | If true, OpenSearch does not include missing or closed indexes in the response. | . | ignore_malformed | Boolean | Use this parameter with the ip_range data type to specify that OpenSearch should ignore malformed fields. If true, OpenSearch does not include entries that do not match the IP range specified in the index in the response. The default is false. | . | cluster_manager_timeout | Time | How long to wait for a connection to the cluster manager node. Default is 30s. | . | timeout | Time | How long to wait for the response to return. Default is 30s. | . | write_index_only | Boolean | Whether OpenSearch should apply mapping updates only to the write index. | . Sample Request . The following request creates a new mapping for the sample-index index: . PUT /sample-index/_mapping { \"properties\": { \"age\": { \"type\": \"integer\" }, \"occupation\":{ \"type\": \"text\" } } } . copy . Sample Response . Upon success, the response returns \"acknowledged\": true. { \"acknowledged\": true } . ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/put-mapping/#optional-query-parameters",
    "relUrl": "/api-reference/index-apis/put-mapping/#optional-query-parameters"
  },"2241": {
    "doc": "Shrink index",
    "title": "Shrink index",
    "content": "The shrink index API operation moves all of your data in an existing index into a new index with fewer primary shards. ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/shrink-index/",
    "relUrl": "/api-reference/index-apis/shrink-index/"
  },"2242": {
    "doc": "Shrink index",
    "title": "Example",
    "content": "POST /my-old-index/_shrink/my-new-index { \"settings\": { \"index.number_of_replicas\": 4, \"index.number_of_shards\": 3 }, \"aliases\":{ \"new-index-alias\": {} } } . copy . ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/shrink-index/#example",
    "relUrl": "/api-reference/index-apis/shrink-index/#example"
  },"2243": {
    "doc": "Shrink index",
    "title": "Path and HTTP methods",
    "content": "POST /&lt;index-name&gt;/_shrink/&lt;target-index&gt; PUT /&lt;index-name&gt;/_shrink/&lt;target-index&gt; . When creating new indexes with this operation, remember that OpenSearch indexes have the following naming restrictions: . | All letters must be lowercase. | Index names can’t begin with underscores (_) or hyphens (-). | Index names can’t contain spaces, commas, or the following characters: . :, \", *, +, /, \\, |, ?, #, &gt;, or &lt; . | . ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/shrink-index/#path-and-http-methods",
    "relUrl": "/api-reference/index-apis/shrink-index/#path-and-http-methods"
  },"2244": {
    "doc": "Shrink index",
    "title": "URL parameters",
    "content": "The shrink index API operation requires you to specify both the source index and the target index. All other parameters are optional. | Parameter | Type | description | . | &lt;index-name&gt; | String | The index to shrink. | . | &lt;target-index&gt; | String | The target index to shrink the source index into. | . | wait_for_active_shards | String | Specifies the number of active shards that must be available before OpenSearch processes the request. Default is 1 (only the primary shard). Set to all or a positive integer. Values greater than 1 require replicas. For example, if you specify a value of 3, the index must have two replicas distributed across two additional nodes for the request to succeed. | . | master_timeout | Time | How long to wait for a connection to the master node. Default is 30s. | . | timeout | Time | How long to wait for the request to return a response. Default is 30s. | . | wait_for_completion | Boolean | When set to false, the request returns immediately instead of after the operation is finished. To monitor the operation status, use the Tasks API with the task ID returned by the request. Default is true. | . | task_execution_timeout | Time | The explicit task execution timeout. Only useful when wait_for_completion is set to false. Default is 1h. | . ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/shrink-index/#url-parameters",
    "relUrl": "/api-reference/index-apis/shrink-index/#url-parameters"
  },"2245": {
    "doc": "Shrink index",
    "title": "Request body",
    "content": "You can use the request body to configure some index settings for the target index. All fields are optional. | Field | Type | Description | . | alias | Object | Sets an alias for the target index. Can have the fields filter, index_routing, is_hidden, is_write_index, routing, or search_routing. See Index Aliases. | . | settings | Object | Index settings you can apply to your target index. See Index Settings. | . | max_shard_size | Bytes | Specifies the maximum size of a primary shard in the target index. Because max_shard_size conflicts with the index.number_of_shards setting, you cannot set both of them at the same time. | . The max_shard_size parameter . The max_shard_size parameter specifies the maximum size of a primary shard in the target index. OpenSearch uses max_shard_size and the total storage for all primary shards in the source index to calculate the number of primary shards and their size for the target index. The primary shard count of the target index is the smallest factor of the source index’s primary shard count for which the shard size does not exceed max_shard_size. For example, if the source index has 8 primary shards, they occupy a total of 400 GB of storage, and the max_shard_size is equal to 150 GB, OpenSearch calculates the number of primary shards in the target index using the following algorithm: . | Calculate the minimum number of primary shards as 400/150, rounded to the nearest whole integer. The minimum number of primary shards is 3. | Calculate the number of primary shards as the smallest factor of 8 that is greater than 3. The number of primary shards is 4. | . The maximum number of primary shards for the target index is equal to the number of primary shards in the source index because the shrink operation is used to reduce the primary shard count. As an example, consider a source index with 5 primary shards that occupy a total of 600 GB of storage. If max_shard_size is 100 GB, the minimum number of primary shards is 600/100, which is 6. However, because the number of primary shards in the source index is smaller than 6, the number of primary shards in the target index is set to 5. The minimum number of primary shards for the target index is 1. ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/shrink-index/#request-body",
    "relUrl": "/api-reference/index-apis/shrink-index/#request-body"
  },"2246": {
    "doc": "Split index",
    "title": "Split index",
    "content": "The split index API operation splits an existing read-only index into a new index, cutting each primary shard into some amount of primary shards in the new index. ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/split/",
    "relUrl": "/api-reference/index-apis/split/"
  },"2247": {
    "doc": "Split index",
    "title": "Example",
    "content": "PUT /sample-index1/_split/split-index1 { \"settings\": { \"index\": { \"number_of_shards\": 4, \"number_of_replicas\": 2 } }, \"aliases\": { \"sample-alias1\": {} } } . copy . ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/split/#example",
    "relUrl": "/api-reference/index-apis/split/#example"
  },"2248": {
    "doc": "Split index",
    "title": "Path and HTTP methods",
    "content": "POST /&lt;source-index&gt;/_split/&lt;target-index&gt; PUT /&lt;source-index&gt;/_split/&lt;target-index&gt; . ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/split/#path-and-http-methods",
    "relUrl": "/api-reference/index-apis/split/#path-and-http-methods"
  },"2249": {
    "doc": "Split index",
    "title": "Index naming restrictions",
    "content": "OpenSearch indexes have the following naming restrictions: . | All letters must be lowercase. | Index names can’t begin with underscores (_) or hyphens (-). | Index names can’t contain spaces, commas, or the following characters: . :, \", *, +, /, \\, |, ?, #, &gt;, or &lt; . | . ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/split/#index-naming-restrictions",
    "relUrl": "/api-reference/index-apis/split/#index-naming-restrictions"
  },"2250": {
    "doc": "Split index",
    "title": "URL parameters",
    "content": "Your request must include the source and target indexes. All split index parameters are optional. | Parameter | Type | Description | . | &lt;source-index&gt; | String | The source index to split. | . | &lt;target-index&gt; | String | The index to create. | . | wait_for_active_shards | String | The number of active shards that must be available before OpenSearch processes the request. Default is 1 (only the primary shard). Set to all or a positive integer. Values greater than 1 require replicas. For example, if you specify a value of 3, the index must have two replicas distributed across two additional nodes for the operation to succeed. | . | master_timeout | Time | How long to wait for a connection to the master node. Default is 30s. | . | timeout | Time | How long to wait for the request to return. Default is 30s. | . | wait_for_completion | Boolean | When set to false, the request returns immediately instead of after the operation is finished. To monitor the operation status, use the Tasks API with the task ID returned by the request. Default is true. | . | task_execution_timeout | Time | The explicit task execution timeout. Only useful when wait_for_completion is set to false. Default is 1h. | . ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/split/#url-parameters",
    "relUrl": "/api-reference/index-apis/split/#url-parameters"
  },"2251": {
    "doc": "Split index",
    "title": "Request body",
    "content": "The split index API operation creates a new target index, so you can specify any index settings and aliases to apply to the target index. ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/split/#request-body",
    "relUrl": "/api-reference/index-apis/split/#request-body"
  },"2252": {
    "doc": "Split index",
    "title": "Response",
    "content": "{ \"acknowledged\": true, \"shards_acknowledged\": true, \"index\": \"split-index1\" } . ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/split/#response",
    "relUrl": "/api-reference/index-apis/split/#response"
  },"2253": {
    "doc": "Update settings",
    "title": "Update settings",
    "content": "Introduced 1.0 . You can use the update settings API operation to update index-level settings. You can change dynamic index settings at any time, but static settings cannot be changed after index creation. For more information about static and dynamic index settings, see Create index. Aside from the static and dynamic index settings, you can also update individual plugins’ settings. To get the full list of updatable settings, run GET &lt;target-index&gt;/_settings?include_defaults=true. ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/update-settings/",
    "relUrl": "/api-reference/index-apis/update-settings/"
  },"2254": {
    "doc": "Update settings",
    "title": "Example",
    "content": "PUT /sample-index1/_settings { \"index.plugins.index_state_management.rollover_skip\": true, \"index\": { \"number_of_replicas\": 4 } } . copy . ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/update-settings/#example",
    "relUrl": "/api-reference/index-apis/update-settings/#example"
  },"2255": {
    "doc": "Update settings",
    "title": "Path and HTTP methods",
    "content": "PUT /&lt;target-index&gt;/_settings . ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/update-settings/#path-and-http-methods",
    "relUrl": "/api-reference/index-apis/update-settings/#path-and-http-methods"
  },"2256": {
    "doc": "Update settings",
    "title": "URL parameters",
    "content": "All update settings parameters are optional. | Parameter | Data type | Description | . | allow_no_indices | Boolean | Whether to ignore wildcards that don’t match any indexes. Default is true. | . | expand_wildcards | String | Expands wildcard expressions to different indexes. Combine multiple values with commas. Available values are all (match all indexes), open (match open indexes), closed (match closed indexes), hidden (match hidden indexes), and none (do not accept wildcard expressions), which must be used with open, closed, or both. Default is open. | . | flat_settings | Boolean | Whether to return settings in the flat form, which can improve readability, especially for heavily nested settings. For example, the flat form of “index”: { “creation_date”: “123456789” } is “index.creation_date”: “123456789”. | . | ignore_unavailable | Boolean | If true, OpenSearch does not include missing or closed indexes in the response. | . | preserve_existing | Boolean | Whether to preserve existing index settings. Default is false. | . | master_timeout | Time | How long to wait for a connection to the master node. Default is 30s. | . | timeout | Time | How long to wait for a connection to return. Default is 30s. | . ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/update-settings/#url-parameters",
    "relUrl": "/api-reference/index-apis/update-settings/#url-parameters"
  },"2257": {
    "doc": "Update settings",
    "title": "Request body",
    "content": "The request body must all of the index settings that you want to update. { \"index.plugins.index_state_management.rollover_skip\": true, \"index\": { \"number_of_replicas\": 4 } } . ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/update-settings/#request-body",
    "relUrl": "/api-reference/index-apis/update-settings/#request-body"
  },"2258": {
    "doc": "Update settings",
    "title": "Response",
    "content": "{ \"acknowledged\": true } . ",
    "url": "https://vagimeli.github.io/api-reference/index-apis/update-settings/#response",
    "relUrl": "/api-reference/index-apis/update-settings/#response"
  },"2259": {
    "doc": "REST API reference",
    "title": "REST API reference",
    "content": "OpenSearch uses its REST API for most operations. This incomplete section includes REST API paths, HTTP verbs, supported parameters, request body details, and example responses. In general, the OpenSearch REST API is no different from the Elasticsearch OSS REST API; most client code that worked with Elasticsearch OSS should also work with OpenSearch. ",
    "url": "https://vagimeli.github.io/api-reference/index/",
    "relUrl": "/api-reference/index/"
  },"2260": {
    "doc": "Create or update ingest pipeline",
    "title": "Create and update a pipeline",
    "content": "The create ingest pipeline API operation creates or updates an ingest pipeline. Each pipeline requires an ingest definition defining how each processor transforms your documents. ",
    "url": "https://vagimeli.github.io/api-reference/ingest-apis/create-update-ingest/#create-and-update-a-pipeline",
    "relUrl": "/api-reference/ingest-apis/create-update-ingest/#create-and-update-a-pipeline"
  },"2261": {
    "doc": "Create or update ingest pipeline",
    "title": "Example",
    "content": "PUT _ingest/pipeline/12345 { \"description\" : \"A description for your pipeline\", \"processors\" : [ { \"set\" : { \"field\": \"field-name\", \"value\": \"value\" } } ] } . copy . ",
    "url": "https://vagimeli.github.io/api-reference/ingest-apis/create-update-ingest/#example",
    "relUrl": "/api-reference/ingest-apis/create-update-ingest/#example"
  },"2262": {
    "doc": "Create or update ingest pipeline",
    "title": "Path and HTTP methods",
    "content": "PUT _ingest/pipeline/{id} . ",
    "url": "https://vagimeli.github.io/api-reference/ingest-apis/create-update-ingest/#path-and-http-methods",
    "relUrl": "/api-reference/ingest-apis/create-update-ingest/#path-and-http-methods"
  },"2263": {
    "doc": "Create or update ingest pipeline",
    "title": "Request body fields",
    "content": "| Field | Required | Type | Description | . | description | Optional | string | Description of your ingest pipeline. | . | processors | Required | Array of processor objects | A processor that transforms documents. Runs in the order specified. Appears in index once ran. | . { \"description\" : \"A description for your pipeline\", \"processors\" : [ { \"set\" : { \"field\": \"field-name\", \"value\": \"value\" } } ] } . ",
    "url": "https://vagimeli.github.io/api-reference/ingest-apis/create-update-ingest/#request-body-fields",
    "relUrl": "/api-reference/ingest-apis/create-update-ingest/#request-body-fields"
  },"2264": {
    "doc": "Create or update ingest pipeline",
    "title": "URL parameters",
    "content": "All URL parameters are optional. | Parameter | Type | Description | . | master_timeout | time | How long to wait for a connection to the master node. | . | timeout | time | How long to wait for the request to return. | . ",
    "url": "https://vagimeli.github.io/api-reference/ingest-apis/create-update-ingest/#url-parameters",
    "relUrl": "/api-reference/ingest-apis/create-update-ingest/#url-parameters"
  },"2265": {
    "doc": "Create or update ingest pipeline",
    "title": "Response",
    "content": "{ \"acknowledged\" : true } . ",
    "url": "https://vagimeli.github.io/api-reference/ingest-apis/create-update-ingest/#response",
    "relUrl": "/api-reference/ingest-apis/create-update-ingest/#response"
  },"2266": {
    "doc": "Create or update ingest pipeline",
    "title": "Create or update ingest pipeline",
    "content": " ",
    "url": "https://vagimeli.github.io/api-reference/ingest-apis/create-update-ingest/",
    "relUrl": "/api-reference/ingest-apis/create-update-ingest/"
  },"2267": {
    "doc": "Delete a pipeline",
    "title": "Delete a pipeline",
    "content": "If you no longer want to use an ingest pipeline, use the delete ingest pipeline API operation. ",
    "url": "https://vagimeli.github.io/api-reference/ingest-apis/delete-ingest/",
    "relUrl": "/api-reference/ingest-apis/delete-ingest/"
  },"2268": {
    "doc": "Delete a pipeline",
    "title": "Example",
    "content": "DELETE _ingest/pipeline/12345 . copy . ",
    "url": "https://vagimeli.github.io/api-reference/ingest-apis/delete-ingest/#example",
    "relUrl": "/api-reference/ingest-apis/delete-ingest/#example"
  },"2269": {
    "doc": "Delete a pipeline",
    "title": "Path and HTTP methods",
    "content": "Delete an ingest pipeline based on that pipeline’s ID. DELETE _ingest/pipeline/ . ",
    "url": "https://vagimeli.github.io/api-reference/ingest-apis/delete-ingest/#path-and-http-methods",
    "relUrl": "/api-reference/ingest-apis/delete-ingest/#path-and-http-methods"
  },"2270": {
    "doc": "Delete a pipeline",
    "title": "URL parameters",
    "content": "All URL parameters are optional. | Parameter | Type | Description | . | master_timeout | time | How long to wait for a connection to the master node. | . | timeout | time | How long to wait for the request to return. | . ",
    "url": "https://vagimeli.github.io/api-reference/ingest-apis/delete-ingest/#url-parameters",
    "relUrl": "/api-reference/ingest-apis/delete-ingest/#url-parameters"
  },"2271": {
    "doc": "Delete a pipeline",
    "title": "Response",
    "content": "{ \"acknowledged\" : true } . ",
    "url": "https://vagimeli.github.io/api-reference/ingest-apis/delete-ingest/#response",
    "relUrl": "/api-reference/ingest-apis/delete-ingest/#response"
  },"2272": {
    "doc": "IP geolocation processor",
    "title": "IP geolocation processor",
    "content": "Introduced 2.8 . Information about the geolocation of an IP address can be used for a variety of purposes: . | Content personalization: You can use IP geolocation information to personalize content for your users based on their location. For example, you could show different versions of your website to users from different countries. | Security: You can use GeoIP to block access to your website from certain countries. This can be helpful to protect your website from attacks or to comply with regulations. | Analytics: You can use GeoIP to track the geographic location of your website visitors. This information can be used to learn more about your audience and to improve your marketing campaigns. | . The OpenSearch Ip2geo processor adds geographical information about IP addresses based on data from the MaxMind GeoIP2 databases. This processor adds the geolocation information by default under the &lt;field_name&gt; and auto-updates the GeoIP2 databases based on a set interval, keeping geolocation data up-to-date and accurate. ",
    "url": "https://vagimeli.github.io/api-reference/ingest-apis/geoip/",
    "relUrl": "/api-reference/ingest-apis/geoip/"
  },"2273": {
    "doc": "IP geolocation processor",
    "title": "Installing the Ip2geo processor",
    "content": "To install the Ip2geo processor, the opensearch-geospatial plugin must be installed first. Learn more in the Installing plugins documentation. ",
    "url": "https://vagimeli.github.io/api-reference/ingest-apis/geoip/#installing-the-ip2geo-processor",
    "relUrl": "/api-reference/ingest-apis/geoip/#installing-the-ip2geo-processor"
  },"2274": {
    "doc": "IP geolocation processor",
    "title": "Creating the data source",
    "content": "Once you’ve installed the Ip2geo processor, create the IP geolocation data source by defining the endpoint value to download geolocation data and specify the data update interval. The endpoint value must contain valid data formats, for example, . The minimum update interval is 1 day. The maximum is determined by the database provider. The following code example shows how to create a data source using the OpenSearch default endpoint value, which is used if the endpoint value is empty, and update interval of 3 days. Example: JSON POST request . { \"endpoint\" : \"https://geoip.maps.opensearch.org/v1/geolite2-city/manifest.json\", \"update_interval_in_days\" : 3 } . The following code example shows the JSON reponse to the preceding request. A true JSON responvnxcse means the request was successful and the server was able to process the request. If you receive a false JSON reponse, check the request to make sure it is valid, check the URL to make sure it is correct, or try again. Example: JSON response . { \"acknowledged\":true } . ",
    "url": "https://vagimeli.github.io/api-reference/ingest-apis/geoip/#creating-the-data-source",
    "relUrl": "/api-reference/ingest-apis/geoip/#creating-the-data-source"
  },"2275": {
    "doc": "IP geolocation processor",
    "title": "Sending a GET request",
    "content": "To request data from the specifed database you created, send a GET request. Example: GET request . GET https://&lt;host&gt;:&lt;port&gt;/_plugins/geospatial/ip2geo/datasource/_all . Example: GET response . &lt;insert-response-example&gt; . The reponse shows information for each field (for example, name, endpoint, provider) in the data source file, when the data source file last updated successfully or failed (for example, last_succeeded_at_in_epoch_millis), and fields (for example, fields) added to the file since you last updated it. ",
    "url": "https://vagimeli.github.io/api-reference/ingest-apis/geoip/#sending-a-get-request",
    "relUrl": "/api-reference/ingest-apis/geoip/#sending-a-get-request"
  },"2276": {
    "doc": "IP geolocation processor",
    "title": "Updating the data source",
    "content": "To update the data source file, send a PUT request. You can continue using the current endpoint value or change it. Note that if the new endpoint value contains fields that are not in the current data source file, the update fails. You also can change the update interval. Example: PUT request . { \"endpoint\": https://geoip.maps.opensearch.org/v1/geolite2-city/manifest.json \"update_interval_in_days\": 1 } . Example: Response . { \"acknowledged\":true } . ",
    "url": "https://vagimeli.github.io/api-reference/ingest-apis/geoip/#updating-the-data-source",
    "relUrl": "/api-reference/ingest-apis/geoip/#updating-the-data-source"
  },"2277": {
    "doc": "IP geolocation processor",
    "title": "Creating the IP2geo processor",
    "content": "Once the data source is created, you can create the Ip2geo processor. To create the processor, send a PUT request. Example: PUT request . { \"description\":\"convert ip to geo\", \"processors\":[ { \"ip2geo\":{ \"field\":\"_ip\", \"datsource\"::\"test1\" } } ] } #### Example: Response ```json &lt;insert-example-response&gt; . ",
    "url": "https://vagimeli.github.io/api-reference/ingest-apis/geoip/#creating-the-ip2geo-processor",
    "relUrl": "/api-reference/ingest-apis/geoip/#creating-the-ip2geo-processor"
  },"2278": {
    "doc": "IP geolocation processor",
    "title": "Using the IP2geo processor in a pipeline",
    "content": "The following table describes . | Name | Required | Default | Description | . | field | yes | - | The field to get the ip address from for the geographical lookup. | . | target_field | no | ip2geo | The field that will hold the geographical information looked up from the Maxmind database. | . | database | no | geoip.maps.opensearch.org/v1/geolite2-city/manifest.json | The database filename referring to a database the module ships with or a custom database in the ingest-geoip config directory. | . | properties | no | Default field depends on what is available in database. | Controls what properties are added to the target_field based on the geoip lookup. | . | ignore_missing | no | false | If true and field does not exist, the processor quietly exits without modifying the document. | . | first_only | no | true | If true only first found geoip data will be returned, even if field contains array. | . The following code is an example of using and adds the geographical information to the `geoip` field based on the `ip` field. Simulating the pipeline . To simulate the pipeline, specify the pipeline in the POST request. Example: POST request . &lt;insert-example-request&gt; . ",
    "url": "https://vagimeli.github.io/api-reference/ingest-apis/geoip/#using-the-ip2geo-processor-in-a-pipeline",
    "relUrl": "/api-reference/ingest-apis/geoip/#using-the-ip2geo-processor-in-a-pipeline"
  },"2279": {
    "doc": "IP geolocation processor",
    "title": "Deleting the Ip2geo processor",
    "content": "To delete the IP2geo processor, send a DELETE request. Example: DELETE request . { DELETE https://&lt;host&gt;:&lt;port&gt;/_ingest/pipeline/&lt;processor&gt; } . ",
    "url": "https://vagimeli.github.io/api-reference/ingest-apis/geoip/#deleting-the-ip2geo-processor",
    "relUrl": "/api-reference/ingest-apis/geoip/#deleting-the-ip2geo-processor"
  },"2280": {
    "doc": "IP geolocation processor",
    "title": "Deleting the data source",
    "content": "To delete the data source, send a DELETE request. Note that if you have another processors that uses the data source, the delete fails. To delete the dat source, you must delete all processors associated with the data source. Example: DELETE request . { DELETE https://&lt;host&gt;:&lt;port&gt;/_plugins/geospatial/ip2geo/datasource/_all } . Example: Failed DELETE request . &lt;insert-example-failed-request&gt; . ",
    "url": "https://vagimeli.github.io/api-reference/ingest-apis/geoip/#deleting-the-data-source",
    "relUrl": "/api-reference/ingest-apis/geoip/#deleting-the-data-source"
  },"2281": {
    "doc": "IP geolocation processor",
    "title": "Next steps",
    "content": "&lt;Do we want to link to any Data Prepper processor information?&gt; &lt;What other documentation or GitHub resources should we include?&gt; . ",
    "url": "https://vagimeli.github.io/api-reference/ingest-apis/geoip/#next-steps",
    "relUrl": "/api-reference/ingest-apis/geoip/#next-steps"
  },"2282": {
    "doc": "Get ingest pipeline",
    "title": "Get ingest pipeline",
    "content": "After you create a pipeline, use the get ingest pipeline API operation to return all the information about a specific ingest pipeline. ",
    "url": "https://vagimeli.github.io/api-reference/ingest-apis/get-ingest/",
    "relUrl": "/api-reference/ingest-apis/get-ingest/"
  },"2283": {
    "doc": "Get ingest pipeline",
    "title": "Example",
    "content": "GET _ingest/pipeline/12345 . copy . ",
    "url": "https://vagimeli.github.io/api-reference/ingest-apis/get-ingest/#example",
    "relUrl": "/api-reference/ingest-apis/get-ingest/#example"
  },"2284": {
    "doc": "Get ingest pipeline",
    "title": "Path and HTTP methods",
    "content": "Return all ingest pipelines. GET _ingest/pipeline . Returns a single ingest pipeline based on the pipeline’s ID. GET _ingest/pipeline/{id} . ",
    "url": "https://vagimeli.github.io/api-reference/ingest-apis/get-ingest/#path-and-http-methods",
    "relUrl": "/api-reference/ingest-apis/get-ingest/#path-and-http-methods"
  },"2285": {
    "doc": "Get ingest pipeline",
    "title": "URL parameters",
    "content": "All parameters are optional. | Parameter | Type | Description | . | master_timeout | time | How long to wait for a connection to the master node. | . ",
    "url": "https://vagimeli.github.io/api-reference/ingest-apis/get-ingest/#url-parameters",
    "relUrl": "/api-reference/ingest-apis/get-ingest/#url-parameters"
  },"2286": {
    "doc": "Get ingest pipeline",
    "title": "Response",
    "content": "{ \"pipeline-id\" : { \"description\" : \"A description for your pipeline\", \"processors\" : [ { \"set\" : { \"field\" : \"field-name\", \"value\" : \"value\" } } ] } } . ",
    "url": "https://vagimeli.github.io/api-reference/ingest-apis/get-ingest/#response",
    "relUrl": "/api-reference/ingest-apis/get-ingest/#response"
  },"2287": {
    "doc": "Ingest APIs",
    "title": "Ingest APIs",
    "content": "Before you index your data, OpenSearch’s ingest APIs help transform your data by creating and managing ingest pipelines. Pipelines consist of processors, customizable tasks that run in the order they appear in the request body. The transformed data appears in your index after each of the processor completes. Ingest pipelines in OpenSearch can only be managed using ingest API operations. When using ingest in production environments, your cluster should contain at least one node with the node roles permission set to ingest. For more information on setting up node roles within a cluster, see Cluster Formation. ",
    "url": "https://vagimeli.github.io/api-reference/ingest-apis/index/",
    "relUrl": "/api-reference/ingest-apis/index/"
  },"2288": {
    "doc": "Simulate an ingest pipeline",
    "title": "Simulate a pipeline",
    "content": "Simulates an ingest pipeline with any example documents you specify. ",
    "url": "https://vagimeli.github.io/api-reference/ingest-apis/simulate-ingest/#simulate-a-pipeline",
    "relUrl": "/api-reference/ingest-apis/simulate-ingest/#simulate-a-pipeline"
  },"2289": {
    "doc": "Simulate an ingest pipeline",
    "title": "Example",
    "content": "POST /_ingest/pipeline/35678/_simulate { \"docs\": [ { \"_index\": \"index\", \"_id\": \"id\", \"_source\": { \"location\": \"document-name\" } }, { \"_index\": \"index\", \"_id\": \"id\", \"_source\": { \"location\": \"document-name\" } } ] } . copy . ",
    "url": "https://vagimeli.github.io/api-reference/ingest-apis/simulate-ingest/#example",
    "relUrl": "/api-reference/ingest-apis/simulate-ingest/#example"
  },"2290": {
    "doc": "Simulate an ingest pipeline",
    "title": "Path and HTTP methods",
    "content": "Simulate the last ingest pipeline created. GET _ingest/pipeline/_simulate POST _ingest/pipeline/_simulate . Simulate a single pipeline based on the pipeline’s ID. GET _ingest/pipeline/{id}/_simulate POST _ingest/pipeline/{id}/_simulate . ",
    "url": "https://vagimeli.github.io/api-reference/ingest-apis/simulate-ingest/#path-and-http-methods",
    "relUrl": "/api-reference/ingest-apis/simulate-ingest/#path-and-http-methods"
  },"2291": {
    "doc": "Simulate an ingest pipeline",
    "title": "URL parameters",
    "content": "All URL parameters are optional. | Parameter | Type | Description | . | verbose | boolean | Verbose mode. Display data output for each processor in executed pipeline. | . ",
    "url": "https://vagimeli.github.io/api-reference/ingest-apis/simulate-ingest/#url-parameters",
    "relUrl": "/api-reference/ingest-apis/simulate-ingest/#url-parameters"
  },"2292": {
    "doc": "Simulate an ingest pipeline",
    "title": "Request body fields",
    "content": "| Field | Required | Type | Description | . | pipeline | Optional | object | The pipeline you want to simulate. When included without the pipeline {id} inside the request path, the response simulates the last pipeline created. | . | docs | Required | array of objects | The documents you want to use to test the pipeline. | . The docs field can include the following subfields: . | Field | Required | Type | Description | . | id | Optional | string | An optional identifier for the document. The identifier cannot be used elsewhere in the index. | . | index | Optional | string | The index where the document’s transformed data appears. | . | source | Required | object | The document’s JSON body. | . ",
    "url": "https://vagimeli.github.io/api-reference/ingest-apis/simulate-ingest/#request-body-fields",
    "relUrl": "/api-reference/ingest-apis/simulate-ingest/#request-body-fields"
  },"2293": {
    "doc": "Simulate an ingest pipeline",
    "title": "Response",
    "content": "Responses vary based on which path and HTTP method you choose. Specify pipeline in request body . { \"docs\" : [ { \"doc\" : { \"_index\" : \"index\", \"_id\" : \"id\", \"_source\" : { \"location\" : \"new-new\", \"field2\" : \"_value\" }, \"_ingest\" : { \"timestamp\" : \"2022-02-07T18:47:57.479230835Z\" } } }, { \"doc\" : { \"_index\" : \"index\", \"_id\" : \"id\", \"_source\" : { \"location\" : \"new-new\", \"field2\" : \"_value\" }, \"_ingest\" : { \"timestamp\" : \"2022-02-07T18:47:57.47933496Z\" } } } ] } . Specify pipeline ID inside HTTP path . { \"docs\" : [ { \"doc\" : { \"_index\" : \"index\", \"_id\" : \"id\", \"_source\" : { \"field-name\" : \"value\", \"location\" : \"document-name\" }, \"_ingest\" : { \"timestamp\" : \"2022-02-03T21:47:05.382744877Z\" } } }, { \"doc\" : { \"_index\" : \"index\", \"_id\" : \"id\", \"_source\" : { \"field-name\" : \"value\", \"location\" : \"document-name\" }, \"_ingest\" : { \"timestamp\" : \"2022-02-03T21:47:05.382803544Z\" } } } ] } . Receive verbose response . With the verbose parameter set to true, the response shows how each processor transforms the specified document. { \"docs\" : [ { \"processor_results\" : [ { \"processor_type\" : \"set\", \"status\" : \"success\", \"doc\" : { \"_index\" : \"index\", \"_id\" : \"id\", \"_source\" : { \"field-name\" : \"value\", \"location\" : \"document-name\" }, \"_ingest\" : { \"pipeline\" : \"35678\", \"timestamp\" : \"2022-02-03T21:45:09.414049004Z\" } } } ] }, { \"processor_results\" : [ { \"processor_type\" : \"set\", \"status\" : \"success\", \"doc\" : { \"_index\" : \"index\", \"_id\" : \"id\", \"_source\" : { \"field-name\" : \"value\", \"location\" : \"document-name\" }, \"_ingest\" : { \"pipeline\" : \"35678\", \"timestamp\" : \"2022-02-03T21:45:09.414093212Z\" } } } ] } ] } . ",
    "url": "https://vagimeli.github.io/api-reference/ingest-apis/simulate-ingest/#response",
    "relUrl": "/api-reference/ingest-apis/simulate-ingest/#response"
  },"2294": {
    "doc": "Simulate an ingest pipeline",
    "title": "Simulate an ingest pipeline",
    "content": " ",
    "url": "https://vagimeli.github.io/api-reference/ingest-apis/simulate-ingest/",
    "relUrl": "/api-reference/ingest-apis/simulate-ingest/"
  },"2295": {
    "doc": "Multi-search",
    "title": "Multi-search",
    "content": "Introduced 1.0 . As the name suggests, the multi-search operation lets you bundle multiple search requests into a single request. OpenSearch then executes the searches in parallel, so you get back the response more quickly compared to sending one request per search. OpenSearch executes each search independently, so the failure of one doesn’t affect the others. ",
    "url": "https://vagimeli.github.io/api-reference/multi-search/",
    "relUrl": "/api-reference/multi-search/"
  },"2296": {
    "doc": "Multi-search",
    "title": "Example",
    "content": "GET _msearch { \"index\": \"opensearch_dashboards_sample_data_logs\"} { \"query\": { \"match_all\": {} }, \"from\": 0, \"size\": 10} { \"index\": \"opensearch_dashboards_sample_data_ecommerce\", \"search_type\": \"dfs_query_then_fetch\"} { \"query\": { \"match_all\": {} } } . copy . ",
    "url": "https://vagimeli.github.io/api-reference/multi-search/#example",
    "relUrl": "/api-reference/multi-search/#example"
  },"2297": {
    "doc": "Multi-search",
    "title": "Path and HTTP methods",
    "content": "GET _msearch GET &lt;indices&gt;/_msearch POST _msearch POST &lt;indices&gt;/_msearch . ",
    "url": "https://vagimeli.github.io/api-reference/multi-search/#path-and-http-methods",
    "relUrl": "/api-reference/multi-search/#path-and-http-methods"
  },"2298": {
    "doc": "Multi-search",
    "title": "Request body",
    "content": "The multi-search request body follows this pattern: . Metadata\\n Query\\n Metadata\\n Query\\n . | Metadata lines include options, such as which indexes to search and the type of search. | Query lines use the query DSL. | . Just like the bulk operation, the JSON doesn’t need to be minified—spaces are fine—but it does need to be on a single line. OpenSearch uses newline characters to parse multi-search requests and requires that the request body end with a newline character. ",
    "url": "https://vagimeli.github.io/api-reference/multi-search/#request-body",
    "relUrl": "/api-reference/multi-search/#request-body"
  },"2299": {
    "doc": "Multi-search",
    "title": "URL parameters and metadata options",
    "content": "All multi-search URL parameters are optional. Some can also be applied per-search as part of each metadata line. | Parameter | Type | Description | Supported in metadata line | . | allow_no_indices | Boolean | Whether to ignore wildcards that don’t match any indexes. Default is true. | Yes | . | cancel_after_time_interval | Time | The time after which the search request will be canceled. Supported at both parent and child request levels. The order of precedence is: 1. Child-level parameter 2. Parent-level parameter 3. Cluster setting.Default is -1. | Yes | . | css_minimize_roundtrips | Boolean | Whether OpenSearch should try to minimize the number of network round trips between the coordinating node and remote clusters (only applicable to cross-cluster search requests). Default is true. | No | . | expand_wildcards | Enum | Expands wildcard expressions to concrete indexes. Combine multiple values with commas. Supported values are all, open, closed, hidden, and none. Default is open. | Yes | . | ignore_unavailable | Boolean | If an index from the indexes list doesn’t exist, whether to ignore it rather than fail the query. Default is false. | Yes | . | max_concurrent_searches | Integer | The maximum number of concurrent searches. The default depends on your node count and search thread pool size. Higher values can improve performance, but risk overloading the cluster. | No | . | max_concurrent_shard_requests | Integer | Maximum number of concurrent shard requests that each search executes per node. Default is 5. Higher values can improve performance, but risk overloading the cluster. | No | . | pre_filter_shard_size | Integer | Default is 128. | No | . | rest_total_hits_as_int | String | Whether the hits.total property is returned as an integer (true) or an object (false). Default is false. | No | . | search_type | String | Affects relevance score. Valid options are query_then_fetch and dfs_query_then_fetch. query_then_fetch scores documents using term and document frequencies for the shard (faster, less accurate), whereas dfs_query_then_fetch uses term and document frequencies across all shards (slower, more accurate). Default is query_then_fetch. | Yes | . | typed_keys | Boolean | Whether to prefix aggregation names with their internal types in the response. Default is false. | No | . ",
    "url": "https://vagimeli.github.io/api-reference/multi-search/#url-parameters-and-metadata-options",
    "relUrl": "/api-reference/multi-search/#url-parameters-and-metadata-options"
  },"2300": {
    "doc": "Multi-search",
    "title": "Metadata-only options",
    "content": "Some options can’t be applied as URL parameters to the entire request. Instead, you can apply them per-search as part of each metadata line. All are optional. | Option | Type | Description | . | index | String, string array | If you don’t specify an index or multiple indexes as part of the URL (or want to override the URL value for an individual search), you can include it here. Examples include \"logs-*\" and [\"my-store\", \"sample_data_ecommerce\"]. | . | preference | String | The nodes or shards that you’d like to perform the search. This setting can be useful for testing, but in most situations, the default behavior provides the best search latencies. Options include _local, _only_local, _prefer_nodes, _only_nodes, and _shards. These last three options accept a list of nodes or shards. Examples include \"_only_nodes:data-node1,data-node2\" and \"_shards:0,1. | . | request_cache | Boolean | Whether to cache results, which can improve latency for repeat searches. Default is to use the index.requests.cache.enable setting for the index (which defaults to true for new indexes). | . | routing | String | Comma-separated custom routing values, for example, \"routing\": \"value1,value2,value3\". | . ",
    "url": "https://vagimeli.github.io/api-reference/multi-search/#metadata-only-options",
    "relUrl": "/api-reference/multi-search/#metadata-only-options"
  },"2301": {
    "doc": "Multi-search",
    "title": "Response",
    "content": "OpenSearch returns an array with the results of each search in the same order as the multi-search request. { \"took\" : 2150, \"responses\" : [ { \"took\" : 2149, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 10000, \"relation\" : \"gte\" }, \"max_score\" : 1.0, \"hits\" : [ { \"_index\" : \"opensearch_dashboards_sample_data_logs\", \"_id\" : \"_fnhBXsBgv2Zxgu9dZ8Y\", \"_score\" : 1.0, \"_source\" : { \"agent\" : \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; .NET CLR 1.1.4322)\", \"bytes\" : 4657, \"clientip\" : \"213.116.129.196\", \"extension\" : \"zip\", \"geo\" : { \"srcdest\" : \"CN:US\", \"src\" : \"CN\", \"dest\" : \"US\", \"coordinates\" : { \"lat\" : 42.35083333, \"lon\" : -86.25613889 } }, \"host\" : \"artifacts.opensearch.org\", \"index\" : \"opensearch_dashboards_sample_data_logs\", \"ip\" : \"213.116.129.196\", \"machine\" : { \"ram\" : 16106127360, \"os\" : \"ios\" }, \"memory\" : null, \"message\" : \"213.116.129.196 - - [2018-07-30T14:12:11.387Z] \\\"GET /opensearch_dashboards/opensearch_dashboards-1.0.0-windows-x86_64.zip HTTP/1.1\\\" 200 4657 \\\"-\\\" \\\"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; .NET CLR 1.1.4322)\\\"\", \"phpmemory\" : null, \"referer\" : \"http://twitter.com/success/ellison-onizuka\", \"request\" : \"/opensearch_dashboards/opensearch_dashboards-1.0.0-windows-x86_64.zip\", \"response\" : 200, \"tags\" : [ \"success\", \"info\" ], \"timestamp\" : \"2021-08-02T14:12:11.387Z\", \"url\" : \"https://artifacts.opensearch.org/downloads/opensearch_dashboards/opensearch_dashboards-1.0.0-windows-x86_64.zip\", \"utc_time\" : \"2021-08-02T14:12:11.387Z\", \"event\" : { \"dataset\" : \"sample_web_logs\" } } }, ... ] }, \"status\" : 200 }, { \"took\" : 1473, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 4675, \"relation\" : \"eq\" }, \"max_score\" : 1.0, \"hits\" : [ { \"_index\" : \"opensearch_dashboards_sample_data_ecommerce\", \"_id\" : \"efnhBXsBgv2Zxgu9ap7e\", \"_score\" : 1.0, \"_source\" : { \"category\" : [ \"Women's Clothing\" ], \"currency\" : \"EUR\", \"customer_first_name\" : \"Gwen\", \"customer_full_name\" : \"Gwen Dennis\", \"customer_gender\" : \"FEMALE\", \"customer_id\" : 26, \"customer_last_name\" : \"Dennis\", \"customer_phone\" : \"\", \"day_of_week\" : \"Tuesday\", \"day_of_week_i\" : 1, \"email\" : \"gwen@dennis-family.zzz\", \"manufacturer\" : [ \"Tigress Enterprises\", \"Gnomehouse mom\" ], \"order_date\" : \"2021-08-10T16:24:58+00:00\", \"order_id\" : 576942, \"products\" : [ { \"base_price\" : 32.99, \"discount_percentage\" : 0, \"quantity\" : 1, \"manufacturer\" : \"Tigress Enterprises\", \"tax_amount\" : 0, \"product_id\" : 22182, \"category\" : \"Women's Clothing\", \"sku\" : \"ZO0036600366\", \"taxless_price\" : 32.99, \"unit_discount_amount\" : 0, \"min_price\" : 14.85, \"_id\" : \"sold_product_576942_22182\", \"discount_amount\" : 0, \"created_on\" : \"2016-12-20T16:24:58+00:00\", \"product_name\" : \"Jersey dress - black/red\", \"price\" : 32.99, \"taxful_price\" : 32.99, \"base_unit_price\" : 32.99 }, { \"base_price\" : 28.99, \"discount_percentage\" : 0, \"quantity\" : 1, \"manufacturer\" : \"Gnomehouse mom\", \"tax_amount\" : 0, \"product_id\" : 14230, \"category\" : \"Women's Clothing\", \"sku\" : \"ZO0234902349\", \"taxless_price\" : 28.99, \"unit_discount_amount\" : 0, \"min_price\" : 13.05, \"_id\" : \"sold_product_576942_14230\", \"discount_amount\" : 0, \"created_on\" : \"2016-12-20T16:24:58+00:00\", \"product_name\" : \"Blouse - june bug\", \"price\" : 28.99, \"taxful_price\" : 28.99, \"base_unit_price\" : 28.99 } ], \"sku\" : [ \"ZO0036600366\", \"ZO0234902349\" ], \"taxful_total_price\" : 61.98, \"taxless_total_price\" : 61.98, \"total_quantity\" : 2, \"total_unique_products\" : 2, \"type\" : \"order\", \"user\" : \"gwen\", \"geoip\" : { \"country_iso_code\" : \"US\", \"location\" : { \"lon\" : -118.2, \"lat\" : 34.1 }, \"region_name\" : \"California\", \"continent_name\" : \"North America\", \"city_name\" : \"Los Angeles\" }, \"event\" : { \"dataset\" : \"sample_ecommerce\" } } }, ... ] }, \"status\" : 200 } ] } . ",
    "url": "https://vagimeli.github.io/api-reference/multi-search/#response",
    "relUrl": "/api-reference/multi-search/#response"
  },"2302": {
    "doc": "Nodes APIs",
    "title": "Nodes API",
    "content": "The nodes API makes it possible to retrieve information about individual nodes within your cluster. ",
    "url": "https://vagimeli.github.io/api-reference/nodes-apis/index/#nodes-api",
    "relUrl": "/api-reference/nodes-apis/index/#nodes-api"
  },"2303": {
    "doc": "Nodes APIs",
    "title": "Node filters",
    "content": "Use the &lt;node-filters&gt; parameter to filter the target set of nodes in the API response. | Parameter | Type | Description | . | &lt;node-filters&gt; | String | A comma-separated list of resolution mechanisms that OpenSearch uses to identify cluster nodes. | . Node filters support several node resolution mechanisms: . | Predefined constants: _local, _cluster_manager, or _all. | An exact match for nodeID | A simple case-sensitive wildcard pattern matching for node-name, host-name, or host-IP-address. | Node roles where the &lt;bool&gt; value is set either to true or false: . | cluster_manager:&lt;bool&gt; refers to all cluster manager-eligible nodes. | data:&lt;bool&gt; refers to all data nodes. | ingest:&lt;bool&gt; refers to all ingest nodes. | voting_only:&lt;bool&gt; refers to all voting-only nodes. | ml:&lt;bool&gt; refers to all machine learning (ML) nodes. | coordinating_only:&lt;bool&gt; refers to all coordinating-only nodes. | . | A simple case-sensitive wildcard pattern matching for node attributes: &lt;node attribute*&gt;:&lt;attribute value*&gt;. The wildcard matching pattern can be used in both the key and value at the same time. | . Resolution mechanisms are applied sequentially in the order specified by the client. Each mechanism specification can either add or remove nodes. To get statistics from the elected cluster manager node only, use the following query : . GET /_nodes/_cluster_manager/stats . copy . To get statistics from nodes that are data-only nodes, use the following query: . GET /_nodes/data:true/stats . copy . Order of resolution mechanisms . The order of resolution mechanisms is applied sequentially, and each can add or remove nodes. The following examples yield different results. To get statistics from all the nodes except the cluster manager node, use the following query: . GET /_nodes/_all,cluster_manager:false/stats . copy . However, if you switch the resolution mechanisms, the result will include all the cluster nodes, including the cluster manager node: . GET /_nodes/cluster_manager:false,_all/stats . copy . ",
    "url": "https://vagimeli.github.io/api-reference/nodes-apis/index/#node-filters",
    "relUrl": "/api-reference/nodes-apis/index/#node-filters"
  },"2304": {
    "doc": "Nodes APIs",
    "title": "Nodes APIs",
    "content": " ",
    "url": "https://vagimeli.github.io/api-reference/nodes-apis/index/",
    "relUrl": "/api-reference/nodes-apis/index/"
  },"2305": {
    "doc": "Nodes hot threads",
    "title": "Nodes hot threads",
    "content": "The nodes hot threads endpoint provides information about busy JVM threads for selected cluster nodes. It provides a unique view of the of activity each node. ",
    "url": "https://vagimeli.github.io/api-reference/nodes-apis/nodes-hot-threads/",
    "relUrl": "/api-reference/nodes-apis/nodes-hot-threads/"
  },"2306": {
    "doc": "Nodes hot threads",
    "title": "Example",
    "content": "GET /_nodes/hot_threads . copy . ",
    "url": "https://vagimeli.github.io/api-reference/nodes-apis/nodes-hot-threads/#example",
    "relUrl": "/api-reference/nodes-apis/nodes-hot-threads/#example"
  },"2307": {
    "doc": "Nodes hot threads",
    "title": "Path and HTTP methods",
    "content": "GET /_nodes/hot_threads GET /_nodes/&lt;nodeId&gt;/hot_threads . ",
    "url": "https://vagimeli.github.io/api-reference/nodes-apis/nodes-hot-threads/#path-and-http-methods",
    "relUrl": "/api-reference/nodes-apis/nodes-hot-threads/#path-and-http-methods"
  },"2308": {
    "doc": "Nodes hot threads",
    "title": "Path parameters",
    "content": "You can include the following optional path parameter in your request. | Parameter | Type | Description | . | nodeId | String | A comma-separated list of node IDs used to filter results. Supports node filters. Defaults to _all. | . ",
    "url": "https://vagimeli.github.io/api-reference/nodes-apis/nodes-hot-threads/#path-parameters",
    "relUrl": "/api-reference/nodes-apis/nodes-hot-threads/#path-parameters"
  },"2309": {
    "doc": "Nodes hot threads",
    "title": "Query parameters",
    "content": "You can include the following query parameters in your request. All query parameters are optional. | Parameter | Type | Description | . | snapshots | Integer | The number of samples of thread stacktraces. Defaults to 10. | . | interval | Time | The interval between consecutive samples. Defaults to 500ms. | . | threads | Integer | The number of the busiest threads to return information about. Defaults to 3. | . | ignore_idle_threads | Boolean | Don’t show threads that are in known idle states, such as waiting on a socket select or pulling from an empty task queue. Defaults to true. | . | type | String | Supported thread types are cpu, wait, or block. Defaults to cpu. | . | timeout | Time | Sets the time limit for node response. Default value is 30s. | . Example request . GET /_nodes/hot_threads . copy . Example response . ::: {opensearch}{F-ByTQzVQ3GQeYzQJArJGQ}{GxbcLdCATPWggOuQHJAoCw}{127.0.0.1}{127.0.0.1:9300}{dimr}{shard_indexing_pressure_enabled=true} Hot threads at 2022-09-29T19:46:44.533Z, interval=500ms, busiestThreads=3, ignoreIdleThreads=true: 0.1% (455.5micros out of 500ms) cpu usage by thread 'ScheduledMetricCollectorsExecutor' 10/10 snapshots sharing following 2 elements java.base@17.0.4/java.lang.Thread.sleep(Native Method) org.opensearch.performanceanalyzer.collectors.ScheduledMetricCollectorsExecutor.run(ScheduledMetricCollectorsExecutor.java:100) . ",
    "url": "https://vagimeli.github.io/api-reference/nodes-apis/nodes-hot-threads/#query-parameters",
    "relUrl": "/api-reference/nodes-apis/nodes-hot-threads/#query-parameters"
  },"2310": {
    "doc": "Nodes hot threads",
    "title": "Response",
    "content": "Unlike the majority of OpenSearch API responses, this response is in a text format. It consists of one section per each cluster node included in the response. Each section starts with a single line containing the following segments: . | Line segment | Description | . | :::&nbsp; | Line start (a distinct visual symbol). | . | {global-eu-35} | Node name. | . | {uFPbKLDOTlOmdnwUlKW8sw} | NodeId. | . | {OAM8OT5CQAyasWuIDeVyUA} | EphemeralId. | . | {global-eu-35.local} | Host name. | . | {[gdv2:a284:2acv:5fa6:0:3a2:7260:74cf]:9300} | Host address. | . | {dimr} | Node roles (d=data, i=ingest, m=cluster manager, r=remote cluster client). | . | {zone=west-a2, shard_indexing_pressure_enabled=true} | Node attributes. | . Then information about threads of the selected type is provided. ::: {global-eu-35}{uFPbKLDOTlOmdnwUlKW8sw}{OAM8OT5CQAyasWuIDeVyUA}{global-eu-35.local}{[gdv2:a284:2acv:5fa6:0:3a2:7260:74cf]:9300}{dimr}{zone=west-a2, shard_indexing_pressure_enabled=true} Hot threads at 2022-04-01T15:15:27.658Z, interval=500ms, busiestThreads=3, ignoreIdleThreads=true: 0.1% (645micros out of 500ms) cpu usage by thread 'opensearch[global-eu-35][transport_worker][T#7]' 4/10 snapshots sharing following 3 elements io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986) io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) java.base@11.0.14.1/java.lang.Thread.run(Thread.java:829) ::: {global-eu-62}{4knOxAdERlOB19zLQIT1bQ}{HJuZs2HiQ_-8Elj0Fvi_1g}{global-eu-62.local}{[gdv2:a284:2acv:5fa6:0:3a2:bba6:fe3f]:9300}{dimr}{zone=west-a2, shard_indexing_pressure_enabled=true} Hot threads at 2022-04-01T15:15:27.659Z, interval=500ms, busiestThreads=3, ignoreIdleThreads=true: 18.7% (93.4ms out of 500ms) cpu usage by thread 'opensearch[global-eu-62][transport_worker][T#3]' 6/10 snapshots sharing following 3 elements io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986) io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) java.base@11.0.14.1/java.lang.Thread.run(Thread.java:829) ::: {global-eu-44}{8WW3hrkcTwGvgah_L8D_jw}{Sok7spHISFyol0jFV6i0kw}{global-eu-44.local}{[gdv2:a284:2acv:5fa6:0:3a2:9120:e79e]:9300}{dimr}{zone=west-a2, shard_indexing_pressure_enabled=true} Hot threads at 2022-04-01T15:15:27.659Z, interval=500ms, busiestThreads=3, ignoreIdleThreads=true: 42.6% (212.7ms out of 500ms) cpu usage by thread 'opensearch[global-eu-44][write][T#5]' 2/10 snapshots sharing following 43 elements java.base@11.0.14.1/sun.nio.ch.IOUtil.write1(Native Method) java.base@11.0.14.1/sun.nio.ch.EPollSelectorImpl.wakeup(EPollSelectorImpl.java:254) io.netty.channel.nio.NioEventLoop.wakeup(NioEventLoop.java:787) io.netty.util.concurrent.SingleThreadEventExecutor.execute(SingleThreadEventExecutor.java:846) io.netty.util.concurrent.SingleThreadEventExecutor.execute(SingleThreadEventExecutor.java:815) io.netty.channel.AbstractChannelHandlerContext.safeExecute(AbstractChannelHandlerContext.java:989) io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:796) io.netty.channel.AbstractChannelHandlerContext.writeAndFlush(AbstractChannelHandlerContext.java:758) io.netty.channel.DefaultChannelPipeline.writeAndFlush(DefaultChannelPipeline.java:1020) io.netty.channel.AbstractChannel.writeAndFlush(AbstractChannel.java:311) org.opensearch.transport.netty4.Netty4TcpChannel.sendMessage(Netty4TcpChannel.java:159) app//org.opensearch.transport.OutboundHan... ",
    "url": "https://vagimeli.github.io/api-reference/nodes-apis/nodes-hot-threads/#response",
    "relUrl": "/api-reference/nodes-apis/nodes-hot-threads/#response"
  },"2311": {
    "doc": "Nodes hot threads",
    "title": "Required permissions",
    "content": "If you use the Security plugin, make sure you set the following permissions: cluster:monitor/nodes/hot_threads. ",
    "url": "https://vagimeli.github.io/api-reference/nodes-apis/nodes-hot-threads/#required-permissions",
    "relUrl": "/api-reference/nodes-apis/nodes-hot-threads/#required-permissions"
  },"2312": {
    "doc": "Nodes info",
    "title": "Nodes info",
    "content": "The nodes info API represents mostly static information about your cluster’s nodes, including but not limited to: . | Host system information | JVM | Processor Type | Node settings | Thread pools settings | Installed plugins | . ",
    "url": "https://vagimeli.github.io/api-reference/nodes-apis/nodes-info/",
    "relUrl": "/api-reference/nodes-apis/nodes-info/"
  },"2313": {
    "doc": "Nodes info",
    "title": "Example",
    "content": "To get information about all nodes in a cluster, use the following query: . GET /_nodes . copy . To get thread pool information about the cluster manager node only, use the following query: . GET /_nodes/master:true/thread_pool . copy . ",
    "url": "https://vagimeli.github.io/api-reference/nodes-apis/nodes-info/#example",
    "relUrl": "/api-reference/nodes-apis/nodes-info/#example"
  },"2314": {
    "doc": "Nodes info",
    "title": "Path and HTTP methods",
    "content": "GET /_nodes GET /_nodes/&lt;nodeId&gt; GET /_nodes/&lt;metrics&gt; GET /_nodes/&lt;nodeId&gt;/&lt;metrics&gt; # or full path equivalent GET /_nodes/&lt;nodeId&gt;/info/&lt;metrics&gt; . ",
    "url": "https://vagimeli.github.io/api-reference/nodes-apis/nodes-info/#path-and-http-methods",
    "relUrl": "/api-reference/nodes-apis/nodes-info/#path-and-http-methods"
  },"2315": {
    "doc": "Nodes info",
    "title": "Path parameters",
    "content": "The following table lists the available path parameters. All path parameters are optional. | Parameter | Type | Description | . | nodeId | String | A comma-separated list of nodeIds used to filter results. Supports node filters. Defaults to _all. | . | metrics | String | A comma-separated list of metric groups that will be included in the response. For example, jvm,thread_pool. Defaults to all metrics. | . The following table lists all available metric groups. | Metric | Description | . | settings | A node’s settings. This is a combination of the default settings, custom settings from the configuration file, and dynamically updated settings. | . | os | Static information about the host OS, including version, processor architecture, and available/allocated processors. | . | process | Contains the process ID. | . | jvm | Detailed static information about the running JVM, including arguments. | . | thread_pool | Configured options for all individual thread pools. | . | transport | Mostly static information about the transport layer. | . | http | Mostly static information about the HTTP layer. | . | plugins | Information about installed plugins and modules. | . | ingest | Information about ingest pipelines and available ingest processors. | . | aggregations | Information about available aggregations. | . | indices | Static index settings configured at the node level. | . ",
    "url": "https://vagimeli.github.io/api-reference/nodes-apis/nodes-info/#path-parameters",
    "relUrl": "/api-reference/nodes-apis/nodes-info/#path-parameters"
  },"2316": {
    "doc": "Nodes info",
    "title": "Query parameters",
    "content": "You can include the following query parameters in your request. All query parameters are optional. | Parameter | Type | Description | . | flat_settings | Boolean | Specifies whether to return the settings object of the response in flat format. Default is false. | . | timeout | Time | Sets the time limit for node response. Default value is 30s. | . Example request . The following query requests the process and transport metrics from the cluster manager node: . GET /_nodes/cluster_manager:true/process,transport . copy . Example response . The response contains the metric groups specified in the &lt;metrics&gt; request parameter (in this case, process and transport): . { \"_nodes\": { \"total\": 1, \"successful\": 1, \"failed\": 0 }, \"cluster_name\": \"opensearch\", \"nodes\": { \"VC0d4RgbTM6kLDwuud2XZQ\": { \"name\": \"node-m1-23\", \"transport_address\": \"127.0.0.1:9300\", \"host\": \"127.0.0.1\", \"ip\": \"127.0.0.1\", \"version\": \"1.3.1\", \"build_type\": \"tar\", \"build_hash\": \"c4c0672877bf0f787ca857c7c37b775967f93d81\", \"roles\": [ \"data\", \"ingest\", \"master\", \"remote_cluster_client\" ], \"attributes\": { \"shard_indexing_pressure_enabled\": \"true\" }, \"process\" : { \"refresh_interval_in_millis\": 1000, \"id\": 44584, \"mlockall\": false }, \"transport\": { \"bound_address\": [ \"[::1]:9300\", \"127.0.0.1:9300\" ], \"publish_address\": \"127.0.0.1:9300\", \"profiles\": { } } } } } . ",
    "url": "https://vagimeli.github.io/api-reference/nodes-apis/nodes-info/#query-parameters",
    "relUrl": "/api-reference/nodes-apis/nodes-info/#query-parameters"
  },"2317": {
    "doc": "Nodes info",
    "title": "Response fields",
    "content": "The response contains the basic node identification and build info for every node matching the &lt;nodeId&gt; request parameter. The following table lists the response fields. | Field | Description | . | name | The node’s name. | . | transport_address | The node’s transport address. | . | host | The node’s host address. | . | ip | The node’s host IP address. | . | version | The node’s OpenSearch version. | . | build_type | The node’s build type, like rpm, docker, tar, etc. | . | build_hash | The git commit hash of the build. | . | total_indexing_buffer | The maximum heap size in bytes used to hold newly indexed documents. Once this heap size is exceeded, the documents are written to disk. | . | roles | The list of the node’s roles. | . | attributes | The node’s attributes. | . | os | Information about the OS, including name, version, architecture, refresh interval, and the number of available and allocated processors. | . | process | Information about the currently running process, including PID, refresh interval, and mlockall, which specifies whether the process address space has been successfully locked in memory. | . | jvm | Information about the JVM, including PID, version, memory information, garbage collector information, and arguments. | . | thread_pool | Information about the thread pool. | . | transport | Information about the transport address, including bound address, publish address, and profiles. | . | http | Information about the HTTP address, including bound address, publish address, and maximum content length, in bytes. | . | plugins | Information about the installed plugins, including name, version, OpenSearch version, Java version, description, class name, custom folder name, a list of extended plugins, and has_native_controller, which specifies whether the plugin has a native controller process. | . | modules | Information about the modules, including name, version, OpenSearch version, Java version, description, class name, custom folder name, a list of extended plugins, and has_native_controller, which specifies whether the plugin has a native controller process. Modules are different from plugins because modules are loaded into OpenSearch automatically, while plugins have to be installed manually. | . | ingest | Information about ingest pipelines and processors. | . | aggregations | Information about the available aggregation types. | . ",
    "url": "https://vagimeli.github.io/api-reference/nodes-apis/nodes-info/#response-fields",
    "relUrl": "/api-reference/nodes-apis/nodes-info/#response-fields"
  },"2318": {
    "doc": "Nodes info",
    "title": "Required permissions",
    "content": "If you use the Security plugin, make sure you have the appropriate permissions: cluster:monitor/nodes/info. ",
    "url": "https://vagimeli.github.io/api-reference/nodes-apis/nodes-info/#required-permissions",
    "relUrl": "/api-reference/nodes-apis/nodes-info/#required-permissions"
  },"2319": {
    "doc": "Nodes reload secure settings",
    "title": "Nodes reload secure settings",
    "content": "The nodes reload secure settings endpoint allows you to change secure settings on a node and reload the secure settings without restarting the node. ",
    "url": "https://vagimeli.github.io/api-reference/nodes-apis/nodes-reload-secure/",
    "relUrl": "/api-reference/nodes-apis/nodes-reload-secure/"
  },"2320": {
    "doc": "Nodes reload secure settings",
    "title": "Path and HTTP methods",
    "content": "POST _nodes/reload_secure_settings POST _nodes/&lt;nodeId&gt;/reload_secure_settings . ",
    "url": "https://vagimeli.github.io/api-reference/nodes-apis/nodes-reload-secure/#path-and-http-methods",
    "relUrl": "/api-reference/nodes-apis/nodes-reload-secure/#path-and-http-methods"
  },"2321": {
    "doc": "Nodes reload secure settings",
    "title": "Path parameter",
    "content": "You can include the following optional path parameter in your request. | Parameter | Type | Description | . | nodeId | String | A comma-separated list of nodeIds used to filter results. Supports node filters. Defaults to _all. | . ",
    "url": "https://vagimeli.github.io/api-reference/nodes-apis/nodes-reload-secure/#path-parameter",
    "relUrl": "/api-reference/nodes-apis/nodes-reload-secure/#path-parameter"
  },"2322": {
    "doc": "Nodes reload secure settings",
    "title": "Request fields",
    "content": "The request may include an optional object containing the password for the OpenSearch keystore. { \"secure_settings_password\": \"keystore_password\" } . Example request . The following is an example API request: . POST _nodes/reload_secure_settings . copy . Example response . The following is an example response: . { \"_nodes\" : { \"total\" : 1, \"successful\" : 1, \"failed\" : 0 }, \"cluster_name\" : \"opensearch-cluster\", \"nodes\" : { \"t7uqHu4SSuWObK3ElkCRfw\" : { \"name\" : \"opensearch-node1\" } } } . ",
    "url": "https://vagimeli.github.io/api-reference/nodes-apis/nodes-reload-secure/#request-fields",
    "relUrl": "/api-reference/nodes-apis/nodes-reload-secure/#request-fields"
  },"2323": {
    "doc": "Nodes reload secure settings",
    "title": "Required permissions",
    "content": "If you use the Security plugin, make sure you set the following permissions: cluster:manage/nodes. ",
    "url": "https://vagimeli.github.io/api-reference/nodes-apis/nodes-reload-secure/#required-permissions",
    "relUrl": "/api-reference/nodes-apis/nodes-reload-secure/#required-permissions"
  },"2324": {
    "doc": "Nodes stats",
    "title": "Nodes stats",
    "content": "The nodes stats API returns statistics about your cluster. ",
    "url": "https://vagimeli.github.io/api-reference/nodes-apis/nodes-stats/",
    "relUrl": "/api-reference/nodes-apis/nodes-stats/"
  },"2325": {
    "doc": "Nodes stats",
    "title": "Path and HTTP methods",
    "content": "GET /_nodes/stats GET /_nodes/&lt;node_id&gt;/stats GET /_nodes/stats/&lt;metric&gt; GET /_nodes/&lt;node_id&gt;/stats/&lt;metric&gt; GET /_nodes/stats/&lt;metric&gt;/&lt;index_metric&gt; GET /_nodes/&lt;node_id&gt;/stats/&lt;metric&gt;/&lt;index_metric&gt; . ",
    "url": "https://vagimeli.github.io/api-reference/nodes-apis/nodes-stats/#path-and-http-methods",
    "relUrl": "/api-reference/nodes-apis/nodes-stats/#path-and-http-methods"
  },"2326": {
    "doc": "Nodes stats",
    "title": "Path parameters",
    "content": "The following table lists the available path parameters. All path parameters are optional. | Parameter | Type | Description | . | nodeId | String | A comma-separated list of nodeIds used to filter results. Supports node filters. Defaults to _all. | . | metric | String | A comma-separated list of metric groups that will be included in the response. For example, jvm,fs. See the list of all metrics below. Defaults to all metrics. | . | index_metric | String | A comma-separated list of index metric groups that will be included in the response. For example, docs,store. See the list of all index metrics below. Defaults to all index metrics. | . The following table lists all available metric groups. | Metric | Description | . | indices | Index statistics, such as size, document count, and search, index, and delete times for documents. | . | os | Statistics about the host OS, including load, memory, and swapping. | . | process | Statistics about processes, including their memory consumption, open file descriptors, and CPU usage. | . | jvm | Statistics about the JVM, including memory pool, buffer pool, and garbage collection, and the number of loaded classes. | . | fs | File system statistics, such as read/write statistics, data path, and free disk space. | . | transport | Transport layer statistics about send/receive in cluster communication. | . | http | Statistics about the HTTP layer. | . | breaker | Statistics about the field data circuit breakers. | . | script | Statistics about scripts, such as compilations and cache evictions. | . | discovery | Statistics about cluster states. | . | ingest | Statistics about ingest pipelines. | . | adaptive_selection | Statistics about adaptive replica selection, which selects an eligible node using shard allocation awareness. | . | script_cache | Statistics about script cache. | . | indexing_pressure | Statistics about the node’s indexing pressure. | . | shard_indexing_pressure | Statistics about shard indexing pressure. | . To filter the information returned for the indices metric, you can use specific index_metric values. You can use these only when you use the following query types: . GET _nodes/stats/ GET _nodes/stats/_all GET _nodes/stats/indices . The following index metrics are supported: . | docs | store | indexing | get | search | merge | refresh | flush | warmer | query_cache | fielddata | completion | segments | translog | request_cache | . For example, the following query requests statistics for docs and search: . GET _nodes/stats/indices/docs,search . copy . ",
    "url": "https://vagimeli.github.io/api-reference/nodes-apis/nodes-stats/#path-parameters",
    "relUrl": "/api-reference/nodes-apis/nodes-stats/#path-parameters"
  },"2327": {
    "doc": "Nodes stats",
    "title": "Query parameters",
    "content": "The following table lists the available query parameters. All query parameters are optional. | Parameter | Type | Description | . | completion_fields | String | The fields to include in completion statistics. Supports comma-separated lists and wildcard expressions. | . | fielddata_fields | String | The fields to include in fielddata statistics. Supports comma-separated lists and wildcard expressions. | . | fields | String | The fields to include. Supports comma-separated lists and wildcard expressions. | . | groups | String | A comma-separated list of search groups to include in the search statistics. | . | level | String | Specifies whether statistics are aggregated at the cluster, index, or shard level. Valid values are indices, node, and shard. | . | timeout | Time | Sets the time limit for node response. Default is 30s. | . | include_segment_file_sizes | Boolean | If segment statistics are requested, this field specifies to return the aggregated disk usage of every Lucene index file. Default is false. | . Example request . GET _nodes/stats/ . copy . Example response . { \"_nodes\" : { \"total\" : 1, \"successful\" : 1, \"failed\" : 0 }, \"cluster_name\" : \"docker-cluster\", \"nodes\" : { \"F-ByTQzVQ3GQeYzQJArJGQ\" : { \"timestamp\" : 1664484195257, \"name\" : \"opensearch\", \"transport_address\" : \"127.0.0.1:9300\", \"host\" : \"127.0.0.1\", \"ip\" : \"127.0.0.1:9300\", \"roles\" : [ \"cluster_manager\", \"data\", \"ingest\", \"remote_cluster_client\" ], \"attributes\" : { \"shard_indexing_pressure_enabled\" : \"true\" }, \"indices\" : { \"docs\" : { \"count\" : 13160, \"deleted\" : 12 }, \"store\" : { \"size_in_bytes\" : 6263461, \"reserved_in_bytes\" : 0 }, \"indexing\" : { \"index_total\" : 0, \"index_time_in_millis\" : 0, \"index_current\" : 0, \"index_failed\" : 0, \"delete_total\" : 204, \"delete_time_in_millis\" : 427, \"delete_current\" : 0, \"noop_update_total\" : 0, \"is_throttled\" : false, \"throttle_time_in_millis\" : 0 }, \"get\" : { \"total\" : 4, \"time_in_millis\" : 18, \"exists_total\" : 4, \"exists_time_in_millis\" : 18, \"missing_total\" : 0, \"missing_time_in_millis\" : 0, \"current\" : 0 }, \"search\" : { \"open_contexts\": 4, \"query_total\": 194, \"query_time_in_millis\": 467, \"query_current\": 0, \"fetch_total\": 194, \"fetch_time_in_millis\": 143, \"fetch_current\": 0, \"scroll_total\": 0, \"scroll_time_in_millis\": 0, \"scroll_current\": 0, \"point_in_time_total\": 0, \"point_in_time_time_in_millis\": 0, \"point_in_time_current\": 0, \"suggest_total\": 0, \"suggest_time_in_millis\": 0, \"suggest_current\": 0 }, \"merges\" : { \"current\" : 0, \"current_docs\" : 0, \"current_size_in_bytes\" : 0, \"total\" : 1, \"total_time_in_millis\" : 5, \"total_docs\" : 12, \"total_size_in_bytes\" : 3967, \"total_stopped_time_in_millis\" : 0, \"total_throttled_time_in_millis\" : 0, \"total_auto_throttle_in_bytes\" : 251658240 }, \"refresh\" : { \"total\" : 74, \"total_time_in_millis\" : 201, \"external_total\" : 57, \"external_total_time_in_millis\" : 314, \"listeners\" : 0 }, \"flush\" : { \"total\" : 28, \"periodic\" : 28, \"total_time_in_millis\" : 1261 }, \"warmer\" : { \"current\" : 0, \"total\" : 45, \"total_time_in_millis\" : 99 }, \"query_cache\" : { \"memory_size_in_bytes\" : 0, \"total_count\" : 0, \"hit_count\" : 0, \"miss_count\" : 0, \"cache_size\" : 0, \"cache_count\" : 0, \"evictions\" : 0 }, \"fielddata\" : { \"memory_size_in_bytes\" : 356, \"evictions\" : 0 }, \"completion\" : { \"size_in_bytes\" : 0, \"fields\" : { } }, \"segments\" : { \"count\" : 17, \"memory_in_bytes\" : 0, \"terms_memory_in_bytes\" : 0, \"stored_fields_memory_in_bytes\" : 0, \"term_vectors_memory_in_bytes\" : 0, \"norms_memory_in_bytes\" : 0, \"points_memory_in_bytes\" : 0, \"doc_values_memory_in_bytes\" : 0, \"index_writer_memory_in_bytes\" : 0, \"version_map_memory_in_bytes\" : 0, \"fixed_bit_set_memory_in_bytes\" : 288, \"max_unsafe_auto_id_timestamp\" : -1, \"file_sizes\" : { } }, \"translog\" : { \"operations\" : 12, \"size_in_bytes\" : 1452, \"uncommitted_operations\" : 12, \"uncommitted_size_in_bytes\" : 1452, \"earliest_last_modified_age\" : 164160 }, \"request_cache\" : { \"memory_size_in_bytes\" : 1649, \"evictions\" : 0, \"hit_count\" : 0, \"miss_count\" : 18 }, \"recovery\" : { \"current_as_source\" : 0, \"current_as_target\" : 0, \"throttle_time_in_millis\" : 0 } }, \"os\" : { \"timestamp\" : 1664484195263, \"cpu\" : { \"percent\" : 0, \"load_average\" : { \"1m\" : 0.0, \"5m\" : 0.0, \"15m\" : 0.0 } }, \"mem\" : { \"total_in_bytes\" : 13137076224, \"free_in_bytes\" : 9265442816, \"used_in_bytes\" : 3871633408, \"free_percent\" : 71, \"used_percent\" : 29 }, \"swap\" : { \"total_in_bytes\" : 4294967296, \"free_in_bytes\" : 4294967296, \"used_in_bytes\" : 0 }, \"cgroup\" : { \"cpuacct\" : { \"control_group\" : \"/\", \"usage_nanos\" : 338710071600 }, \"cpu\" : { \"control_group\" : \"/\", \"cfs_period_micros\" : 100000, \"cfs_quota_micros\" : -1, \"stat\" : { \"number_of_elapsed_periods\" : 0, \"number_of_times_throttled\" : 0, \"time_throttled_nanos\" : 0 } }, \"memory\" : { \"control_group\" : \"/\", \"limit_in_bytes\" : \"9223372036854771712\", \"usage_in_bytes\" : \"1432346624\" } } }, \"process\" : { \"timestamp\" : 1664484195263, \"open_file_descriptors\" : 556, \"max_file_descriptors\" : 65536, \"cpu\" : { \"percent\" : 0, \"total_in_millis\" : 170870 }, \"mem\" : { \"total_virtual_in_bytes\" : 6563344384 } }, \"jvm\" : { \"timestamp\" : 1664484195264, \"uptime_in_millis\" : 21232111, \"mem\" : { \"heap_used_in_bytes\" : 308650480, \"heap_used_percent\" : 57, \"heap_committed_in_bytes\" : 536870912, \"heap_max_in_bytes\" : 536870912, \"non_heap_used_in_bytes\" : 147657128, \"non_heap_committed_in_bytes\" : 152502272, \"pools\" : { \"young\" : { \"used_in_bytes\" : 223346688, \"max_in_bytes\" : 0, \"peak_used_in_bytes\" : 318767104, \"peak_max_in_bytes\" : 0, \"last_gc_stats\" : { \"used_in_bytes\" : 0, \"max_in_bytes\" : 0, \"usage_percent\" : -1 } }, \"old\" : { \"used_in_bytes\" : 67068928, \"max_in_bytes\" : 536870912, \"peak_used_in_bytes\" : 67068928, \"peak_max_in_bytes\" : 536870912, \"last_gc_stats\" : { \"used_in_bytes\" : 34655744, \"max_in_bytes\" : 536870912, \"usage_percent\" : 6 } }, \"survivor\" : { \"used_in_bytes\" : 18234864, \"max_in_bytes\" : 0, \"peak_used_in_bytes\" : 32721280, \"peak_max_in_bytes\" : 0, \"last_gc_stats\" : { \"used_in_bytes\" : 18234864, \"max_in_bytes\" : 0, \"usage_percent\" : -1 } } } }, \"threads\" : { \"count\" : 80, \"peak_count\" : 80 }, \"gc\" : { \"collectors\" : { \"young\" : { \"collection_count\" : 18, \"collection_time_in_millis\" : 199 }, \"old\" : { \"collection_count\" : 0, \"collection_time_in_millis\" : 0 } } }, \"buffer_pools\" : { \"mapped\" : { \"count\" : 23, \"used_in_bytes\" : 6232113, \"total_capacity_in_bytes\" : 6232113 }, \"direct\" : { \"count\" : 63, \"used_in_bytes\" : 9050069, \"total_capacity_in_bytes\" : 9050068 }, \"mapped - 'non-volatile memory'\" : { \"count\" : 0, \"used_in_bytes\" : 0, \"total_capacity_in_bytes\" : 0 } }, \"classes\" : { \"current_loaded_count\" : 20693, \"total_loaded_count\" : 20693, \"total_unloaded_count\" : 0 } }, \"thread_pool\" : { \"OPENSEARCH_ML_TASK_THREAD_POOL\" : { \"threads\" : 0, \"queue\" : 0, \"active\" : 0, \"rejected\" : 0, \"largest\" : 0, \"completed\" : 0 }, \"ad-batch-task-threadpool\" : { \"threads\" : 0, \"queue\" : 0, \"active\" : 0, \"rejected\" : 0, \"largest\" : 0, \"completed\" : 0 }, ... }, \"fs\" : { \"timestamp\" : 1664484195264, \"total\" : { \"total_in_bytes\" : 269490393088, \"free_in_bytes\" : 261251477504, \"available_in_bytes\" : 247490805760 }, \"data\" : [ { \"path\" : \"/usr/share/opensearch/data/nodes/0\", \"mount\" : \"/ (overlay)\", \"type\" : \"overlay\", \"total_in_bytes\" : 269490393088, \"free_in_bytes\" : 261251477504, \"available_in_bytes\" : 247490805760 } ], \"io_stats\" : { } }, \"transport\" : { \"server_open\" : 0, \"total_outbound_connections\" : 0, \"rx_count\" : 0, \"rx_size_in_bytes\" : 0, \"tx_count\" : 0, \"tx_size_in_bytes\" : 0 }, \"http\" : { \"current_open\" : 5, \"total_opened\" : 1108 }, \"breakers\" : { \"request\" : { \"limit_size_in_bytes\" : 322122547, \"limit_size\" : \"307.1mb\", \"estimated_size_in_bytes\" : 0, \"estimated_size\" : \"0b\", \"overhead\" : 1.0, \"tripped\" : 0 }, \"fielddata\" : { \"limit_size_in_bytes\" : 214748364, \"limit_size\" : \"204.7mb\", \"estimated_size_in_bytes\" : 356, \"estimated_size\" : \"356b\", \"overhead\" : 1.03, \"tripped\" : 0 }, \"in_flight_requests\" : { \"limit_size_in_bytes\" : 536870912, \"limit_size\" : \"512mb\", \"estimated_size_in_bytes\" : 0, \"estimated_size\" : \"0b\", \"overhead\" : 2.0, \"tripped\" : 0 }, \"parent\" : { \"limit_size_in_bytes\" : 510027366, \"limit_size\" : \"486.3mb\", \"estimated_size_in_bytes\" : 308650480, \"estimated_size\" : \"294.3mb\", \"overhead\" : 1.0, \"tripped\" : 0 } }, \"script\" : { \"compilations\" : 0, \"cache_evictions\" : 0, \"compilation_limit_triggered\" : 0 }, \"discovery\" : { \"cluster_state_queue\" : { \"total\" : 0, \"pending\" : 0, \"committed\" : 0 }, \"published_cluster_states\" : { \"full_states\" : 2, \"incompatible_diffs\" : 0, \"compatible_diffs\" : 10 } }, \"ingest\" : { \"total\" : { \"count\" : 0, \"time_in_millis\" : 0, \"current\" : 0, \"failed\" : 0 }, \"pipelines\" : { } }, \"adaptive_selection\" : { \"F-ByTQzVQ3GQeYzQJArJGQ\" : { \"outgoing_searches\" : 0, \"avg_queue_size\" : 0, \"avg_service_time_ns\" : 501024, \"avg_response_time_ns\" : 794105, \"rank\" : \"0.8\" } }, \"script_cache\" : { \"sum\" : { \"compilations\" : 0, \"cache_evictions\" : 0, \"compilation_limit_triggered\" : 0 }, \"contexts\" : [ { \"context\" : \"aggregation_selector\", \"compilations\" : 0, \"cache_evictions\" : 0, \"compilation_limit_triggered\" : 0 }, { \"context\" : \"aggs\", \"compilations\" : 0, \"cache_evictions\" : 0, \"compilation_limit_triggered\" : 0 }, ... ] }, \"indexing_pressure\" : { \"memory\" : { \"current\" : { \"combined_coordinating_and_primary_in_bytes\" : 0, \"coordinating_in_bytes\" : 0, \"primary_in_bytes\" : 0, \"replica_in_bytes\" : 0, \"all_in_bytes\" : 0 }, \"total\" : { \"combined_coordinating_and_primary_in_bytes\" : 40256, \"coordinating_in_bytes\" : 40256, \"primary_in_bytes\" : 45016, \"replica_in_bytes\" : 0, \"all_in_bytes\" : 40256, \"coordinating_rejections\" : 0, \"primary_rejections\" : 0, \"replica_rejections\" : 0 }, \"limit_in_bytes\" : 53687091 } }, \"shard_indexing_pressure\" : { \"stats\" : { }, \"total_rejections_breakup_shadow_mode\" : { \"node_limits\" : 0, \"no_successful_request_limits\" : 0, \"throughput_degradation_limits\" : 0 }, \"enabled\" : false, \"enforced\" : false } } } } . ",
    "url": "https://vagimeli.github.io/api-reference/nodes-apis/nodes-stats/#query-parameters",
    "relUrl": "/api-reference/nodes-apis/nodes-stats/#query-parameters"
  },"2328": {
    "doc": "Nodes stats",
    "title": "Response fields",
    "content": "The following table lists all response fields. | Field | Data type | Description | . | _nodes | Object | Statistics about the nodes that are returned. | . | _nodes.total | Integer | The total number of nodes for this request. | . | _nodes.successful | Integer | The number of nodes for which the request was successful. | . | _nodes.failed | Integer | The number of nodes for which the request failed. If there are nodes for which the request failed, the failure message is included. | . | cluster_name | String | The name of the cluster. | . | nodes | Object | Statistics for the nodes included in this request. | . nodes . The nodes object contains all nodes that are returned by the request, along with their IDs. Each node has the following properties. | Field | Data type | Description | . | timestamp | Integer | The time the nodes statistics were collected, in milliseconds since the epoch. | . | name | String | The name of the node. | . | transport_address | IP address | The host and port of the transport layer that is used by nodes in a cluster to communicate internally. | . | host | IP address | The network host of the node. | . | ip | IP address | The IP address and port of the node. | . | roles | Array | The roles of the node (for example, cluster_manager, data, or ingest). | . | attributes | Object | The attributes of the node (for example, shard_indexing_pressure_enabled). | . | indices | Object | Index statistics for each index that has shards on the node. | . | os | Object | Statistics about the OS for the node. | . | process | Object | Process statistics for the node. | . | jvm | Object | Statistics about the JVM for the node. | . | thread_pool | Object | Statistics about each thread pool for the node. | . | fs | Object | Statistics about the file stores for the node. | . | transport | Object | Transport statistics for the node. | . | http | Object | HTTP statistics for the node. | . | http.current_open | Integer | The number of currently open HTTP connections for the node. | . | http.total_opened | Integer | The total number of HTTP connections the node has opened since it started. | . | breakers | Object | Statistics about the circuit breakers for the node. | . | script | Object | Script statistics for the node. | . | script_cache | Object | Script cache statistics for the node. | . | discovery | Object | Node discovery statistics for the node. | . | ingest | Object | Ingest statistics for the node. | . | adaptive_selection | Object | Statistics about adaptive selections for the node. | . | indexing_pressure | Object | Statistics related to the node’s indexing pressure. | . | shard_indexing_pressure | Object | Statistics related to indexing pressure at the shard level. | . | search_backpressure | Object | Statistics related to search backpressure. | . indices . The indices object contains the index statistics for each index with shards on this node. Each index has the following properties. | Field | Field type | Description | . | docs | Object | Document statistics for all primary shards that exist on the node. | . | docs.count | Integer | The number of documents reported by Lucene. Excludes deleted documents and recently indexed documents that are not yet assigned to a segment. Nested documents are counted separately. | . | docs.deleted | Integer | The number of deleted documents reported by Lucene. Excludes recent deletion operations that have not yet affect the segment. | . | store | Object | Statistics about the shard sizes of the shards on the node. | . | store.size_in_bytes | Integer | Total size of all shards on the node. | . | store.reserved_in_bytes | Integer | The predicted number of bytes the shard store will grow to be because of activities such as restoring snapshots and peer recoveries. | . | indexing | Object | Statistics about indexing operations for the node. | . | indexing.index_total | Integer | The total number of indexing operations on the node. | . | indexing.index_time_in_millis | Integer | The total time for all indexing operations, in milliseconds. | . | indexing.index_current | Integer | The number of indexing operations that are currently running. | . | indexing.index_failed | Integer | The number of indexing operations that have failed. | . | indexing.delete_total | Integer | The total number of deletions. | . | indexing.delete_time_in_millis | Integer | The total time for all deletion operations, in milliseconds. | . | indexing.delete_current | Integer | The number of deletion operations that are currently running. | . | indexing.noop_update_total | Integer | The total number of noop operations. | . | indexing.is_throttled | Boolean | Specifies whether any operations were throttled. | . | indexing.throttle_time_in_millis | Integer | The total time for throttling operations, in milliseconds. | . | get | Object | Statistics about the get operations for the node. | . | get.total | Integer | The total number of get operations. | . | get.time_in_millis | Integer | The total time for all get operations, in milliseconds. | . | get.exists_total | Integer | The total number of successful get operations. | . | get.exists_time_in_millis | Integer | The total time for all successful get operations, in milliseconds. | . | get.missing_total | Integer | The number of failed get operations. | . | get.missing_time_in_millis | Integer | The total time for all failed get operations, in milliseconds. | . | get.current | Integer | The number of get operations that are currently running. | . | search | Object | Statistics about the search operations for the node. | . | search.point_in_time_total | Integer | The total number of Point in Time contexts that have been created (completed and active) since the node last restarted. | . | search.point_in_time_time_in_millis | Integer | The amount of time that Point in Time contexts have been held open since the node last restarted, in milliseconds. | . | search.point_in_time_current | Integer | The number of Point in Time contexts currently open. | . | search.open_contexts | Integer | The number of open search contexts. | . | search.query_total | Integer | The total number of query operations. | . | search.query_time_in_millis | Integer | The total time for all query operations, in milliseconds. | . | search.query_current | Integer | The number of query operations that are currently running. | . | search.fetch_total | Integer | The total number of fetch operations. | . | search.fetch_time_in_millis | Integer | The total time for all fetch operations, in milliseconds. | . | search.fetch_current | Integer | The number of fetch operations that are currently running. | . | search.scroll_total | Integer | The total number of scroll operations. | . | search.scroll_time_in_millis | Integer | The total time for all scroll operations, in milliseconds. | . | search.scroll_current | Integer | The number of scroll operations that are currently running. | . | search.suggest_total | Integer | The total number of suggest operations. | . | search.suggest_time_in_millis | Integer | The total time for all suggest operations, in milliseconds. | . | search.suggest_current | Integer | The number of suggest operations that are currently running. | . | merges | Object | Statistics about merge operations for the node. | . | merges.current | Integer | The number of merge operations that are currently running. | . | merges.current_docs | Integer | The number of document merges that are currently running. | . | merges.current_size_in_bytes | Integer | The memory size, in bytes, that is used to perform current merge operations. | . | merges.total | Integer | The total number of merge operations. | . | merges.total_time_in_millis | Integer | The total time for merges, in milliseconds. | . | merges.total_docs | Integer | The total number of documents that have been merged. | . | merges.total_size_in_bytes | Integer | The total size of all merged documents, in bytes. | . | merges.total_stopped_time_in_millis | Integer | The total time spent on stopping merge operations, in milliseconds. | . | merges.total_throttled_time_in_millis | Integer | The total time spent on throttling merge operations, in milliseconds. | . | merges.total_auto_throttle_in_bytes | Integer | The total size of automatically throttled merge operations, in bytes. | . | refresh | Object | Statistics about refresh operations for the node. | . | refresh.total | Integer | The total number of refresh operations. | . | refresh.total_time_in_millis | Integer | The total time for all refresh operations, in milliseconds. | . | refresh.external_total | Integer | The total number of external refresh operations. | . | refresh.external_total_time_in_millis | Integer | The total time for all external refresh operations, in milliseconds. | . | refresh.listeners | Integer | The number of refresh listeners. | . | flush | Object | Statistics about flush operations for the node. | . | flush.total | Integer | The total number of flush operations. | . | flush.periodic | Integer | The total number of periodic flush operations. | . | flush.total_time_in_millis | Integer | The total time for all flush operations, in milliseconds. | . | warmer | Object | Statistics about the index warming operations for the node. | . | warmer.current | Integer | The number of current index warming operations. | . | warmer.total | Integer | The total number of index warming operations. | . | warmer.total_time_in_millis | Integer | The total time for all index warming operations, in milliseconds. | . | query_cache | Statistics about query cache operations for the node. |   | . | query_cache.memory_size_in_bytes | Integer | The amount of memory used for the query cache for all shards in the node. | . | query_cache.total_count | Integer | The total number of hits, misses, and cached queries in the query cache. | . | query_cache.hit_count | Integer | The total number of hits in the query cache. | . | query_cache.miss_count | Integer | The total number of misses in the query cache. | . | query_cache.cache_size | Integer | The size of the query cache, in bytes. | . | query_cache.cache_count | Integer | The number of queries in the query cache. | . | query_cache.evictions | Integer | The number of evictions in the query cache. | . | fielddata | Object | Statistics about the field data cache for all shards in the node. | . | fielddata.memory_size_in_bytes | Integer | The total amount of memory used for the field data cache for all shards in the node. | . | fielddata.evictions | Integer | The number of evictions in the field data cache. | . | fielddata.fields | Object | Contains all field data fields. | . | completion | Object | Statistics about completions for all shards in the node. | . | completion.size_in_bytes | Integer | The total amount of memory used for completion for all shards in the node, in bytes. | . | completion.fields | Object | Contains completion fields. | . | segments | Object | Statistics about segments for all shards in the node. | . | segments.count | Integer | The total number of segments. | . | segments.memory_in_bytes | Integer | The total amount of memory, in bytes. | . | segments.terms_memory_in_bytes | Integer | The total amount of memory used for terms, in bytes. | . | segments.stored_fields_memory_in_bytes | Integer | The total amount of memory used for stored fields, in bytes. | . | segments.term_vectors_memory_in_bytes | Integer | The total amount of memory used for term vectors, in bytes. | . | segments.norms_memory_in_bytes | Integer | The total amount of memory used for normalization factors, in bytes. | . | segments.points_memory_in_bytes | Integer | The total amount of memory used for points, in bytes. | . | segments.doc_values_memory_in_bytes | Integer | The total amount of memory used for doc values, in bytes. | . | segments.index_writer_memory_in_bytes | Integer | The total amount of memory used by all index writers, in bytes. | . | segments.version_map_memory_in_bytes | Integer | The total amount of memory used by all version maps, in bytes. | . | segments.fixed_bit_set_memory_in_bytes | Integer | The total amount of memory used by fixed bit sets, in bytes. Fixed bit sets are used for nested objects and join fields. | . | segments.max_unsafe_auto_id_timestamp | Integer | The timestamp for the most recently retired indexing request, in milliseconds since the epoch. | . | segments.file_sizes | Integer | Statistics about the size of the segment files. | . | translog | Object | Statistics about transaction log operations for the node. | . | translog.operations | Integer | The number of translog operations. | . | translog.size_in_bytes | Integer | The size of the translog, in bytes. | . | translog.uncommitted_operations | Integer | The number of uncommitted translog operations. | . | translog.uncommitted_size_in_bytes | Integer | The size of uncommitted translog operations, in bytes. | . | translog.earliest_last_modified_age | Integer | The earliest last modified age for the translog. | . | request_cache | Object | Statistics about the request cache for the node. | . | request_cache.memory_size_in_bytes | Integer | The memory size used by the request cache, in bytes. | . | request_cache.evictions | Integer | The number of request cache evictions. | . | request_cache.hit_count | Integer | The number of request cache hits. | . | request_cache.miss_count | Integer | The number of request cache misses. | . | recovery | Object | Statistics about recovery operations for the node. | . | recovery.current_as_source | Integer | The number of recovery operations that have used an index shard as a source. | . | recovery.current_as_target | Integer | The number of recovery operations that have used an index shard as a target. | . | recovery.throttle_time_in_millis | Integer | The delay of recovery operations due to throttling, in milliseconds. | . os . The os object has the OS statistics for the node and has the following properties. | Field | Field type | Description | . | timestamp | Integer | The last refresh time for the OS statistics, in milliseconds since the epoch. | . | cpu | Object | Statistics about the node’s CPU usage. | . | cpu.percent | Integer | Recent CPU usage for the system. | . | cpu.load_average | Object | Statistics about load averages for the system. | . | cpu.load_average.1m | Float | The load average for the system for the time period of one minute. | . | cpu.load_average.5m | Float | The load average for the system for the time period of five minutes. | . | cpu.load_average.15m | Float | The load average for the system for the time period of 15 minutes. | . | cpu.mem | Object | Statistics about memory usage for the node. | . | cpu.mem.total_in_bytes | Integer | The total amount of physical memory, in bytes. | . | cpu.mem.free_in_bytes | Integer | The total amount of free physical memory, in bytes. | . | cpu.mem.used_in_bytes | Integer | The total amount of used physical memory, in bytes. | . | cpu.mem.free_percent | Integer | The percentage of memory that is free. | . | cpu.mem.used_percent | Integer | The percentage of memory that is used. | . | cpu.swap | Object | Statistics about swap space for the node. | . | cpu.swap.total_in_bytes | Integer | The total amount of swap space, in bytes. | . | cpu.swap.free_in_bytes | Integer | The total amount of free swap space, in bytes. | . | cpu.swap.used_in_bytes | Integer | The total amount of used swap space, in bytes. | . | cpu.cgroup | Object | Contains cgroup statistics for the node. Returned for Linux only. | . | cpu.cgroup.cpuacct | Object | Statistics about the cpuacct control group for the node. | . | cpu.cgroup.cpu | Object | Statistics about the CPU control group for the node. | . | cpu.cgroup.memory | Object | Statistics about the memory control group for the node. | . process . The process object contains process statistics for the node and has the following properties. | Field | Field type | Description | . | timestamp | Integer | The last refresh time for the process statistics, in milliseconds since the epoch. | . | open_file_descriptors | Integer | The number of opened file descriptors associated with the current process. | . | max_file_descriptors | Integer | The maximum number of file descriptors for the system. | . | cpu | Object | Statistics about the CPU for the node. | . | cpu.percent | Integer | The percentage of CPU usage for the process. | . | cpu.total_in_millis | Integer | The total CPU time used by the process on which the JVM is running, in milliseconds. | . | mem | Object | Statistics about the memory for the node. | . | mem.total_virtual_in_bytes | Integer | The total amount of virtual memory that is guaranteed to be available to the process that is currently running, in bytes. | . jvm . The jvm object contains statistics about the JVM for the node and has the following properties. | Field | Field type | Description | . | timestamp | Integer | The last refresh time for the JVM statistics, in milliseconds since the epoch. | . | uptime_in_millis | Integer | The JVM uptime, in milliseconds. | . | mem | Object | Statistics for the JVM memory usage on the node. | . | mem.heap_used_in_bytes | Integer | The amount of memory that is currently being used, in bytes. | . | mem.heap_used_percent | Integer | The percentage of memory that is currently used by the heap. | . | mem.heap_committed_in_bytes | Integer | The amount of memory available for use by the heap, in bytes. | . | mem.heap_max_in_bytes | Integer | The maximum amount of memory available for use by the heap, in bytes. | . | mem.non_heap_used_in_bytes | Integer | The amount of non-heap memory that is currently used, in bytes. | . | mem.non_heap_committed_in_bytes | Integer | The maximum amount of non-heap memory available for use, in bytes. | . | mem.pools | Object | Statistics about heap memory usage for the node. | . | mem.pools.young | Object | Statistics about the young generation heap memory usage for the node. Contains the amount of memory used, the maximum amount of memory available, and the peak amount of memory used. | . | mem.pools.old | Object | Statistics about the old generation heap memory usage for the node. Contains the amount of memory used, the maximum amount of memory available, and the peak amount of memory used. | . | mem.pools.survivor | Object | Statistics about the survivor space memory usage for the node. Contains the amount of memory used, the maximum amount of memory available, and the peak amount of memory used. | . | threads | Object | Statistics about the JVM thread usage for the node. | . | threads.count | Integer | The number of threads that are currently active in the JVM. | . | threads.peak_count | Integer | The maximum number of threads in the JVM. | . | gc.collectors | Object | Statistics about the JVM garbage collectors for the node. | . | gc.collectors.young | Integer | Statistics about JVM garbage collectors that collect young generation objects. | . | gc.collectors.young.collection_count | Integer | The number of garbage collectors that collect young generation objects. | . | gc.collectors.young.collection_time_in_millis | Integer | The total time spent on garbage collection of young generation objects, in milliseconds. | . | gc.collectors.old | Integer | Statistics about JVM garbage collectors that collect old generation objects. | . | gc.collectors.old.collection_count | Integer | The number of garbage collectors that collect old generation objects. | . | gc.collectors.old.collection_time_in_millis | Integer | The total time spent on garbage collection of old generation objects, in milliseconds. | . | buffer_pools | Object | Statistics about the JVM buffer pools for the node. | . | buffer_pools.mapped | Object | Statistics about the mapped JVM buffer pools for the node. | . | buffer_pools.mapped.count | Integer | The number of mapped buffer pools. | . | buffer_pools.mapped.used_in_bytes | Integer | The amount of memory used by mapped buffer pools, in bytes. | . | buffer_pools.mapped.total_capacity_in_bytes | Integer | The total capacity of the mapped buffer pools, in bytes. | . | buffer_pools.direct | Object | Statistics about the direct JVM buffer pools for the node. | . | buffer_pools.direct.count | Integer | The number of direct buffer pools. | . | buffer_pools.direct.used_in_bytes | Integer | The amount of memory used by direct buffer pools, in bytes. | . | buffer_pools.direct.total_capacity_in_bytes | Integer | The total capacity of the direct buffer pools, in bytes. | . | classes | Object | Statistics about the classes loaded by the JVM for the node. | . | classes.current_loaded_count | Integer | The number of classes currently loaded by the JVM. | . | classes.total_loaded_count | Integer | The total number of classes loaded by the JVM since it started. | . | classes.total_unloaded_count | Integer | The total number of classes unloaded by the JVM since it started. | . thread_pool . The thread_pool object contains a list of all thread pools. Each thread pool is a nested object specified by its ID with the properties listed below. | Field | Field type | Description | . | threads | Integer | The number of threads in the pool. | . | queue | Integer | The number of threads in queue. | . | active | Integer | The number of active threads in the pool. | . | rejected | Integer | The number of tasks that have been rejected. | . | largest | Integer | The peak number of threads in the pool. | . | completed | Integer | The number of tasks completed. | . fs . The fs object represents statistics about the file stores for the node. It has the following properties. | Field | Field type | Description | . | timestamp | Integer | The last refresh time for the file store statistics, in milliseconds since the epoch. | . | total | Object | Statistics for all file stores of the node. | . | total.total_in_bytes | Integer | The total memory size of all file stores, in bytes. | . | total.free_in_bytes | Integer | The total unallocated disk space in all file stores, in bytes. | . | total.available_in_bytes | Integer | The total disk space available to the JVM on all file stores. Represents the actual amount of memory, in bytes, that OpenSearch can use. | . | data | Array | The list of all file stores. Each file store has the properties listed below. | . | data.path | String | The path to the file store. | . | data.mount | String | The mount point of the file store. | . | data.type | String | The type of the file store (for example, overlay). | . | data.total_in_bytes | Integer | The total size of the file store, in bytes. | . | data.free_in_bytes | Integer | The total unallocated disk space in the file store, in bytes. | . | data.available_in_bytes | Integer | The total amount of disk space available to the JVM for the file store, in bytes. | . | io_stats | Object | I/O statistics for the node (Linux only). Includes devices, read and write operations, and the I/O operation time. | . transport . The transport object has the following properties. | Field | Field type | Description | . | server_open | Integer | The number of open inbound TCP connections that OpenSearch nodes use for internal communication. | . | total_outbound_connections | Integer | The total number of outbound transport connections that the node has opened since it started. | . | rx_count | Integer | The total number of RX (receive) packets the node received during internal communication. | . | rx_size_in_bytes | Integer | The total size of RX packets the node received during internal communication, in bytes. | . | tx_count | Integer | The total number of TX (transmit) packets the node sent during internal communication. | . | tx_size_in_bytes | Integer | The total size of TX (transmit) packets the node sent during internal communication, in bytes. | . breakers . The breakers object contains statistics about the circuit breakers for the node. Each circuit breaker is a nested object listed by name and contains the following properties. | Field | Field type | Description | . | limit_size_in_bytes | Integer | The memory limit for the circuit breaker, in bytes. | . | limit_size | Byte value | The memory limit for the circuit breaker in human-readable format (for example, 307.1mb). | . | estimated_size_in_bytes | Integer | The estimated memory used for the operation, in bytes. | . | estimated_size | Byte value | The estimated memory used for the operation in human-readable format (for example, 356b). | . | overhead | Float | A factor that all estimates are multiplied by to calculate the final estimate. | . | tripped | Integer | The total number of times the circuit breaker has been activated to prevent an out-of-memory error. | . script and script_cache . The script and script_cache objects have the following properties. | Field | Field type | Description | . | script | Object | Script statistics for the node. | . | script.compilations | Integer | The total number of script compilations for the node. | . | script.cache_evictions | Integer | The total number of times the script cache has purged old data. | . | script.compilation_limit_triggered | Integer | The total number of times script compilation was limited by a circuit breaker. | . | script_cache | Object | Script cache statistics for the node. | . | script_cache.sum.compilations | Integer | The total number of script compilations in the cache for the node. | . | script_cache.sum.cache_evictions | Integer | The total number of times the script cache has purged old data. | . | script_cache.sum.compilation_limit_triggered | Integer | The total number of times script compilation in the cache was limited by a circuit breaker. | . | script_cache.contexts | Array of objects | The list of contexts for the script cache. Each context contains its name, the number of compilations, the number of cache evictions, and the number of times the script was limited by a circuit breaker. | . discovery . The discovery object contains the node discovery statistics and has the following properties. | Field | Field type | Description | . | cluster_state_queue | Object | Cluster state queue statistics for the node. | . | cluster_state_queue.total | Integer | The total number of cluster states in the queue. | . | cluster_state_queue.pending | Integer | The number of pending cluster states in the queue. | . | cluster_state_queue.committed | Integer | The number of committed cluster states in the queue. | . | published_cluster_states | Object | Statistics for the published cluster states for the node. | . | published_cluster_states.full_states | Integer | The number of published cluster states. | . | published_cluster_states.incompatible_diffs | Integer | The number of incompatible differences between published cluster states. | . | published_cluster_states.compatible_diffs | Integer | The number of compatible differences between published cluster states. | . ingest . The ingest object contains the ingest statistics and has the following properties. | Field | Field type | Description | . | total | Integer | Ingest statistics for the node’s lifetime. | . | total.count | Integer | The total number of documents ingested by the node. | . | total.time_in_millis | Integer | The total amount of time for preprocessing ingest documents, in milliseconds. | . | total.current | Integer | The total number of documents that are currently being ingested by the node. | . | total.failed | Integer | The total number of failed ingestions for the node. | . | pipelines | Object | Ingest pipeline statistics for the node. Each pipeline is a nested object specified by its ID with the properties listed below. | . | pipelines.id.count | Integer | The number of documents preprocessed by the ingest pipeline. | . | pipelines.id.time_in_millis | Integer | The total amount of time for preprocessing documents in the ingest pipeline, in milliseconds. | . | pipelines.id.failed | Integer | The total number of failed ingestions for the ingest pipeline. | . | pipelines.id.processors | Array of objects | Statistics for the ingest processors. Includes the number of documents that are currently transformed, the total number of transformed documents, the number of failed transformations, and the time spent transforming documents. | . adaptive_selection . The adaptive_selection object contains the adaptive selection statistics. Each entry is specified by the node ID and has the properties listed below. | Field | Field type | Description | . | outgoing_searches | Integer | The number of outgoing search requests for the node. | . | avg_queue_size | Integer | The rolling average queue size of search requests for the node (exponentially weighted). | . | avg_service_time_ns | Integer | The rolling average service time for search requests, in nanoseconds (exponentially weighted). | . | avg_response_time_ns | Integer | The rolling average response time for search requests, in nanoseconds (exponentially weighted). | . | rank | String | The node’s rank that is used to select shards when routing requests. | . indexing_pressure . The indexing_pressure object contains the indexing pressure statistics and has the following properties. | Field | Field type | Description | . | memory | Object | Statistics related to memory consumption for the indexing load. | . | memory.current | Object | Statistics related to memory consumption for the current indexing load. | . | memory.current.combined_coordinating_and_primary_in_bytes | Integer | The total memory used by indexing requests in the coordinating or primary stages, in bytes. A node can reuse the coordinating memory if the primary stage is run locally, so the total memory does not necessarily equal the sum of the coordinating and primary stage memory usage. | . | memory.current.coordinating_in_bytes | The total memory consumed by indexing requests in the coordinating stage, in bytes. |   | . | memory.current.primary_in_bytes | Integer | The total memory consumed by indexing requests in the primary stage, in bytes. | . | memory.current.replica_in_bytes | Integer | The total memory consumed by indexing requests in the replica stage, in bytes. | . | memory.current.all_in_bytes | Integer | The total memory consumed by indexing requests in the coordinating, primary, or replica stages. | . shard_indexing_pressure . The shard_indexing_pressure object contains the shard indexing pressure statistics and has the following properties. | Field | Field type | Description | . | stats | Object | Statistics about shard indexing pressure. | . | total_rejections_breakup_shadow_mode | Object | If running in shadow mode, the total_rejections_breakup_shadow_mode object contains statistics about the request rejection criteria of all shards in the node. | . | total_rejections_breakup_shadow_mode.node_limits | Integer | The total number of rejections due to the node memory limit. When all shards reach the memory limit assigned to the node (for example, 10% of heap size), the shard is unable to take in more traffic on the node, and the indexing request is rejected. | . | total_rejections_breakup_shadow_mode.no_successful_request_limits | Integer | The total number of rejections when the node occupancy level is breaching its soft limit and the shard has multiple outstanding requests that are waiting to be executed. In this case, additional indexing requests are rejected until the system recovers. | . | total_rejections_breakup_shadow_mode.throughput_degradation_limits | Integer | The total number of rejections when the node occupancy level is breaching its soft limit and there is a constant deterioration in the request turnaround at the shard level. In this case, additional indexing requests are rejected until the system recovers. | . | enabled | Boolean | Specifies whether the shard indexing pressure feature is turned on for the node. | . | enforced | Boolean | If true, the shard indexing pressure runs in enforced mode (there are rejections). If false, the shard indexing pressure runs in shadow mode (there are no rejections, but statistics are recorded and can be retrieved in the total_rejections_breakup_shadow_mode object). Only applicable if shard indexing pressure is enabled. | . ",
    "url": "https://vagimeli.github.io/api-reference/nodes-apis/nodes-stats/#response-fields",
    "relUrl": "/api-reference/nodes-apis/nodes-stats/#response-fields"
  },"2329": {
    "doc": "Nodes stats",
    "title": "Required permissions",
    "content": "If you use the Security plugin, make sure you have the appropriate permissions: cluster:monitor/nodes/stats. ",
    "url": "https://vagimeli.github.io/api-reference/nodes-apis/nodes-stats/#required-permissions",
    "relUrl": "/api-reference/nodes-apis/nodes-stats/#required-permissions"
  },"2330": {
    "doc": "Nodes usage",
    "title": "Nodes usage",
    "content": "The nodes usage endpoint returns low-level information about REST action usage on nodes. ",
    "url": "https://vagimeli.github.io/api-reference/nodes-apis/nodes-usage/",
    "relUrl": "/api-reference/nodes-apis/nodes-usage/"
  },"2331": {
    "doc": "Nodes usage",
    "title": "Path and HTTP methods",
    "content": "GET _nodes/usage GET _nodes/&lt;nodeId&gt;/usage GET _nodes/usage/&lt;metric&gt; GET _nodes/&lt;nodeId&gt;/usage/&lt;metric&gt; . ",
    "url": "https://vagimeli.github.io/api-reference/nodes-apis/nodes-usage/#path-and-http-methods",
    "relUrl": "/api-reference/nodes-apis/nodes-usage/#path-and-http-methods"
  },"2332": {
    "doc": "Nodes usage",
    "title": "Path parameters",
    "content": "You can include the following optional path parameters in your request. | Parameter | Type | Description | . | nodeId | String | A comma-separated list of nodeIds used to filter results. Supports node filters. Defaults to _all. | . | metric | String | The metrics that will be included in the response. You can set the string to either _all or rest_actions. rest_actions returns the total number of times an action has been called on the node. _all returns all stats from the node. Defaults to _all. | . ",
    "url": "https://vagimeli.github.io/api-reference/nodes-apis/nodes-usage/#path-parameters",
    "relUrl": "/api-reference/nodes-apis/nodes-usage/#path-parameters"
  },"2333": {
    "doc": "Nodes usage",
    "title": "Query parameters",
    "content": "You can include the following optional query parameters in your request. | Parameter | Type | Description | . | timeout | Time | Sets the time limit for a response from the node. Default is 30s. | . | cluster_manager_timeout | Time | Sets the time limit for a response from the cluster manager. Default is 30s. | . Example request . The following request returns usage details for all nodes: . GET _nodes/usage . copy . Example response . The following is an example response: . { \"_nodes\" : { \"total\" : 1, \"successful\" : 1, \"failed\" : 0 }, \"cluster_name\" : \"opensearch-cluster\", \"nodes\" : { \"t7uqHu4SSuWObK3ElkCRfw\" : { \"timestamp\" : 1665695174312, \"since\" : 1663994849643, \"rest_actions\" : { \"opendistro_get_rollup_action\" : 3, \"nodes_usage_action\" : 1, \"list_dangling_indices\" : 1, \"get_index_template_action\" : 258, \"nodes_info_action\" : 152665, \"get_mapping_action\" : 259, \"get_data_streams_action\" : 12, \"cat_indices_action\" : 6, \"get_indices_action\" : 3, \"ism_explain_action\" : 7, \"nodes_reload_action\" : 1, \"get_policy_action\" : 3, \"PerformanceAnalyzerClusterConfigAction\" : 2, \"index_policy_action\" : 1, \"rank_eval_action\" : 3, \"search_action\" : 592, \"get_aliases_action\" : 258, \"document_mget_action\" : 2, \"document_get_action\" : 30, \"count_action\" : 1, \"main_action\" : 1 }, \"aggregations\" : { } } } } . ",
    "url": "https://vagimeli.github.io/api-reference/nodes-apis/nodes-usage/#query-parameters",
    "relUrl": "/api-reference/nodes-apis/nodes-usage/#query-parameters"
  },"2334": {
    "doc": "Nodes usage",
    "title": "Required permissions",
    "content": "If you use the Security plugin, make sure you set the following permissions: cluster:manage/nodes or cluster:monitor/nodes. ",
    "url": "https://vagimeli.github.io/api-reference/nodes-apis/nodes-usage/#required-permissions",
    "relUrl": "/api-reference/nodes-apis/nodes-usage/#required-permissions"
  },"2335": {
    "doc": "Popular APIs",
    "title": "Popular APIs",
    "content": "This page contains example requests for popular OpenSearch operations. . | Create index with non-default settings | Index a document with a random ID | Index a document with a specific ID | Index several documents at once | List all indices | Open or close all indices that match a pattern | Delete all indices that match a pattern | Create an index alias | List all aliases | Search an index or all indices that match a pattern | Get cluster settings, including defaults | Change disk watermarks (or other cluster settings) | Get cluster health | List nodes in the cluster | Get node statistics | Get snapshots in a repository | Take a snapshot | Restore a snapshot | . ",
    "url": "https://vagimeli.github.io/api-reference/popular-api/",
    "relUrl": "/api-reference/popular-api/"
  },"2336": {
    "doc": "Popular APIs",
    "title": "Create index with non-default settings",
    "content": "PUT my-logs { \"settings\": { \"number_of_shards\": 4, \"number_of_replicas\": 2 }, \"mappings\": { \"properties\": { \"title\": { \"type\": \"text\" }, \"year\": { \"type\": \"integer\" } } } } . ",
    "url": "https://vagimeli.github.io/api-reference/popular-api/#create-index-with-non-default-settings",
    "relUrl": "/api-reference/popular-api/#create-index-with-non-default-settings"
  },"2337": {
    "doc": "Popular APIs",
    "title": "Index a document with a random ID",
    "content": "POST my-logs/_doc { \"title\": \"Your Name\", \"year\": \"2016\" } . ",
    "url": "https://vagimeli.github.io/api-reference/popular-api/#index-a-document-with-a-random-id",
    "relUrl": "/api-reference/popular-api/#index-a-document-with-a-random-id"
  },"2338": {
    "doc": "Popular APIs",
    "title": "Index a document with a specific ID",
    "content": "PUT my-logs/_doc/1 { \"title\": \"Weathering with You\", \"year\": \"2019\" } . ",
    "url": "https://vagimeli.github.io/api-reference/popular-api/#index-a-document-with-a-specific-id",
    "relUrl": "/api-reference/popular-api/#index-a-document-with-a-specific-id"
  },"2339": {
    "doc": "Popular APIs",
    "title": "Index several documents at once",
    "content": "The blank line at the end of the request body is required. If you omit the _id field, OpenSearch generates a random ID. POST _bulk { \"index\": { \"_index\": \"my-logs\", \"_id\": \"2\" } } { \"title\": \"The Garden of Words\", \"year\": 2013 } { \"index\" : { \"_index\": \"my-logs\", \"_id\" : \"3\" } } { \"title\": \"5 Centimeters Per Second\", \"year\": 2007 } . ",
    "url": "https://vagimeli.github.io/api-reference/popular-api/#index-several-documents-at-once",
    "relUrl": "/api-reference/popular-api/#index-several-documents-at-once"
  },"2340": {
    "doc": "Popular APIs",
    "title": "List all indices",
    "content": "GET _cat/indices?v&amp;expand_wildcards=all . ",
    "url": "https://vagimeli.github.io/api-reference/popular-api/#list-all-indices",
    "relUrl": "/api-reference/popular-api/#list-all-indices"
  },"2341": {
    "doc": "Popular APIs",
    "title": "Open or close all indices that match a pattern",
    "content": "POST my-logs*/_open POST my-logs*/_close . ",
    "url": "https://vagimeli.github.io/api-reference/popular-api/#open-or-close-all-indices-that-match-a-pattern",
    "relUrl": "/api-reference/popular-api/#open-or-close-all-indices-that-match-a-pattern"
  },"2342": {
    "doc": "Popular APIs",
    "title": "Delete all indices that match a pattern",
    "content": "DELETE my-logs* . ",
    "url": "https://vagimeli.github.io/api-reference/popular-api/#delete-all-indices-that-match-a-pattern",
    "relUrl": "/api-reference/popular-api/#delete-all-indices-that-match-a-pattern"
  },"2343": {
    "doc": "Popular APIs",
    "title": "Create an index alias",
    "content": "This request creates the alias my-logs-today for the index my-logs-2019-11-13. PUT my-logs-2019-11-13/_alias/my-logs-today . ",
    "url": "https://vagimeli.github.io/api-reference/popular-api/#create-an-index-alias",
    "relUrl": "/api-reference/popular-api/#create-an-index-alias"
  },"2344": {
    "doc": "Popular APIs",
    "title": "List all aliases",
    "content": "GET _cat/aliases?v . ",
    "url": "https://vagimeli.github.io/api-reference/popular-api/#list-all-aliases",
    "relUrl": "/api-reference/popular-api/#list-all-aliases"
  },"2345": {
    "doc": "Popular APIs",
    "title": "Search an index or all indices that match a pattern",
    "content": "GET my-logs/_search?q=test GET my-logs*/_search?q=test . ",
    "url": "https://vagimeli.github.io/api-reference/popular-api/#search-an-index-or-all-indices-that-match-a-pattern",
    "relUrl": "/api-reference/popular-api/#search-an-index-or-all-indices-that-match-a-pattern"
  },"2346": {
    "doc": "Popular APIs",
    "title": "Get cluster settings, including defaults",
    "content": "GET _cluster/settings?include_defaults=true . ",
    "url": "https://vagimeli.github.io/api-reference/popular-api/#get-cluster-settings-including-defaults",
    "relUrl": "/api-reference/popular-api/#get-cluster-settings-including-defaults"
  },"2347": {
    "doc": "Popular APIs",
    "title": "Change disk watermarks (or other cluster settings)",
    "content": "PUT _cluster/settings { \"transient\": { \"cluster.routing.allocation.disk.watermark.low\": \"80%\", \"cluster.routing.allocation.disk.watermark.high\": \"85%\" } } . ",
    "url": "https://vagimeli.github.io/api-reference/popular-api/#change-disk-watermarks-or-other-cluster-settings",
    "relUrl": "/api-reference/popular-api/#change-disk-watermarks-or-other-cluster-settings"
  },"2348": {
    "doc": "Popular APIs",
    "title": "Get cluster health",
    "content": "GET _cluster/health . ",
    "url": "https://vagimeli.github.io/api-reference/popular-api/#get-cluster-health",
    "relUrl": "/api-reference/popular-api/#get-cluster-health"
  },"2349": {
    "doc": "Popular APIs",
    "title": "List nodes in the cluster",
    "content": "GET _cat/nodes?v . ",
    "url": "https://vagimeli.github.io/api-reference/popular-api/#list-nodes-in-the-cluster",
    "relUrl": "/api-reference/popular-api/#list-nodes-in-the-cluster"
  },"2350": {
    "doc": "Popular APIs",
    "title": "Get node statistics",
    "content": "GET _nodes/stats . ",
    "url": "https://vagimeli.github.io/api-reference/popular-api/#get-node-statistics",
    "relUrl": "/api-reference/popular-api/#get-node-statistics"
  },"2351": {
    "doc": "Popular APIs",
    "title": "Get snapshots in a repository",
    "content": "GET _snapshot/my-repository/_all . ",
    "url": "https://vagimeli.github.io/api-reference/popular-api/#get-snapshots-in-a-repository",
    "relUrl": "/api-reference/popular-api/#get-snapshots-in-a-repository"
  },"2352": {
    "doc": "Popular APIs",
    "title": "Take a snapshot",
    "content": "PUT _snapshot/my-repository/my-snapshot . ",
    "url": "https://vagimeli.github.io/api-reference/popular-api/#take-a-snapshot",
    "relUrl": "/api-reference/popular-api/#take-a-snapshot"
  },"2353": {
    "doc": "Popular APIs",
    "title": "Restore a snapshot",
    "content": "POST _snapshot/my-repository/my-snapshot/_restore { \"indices\": \"-.opendistro_security\", \"include_global_state\": false } . ",
    "url": "https://vagimeli.github.io/api-reference/popular-api/#restore-a-snapshot",
    "relUrl": "/api-reference/popular-api/#restore-a-snapshot"
  },"2354": {
    "doc": "Ranking evaluation",
    "title": "Ranking evaluation",
    "content": "The rank eval endpoint allows you to evaluate the quality of ranked search results. ",
    "url": "https://vagimeli.github.io/api-reference/rank-eval/",
    "relUrl": "/api-reference/rank-eval/"
  },"2355": {
    "doc": "Ranking evaluation",
    "title": "Path and HTTP methods",
    "content": "GET &lt;index_name&gt;/_rank_eval POST &lt;index_name&gt;/_rank_eval . ",
    "url": "https://vagimeli.github.io/api-reference/rank-eval/#path-and-http-methods",
    "relUrl": "/api-reference/rank-eval/#path-and-http-methods"
  },"2356": {
    "doc": "Ranking evaluation",
    "title": "Query parameters",
    "content": "Query parameters are optional. | Parameter | Data type | Description | . | ignore_unavailable | Boolean | Defaults to false. When set to false the response body will return an error if an index is closed or missing. | . | allow_no_indices | Boolean | Defaults to true. When set to false the response body will return an error if a wildcard expression points to indexes that are closed or missing. | . | expand_wildcards | String | Expand wildcard expressions for indexes that are open, closed, hidden, none, or all. | . | search_type | String | Set search type to either query_then_fetch or dfs_query_then_fetch. | . ",
    "url": "https://vagimeli.github.io/api-reference/rank-eval/#query-parameters",
    "relUrl": "/api-reference/rank-eval/#query-parameters"
  },"2357": {
    "doc": "Ranking evaluation",
    "title": "Request fields",
    "content": "The request body must contain at least one parameter. | Field Type | Description | . | id | Document or template ID. | . | requests | Set multiple search requests within the request field section. | . | ratings | Document relevance score. | . | k | The number of documents returned per query. Default is set to 10. | . | relevant_rating_threshold | The threshold at which documents are considered relevant. Default is set to 1. | . | normalize | Discounted cumulative gain will be calculated when set to true. | . | maximum_relevance | Sets the maximum relevance score when using the expected reciprocal rank metric. | . | ignore_unlabeled | Defaults to false. Unlabeled documents are ignored when set to true. | . | template_id | Template ID. | . | params | Parameters used in the template. | . Example request . GET shakespeare/_rank_eval { \"requests\": [ { \"id\": \"books_query\", \"request\": { \"query\": { \"match\": { \"text\": \"thou\" } } }, \"ratings\": [ { \"_index\": \"shakespeare\", \"_id\": \"80\", \"rating\": 0 }, { \"_index\": \"shakespeare\", \"_id\": \"115\", \"rating\": 1 }, { \"_index\": \"shakespeare\", \"_id\": \"117\", \"rating\": 2 } ] }, { \"id\": \"words_query\", \"request\": { \"query\": { \"match\": { \"text\": \"art\" } } }, \"ratings\": [ { \"_index\": \"shakespeare\", \"_id\": \"115\", \"rating\": 2 } ] } ] } . copy . Example response . { \"rank_eval\": { \"metric_score\": 0.7, \"details\": { \"query_1\": { \"metric_score\": 0.9, \"unrated_docs\": [ { \"_index\": \"shakespeare\", \"_id\": \"1234567\" }, ... ], \"hits\": [ { \"hit\": { \"_index\": \"shakespeare\", \"_type\": \"page\", \"_id\": \"1234567\", \"_score\": 5.123456789 }, \"rating\": 1 }, ... ], \"metric_details\": { \"precision\": { \"relevant_docs_retrieved\": 3, \"docs_retrieved\": 6 } } }, \"query_2\": { [... ] } }, \"failures\": { [... ] } } } . ",
    "url": "https://vagimeli.github.io/api-reference/rank-eval/#request-fields",
    "relUrl": "/api-reference/rank-eval/#request-fields"
  },"2358": {
    "doc": "Reload search analyzer",
    "title": "Reload search analyzer",
    "content": "The reload search analyzer API operation detects any changes to synonym files for any configured search analyzers. The reload search analyzer request needs to be run on all nodes. Additionally, the synonym token filter must be set to true. ",
    "url": "https://vagimeli.github.io/api-reference/reload-search-analyzer/",
    "relUrl": "/api-reference/reload-search-analyzer/"
  },"2359": {
    "doc": "Reload search analyzer",
    "title": "Path and HTTP methods",
    "content": "POST /&lt;index&gt;/_reload_search_analyzers GET /&lt;index&gt;/_reload_search_analyzers . ",
    "url": "https://vagimeli.github.io/api-reference/reload-search-analyzer/#path-and-http-methods",
    "relUrl": "/api-reference/reload-search-analyzer/#path-and-http-methods"
  },"2360": {
    "doc": "Reload search analyzer",
    "title": "Request body fields",
    "content": "Request body parameters are optional. | Field Type | Data type | Description | . | allow_no_indices | Boolean | When set to false, an error is returned for indexes that are closed or missing and match any wildcard expression. Default is set to true. | . | expand_wildcards | String | Allows you to set the wildcards that can be matched to a type of index. Available options are open, closed, all, none, and hidden. Default is set to open. | . | ignore_unavailable | Boolean | If an index is closed or missing, an error is returned when ignore_unavailable is set to false. Default is set to false. | . ",
    "url": "https://vagimeli.github.io/api-reference/reload-search-analyzer/#request-body-fields",
    "relUrl": "/api-reference/reload-search-analyzer/#request-body-fields"
  },"2361": {
    "doc": "Reload search analyzer",
    "title": "Examples",
    "content": "The following are an example request and response. Example request . POST /shakespeare/_reload_search_analyzers . copy . Example response . { \"_shards\": { \"total\": 1, \"successful\": 1, \"failed\": 0 }, \"reload_details\": [ { \"index\": \"shakespeare\", \"reloaded_analyzers\": [ \"analyzers-synonyms-test\" ], \"reloaded_node_ids\": [ \"opensearch-node1\" ] } ] } . ",
    "url": "https://vagimeli.github.io/api-reference/reload-search-analyzer/#examples",
    "relUrl": "/api-reference/reload-search-analyzer/#examples"
  },"2362": {
    "doc": "Remote cluster information",
    "title": "Remote cluster information",
    "content": "Introduced 1.0 . This operation provides connection information for any remote OpenSearch clusters that you’ve configured for the local cluster, such as the remote cluster alias, connection mode (sniff or proxy), IP addresses for seed nodes, and timeout settings. The response is more comprehensive and useful than a call to _cluster/settings, which only includes the cluster alias and seed nodes. ",
    "url": "https://vagimeli.github.io/api-reference/remote-info/",
    "relUrl": "/api-reference/remote-info/"
  },"2363": {
    "doc": "Remote cluster information",
    "title": "Path and HTTP methods",
    "content": "GET _remote/info . copy . ",
    "url": "https://vagimeli.github.io/api-reference/remote-info/#path-and-http-methods",
    "relUrl": "/api-reference/remote-info/#path-and-http-methods"
  },"2364": {
    "doc": "Remote cluster information",
    "title": "Response",
    "content": "{ \"opensearch-cluster2\": { \"connected\": true, \"mode\": \"sniff\", \"seeds\": [ \"172.28.0.2:9300\" ], \"num_nodes_connected\": 1, \"max_connections_per_cluster\": 3, \"initial_connect_timeout\": \"30s\", \"skip_unavailable\": false } } . ",
    "url": "https://vagimeli.github.io/api-reference/remote-info/#response",
    "relUrl": "/api-reference/remote-info/#response"
  },"2365": {
    "doc": "Create or Update Stored Script",
    "title": "Create or update stored script",
    "content": "Creates or updates a stored script or search template. For additional information about Painless scripting, see: . | k-NN Painless Scripting extensions. | k-NN. | . Path parameters . | Parameter | Data type | Description | . | script-id | String | Stored script or search template ID. Must be unique across the cluster. Required. | . Query parameters . All parameters are optional. | Parameter | Data type | Description | . | context | String | Context in which the script or search template is to run. To prevent errors, the API immediately compiles the script or template in this context. | . | cluster_manager_timeout | Time | Amount of time to wait for a connection to the cluster manager. Defaults to 30 seconds. | . | timeout | Time | The period of time to wait for a response. If a response is not received before the timeout value, the request fails and returns an error. Defaults to 30 seconds. | . Request fields . | Field | Data type | Description | . | script | Object | Defines the script or search template, its parameters, and its language. See Script object below. | . Script object . | Field | Data type | Description | . | lang | String | Scripting language. Required. | . | source | String or Object | Required. For scripts, a string with the contents of the script. For search templates, an object that defines the search template. Supports the same parameters as the Search API request body. Search templates also support Mustache variables. | . Example request . The sample uses an index called books with the following documents: . {\"index\":{\"_id\":1}} {\"name\":\"book1\",\"author\":\"Faustine\",\"ratings\":[4,3,5]} {\"index\":{\"_id\":2}} {\"name\":\"book2\",\"author\":\"Amit\",\"ratings\":[5,5,5]} {\"index\":{\"_id\":3}} {\"name\":\"book3\",\"author\":\"Gilroy\",\"ratings\":[2,1,5]} . The following request creates the Painless script my-first-script. It sums the ratings for each book and displays the sum in the output. PUT _scripts/my-first-script { \"script\": { \"lang\": \"painless\", \"source\": \"\"\" int total = 0; for (int i = 0; i &lt; doc['ratings'].length; ++i) { total += doc['ratings'][i]; } return total; \"\"\" } } . copy . The example above uses the syntax of the Dev Tools console in OpenSearch Dashboards. You can also use a curl request. The following curl request is equivalent to the previous Dashboards console example: . curl -XPUT \"http://opensearch:9200/_scripts/my-first-script\" -H 'Content-Type: application/json' -d' { \"script\": { \"lang\": \"painless\", \"source\": \"\\n int total = 0;\\n for (int i = 0; i &lt; doc['\\''ratings'\\''].length; ++i) {\\n total += doc['\\''ratings'\\''][i];\\n }\\n return total;\\n \" } }' . copy . The following request creates the Painless script my-first-script, which sums the ratings for each book and displays the sum in the output: . PUT _scripts/my-first-script { \"script\": { \"lang\": \"painless\", \"source\": \"\"\" int total = 0; for (int i = 0; i &lt; doc['ratings'].length; ++i) { total += doc['ratings'][i]; } return total; \"\"\" } } . copy . See Execute Painless stored script for information about running the script. Example response . The PUT _scripts/my-first-script request returns the following field: . { \"acknowledged\" : true } . To determine whether the script was successfully created, use the Get stored script API, passing the script name as the script path parameter. Response fields . | Field | Data type | Description | . | acknowledged | Boolean | Whether the request was received. | . ",
    "url": "https://vagimeli.github.io/api-reference/script-apis/create-stored-script/#create-or-update-stored-script",
    "relUrl": "/api-reference/script-apis/create-stored-script/#create-or-update-stored-script"
  },"2366": {
    "doc": "Create or Update Stored Script",
    "title": "Creating or updating a stored script with parameters",
    "content": "The Painless script supports params to pass variables to the script. Example . The following request creates the Painless script multiplier-script. The request sums the ratings for each book, multiplies the summed value by the multiplier parameter, and displays the result in the output: . PUT _scripts/multiplier-script { \"script\": { \"lang\": \"painless\", \"source\": \"\"\" int total = 0; for (int i = 0; i &lt; doc['ratings'].length; ++i) { total += doc['ratings'][i]; } return total * params['multiplier']; \"\"\" } } . copy . Example response . The PUT _scripts/multiplier-script request returns the following field: . { \"acknowledged\" : true } . ",
    "url": "https://vagimeli.github.io/api-reference/script-apis/create-stored-script/#creating-or-updating-a-stored-script-with-parameters",
    "relUrl": "/api-reference/script-apis/create-stored-script/#creating-or-updating-a-stored-script-with-parameters"
  },"2367": {
    "doc": "Create or Update Stored Script",
    "title": "Create or Update Stored Script",
    "content": " ",
    "url": "https://vagimeli.github.io/api-reference/script-apis/create-stored-script/",
    "relUrl": "/api-reference/script-apis/create-stored-script/"
  },"2368": {
    "doc": "Delete Script",
    "title": "Delete script",
    "content": "Deletes a stored script . Path parameters . Path parameters are optional. | Parameter | Data type | Description | . | script-id | String | ID of script to delete. | . Query parameters . | Parameter | Data type | Description | . | cluster_manager_timeout | Time | Amount of time to wait for a connection to the cluster manager. Optional, defaults to 30s. | . | timeout | Time | The period of time to wait for a response. If a response is not received before the timeout value, the request will be dropped. | . Example request . The following request deletes the my-first-script script: . DELETE _scripts/my-script . copy . Example response . The DELETE _scripts/my-first-script request returns the following field: . { \"acknowledged\" : true } . To determine whether the stored script was successfully deleted, use the Get stored script API, passing the script name as the script path parameter. Response fields . The request returns the following response fields: . | Field | Data type | Description | . | acknowledged | Boolean | Whether the delete script request was received. | . ",
    "url": "https://vagimeli.github.io/api-reference/script-apis/delete-script/#delete-script",
    "relUrl": "/api-reference/script-apis/delete-script/#delete-script"
  },"2369": {
    "doc": "Delete Script",
    "title": "Delete Script",
    "content": " ",
    "url": "https://vagimeli.github.io/api-reference/script-apis/delete-script/",
    "relUrl": "/api-reference/script-apis/delete-script/"
  },"2370": {
    "doc": "Execute Painless script",
    "title": "Execute Painless script",
    "content": "The Execute Painless script API allows you to run a script that is not stored. Path and HTTP methods . GET /_scripts/painless/_execute POST /_scripts/painless/_execute . Request fields . | Field | Description | . | script | The script to run. Required | . | context | A context for the script. Optional. Default is painless_test. | . | context_setup | Specifies additional parameters for the context. Optional. | . Example request . The following request uses the default painless_context for the script: . GET /_scripts/painless/_execute { \"script\": { \"source\": \"(params.x + params.y)/ 2\", \"params\": { \"x\": 80, \"y\": 100 } } } . copy . Example response . The response contains the average of two script parameters: . { \"result\" : \"90\" } . Response fields . | Field | Description | . | result | The script result. | . ",
    "url": "https://vagimeli.github.io/api-reference/script-apis/exec-script/",
    "relUrl": "/api-reference/script-apis/exec-script/"
  },"2371": {
    "doc": "Execute Painless script",
    "title": "Script contexts",
    "content": "Choose different contexts to control the variables that are available to the script and the result’s return type. The default context is painless_test. Painless test context . The painless_test context is the default script context that provides only the params variable to the script. The returned result is always converted to a string. See the preceding example request for a usage example. Filter context . The filter context runs the script as if the script were inside a script query. You must provide a test document in the context. The _source, stored fields, and _doc variables will be available to the script. You can specify the following parameters for the filter context in the context_setup. | Parameter | Description | . | document | The document that is indexed in memory temporarily and available to the script. | . | index | The name of the index that contains a mapping for the document. | . For example, first create an index with a mapping for a test document: . PUT /testindex1 { \"mappings\": { \"properties\": { \"grad\": { \"type\": \"boolean\" }, \"gpa\": { \"type\": \"float\" } } } } . copy . Run a script to determine if a student is eligible to graduate with honors: . POST /_scripts/painless/_execute { \"script\": { \"source\": \"doc['grad'].value == true &amp;&amp; doc['gpa'].value &gt;= params.min_honors_gpa\", \"params\": { \"min_honors_gpa\": 3.5 } }, \"context\": \"filter\", \"context_setup\": { \"index\": \"testindex1\", \"document\": { \"grad\": true, \"gpa\": 3.79 } } } . copy . The response contains the result: . { \"result\" : true } . Score context . The score context runs a script as if the script were in a script_score function in a function_score query. You can specify the following parameters for the score context in the context_setup. | Parameter | Description | . | document | The document that is indexed in memory temporarily and available to the script. | . | index | The name of the index that contains a mapping for the document. | . | query | If the script uses the _score parameter, the query can specify to use the _score field to compute the score. | . For example, first create an index with a mapping for a test document: . PUT /testindex1 { \"mappings\": { \"properties\": { \"gpa_4_0\": { \"type\": \"float\" } } } } . copy . Run a script that converts a GPA on a 4.0 scale into a different scale that is provided as a parameter: . POST /_scripts/painless/_execute { \"script\": { \"source\": \"doc['gpa_4_0'].value * params.max_gpa / 4.0\", \"params\": { \"max_gpa\": 5.0 } }, \"context\": \"score\", \"context_setup\": { \"index\": \"testindex1\", \"document\": { \"gpa_4_0\": 3.5 } } } . copy . The response contains the result: . { \"result\" : 4.375 } . ",
    "url": "https://vagimeli.github.io/api-reference/script-apis/exec-script/#script-contexts",
    "relUrl": "/api-reference/script-apis/exec-script/#script-contexts"
  },"2372": {
    "doc": "Execute Painless stored script",
    "title": "Execute Painless stored script",
    "content": "Runs a stored script written in the Painless language. OpenSearch provides several ways to run a script; the following sections show how to run a script by passing script information in the request body of a GET &lt;index&gt;/_search request. Request fields . | Field | Data type | Description | . | query | Object | A filter that specifies documents to process. | . | script_fields | Object | Fields to include in output. | . | script | Object | ID of the script that produces a value for a field. | . Example request . The following request runs the stored script that was created in Create or update stored script. The script sums the ratings for each book and displays the sum in the total_ratings field in the output. | The script’s target is the books index. | The \"match_all\": {} property value is an empty object indicating to process each document in the index. | The total_ratings field value is the result of the my-first-script execution. See Create or update stored script. | . GET books/_search { \"query\": { \"match_all\": {} }, \"script_fields\": { \"total_ratings\": { \"script\": { \"id\": \"my-first-script\" } } } } . copy . Example response . The GET books/_search request returns the following fields: . { \"took\" : 2, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 3, \"relation\" : \"eq\" }, \"max_score\" : 1.0, \"hits\" : [ { \"_index\" : \"books\", \"_id\" : \"1\", \"_score\" : 1.0, \"fields\" : { \"total_ratings\" : [ 12 ] } }, { \"_index\" : \"books\", \"_id\" : \"2\", \"_score\" : 1.0, \"fields\" : { \"total_ratings\" : [ 15 ] } }, { \"_index\" : \"books\", \"_id\" : \"3\", \"_score\" : 1.0, \"fields\" : { \"total_ratings\" : [ 8 ] } } ] } } . Response fields . | Field | Data type | Description | . | took | Integer | How long the operation took in milliseconds. | . | timed_out | Boolean | Whether the operation timed out. | . | _shards | Object | Total number of shards processed and also the total number of successful, skipped, and not processed. | . | hits | Object | Contains high-level information about the documents processed and an array of hits objects. See Hits object. | . Hits object . | Field | Data type | Description | . | total | Object | Total number of documents processed and their relationship to the match request field. | . | max_score | Double | Highest relevance score returned from all the hits. | . | hits | Array | Information about each document that was processed. See Document object. | . Document object . | Field | Data type | Description | . | _index | String | Index that contains the document. | . | _id | String | Document ID. | . | _score | Float | Document’s relevance score. | . | fields | Object | Fields and their value returned from the script. | . ",
    "url": "https://vagimeli.github.io/api-reference/script-apis/exec-stored-script/",
    "relUrl": "/api-reference/script-apis/exec-stored-script/"
  },"2373": {
    "doc": "Execute Painless stored script",
    "title": "Running a Painless stored script with parameters",
    "content": "To pass different parameters to the script each time when running a query, define params in script_fields. Example . The following request runs the stored script that was created in Create or update stored script. The script sums the ratings for each book, multiplies the summed value by the multiplier parameter, and displays the result in the output. | The script’s target is the books index. | The \"match_all\": {} property value is an empty object, indicating that it processes each document in the index. | The total_ratings field value is the result of the multiplier-script execution. See Creating or updating a stored script with parameters. | \"multiplier\": 2 in the params field is a variable passed to the stored script multiplier-script: . | . GET books/_search { \"query\": { \"match_all\": {} }, \"script_fields\": { \"total_ratings\": { \"script\": { \"id\": \"multiplier-script\", \"params\": { \"multiplier\": 2 } } } } } . copy . Example response . { \"took\" : 12, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 3, \"relation\" : \"eq\" }, \"max_score\" : 1.0, \"hits\" : [ { \"_index\" : \"books\", \"_type\" : \"_doc\", \"_id\" : \"3\", \"_score\" : 1.0, \"fields\" : { \"total_ratings\" : [ 16 ] } }, { \"_index\" : \"books\", \"_type\" : \"_doc\", \"_id\" : \"2\", \"_score\" : 1.0, \"fields\" : { \"total_ratings\" : [ 30 ] } }, { \"_index\" : \"books\", \"_type\" : \"_doc\", \"_id\" : \"1\", \"_score\" : 1.0, \"fields\" : { \"total_ratings\" : [ 24 ] } } ] } } . ",
    "url": "https://vagimeli.github.io/api-reference/script-apis/exec-stored-script/#running-a-painless-stored-script-with-parameters",
    "relUrl": "/api-reference/script-apis/exec-stored-script/#running-a-painless-stored-script-with-parameters"
  },"2374": {
    "doc": "Execute Painless stored script",
    "title": "Sort results using painless stored script",
    "content": "You can use painless stored script to sort results. Sample request . GET books/_search { \"query\": { \"match_all\": {} }, \"script_fields\": { \"total_ratings\": { \"script\": { \"id\": \"multiplier-script\", \"params\": { \"multiplier\": 2 } } } }, \"sort\": { \"_script\": { \"type\": \"number\", \"script\": { \"id\": \"multiplier-script\", \"params\": { \"multiplier\": 2 } }, \"order\": \"desc\" } } } . Sample response . { \"took\" : 90, \"timed_out\" : false, \"_shards\" : { \"total\" : 5, \"successful\" : 5, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 3, \"relation\" : \"eq\" }, \"max_score\" : null, \"hits\" : [ { \"_index\" : \"books\", \"_type\" : \"_doc\", \"_id\" : \"2\", \"_score\" : null, \"fields\" : { \"total_ratings\" : [ 30 ] }, \"sort\" : [ 30.0 ] }, { \"_index\" : \"books\", \"_type\" : \"_doc\", \"_id\" : \"1\", \"_score\" : null, \"fields\" : { \"total_ratings\" : [ 24 ] }, \"sort\" : [ 24.0 ] }, { \"_index\" : \"books\", \"_type\" : \"_doc\", \"_id\" : \"3\", \"_score\" : null, \"fields\" : { \"total_ratings\" : [ 16 ] }, \"sort\" : [ 16.0 ] } ] } } . ",
    "url": "https://vagimeli.github.io/api-reference/script-apis/exec-stored-script/#sort-results-using-painless-stored-script",
    "relUrl": "/api-reference/script-apis/exec-stored-script/#sort-results-using-painless-stored-script"
  },"2375": {
    "doc": "Get Stored Script Contexts",
    "title": "Get stored script contexts",
    "content": "Retrieves all contexts for stored scripts. Example request . GET _script_context . copy . Example response . The GET _script_context request returns the following fields: . { \"contexts\" : [ { \"name\" : \"aggregation_selector\", \"methods\" : [ { \"name\" : \"execute\", \"return_type\" : \"boolean\", \"params\" : [ ] }, { \"name\" : \"getParams\", \"return_type\" : \"java.util.Map\", \"params\" : [ ] } ] }, { \"name\" : \"aggs\", \"methods\" : [ { \"name\" : \"execute\", \"return_type\" : \"java.lang.Object\", \"params\" : [ ] }, { \"name\" : \"getDoc\", \"return_type\" : \"java.util.Map\", \"params\" : [ ] }, { \"name\" : \"getParams\", \"return_type\" : \"java.util.Map\", \"params\" : [ ] }, { \"name\" : \"get_score\", \"return_type\" : \"java.lang.Number\", \"params\" : [ ] }, { \"name\" : \"get_value\", \"return_type\" : \"java.lang.Object\", \"params\" : [ ] } ] }, { \"name\" : \"aggs_combine\", \"methods\" : [ { \"name\" : \"execute\", \"return_type\" : \"java.lang.Object\", \"params\" : [ ] }, { \"name\" : \"getParams\", \"return_type\" : \"java.util.Map\", \"params\" : [ ] }, { \"name\" : \"getState\", \"return_type\" : \"java.util.Map\", \"params\" : [ ] } ] }, { \"name\" : \"aggs_init\", \"methods\" : [ { \"name\" : \"execute\", \"return_type\" : \"void\", \"params\" : [ ] }, { \"name\" : \"getParams\", \"return_type\" : \"java.util.Map\", \"params\" : [ ] }, { \"name\" : \"getState\", \"return_type\" : \"java.lang.Object\", \"params\" : [ ] } ] }, { \"name\" : \"aggs_map\", \"methods\" : [ { \"name\" : \"execute\", \"return_type\" : \"void\", \"params\" : [ ] }, { \"name\" : \"getDoc\", \"return_type\" : \"java.util.Map\", \"params\" : [ ] }, { \"name\" : \"getParams\", \"return_type\" : \"java.util.Map\", \"params\" : [ ] }, { \"name\" : \"getState\", \"return_type\" : \"java.util.Map\", \"params\" : [ ] }, { \"name\" : \"get_score\", \"return_type\" : \"double\", \"params\" : [ ] } ] }, { \"name\" : \"aggs_reduce\", \"methods\" : [ { \"name\" : \"execute\", \"return_type\" : \"java.lang.Object\", \"params\" : [ ] }, { \"name\" : \"getParams\", \"return_type\" : \"java.util.Map\", \"params\" : [ ] }, { \"name\" : \"getStates\", \"return_type\" : \"java.util.List\", \"params\" : [ ] } ] }, { \"name\" : \"analysis\", \"methods\" : [ { \"name\" : \"execute\", \"return_type\" : \"boolean\", \"params\" : [ { \"type\" : \"org.opensearch.analysis.common.AnalysisPredicateScript$Token\", \"name\" : \"token\" } ] } ] }, { \"name\" : \"bucket_aggregation\", \"methods\" : [ { \"name\" : \"execute\", \"return_type\" : \"java.lang.Number\", \"params\" : [ ] }, { \"name\" : \"getParams\", \"return_type\" : \"java.util.Map\", \"params\" : [ ] } ] }, { \"name\" : \"field\", \"methods\" : [ { \"name\" : \"execute\", \"return_type\" : \"java.lang.Object\", \"params\" : [ ] }, { \"name\" : \"getDoc\", \"return_type\" : \"java.util.Map\", \"params\" : [ ] }, { \"name\" : \"getParams\", \"return_type\" : \"java.util.Map\", \"params\" : [ ] } ] }, { \"name\" : \"filter\", \"methods\" : [ { \"name\" : \"execute\", \"return_type\" : \"boolean\", \"params\" : [ ] }, { \"name\" : \"getDoc\", \"return_type\" : \"java.util.Map\", \"params\" : [ ] }, { \"name\" : \"getParams\", \"return_type\" : \"java.util.Map\", \"params\" : [ ] } ] }, { \"name\" : \"ingest\", \"methods\" : [ { \"name\" : \"execute\", \"return_type\" : \"void\", \"params\" : [ { \"type\" : \"java.util.Map\", \"name\" : \"ctx\" } ] }, { \"name\" : \"getParams\", \"return_type\" : \"java.util.Map\", \"params\" : [ ] } ] }, { \"name\" : \"interval\", \"methods\" : [ { \"name\" : \"execute\", \"return_type\" : \"boolean\", \"params\" : [ { \"type\" : \"org.opensearch.index.query.IntervalFilterScript$Interval\", \"name\" : \"interval\" } ] } ] }, { \"name\" : \"moving-function\", \"methods\" : [ { \"name\" : \"execute\", \"return_type\" : \"double\", \"params\" : [ { \"type\" : \"java.util.Map\", \"name\" : \"params\" }, { \"type\" : \"double[]\", \"name\" : \"values\" } ] } ] }, { \"name\" : \"number_sort\", \"methods\" : [ { \"name\" : \"execute\", \"return_type\" : \"double\", \"params\" : [ ] }, { \"name\" : \"getDoc\", \"return_type\" : \"java.util.Map\", \"params\" : [ ] }, { \"name\" : \"getParams\", \"return_type\" : \"java.util.Map\", \"params\" : [ ] }, { \"name\" : \"get_score\", \"return_type\" : \"double\", \"params\" : [ ] } ] }, { \"name\" : \"painless_test\", \"methods\" : [ { \"name\" : \"execute\", \"return_type\" : \"java.lang.Object\", \"params\" : [ ] }, { \"name\" : \"getParams\", \"return_type\" : \"java.util.Map\", \"params\" : [ ] } ] }, { \"name\" : \"processor_conditional\", \"methods\" : [ { \"name\" : \"execute\", \"return_type\" : \"boolean\", \"params\" : [ { \"type\" : \"java.util.Map\", \"name\" : \"ctx\" } ] }, { \"name\" : \"getParams\", \"return_type\" : \"java.util.Map\", \"params\" : [ ] } ] }, { \"name\" : \"score\", \"methods\" : [ { \"name\" : \"execute\", \"return_type\" : \"double\", \"params\" : [ { \"type\" : \"org.opensearch.script.ScoreScript$ExplanationHolder\", \"name\" : \"explanation\" } ] }, { \"name\" : \"getDoc\", \"return_type\" : \"java.util.Map\", \"params\" : [ ] }, { \"name\" : \"getParams\", \"return_type\" : \"java.util.Map\", \"params\" : [ ] }, { \"name\" : \"get_score\", \"return_type\" : \"double\", \"params\" : [ ] } ] }, { \"name\" : \"script_heuristic\", \"methods\" : [ { \"name\" : \"execute\", \"return_type\" : \"double\", \"params\" : [ { \"type\" : \"java.util.Map\", \"name\" : \"params\" } ] } ] }, { \"name\" : \"similarity\", \"methods\" : [ { \"name\" : \"execute\", \"return_type\" : \"double\", \"params\" : [ { \"type\" : \"double\", \"name\" : \"weight\" }, { \"type\" : \"org.opensearch.index.similarity.ScriptedSimilarity$Query\", \"name\" : \"query\" }, { \"type\" : \"org.opensearch.index.similarity.ScriptedSimilarity$Field\", \"name\" : \"field\" }, { \"type\" : \"org.opensearch.index.similarity.ScriptedSimilarity$Term\", \"name\" : \"term\" }, { \"type\" : \"org.opensearch.index.similarity.ScriptedSimilarity$Doc\", \"name\" : \"doc\" } ] } ] }, { \"name\" : \"similarity_weight\", \"methods\" : [ { \"name\" : \"execute\", \"return_type\" : \"double\", \"params\" : [ { \"type\" : \"org.opensearch.index.similarity.ScriptedSimilarity$Query\", \"name\" : \"query\" }, { \"type\" : \"org.opensearch.index.similarity.ScriptedSimilarity$Field\", \"name\" : \"field\" }, { \"type\" : \"org.opensearch.index.similarity.ScriptedSimilarity$Term\", \"name\" : \"term\" } ] } ] }, { \"name\" : \"string_sort\", \"methods\" : [ { \"name\" : \"execute\", \"return_type\" : \"java.lang.String\", \"params\" : [ ] }, { \"name\" : \"getDoc\", \"return_type\" : \"java.util.Map\", \"params\" : [ ] }, { \"name\" : \"getParams\", \"return_type\" : \"java.util.Map\", \"params\" : [ ] }, { \"name\" : \"get_score\", \"return_type\" : \"double\", \"params\" : [ ] } ] }, { \"name\" : \"template\", \"methods\" : [ { \"name\" : \"execute\", \"return_type\" : \"java.lang.String\", \"params\" : [ ] }, { \"name\" : \"getParams\", \"return_type\" : \"java.util.Map\", \"params\" : [ ] } ] }, { \"name\" : \"terms_set\", \"methods\" : [ { \"name\" : \"execute\", \"return_type\" : \"java.lang.Number\", \"params\" : [ ] }, { \"name\" : \"getDoc\", \"return_type\" : \"java.util.Map\", \"params\" : [ ] }, { \"name\" : \"getParams\", \"return_type\" : \"java.util.Map\", \"params\" : [ ] } ] }, { \"name\" : \"trigger\", \"methods\" : [ { \"name\" : \"execute\", \"return_type\" : \"boolean\", \"params\" : [ { \"type\" : \"org.opensearch.alerting.script.QueryLevelTriggerExecutionContext\", \"name\" : \"ctx\" } ] }, { \"name\" : \"getParams\", \"return_type\" : \"java.util.Map\", \"params\" : [ ] } ] }, { \"name\" : \"update\", \"methods\" : [ { \"name\" : \"execute\", \"return_type\" : \"void\", \"params\" : [ ] }, { \"name\" : \"getCtx\", \"return_type\" : \"java.util.Map\", \"params\" : [ ] }, { \"name\" : \"getParams\", \"return_type\" : \"java.util.Map\", \"params\" : [ ] } ] } ] } . Response fields . The GET _script_context request returns the following response fields: . | Field | Data type | Description | . | contexts | List | A list of all contexts. See Script object. | . Script context . | Field | Data type | Description | . | name | String | The context name. | . | methods | List | List of the context’s allowable methods. See Script object. | . Context methods . | Field | Data type | Description | . | name | String | Method name. | . | name | String | Type that the method returns (boolean, object, number, and so on). | . | params | List | List of the parameters accepted by the method. See Script object. | . Method parameters . | Field | Data type | Description | . | type | String | Parameter data type. | . | name | String | Parameter name. | . ",
    "url": "https://vagimeli.github.io/api-reference/script-apis/get-script-contexts/#get-stored-script-contexts",
    "relUrl": "/api-reference/script-apis/get-script-contexts/#get-stored-script-contexts"
  },"2376": {
    "doc": "Get Stored Script Contexts",
    "title": "Get Stored Script Contexts",
    "content": " ",
    "url": "https://vagimeli.github.io/api-reference/script-apis/get-script-contexts/",
    "relUrl": "/api-reference/script-apis/get-script-contexts/"
  },"2377": {
    "doc": "Get Script Language",
    "title": "Get script language",
    "content": "The get script language API operation retrieves all supported script languages and their contexts. Example request . GET _script_language . copy . Example response . The GET _script_language request returns the available contexts for each language: . { \"types_allowed\" : [ \"inline\", \"stored\" ], \"language_contexts\" : [ { \"language\" : \"expression\", \"contexts\" : [ \"aggregation_selector\", \"aggs\", \"bucket_aggregation\", \"field\", \"filter\", \"number_sort\", \"score\", \"terms_set\" ] }, { \"language\" : \"mustache\", \"contexts\" : [ \"template\" ] }, { \"language\" : \"opensearch_query_expression\", \"contexts\" : [ \"aggs\", \"filter\" ] }, { \"language\" : \"painless\", \"contexts\" : [ \"aggregation_selector\", \"aggs\", \"aggs_combine\", \"aggs_init\", \"aggs_map\", \"aggs_reduce\", \"analysis\", \"bucket_aggregation\", \"field\", \"filter\", \"ingest\", \"interval\", \"moving-function\", \"number_sort\", \"painless_test\", \"processor_conditional\", \"score\", \"script_heuristic\", \"similarity\", \"similarity_weight\", \"string_sort\", \"template\", \"terms_set\", \"trigger\", \"update\" ] } ] } . Response fields . The request contains the following response fields. | Field | Data type | Description | . | types_allowed | List of strings | The types of scripts that are enabled, determined by the script.allowed_types setting. May contain inline and/or stored. | . | language_contexts | List of objects | A list of objects, each of which maps a supported language to its available contexts. | . | language_contexts.language | String | The name of the registered scripting language. | . | language_contexts.contexts | List of strings | A list of all contexts for the language, determined by the script.allowed_contexts setting. | . ",
    "url": "https://vagimeli.github.io/api-reference/script-apis/get-script-language/#get-script-language",
    "relUrl": "/api-reference/script-apis/get-script-language/#get-script-language"
  },"2378": {
    "doc": "Get Script Language",
    "title": "Get Script Language",
    "content": " ",
    "url": "https://vagimeli.github.io/api-reference/script-apis/get-script-language/",
    "relUrl": "/api-reference/script-apis/get-script-language/"
  },"2379": {
    "doc": "Get Stored Script",
    "title": "Get stored script",
    "content": "Retrieves a stored script. Path parameters . | Parameter | Data type | Description | . | script | String | Stored script or search template name. Required. | . Query parameters . | Parameter | Data type | Description | . | cluster_manager_timeout | Time | Amount of time to wait for a connection to the cluster manager. Optional, defaults to 30s. | . Example request . The following retrieves the my-first-script stored script. GET _scripts/my-first-script . copy . Example response . The GET _scripts/my-first-script request returns the following fields: . { \"_id\" : \"my-first-script\", \"found\" : true, \"script\" : { \"lang\" : \"painless\", \"source\" : \"\"\" int total = 0; for (int i = 0; i &lt; doc['ratings'].length; ++i) { total += doc['ratings'][i]; } return total; \"\"\" } } . Response fields . The GET _scripts/my-first-script request returns the following response fields: . | Field | Data type | Description | . | _id | String | The script’s name. | . | found | Boolean | The requested script exists and was retrieved. | . | script | Object | The script definition. See Script object. | . Script object . | Field | Data type | Description | . | lang | String | The script’s language. | . | source | String | The script’s body. | . ",
    "url": "https://vagimeli.github.io/api-reference/script-apis/get-stored-script/#get-stored-script",
    "relUrl": "/api-reference/script-apis/get-stored-script/#get-stored-script"
  },"2380": {
    "doc": "Get Stored Script",
    "title": "Get Stored Script",
    "content": " ",
    "url": "https://vagimeli.github.io/api-reference/script-apis/get-stored-script/",
    "relUrl": "/api-reference/script-apis/get-stored-script/"
  },"2381": {
    "doc": "Script APIs",
    "title": "Script APIs",
    "content": "The script APIs allow you to work with stored scripts. Stored scripts are part of the cluster state and reduce compilation time and enhance search speed. The default scripting language is Painless. You can perform the following operations on stored scripts: . | Create or update stored script | Execute Painless stored script | Get stored script | Delete script | Get stored script contexts. | Get script language | . ",
    "url": "https://vagimeli.github.io/api-reference/script-apis/index/",
    "relUrl": "/api-reference/script-apis/index/"
  },"2382": {
    "doc": "Scroll",
    "title": "Scroll",
    "content": "Introduced 1.0 . You can use the scroll operation to retrieve a large number of results. For example, for machine learning jobs, you can request an unlimited number of results in batches. To use the scroll operation, add a scroll parameter to the request header with a search context to tell OpenSearch how long you need to keep scrolling. This search context needs to be long enough to process a single batch of results. Because search contexts consume a lot of memory, we suggest you don’t use the scroll operation for frequent user queries. Instead, use the sort parameter with the search_after parameter to scroll responses for user queries. ",
    "url": "https://vagimeli.github.io/api-reference/scroll/",
    "relUrl": "/api-reference/scroll/"
  },"2383": {
    "doc": "Scroll",
    "title": "Example",
    "content": "To set the number of results that you want returned for each batch, use the size parameter: . GET shakespeare/_search?scroll=10m { \"size\": 10000 } . copy . OpenSearch caches the results and returns a scroll ID to access them in batches: . \"_scroll_id\" : \"DXF1ZXJ5QW5kRmV0Y2gBAAAAAAAAAAUWdmpUZDhnRFBUcWFtV21nMmFwUGJEQQ==\" . Pass this scroll ID to the scroll operation to get back the next batch of results: . GET _search/scroll { \"scroll\": \"10m\", \"scroll_id\": \"DXF1ZXJ5QW5kRmV0Y2gBAAAAAAAAAAUWdmpUZDhnRFBUcWFtV21nMmFwUGJEQQ==\" } . copy . Using this scroll ID, you get results in batches of 10,000 as long as the search context is still open. Typically, the scroll ID does not change between requests, but it can change, so make sure to always use the latest scroll ID. If you don’t send the next scroll request within the set search context, the scroll operation does not return any results. If you expect billions of results, use a sliced scroll. Slicing allows you to perform multiple scroll operations for the same request, but in parallel. Set the ID and the maximum number of slices for the scroll: . GET shakespeare/_search?scroll=10m { \"slice\": { \"id\": 0, \"max\": 10 }, \"query\": { \"match_all\": {} } } . copy . With a single scroll ID, you get back 10 results. You can have up to 10 IDs. Close the search context when you’re done scrolling, because the scroll operation continues to consume computing resources until the timeout: . DELETE _search/scroll/DXF1ZXJ5QW5kRmV0Y2gBAAAAAAAAAAcWdmpUZDhnRFBUcWFtV21nMmFwUGJEQQ== . copy . To close all open scroll contexts: . DELETE _search/scroll/_all . copy . The scroll operation corresponds to a specific timestamp. It doesn’t consider documents added after that timestamp as potential results. ",
    "url": "https://vagimeli.github.io/api-reference/scroll/#example",
    "relUrl": "/api-reference/scroll/#example"
  },"2384": {
    "doc": "Scroll",
    "title": "Path and HTTP methods",
    "content": "GET _search/scroll POST _search/scroll . GET _search/scroll/&lt;scroll-id&gt; POST _search/scroll/&lt;scroll-id&gt; . ",
    "url": "https://vagimeli.github.io/api-reference/scroll/#path-and-http-methods",
    "relUrl": "/api-reference/scroll/#path-and-http-methods"
  },"2385": {
    "doc": "Scroll",
    "title": "URL parameters",
    "content": "All scroll parameters are optional. | Parameter | Type | Description | . | scroll | Time | Specifies the amount of time the search context is maintained. | . | scroll_id | String | The scroll ID for the search. | . | rest_total_hits_as_int | Boolean | Whether the hits.total property is returned as an integer (true) or an object (false). Default is false. | . ",
    "url": "https://vagimeli.github.io/api-reference/scroll/#url-parameters",
    "relUrl": "/api-reference/scroll/#url-parameters"
  },"2386": {
    "doc": "Scroll",
    "title": "Response",
    "content": "{ \"succeeded\": true, \"num_freed\": 1 } . ",
    "url": "https://vagimeli.github.io/api-reference/scroll/#response",
    "relUrl": "/api-reference/scroll/#response"
  },"2387": {
    "doc": "Search",
    "title": "Search",
    "content": "Introduced 1.0 . The Search API operation lets you execute a search request to search your cluster for data. ",
    "url": "https://vagimeli.github.io/api-reference/search/",
    "relUrl": "/api-reference/search/"
  },"2388": {
    "doc": "Search",
    "title": "Example",
    "content": "GET /movies/_search { \"query\": { \"match\": { \"text_entry\": \"I am the night\" } } } . copy . ",
    "url": "https://vagimeli.github.io/api-reference/search/#example",
    "relUrl": "/api-reference/search/#example"
  },"2389": {
    "doc": "Search",
    "title": "Path and HTTP Methods",
    "content": "GET /&lt;target-index&gt;/_search GET /_search POST /&lt;target-index&gt;/_search POST /_search . ",
    "url": "https://vagimeli.github.io/api-reference/search/#path-and-http-methods",
    "relUrl": "/api-reference/search/#path-and-http-methods"
  },"2390": {
    "doc": "Search",
    "title": "URL Parameters",
    "content": "All URL parameters are optional. | Parameter | Type | Description | . | allow_no_indices | Boolean | Whether to ignore wildcards that don’t match any indexes. Default is true. | . | allow_partial_search_results | Boolean | Whether to return partial results if the request runs into an error or times out. Default is true. | . | analyzer | String | Analyzer to use in the query string. | . | analyze_wildcard | Boolean | Whether the update operation should include wildcard and prefix queries in the analysis. Default is false. | . | batched_reduce_size | Integer | How many shard results to reduce on a node. Default is 512. | . | cancel_after_time_interval | Time | The time after which the search request will be canceled. Request-level parameter takes precedence over cancel_after_time_interval cluster setting. Default is -1. | . | ccs_minimize_roundtrips | Boolean | Whether to minimize roundtrips between a node and remote clusters. Default is true. | . | default_operator | String | Indicates whether the default operator for a string query should be AND or OR. Default is OR. | . | df | String | The default field in case a field prefix is not provided in the query string. | . | docvalue_fields | String | The fields that OpenSearch should return using their docvalue forms. | . | expand_wildcards | String | Specifies the type of index that wildcard expressions can match. Supports comma-separated values. Valid values are all (match any index), open (match open, non-hidden indexes), closed (match closed, non-hidden indexes), hidden (match hidden indexes), and none (deny wildcard expressions). Default is open. | . | explain | Boolean | Whether to return details about how OpenSearch computed the document’s score. Default is false. | . | from | Integer | The starting index to search from. Default is 0. | . | ignore_throttled | Boolean | Whether to ignore concrete, expanded, or indexes with aliases if indexes are frozen. Default is true. | . | ignore_unavailable | Boolean | Specifies whether to include missing or closed indexes in the response. Default is false. | . | lenient | Boolean | Specifies whether OpenSearch should accept requests if queries have format errors (for example, querying a text field for an integer). Default is false. | . | max_concurrent_shard_requests | Integer | How many concurrent shard requests this request should execute on each node. Default is 5. | . | pre_filter_shard_size | Integer | A prefilter size threshold that triggers a prefilter operation if the request exceeds the threshold. Default is 128 shards. | . | preference | String | Specifies which shard or node OpenSearch should perform the count operation on. | . | q | String | Lucene query string’s query. | . | request_cache | Boolean | Specifies whether OpenSearch should use the request cache. Default is whether it’s enabled in the index’s settings. | . | rest_total_hits_as_int | Boolean | Whether to return hits.total as an integer. Returns an object otherwise. Default is false. | . | routing | String | Value used to route the update by query operation to a specific shard. | . | scroll | Time | How long to keep the search context open. | . | search_type | String | Whether OpenSearch should use global term and document frequencies when calculating relevance scores. Valid choices are query_then_fetch and dfs_query_then_fetch. query_then_fetch scores documents using local term and document frequencies for the shard. It’s usually faster but less accurate. dfs_query_then_fetch scores documents using global term and document frequencies across all shards. It’s usually slower but more accurate. Default is query_then_fetch. | . | seq_no_primary_term | Boolean | Whether to return sequence number and primary term of the last operation of each document hit. | . | size | Integer | How many results to include in the response. | . | sort | List | A comma-separated list of &lt;field&gt; : &lt;direction&gt; pairs to sort by. | . | _source | String | Whether to include the _source field in the response. | . | _source_excludes | List | A comma-separated list of source fields to exclude from the response. | . | _source_includes | List | A comma-separated list of source fields to include in the response. | . | stats | String | Value to associate with the request for additional logging. | . | stored_fields | Boolean | Whether the get operation should retrieve fields stored in the index. Default is false. | . | suggest_field | String | Fields OpenSearch can use to look for similar terms. | . | suggest_mode | String | The mode to use when searching. Available options are always (use suggestions based on the provided terms), popular (use suggestions that have more occurrences), and missing (use suggestions for terms not in the index). | . | suggest_size | Integer | How many suggestions to return. | . | suggest_text | String | The source that suggestions should be based off of. | . | terminate_after | Integer | The maximum number of documents OpenSearch should process before terminating the request. Default is 0. | . | timeout | Time | How long the operation should wait for a response from active shards. Default is 1m. | . | track_scores | Boolean | Whether to return document scores. Default is false. | . | track_total_hits | Boolean or Integer | Whether to return how many documents matched the query. | . | typed_keys | Boolean | Whether returned aggregations and suggested terms should include their types in the response. Default is true. | . | version | Boolean | Whether to include the document version as a match. | . ",
    "url": "https://vagimeli.github.io/api-reference/search/#url-parameters",
    "relUrl": "/api-reference/search/#url-parameters"
  },"2391": {
    "doc": "Search",
    "title": "Request body",
    "content": "All fields are optional. | Field | Type | Description | . | docvalue_fields | Array of objects | The fields that OpenSearch should return using their docvalue forms. Specify a format to return results in a certain format, such as date and time. | . | fields | Array | The fields to search for in the request. Specify a format to return results in a certain format, such as date and time. | . | explain | String | Whether to return details about how OpenSearch computed the document’s score. Default is false. | . | from | Integer | The starting index to search from. Default is 0. | . | indices_boost | Array of objects | Values used to boost the score of specified indexes. Specify in the format of &lt;index&gt; : &lt;boost-multiplier&gt; | . | min_score | Integer | Specify a score threshold to return only documents above the threshold. | . | query | Object | The DSL query to use in the request. | . | seq_no_primary_term | Boolean | Whether to return sequence number and primary term of the last operation of each document hit. | . | size | Integer | How many results to return. Default is 10. | . | _source |   | Whether to include the _source field in the response. | . | stats | String | Value to associate with the request for additional logging. | . | terminate_after | Integer | The maximum number of documents OpenSearch should process before terminating the request. Default is 0. | . | timeout | Time | How long to wait for a response. Default is no timeout. | . | version | Boolean | Whether to include the document version in the response. | . ",
    "url": "https://vagimeli.github.io/api-reference/search/#request-body",
    "relUrl": "/api-reference/search/#request-body"
  },"2392": {
    "doc": "Search",
    "title": "Response body",
    "content": "{ \"took\": 3, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 1, \"relation\": \"eq\" }, \"max_score\": 1.0, \"hits\": [ { \"_index\": \"superheroes\", \"_id\": \"1\", \"_score\": 1.0, \"_source\": { \"superheroes\": [ { \"Hero name\": \"Superman\", \"Real identity\": \"Clark Kent\", \"Age\": 28 }, { \"Hero name\": \"Batman\", \"Real identity\": \"Bruce Wayne\", \"Age\": 26 }, { \"Hero name\": \"Flash\", \"Real identity\": \"Barry Allen\", \"Age\": 28 }, { \"Hero name\": \"Robin\", \"Real identity\": \"Dick Grayson\", \"Age\": 15 } ] } } ] } } . ",
    "url": "https://vagimeli.github.io/api-reference/search/#response-body",
    "relUrl": "/api-reference/search/#response-body"
  },"2393": {
    "doc": "Register Snapshot Repository",
    "title": "Registering or updating a snapshot repository",
    "content": "You can register a new repository in which to store snapshots or update information for an existing repository by using the snapshots API. There are two types of snapshot repositories: . | File system (fs): For instructions on creating an fs repository, see Register repository shared file system. | Amazon Simple Storage Service (Amazon S3) bucket (s3): For instructions on creating an s3 repository, see Register repository Amazon S3. | . For instructions on creating a repository, see Register repository. ",
    "url": "https://vagimeli.github.io/api-reference/snapshots/create-repository/#registering-or-updating-a-snapshot-repository",
    "relUrl": "/api-reference/snapshots/create-repository/#registering-or-updating-a-snapshot-repository"
  },"2394": {
    "doc": "Register Snapshot Repository",
    "title": "Path and HTTP methods",
    "content": "POST /_snapshot/my-first-repo/ PUT /_snapshot/my-first-repo/ . ",
    "url": "https://vagimeli.github.io/api-reference/snapshots/create-repository/#path-and-http-methods",
    "relUrl": "/api-reference/snapshots/create-repository/#path-and-http-methods"
  },"2395": {
    "doc": "Register Snapshot Repository",
    "title": "Path parameters",
    "content": "| Parameter | Data type | Description | . | repository | String | Repository name | . ",
    "url": "https://vagimeli.github.io/api-reference/snapshots/create-repository/#path-parameters",
    "relUrl": "/api-reference/snapshots/create-repository/#path-parameters"
  },"2396": {
    "doc": "Register Snapshot Repository",
    "title": "Request parameters",
    "content": "Request parameters depend on the type of repository: fs or s3. fs repository . | Request field | Description | . | location | The file system directory for snapshots, such as a mounted directory from a file server or a Samba share. Must be accessible by all nodes. Required. | . | chunk_size | Breaks large files into chunks during snapshot operations (e.g. 64mb, 1gb), which is important for cloud storage providers and far less important for shared file systems. Default is null (unlimited). Optional. | . | compress | Whether to compress metadata files. This setting does not affect data files, which might already be compressed, depending on your index settings. Default is false. Optional. | . | max_restore_bytes_per_sec | The maximum rate at which snapshots restore. Default is 40 MB per second (40m). Optional. | . | max_snapshot_bytes_per_sec | The maximum rate at which snapshots take. Default is 40 MB per second (40m). Optional. | . | readonly | Whether the repository is read-only. Useful when migrating from one cluster (\"readonly\": false when registering) to another cluster (\"readonly\": true when registering). Optional. | . Example request . The following example registers an fs repository using the local directory /mnt/snapshots as location. PUT /_snapshot/my-fs-repository { \"type\": \"fs\", \"settings\": { \"location\": \"/mnt/snapshots\" } } . copy . s3 repository . | Request field | Description | . | base_path | The path within the bucket in which you want to store snapshots (for example, my/snapshot/directory). Optional. If not specified, snapshots are stored in the S3 bucket root. | . | bucket | Name of the S3 bucket. Required. | . | buffer_size | The threshold beyond which chunks (of chunk_size) should be broken into pieces (of buffer_size) and sent to S3 using a different API. Default is the smaller of two values: 100 MB or 5% of the Java heap. Valid values are between 5mb and 5gb. We don’t recommend changing this option. | . | canned_acl | S3 has several canned ACLs that the repository-s3 plugin can add to objects as it creates them in S3. Default is private. Optional. | . | chunk_size | Breaks files into chunks during snapshot operations (e.g. 64mb, 1gb), which is important for cloud storage providers and far less important for shared file systems. Default is 1gb. Optional. | . | client | When specifying client settings (e.g. s3.client.default.access_key), you can use a string other than default (e.g. s3.client.backup-role.access_key). If you used an alternate name, change this value to match. Default and recommended value is default. Optional. | . | compress | Whether to compress metadata files. This setting does not affect data files, which might already be compressed, depending on your index settings. Default is false. Optional. | . | max_restore_bytes_per_sec | The maximum rate at which snapshots restore. Default is 40 MB per second (40m). Optional. | . | max_snapshot_bytes_per_sec | The maximum rate at which snapshots take. Default is 40 MB per second (40m). Optional. | . | readonly | Whether the repository is read-only. Useful when migrating from one cluster (\"readonly\": false when registering) to another cluster (\"readonly\": true when registering). Optional. | . | server_side_encryption | Whether to encrypt snapshot files in the S3 bucket. This setting uses AES-256 with S3-managed keys. See Protecting data using server-side encryption. Default is false. Optional. | . | storage_class | Specifies the S3 storage class for the snapshots files. Default is standard. Do not use the glacier and deep_archive storage classes. Optional. | . For the base_path parameter, do not enter the s3:// prefix when entering your S3 bucket details. Only the name of the bucket is required. Example request . The following request registers a new S3 repository called my-opensearch-repo in an existing bucket called my-open-search-bucket. By default, all snapshots are stored in the my/snapshot/directory. PUT /_snapshot/my-opensearch-repo { \"type\": \"s3\", \"settings\": { \"bucket\": \"my-open-search-bucket\", \"base_path\": \"my/snapshot/directory\" } } . copy . Example response . Upon success, the following JSON object is returned: . { \"acknowledged\": true } . To verify that the repository was registered, use the Get snapshot repository API, passing the repository name as the repository path parameter. ",
    "url": "https://vagimeli.github.io/api-reference/snapshots/create-repository/#request-parameters",
    "relUrl": "/api-reference/snapshots/create-repository/#request-parameters"
  },"2397": {
    "doc": "Register Snapshot Repository",
    "title": "Register Snapshot Repository",
    "content": " ",
    "url": "https://vagimeli.github.io/api-reference/snapshots/create-repository/",
    "relUrl": "/api-reference/snapshots/create-repository/"
  },"2398": {
    "doc": "Create Snapshot",
    "title": "Create snapshot",
    "content": "Creates a snapshot within an existing repository. | To learn more about snapshots, see Snapshots. | To view a list of your repositories, see Get snapshot repository. | . Path and HTTP methods . PUT /_snapshot/&lt;repository&gt;/&lt;snapshot&gt; POST /_snapshot/&lt;repository&gt;/&lt;snapshot&gt; . Path parameters . | Parameter | Data type | Description | . | repository | String | Repostory name to contain the snapshot. | . | snapshot | String | Name of Snapshot to create. | . Query parameters . | Parameter | Data type | Description | . | wait_for_completion | Boolean | Whether to wait for snapshot creation to complete before continuing. If you include this parameter, the snapshot definition is returned after completion. | . Request fields . The request body is optional. | Field | Data type | Description | . | indices | String | The indices you want to include in the snapshot. You can use , to create a list of indices, * to specify an index pattern, and - to exclude certain indices. Don’t put spaces between items. Default is all indices. | . | ignore_unavailable | Boolean | If an index from the indices list doesn’t exist, whether to ignore it rather than fail the snapshot. Default is false. | . | include_global_state | Boolean | Whether to include cluster state in the snapshot. Default is true. | . | partial | Boolean | Whether to allow partial snapshots. Default is false, which fails the entire snapshot if one or more shards fails to stor | . Example requests . Request without a body . The following request creates a snapshot called my-first-snapshot in an S3 repository called my-s3-repository. A request body is not included because it is optional. POST _snapshot/my-s3-repository/my-first-snapshot . copy . Request with a body . You can also add a request body to include or exclude certain indices or specify other settings: . PUT _snapshot/my-s3-repository/2 { \"indices\": \"opensearch-dashboards*,my-index*,-my-index-2016\", \"ignore_unavailable\": true, \"include_global_state\": false, \"partial\": false } . copy . Example responses . Upon success, the response content depends on whether you include the wait_for_completion query parameter. wait_for_completion not included . { \"accepted\": true } . To verify that the snapshot was created, use the Get snapshot API, passing the snapshot name as the snapshot path parameter. wait_for_completion included . The snapshot definition is returned. { \"snapshot\" : { \"snapshot\" : \"5\", \"uuid\" : \"ZRH4Zv7cSnuYev2JpLMJGw\", \"version_id\" : 136217927, \"version\" : \"2.0.1\", \"indices\" : [ \".opendistro-reports-instances\", \".opensearch-observability\", \".kibana_1\", \"opensearch_dashboards_sample_data_flights\", \".opensearch-notifications-config\", \".opendistro-reports-definitions\", \"shakespeare\" ], \"data_streams\" : [ ], \"include_global_state\" : true, \"state\" : \"SUCCESS\", \"start_time\" : \"2022-08-10T16:52:15.277Z\", \"start_time_in_millis\" : 1660150335277, \"end_time\" : \"2022-08-10T16:52:18.699Z\", \"end_time_in_millis\" : 1660150338699, \"duration_in_millis\" : 3422, \"failures\" : [ ], \"shards\" : { \"total\" : 7, \"failed\" : 0, \"successful\" : 7 } } } . Response fields . | Field | Data type | Description | . | snapshot | string | Snapshot name. | . | uuid | string | Snapshot’s universally unique identifier (UUID). | . | version_id | int | Build ID of the Open Search version that created the snapshot. | . | version | float | Open Search version that created the snapshot. | . | indices | array | Indices in the snapshot. | . | data_streams | array | Data streams in the snapshot. | . | include_global_state | boolean | Whether the current cluster state is included in the snapshot. | . | start_time | string | Date/time when the snapshot creation process began. | . | start_time_in_millis | long | Time (in milliseconds) when the snapshot creation process began. | . | end_time | string | Date/time when the snapshot creation process ended. | . | end_time_in_millis | long | Time (in milliseconds) when the snapshot creation process ended. | . | duration_in_millis | long | Total time (in milliseconds) that the snapshot creation process lasted. | . | failures | array | Failures, if any, that occured during snapshot creation. | . | shards | object | Total number of shards created along with number of successful and failed shards. | . | state | string | Snapshot status. Possible values: IN_PROGRESS, SUCCESS, FAILED, PARTIAL. | . ",
    "url": "https://vagimeli.github.io/api-reference/snapshots/create-snapshot/#create-snapshot",
    "relUrl": "/api-reference/snapshots/create-snapshot/#create-snapshot"
  },"2399": {
    "doc": "Create Snapshot",
    "title": "Create Snapshot",
    "content": " ",
    "url": "https://vagimeli.github.io/api-reference/snapshots/create-snapshot/",
    "relUrl": "/api-reference/snapshots/create-snapshot/"
  },"2400": {
    "doc": "Delete Snapshot Repository",
    "title": "Delete snapshot repository configuration",
    "content": "Deletes a snapshot repository configuration. A repository in OpenSearch is simply a configuration that maps a repository name to a type (file system or s3 repository) along with other information depending on the type. The configuration is backed by a file system location or an s3 bucket. When you invoke the API, the physical file system or s3 bucket itself is not deleted. Only the configuration is deleted. To learn more about repositories, see Register or update snapshot repository. Path parameters . | Parameter | Data type | Description | . | repository | String | Repository to delete. | . Example request . The following request deletes the my-opensearch-repo repository: . DELETE _snapshot/my-opensearch-repo . copy . Example response . Upon success, the response returns the following JSON object: . { \"acknowledged\" : true } . To verify that the repository was deleted, use the Get snapshot repository API, passing the repository name as the repository path parameter. ",
    "url": "https://vagimeli.github.io/api-reference/snapshots/delete-snapshot-repository/#delete-snapshot-repository-configuration",
    "relUrl": "/api-reference/snapshots/delete-snapshot-repository/#delete-snapshot-repository-configuration"
  },"2401": {
    "doc": "Delete Snapshot Repository",
    "title": "Delete Snapshot Repository",
    "content": " ",
    "url": "https://vagimeli.github.io/api-reference/snapshots/delete-snapshot-repository/",
    "relUrl": "/api-reference/snapshots/delete-snapshot-repository/"
  },"2402": {
    "doc": "Delete Snapshot",
    "title": "Delete snapshot",
    "content": "Deletes a snapshot from a repository. | To learn more about snapshots, see Snapshots. | To view a list of your repositories, see cat repositories. | To view a list of your snapshots, see cat snapshots. | . Path parameters . | Parameter | Data type | Description | . | repository | String | Repostory that contains the snapshot. | . | snapshot | String | Snapshot to delete. | . Example request . The following request deletes a snapshot called my-first-snapshot from the my-opensearch-repo repository: . DELETE _snapshot/my-opensearch-repo/my-first-snapshot . copy . Example response . Upon success, the response returns the following JSON object: . { \"acknowledged\": true } . To verify that the snapshot was deleted, use the Get snapshot API, passing the snapshot name as the snapshot path parameter. ",
    "url": "https://vagimeli.github.io/api-reference/snapshots/delete-snapshot/#delete-snapshot",
    "relUrl": "/api-reference/snapshots/delete-snapshot/#delete-snapshot"
  },"2403": {
    "doc": "Delete Snapshot",
    "title": "Delete Snapshot",
    "content": " ",
    "url": "https://vagimeli.github.io/api-reference/snapshots/delete-snapshot/",
    "relUrl": "/api-reference/snapshots/delete-snapshot/"
  },"2404": {
    "doc": "Get Snapshot Repository",
    "title": "Get snapshot repository.",
    "content": "Retrieves information about a snapshot repository. To learn more about repositories, see Register repository. You can also get details about a snapshot during and after snapshot creation. See Get snapshot status. Path parameters . | Parameter | Data type | Description | . | repository | String | A comma-separated list of snapshot repository names to retrieve. Wildcard (*) expressions are supported including combining wildcards with exclude patterns starting with -. | . Query parameters . | Parameter | Data type | Description | . | local | Boolean | Whether to get information from the local node. Optional, defaults to false. | . | cluster_manager_timeout | Time | Amount of time to wait for a connection to the master node. Optional, defaults to 30 seconds. | . Example request . The following request retrieves information for the my-opensearch-repo repository: . GET /_snapshot/my-opensearch-repo . copy . Example response . Upon success, the response returns repositry information. This sample is for an s3 repository type. { \"my-opensearch-repo\" : { \"type\" : \"s3\", \"settings\" : { \"bucket\" : \"my-open-search-bucket\", \"base_path\" : \"snapshots\" } } } . Response fields . | Field | Data type | Description | . | type | string | Bucket type: fs (file system) or s3 (s3 bucket) | . | bucket | string | S3 bucket name. | . | base_path | string | Folder within the bucket where snapshots are stored. | . ",
    "url": "https://vagimeli.github.io/api-reference/snapshots/get-snapshot-repository/#get-snapshot-repository",
    "relUrl": "/api-reference/snapshots/get-snapshot-repository/#get-snapshot-repository"
  },"2405": {
    "doc": "Get Snapshot Repository",
    "title": "Get Snapshot Repository",
    "content": " ",
    "url": "https://vagimeli.github.io/api-reference/snapshots/get-snapshot-repository/",
    "relUrl": "/api-reference/snapshots/get-snapshot-repository/"
  },"2406": {
    "doc": "Get Snapshot Status",
    "title": "Get snapshot status",
    "content": "Returns details about a snapshot’s state during and after snapshot creation. To learn about snapshot creation, see Create snapshot. If you use the Security plugin, you must have the monitor_snapshot, create_snapshot, or manage cluster privileges. Path parameters . Path parameters are optional. | Parameter | Data type | Description | . | repository | String | Repository containing the snapshot. | . | snapshot | String | Snapshot to return. | . Three request variants provide flexibility: . | GET _snapshot/_status returns the status of all currently running snapshots in all repositories. | GET _snapshot/&lt;repository&gt;/_status returns the status of only currently running snapshots in the specified repository. This is the preferred variant. | GET _snapshot/&lt;repository&gt;/&lt;snapshot&gt;/_status returns the status of all snapshots in the specified repository whether they are running or not. | . Using the API to return state for other than currently running snapshots can be very costly for (1) machine machine resources and (2) processing time if running in the cloud. For each snapshot, each request causes file reads from all a snapshot’s shards. Request fields . | Field | Data type | Description | . | ignore_unavailable | Boolean | How to handles requests for unavailable snapshots. If false, the request returns an error for unavailable snapshots. If true, the request ignores unavailable snapshots, such as those that are corrupted or temporarily cannot be returned. Defaults to false. | . Example request . The following request returns the status of my-first-snapshot in the my-opensearch-repo repository. Unavailable snapshots are ignored. GET _snapshot/my-opensearch-repo/my-first-snapshot/_status { \"ignore_unavailable\": true } . copy . Example response . The example that follows corresponds to the request above in the Example request section. The GET _snapshot/my-opensearch-repo/my-first-snapshot/_status request returns the following fields: . { \"snapshots\" : [ { \"snapshot\" : \"my-first-snapshot\", \"repository\" : \"my-opensearch-repo\", \"uuid\" : \"dCK4Qth-TymRQ7Tu7Iga0g\", \"state\" : \"SUCCESS\", \"include_global_state\" : true, \"shards_stats\" : { \"initializing\" : 0, \"started\" : 0, \"finalizing\" : 0, \"done\" : 7, \"failed\" : 0, \"total\" : 7 }, \"stats\" : { \"incremental\" : { \"file_count\" : 31, \"size_in_bytes\" : 24488927 }, \"total\" : { \"file_count\" : 31, \"size_in_bytes\" : 24488927 }, \"start_time_in_millis\" : 1660666841667, \"time_in_millis\" : 14054 }, \"indices\" : { \".opensearch-observability\" : { \"shards_stats\" : { \"initializing\" : 0, \"started\" : 0, \"finalizing\" : 0, \"done\" : 1, \"failed\" : 0, \"total\" : 1 }, \"stats\" : { \"incremental\" : { \"file_count\" : 1, \"size_in_bytes\" : 208 }, \"total\" : { \"file_count\" : 1, \"size_in_bytes\" : 208 }, \"start_time_in_millis\" : 1660666841868, \"time_in_millis\" : 201 }, \"shards\" : { \"0\" : { \"stage\" : \"DONE\", \"stats\" : { \"incremental\" : { \"file_count\" : 1, \"size_in_bytes\" : 208 }, \"total\" : { \"file_count\" : 1, \"size_in_bytes\" : 208 }, \"start_time_in_millis\" : 1660666841868, \"time_in_millis\" : 201 } } } }, \"shakespeare\" : { \"shards_stats\" : { \"initializing\" : 0, \"started\" : 0, \"finalizing\" : 0, \"done\" : 1, \"failed\" : 0, \"total\" : 1 }, \"stats\" : { \"incremental\" : { \"file_count\" : 4, \"size_in_bytes\" : 18310117 }, \"total\" : { \"file_count\" : 4, \"size_in_bytes\" : 18310117 }, \"start_time_in_millis\" : 1660666842470, \"time_in_millis\" : 13050 }, \"shards\" : { \"0\" : { \"stage\" : \"DONE\", \"stats\" : { \"incremental\" : { \"file_count\" : 4, \"size_in_bytes\" : 18310117 }, \"total\" : { \"file_count\" : 4, \"size_in_bytes\" : 18310117 }, \"start_time_in_millis\" : 1660666842470, \"time_in_millis\" : 13050 } } } }, \"opensearch_dashboards_sample_data_flights\" : { \"shards_stats\" : { \"initializing\" : 0, \"started\" : 0, \"finalizing\" : 0, \"done\" : 1, \"failed\" : 0, \"total\" : 1 }, \"stats\" : { \"incremental\" : { \"file_count\" : 10, \"size_in_bytes\" : 6132245 }, \"total\" : { \"file_count\" : 10, \"size_in_bytes\" : 6132245 }, \"start_time_in_millis\" : 1660666843476, \"time_in_millis\" : 6221 }, \"shards\" : { \"0\" : { \"stage\" : \"DONE\", \"stats\" : { \"incremental\" : { \"file_count\" : 10, \"size_in_bytes\" : 6132245 }, \"total\" : { \"file_count\" : 10, \"size_in_bytes\" : 6132245 }, \"start_time_in_millis\" : 1660666843476, \"time_in_millis\" : 6221 } } } }, \".opendistro-reports-definitions\" : { \"shards_stats\" : { \"initializing\" : 0, \"started\" : 0, \"finalizing\" : 0, \"done\" : 1, \"failed\" : 0, \"total\" : 1 }, \"stats\" : { \"incremental\" : { \"file_count\" : 1, \"size_in_bytes\" : 208 }, \"total\" : { \"file_count\" : 1, \"size_in_bytes\" : 208 }, \"start_time_in_millis\" : 1660666843076, \"time_in_millis\" : 200 }, \"shards\" : { \"0\" : { \"stage\" : \"DONE\", \"stats\" : { \"incremental\" : { \"file_count\" : 1, \"size_in_bytes\" : 208 }, \"total\" : { \"file_count\" : 1, \"size_in_bytes\" : 208 }, \"start_time_in_millis\" : 1660666843076, \"time_in_millis\" : 200 } } } }, \".opendistro-reports-instances\" : { \"shards_stats\" : { \"initializing\" : 0, \"started\" : 0, \"finalizing\" : 0, \"done\" : 1, \"failed\" : 0, \"total\" : 1 }, \"stats\" : { \"incremental\" : { \"file_count\" : 1, \"size_in_bytes\" : 208 }, \"total\" : { \"file_count\" : 1, \"size_in_bytes\" : 208 }, \"start_time_in_millis\" : 1660666841667, \"time_in_millis\" : 201 }, \"shards\" : { \"0\" : { \"stage\" : \"DONE\", \"stats\" : { \"incremental\" : { \"file_count\" : 1, \"size_in_bytes\" : 208 }, \"total\" : { \"file_count\" : 1, \"size_in_bytes\" : 208 }, \"start_time_in_millis\" : 1660666841667, \"time_in_millis\" : 201 } } } }, \".kibana_1\" : { \"shards_stats\" : { \"initializing\" : 0, \"started\" : 0, \"finalizing\" : 0, \"done\" : 1, \"failed\" : 0, \"total\" : 1 }, \"stats\" : { \"incremental\" : { \"file_count\" : 13, \"size_in_bytes\" : 45733 }, \"total\" : { \"file_count\" : 13, \"size_in_bytes\" : 45733 }, \"start_time_in_millis\" : 1660666842673, \"time_in_millis\" : 2007 }, \"shards\" : { \"0\" : { \"stage\" : \"DONE\", \"stats\" : { \"incremental\" : { \"file_count\" : 13, \"size_in_bytes\" : 45733 }, \"total\" : { \"file_count\" : 13, \"size_in_bytes\" : 45733 }, \"start_time_in_millis\" : 1660666842673, \"time_in_millis\" : 2007 } } } }, \".opensearch-notifications-config\" : { \"shards_stats\" : { \"initializing\" : 0, \"started\" : 0, \"finalizing\" : 0, \"done\" : 1, \"failed\" : 0, \"total\" : 1 }, \"stats\" : { \"incremental\" : { \"file_count\" : 1, \"size_in_bytes\" : 208 }, \"total\" : { \"file_count\" : 1, \"size_in_bytes\" : 208 }, \"start_time_in_millis\" : 1660666842270, \"time_in_millis\" : 200 }, \"shards\" : { \"0\" : { \"stage\" : \"DONE\", \"stats\" : { \"incremental\" : { \"file_count\" : 1, \"size_in_bytes\" : 208 }, \"total\" : { \"file_count\" : 1, \"size_in_bytes\" : 208 }, \"start_time_in_millis\" : 1660666842270, \"time_in_millis\" : 200 } } } } } } ] } . Response fields . | Field | Data type | Description | . | repository | String | Name of repository that contains the snapshot. | . | snapshot | String | Snapshot name. | . | uuid | String | Snapshot Universally unique identifier (UUID). | . | state | String | Snapshot’s current status. See Snapshot states. | . | include_global_state | Boolean | Whether the current cluster state is included in the snapshot. | . | shards_stats | Object | Snapshot’s shard counts. See Shard stats. | . | stats | Object | Details of files included in the snapshot. file_count: number of files. size_in_bytes: total of all fie sizes. See Snapshot file stats. | . | index | list of Objects | List of objects that contain information about the indices in the snapshot. See Index objects. | . Snapshot states . | State | Description | . | FAILED | The snapshot terminated in an error and no data was stored. | . | IN_PROGRESS | The snapshot is currently running. | . | PARTIAL | The global cluster state was stored, but data from at least one shard was not stored. The failures property of the Create snapshot response contains additional details. | . | SUCCESS | The snapshot finished and all shards were stored successfully. | . Shard stats . All property values are Integers. | Property | Description | . | initializing | Number of shards that are still initializing. | . | started | Number of shards that have started but not are not finalized. | . | finalizing | Number of shards that are finalizing but are not done. | . | done | Number of shards that initialized, started, and finalized successfully. | . | failed | Number of shards that failed to be included in the snapshot. | . | total | Total number of shards included in the snapshot. | . Snapshot file stats . | Property | Type | Description | . | incremental | Object | Number and size of files that still need to be copied during snapshot creation. For completed snapshots, incremental provides the number and size of files that were not already in the repository and were copied as part of the incremental snapshot. | . | processed | Object | Number and size of files already uploaded to the snapshot. The processed file_count and size_in_bytes are incremented in stats after a file is uploaded. | . | total | Object | Total number and size of files that are referenced by the snapshot. | . | start_time_in_millis | Long | Time (in milliseconds) when snapshot creation began. | . | time_in_millis | Long | Total time (in milliseconds) that the snapshot took to complete. | . Index objects . | Property | Type | Description | . | shards_stats | Object | See Shard stats. | . | stats | Object | See Snapshot file stats. | . | shards | list of Objects | List of objects containing information about the shards that include the snapshot. Properies of the shards are listed below in bold text. stage: Current state of shards in the snapshot. Shard states are: * DONE: Number of shards in the snapshot that were successfully stored in the repository. * FAILURE: Number of shards in the snapshot that were not successfully stored in the repository. * FINALIZE: Number of shards in the snapshot that are in the finalizing stage of being stored in the repository. * INIT: Number of shards in the snapshot that are in the initializing stage of being stored in the repository.* STARTED: Number of shards in the snapshot that are in the started stage of being stored in the repository. stats: See Snapshot file stats. total: Total number and size of files referenced by the snapshot. start_time_in_millis: Time (in milliseconds) when snapshot creation began. time_in_millis: Total time (in milliseconds) that the snapshot took to complete. | . ",
    "url": "https://vagimeli.github.io/api-reference/snapshots/get-snapshot-status/#get-snapshot-status",
    "relUrl": "/api-reference/snapshots/get-snapshot-status/#get-snapshot-status"
  },"2407": {
    "doc": "Get Snapshot Status",
    "title": "Get Snapshot Status",
    "content": " ",
    "url": "https://vagimeli.github.io/api-reference/snapshots/get-snapshot-status/",
    "relUrl": "/api-reference/snapshots/get-snapshot-status/"
  },"2408": {
    "doc": "Get Snapshot",
    "title": "Get snapshot.",
    "content": "Retrieves information about a snapshot. Path parameters . | Parameter | Data type | Description | . | repository | String | The repository that contains the snapshot to retrieve. | . | snapshot | String | Snapshot to retrieve. | . Query parameters . | Parameter | Data type | Description | . | verbose | Boolean | Whether to show all, or just basic snapshot information. If true, returns all information. If false, omits information like start/end times, failures, and shards. Optional, defaults to true. | . | ignore_unavailable | Boolean | How to handle snapshots that are unavailable (corrupted or otherwise temporarily can’t be returned). If true and the snapshot is unavailable, the request does not return the snapshot. If false and the snapshot is unavailable, the request returns an error. Optional, defaults to false. | . Example request . The following request retrieves information for the my-first-snapshot located in the my-opensearch-repo repository: . GET _snapshot/my-opensearch-repo/my-first-snapshot . copy . Example response . Upon success, the response returns snapshot information: . { \"snapshots\" : [ { \"snapshot\" : \"my-first-snapshot\", \"uuid\" : \"3P7Qa-M8RU6l16Od5n7Lxg\", \"version_id\" : 136217927, \"version\" : \"2.0.1\", \"indices\" : [ \".opensearch-observability\", \".opendistro-reports-instances\", \".opensearch-notifications-config\", \"shakespeare\", \".opendistro-reports-definitions\", \"opensearch_dashboards_sample_data_flights\", \".kibana_1\" ], \"data_streams\" : [ ], \"include_global_state\" : true, \"state\" : \"SUCCESS\", \"start_time\" : \"2022-08-11T20:30:00.399Z\", \"start_time_in_millis\" : 1660249800399, \"end_time\" : \"2022-08-11T20:30:14.851Z\", \"end_time_in_millis\" : 1660249814851, \"duration_in_millis\" : 14452, \"failures\" : [ ], \"shards\" : { \"total\" : 7, \"failed\" : 0, \"successful\" : 7 } } ] } . Response fields . | Field | Data type | Description | . | snapshot | string | Snapshot name. | . | uuid | string | Snapshot’s universally unique identifier (UUID). | . | version_id | int | Build ID of the Open Search version that created the snapshot. | . | version | float | Open Search version that created the snapshot. | . | indices | array | Indices in the snapshot. | . | data_streams | array | Data streams in the snapshot. | . | include_global_state | boolean | Whether the current cluster state is included in the snapshot. | . | start_time | string | Date/time when the snapshot creation process began. | . | start_time_in_millis | long | Time (in milliseconds) when the snapshot creation process began. | . | end_time | string | Date/time when the snapshot creation process ended. | . | end_time_in_millis | long | Time (in milliseconds) when the snapshot creation process ended. | . | duration_in_millis | long | Total time (in milliseconds) that the snapshot creation process lasted. | . | failures | array | Failures, if any, that occured during snapshot creation. | . | shards | object | Total number of shards created along with number of successful and failed shards. | . | state | string | Snapshot status. Possible values: IN_PROGRESS, SUCCESS, FAILED, PARTIAL. | . ",
    "url": "https://vagimeli.github.io/api-reference/snapshots/get-snapshot/#get-snapshot",
    "relUrl": "/api-reference/snapshots/get-snapshot/#get-snapshot"
  },"2409": {
    "doc": "Get Snapshot",
    "title": "Get Snapshot",
    "content": " ",
    "url": "https://vagimeli.github.io/api-reference/snapshots/get-snapshot/",
    "relUrl": "/api-reference/snapshots/get-snapshot/"
  },"2410": {
    "doc": "Snapshot APIs",
    "title": "Snapshot APIs",
    "content": "The snapshot APIs allow you to manage snapshots and snapshot repositories. ",
    "url": "https://vagimeli.github.io/api-reference/snapshots/index/",
    "relUrl": "/api-reference/snapshots/index/"
  },"2411": {
    "doc": "Restore Snapshot",
    "title": "Restore Snapshot",
    "content": "Restores a snapshot of a cluster or specified data streams and indices. | For information about indices and clusters, see Introduction to OpenSearch. | For information about data streams, see Data streams. | . If open indexes with the same name that you want to restore already exist in the cluster, you must close, delete, or rename the indexes. See Example request for information about renaming an index. See Close index for information about closing an index. Path parameters . | Parameter | Data type | Description | . | repository | String | Repository containing the snapshot to restore. | . | snapshot | String | Snapshot to restore. | . Query parameters . | Parameter | Data type | Description | . | wait_for_completion | Boolean | Whether to wait for snapshot restoration to complete before continuing. | . Request fields . All request body parameters are optional. | Parameter | Data type | Description | . | ignore_unavailable | Boolean | How to handle data streams or indices that are missing or closed. If false, the request returns an error for any data stream or index that is missing or closed. If true, the request ignores data streams and indices in indices that are missing or closed. Defaults to false. | . | ignore_index_settings | Boolean | A comma-delimited list of index settings that you don’t want to restore from a snapshot. | . | include_aliases | Boolean | How to handle index aliases from the original snapshot. If true, index aliases from the original snapshot are restored. If false, aliases along with associated indices are not restored. Defaults to true. | . | include_global_state | Boolean | Whether to restore the current cluster state1. If false, the cluster state is not restored. If true, the current cluster state is restored. Defaults to false. | . | index_settings | String | A comma-delimited list of settings to add or change in all restored indices. Use this parameter to override index settings during snapshot restoration. For data streams, these index settings are applied to the restored backing indices. | . | indices | String | A comma-delimited list of data streams and indices to restore from the snapshot. Multi-index syntax is supported. By default, a restore operation includes all data streams and indices in the snapshot. If this argument is provided, the restore operation only includes the data streams and indices that you specify. | . | partial | Boolean | How the restore operation will behave if indices in the snapshot do not have all primary shards available. If false, the entire restore operation fails if any indices in the snapshot do not have all primary shards available. If true, allows the restoration of a partial snapshot of indices with unavailable shards. Only shards that were successfully included in the snapshot are restored. All missing shards are recreated as empty. By default, the entire restore operation fails if one or more indices included in the snapshot do not have all primary shards available. To change this behavior, set partial to true. Defaults to false. | . | rename_pattern | String | The pattern to apply to restored data streams and indices. Data streams and indices matching the rename pattern will be renamed according to rename_replacement. The rename pattern is applied as defined by the regular expression that supports referencing the original text. The request fails if two or more data streams or indices are renamed into the same name. If you rename a restored data stream, its backing indices are also renamed. For example, if you rename the logs data stream to recovered-logs, the backing index .ds-logs-1 is renamed to .ds-recovered-logs-1. If you rename a restored stream, ensure an index template matches the new stream name. If there are no matching index template names, the stream cannot roll over and new backing indices are not created. | . | rename_replacement | String | The rename replacement string. See rename_pattern for more information. | . | wait_for_completion | Boolean | Whether to return a response after the restore operation has completed. If false, the request returns a response when the restore operation initializes. If true, the request returns a response when the restore operation completes. Defaults to false. | . 1The cluster state includes: . | Persistent cluster settings | Index templates | Legacy index templates | Ingest pipelines | Index lifecycle policies | . Example request . The following request restores the opendistro-reports-definitions index from my-first-snapshot. The rename_pattern and rename_replacement combination causes the index to be renamed to opendistro-reports-definitions_restored because duplicate open index names in a cluster are not allowed. POST /_snapshot/my-opensearch-repo/my-first-snapshot/_restore { \"indices\": \"opendistro-reports-definitions\", \"ignore_unavailable\": true, \"include_global_state\": false, \"rename_pattern\": \"(.+)\", \"rename_replacement\": \"$1_restored\", \"include_aliases\": false } . Example response . Upon success, the response returns the following JSON object: . { \"snapshot\" : { \"snapshot\" : \"my-first-snapshot\", \"indices\" : [ ], \"shards\" : { \"total\" : 0, \"failed\" : 0, \"successful\" : 0 } } } . Except for the snapshot name, all properties are empty or 0. This is because any changes made to the volume after the snapshot was generated are lost. However, if you invoke the Get snapshot API to examine the snapshot, a fully populated snapshot object is returned. Response fields . | Field | Data type | Description | . | snapshot | string | Snapshot name. | . | indices | array | Indices in the snapshot. | . | shards | object | Total number of shards created along with number of successful and failed shards. | . If open indices in a snapshot already exist in a cluster, and you don’t delete, close, or rename them, the API returns an error like the following: . { \"error\" : { \"root_cause\" : [ { \"type\" : \"snapshot_restore_exception\", \"reason\" : \"[my-opensearch-repo:my-first-snapshot/dCK4Qth-TymRQ7Tu7Iga0g] cannot restore index [.opendistro-reports-definitions] because an open index with same name already exists in the cluster. Either close or delete the existing index or restore the index under a different name by providing a rename pattern and replacement name\" } ], \"type\" : \"snapshot_restore_exception\", \"reason\" : \"[my-opensearch-repo:my-first-snapshot/dCK4Qth-TymRQ7Tu7Iga0g] cannot restore index [.opendistro-reports-definitions] because an open index with same name already exists in the cluster. Either close or delete the existing index or restore the index under a different name by providing a rename pattern and replacement name\" }, \"status\" : 500 } . ",
    "url": "https://vagimeli.github.io/api-reference/snapshots/restore-snapshot/",
    "relUrl": "/api-reference/snapshots/restore-snapshot/"
  },"2412": {
    "doc": "Verify Snaphot Repository",
    "title": "Verify snapshot repository",
    "content": "Verifies that a snapshot repository is functional. Verifies the repository on each node in a cluster. If verification is successful, the verify snapshot repository API returns a list of nodes connected to the snapshot repository. If verification failed, the API returns an error. If you use the Security plugin, you must have the manage cluster privilege. Path parameters . Path parameters are optional. | Parameter | Data type | Description | . | repository | String | Name of repository to verify. | . Query parameters . | Parameter | Data type | Description | . | cluster_manager_timeout | Time | Amount of time to wait for a connection to the master node. Optional, defaults to 30s. | . | timeout | Time | The period of time to wait for a response. If a response is not received before the timeout value, the request fails and returns an error. Defaults to 30s. | . Example request . The following request verifies that the my-opensearch-repo is functional: . POST /_snapshot/my-opensearch-repo/_verify?timeout=0s&amp;cluster_manager_timeout=50s . Example response . The example that follows corresponds to the request above in the Example request section. The POST /_snapshot/my-opensearch-repo/_verify?timeout=0s&amp;cluster_manager_timeout=50s request returns the following fields: . { \"nodes\" : { \"by1kztwTRoeCyg4iGU5Y8A\" : { \"name\" : \"opensearch-node1\" } } } . In the preceding sample, one node is connected to the snapshot repository. If more were connected, you would see them in the response. Example: . { \"nodes\" : { \"lcfL6jv2jo6sMEtp4idMvg\" : { \"name\" : \"node-1\" }, \"rEPtFT/B+cuuOHnQn0jy4s\" : { \"name\" : \"node-2\" } } . Response fields . | Field | Data type | Description | . | nodes | Object | A list (not an array) of nodes connected to the snapshot repository. Each node itself is a property where the node ID is the key and the name has an ID (Object) and a name (String). | . ",
    "url": "https://vagimeli.github.io/api-reference/snapshots/verify-snapshot-repository/#verify-snapshot-repository",
    "relUrl": "/api-reference/snapshots/verify-snapshot-repository/#verify-snapshot-repository"
  },"2413": {
    "doc": "Verify Snaphot Repository",
    "title": "Verify Snaphot Repository",
    "content": " ",
    "url": "https://vagimeli.github.io/api-reference/snapshots/verify-snapshot-repository/",
    "relUrl": "/api-reference/snapshots/verify-snapshot-repository/"
  },"2414": {
    "doc": "Tasks",
    "title": "Tasks",
    "content": "Introduced 1.0 . A task is any operation you run in a cluster. For example, searching your data collection of books for a title or author name is a task. When you run OpenSearch, a task is automatically created to monitor your cluster’s health and performance. For more information about all of the tasks currently executing in your cluster, you can use the tasks API operation. The following request returns information about all of your tasks: . GET _tasks . copy . By including a task ID, you can get information specific to a particular task. Note that a task ID consists of a node’s identifying string and the task’s numerical ID. For example, if your node’s identifying string is nodestring and the task’s numerical ID is 1234, then your task ID is nodestring:1234. You can find this information by running the tasks operation: . GET _tasks/&lt;task_id&gt; . copy . Note that if a task finishes running, it won’t be returned as part of your request. For an example of a task that takes a little longer to finish, you can run the _reindex API operation on a larger document, and then run tasks. Sample Response . { \"nodes\": { \"Mgqdm0r9SEGClWxp_RbnaQ\": { \"name\": \"opensearch-node1\", \"transport_address\": \"172.18.0.3:9300\", \"host\": \"172.18.0.3\", \"ip\": \"172.18.0.3:9300\", \"roles\": [ \"data\", \"ingest\", \"master\", \"remote_cluster_client\" ], \"tasks\": { \"Mgqdm0r9SEGClWxp_RbnaQ:17416\": { \"node\": \"Mgqdm0r9SEGClWxp_RbnaQ\", \"id\": 17416, \"type\": \"transport\", \"action\": \"cluster:monitor/tasks/lists\", \"start_time_in_millis\": 1613599752458, \"running_time_in_nanos\": 994000, \"cancellable\": false, \"headers\": {} } }, \"Mgqdm0r9SEGClWxp_RbnaQ:17413\": { \"node\": \"Mgqdm0r9SEGClWxp_RbnaQ\", \"id\": 17413, \"type\": \"transport\", \"action\": \"indices:data/write/bulk\", \"start_time_in_millis\": 1613599752286, \"running_time_in_nanos\": 172846500, \"cancellable\": false, \"parent_task_id\": \"Mgqdm0r9SEGClWxp_RbnaQ:17366\", \"headers\": {} }, \"Mgqdm0r9SEGClWxp_RbnaQ:17366\": { \"node\": \"Mgqdm0r9SEGClWxp_RbnaQ\", \"id\": 17366, \"type\": \"transport\", \"action\": \"indices:data/write/reindex\", \"start_time_in_millis\": 1613599750929, \"running_time_in_nanos\": 1529733100, \"cancellable\": true, \"headers\": {} } } } } } . You can also use the following parameters with your query. | Parameter | Data type | Description | . | nodes | List | A comma-separated list of node IDs or names to limit the returned information. Use _local to return information from the node you’re connecting to, specify the node name to get information from specific nodes, or keep the parameter empty to get information from all nodes. | . | actions | List | A comma-separated list of actions that should be returned. Keep empty to return all. | . | detailed | Boolean | Returns detailed task information. (Default: false) | . | parent_task_id | String | Returns tasks with a specified parent task ID (node_id:task_number). Keep empty or set to -1 to return all. | . | wait_for_completion | Boolean | Waits for the matching tasks to complete. (Default: false) | . | group_by | Enum | Groups tasks by parent/child relationships or nodes. (Default: nodes) | . | timeout | Time | An explicit operation timeout. (Default: 30 seconds) | . | master_timeout | Time | The time to wait for a connection to the primary node. (Default: 30 seconds) | . For example, this request returns tasks currently running on a node named opensearch-node1: . Sample Request . GET /_tasks?nodes=opensearch-node1 . copy . Sample Response . { \"nodes\": { \"Mgqdm0r9SEGClWxp_RbnaQ\": { \"name\": \"opensearch-node1\", \"transport_address\": \"sample_address\", \"host\": \"sample_host\", \"ip\": \"sample_ip\", \"roles\": [ \"data\", \"ingest\", \"master\", \"remote_cluster_client\" ], \"tasks\": { \"Mgqdm0r9SEGClWxp_RbnaQ:24578\": { \"node\": \"Mgqdm0r9SEGClWxp_RbnaQ\", \"id\": 24578, \"type\": \"transport\", \"action\": \"cluster:monitor/tasks/lists\", \"start_time_in_millis\": 1611612517044, \"running_time_in_nanos\": 638700, \"cancellable\": false, \"headers\": {} }, \"Mgqdm0r9SEGClWxp_RbnaQ:24579\": { \"node\": \"Mgqdm0r9SEGClWxp_RbnaQ\", \"id\": 24579, \"type\": \"direct\", \"action\": \"cluster:monitor/tasks/lists[n]\", \"start_time_in_millis\": 1611612517044, \"running_time_in_nanos\": 222200, \"cancellable\": false, \"parent_task_id\": \"Mgqdm0r9SEGClWxp_RbnaQ:24578\", \"headers\": {} } } } } } . The following request returns detailed information about active search tasks: . Sample Request . curl -XGET \"localhost:9200/_tasks?actions=*search&amp;detailed . copy . Sample Response . { \"nodes\" : { \"CRqNwnEeRXOjeTSYYktw-A\" : { \"name\" : \"runTask-0\", \"transport_address\" : \"127.0.0.1:9300\", \"host\" : \"127.0.0.1\", \"ip\" : \"127.0.0.1:9300\", \"roles\" : [ \"cluster_manager\", \"data\", \"ingest\", \"remote_cluster_client\" ], \"attributes\" : { \"testattr\" : \"test\", \"shard_indexing_pressure_enabled\" : \"true\" }, \"tasks\" : { \"CRqNwnEeRXOjeTSYYktw-A:677\" : { \"node\" : \"CRqNwnEeRXOjeTSYYktw-A\", \"id\" : 677, \"type\" : \"transport\", \"action\" : \"indices:data/read/search\", \"description\" : \"indices[], search_type[QUERY_THEN_FETCH], source[{\\\"query\\\":{\\\"query_string\\\":&lt;QUERY_STRING&gt;}}]\", \"start_time_in_millis\" : 1660106254525, \"running_time_in_nanos\" : 1354236, \"cancellable\" : true, \"cancelled\" : false, \"headers\" : { }, \"resource_stats\" : { \"total\" : { \"cpu_time_in_nanos\" : 0, \"memory_in_bytes\" : 0 } } } } } } } . ",
    "url": "https://vagimeli.github.io/api-reference/tasks/",
    "relUrl": "/api-reference/tasks/"
  },"2415": {
    "doc": "Tasks",
    "title": "Task canceling",
    "content": "After getting a list of tasks, you can cancel all cancelable tasks with the following request: . POST _tasks/_cancel . copy . Note that not all tasks are cancelable. To see if a task is cancelable, refer to the cancellable field in the response to your tasks API request. You can also cancel a task by including a specific task ID. POST _tasks/&lt;task_id&gt;/_cancel . copy . The cancel operation supports the same parameters as the tasks operation. The following example shows how to cancel all cancelable tasks on multiple nodes. POST _tasks/_cancel?nodes=opensearch-node1,opensearch-node2 . copy . ",
    "url": "https://vagimeli.github.io/api-reference/tasks/#task-canceling",
    "relUrl": "/api-reference/tasks/#task-canceling"
  },"2416": {
    "doc": "Tasks",
    "title": "Attaching headers to tasks",
    "content": "To associate requests with tasks for better tracking, you can provide a X-Opaque-Id:&lt;ID_number&gt; header as part of the HTTPS request reader of your curl command. The API will attach the specified header in the returned result. Usage: . curl -i -H \"X-Opaque-Id: 111111\" \"https://localhost:9200/_tasks\" -u 'admin:admin' --insecure . copy . The _tasks operation returns the following result. HTTP/1.1 200 OK X-Opaque-Id: 111111 content-type: application/json; charset=UTF-8 content-length: 768 { \"nodes\": { \"Mgqdm0r9SEGClWxp_RbnaQ\": { \"name\": \"opensearch-node1\", \"transport_address\": \"172.18.0.4:9300\", \"host\": \"172.18.0.4\", \"ip\": \"172.18.0.4:9300\", \"roles\": [ \"data\", \"ingest\", \"master\", \"remote_cluster_client\" ], \"tasks\": { \"Mgqdm0r9SEGClWxp_RbnaQ:30072\": { \"node\": \"Mgqdm0r9SEGClWxp_RbnaQ\", \"id\": 30072, \"type\": \"direct\", \"action\": \"cluster:monitor/tasks/lists[n]\", \"start_time_in_millis\": 1613166701725, \"running_time_in_nanos\": 245400, \"cancellable\": false, \"parent_task_id\": \"Mgqdm0r9SEGClWxp_RbnaQ:30071\", \"headers\": { \"X-Opaque-Id\": \"111111\" } }, \"Mgqdm0r9SEGClWxp_RbnaQ:30071\": { \"node\": \"Mgqdm0r9SEGClWxp_RbnaQ\", \"id\": 30071, \"type\": \"transport\", \"action\": \"cluster:monitor/tasks/lists\", \"start_time_in_millis\": 1613166701725, \"running_time_in_nanos\": 658200, \"cancellable\": false, \"headers\": { \"X-Opaque-Id\": \"111111\" } } } } } } . This operation supports the same parameters as the tasks operation. The following example shows how you can associate X-Opaque-Id with specific tasks: . curl -i -H \"X-Opaque-Id: 123456\" \"https://localhost:9200/_tasks?nodes=opensearch-node1\" -u 'admin:admin' --insecure . copy . ",
    "url": "https://vagimeli.github.io/api-reference/tasks/#attaching-headers-to-tasks",
    "relUrl": "/api-reference/tasks/#attaching-headers-to-tasks"
  },"2417": {
    "doc": "Supported units",
    "title": "Supported units",
    "content": "OpenSearch supports the following units for all REST operations: . | Unit | Description | Example | . | Times | The supported units for time are d for days, h for hours, m for minutes, s for seconds, ms for milliseconds, micros for microseconds, and nanos for nanoseconds. | 5d or 7h | . | Bytes | The supported units for byte size are b for bytes, kb for kibibytes, mb for mebibytes, gb for gibibytes, tb for tebibytes, and pb for pebibytes. Despite the base-10 abbreviations, these units are base-2; 1kb is 1,024 bytes, 1mb is 1,048,576 bytes, etc. | 7kb or 6gb | . | Distances | The supported units for distance are mi for miles, yd for yards, ft for feet, in for inches, km for kilometers, m for meters, cm for centimeters, mm for millimeters, and nmi or NM for nautical miles. | 5mi or 4ft | . | Quantities without units | For large values that don’t have a unit, use k for kilo, m for mega, g for giga, t for tera, and p for peta. | 5k for 5,000 | . To convert output units to human-readable values, see Common REST parameters. ",
    "url": "https://vagimeli.github.io/api-reference/units/",
    "relUrl": "/api-reference/units/"
  },"2418": {
    "doc": "Common issues",
    "title": "Common issues",
    "content": "This page contains a list of common issues and workarounds. ",
    "url": "https://vagimeli.github.io/troubleshoot/index/",
    "relUrl": "/troubleshoot/index/"
  },"2419": {
    "doc": "Common issues",
    "title": "OpenSearch Dashboards fails to start",
    "content": "If you encounter the error FATAL Error: Request Timeout after 30000ms during startup, try running OpenSearch Dashboards on a more powerful machine. We recommend four CPU cores and 8 GB of RAM. ",
    "url": "https://vagimeli.github.io/troubleshoot/index/#opensearch-dashboards-fails-to-start",
    "relUrl": "/troubleshoot/index/#opensearch-dashboards-fails-to-start"
  },"2420": {
    "doc": "Common issues",
    "title": "Requests to OpenSearch Dashboards fail with “Request must contain a osd-xsrf header”",
    "content": "If you run legacy Kibana OSS scripts against OpenSearch Dashboards—for example, curl commands that import saved objects from a file—they might fail with the following error: . {\"status\": 400, \"body\": \"Request must contain a osd-xsrf header.\"} . In this case, your scripts likely include the \"kbn-xsrf: true\" header. Switch it to the osd-xsrf: true header: . curl -XPOST -u 'admin:admin' 'https://DASHBOARDS_ENDPOINT/api/saved_objects/_import' -H 'osd-xsrf:true' --form file=@export.ndjson . ",
    "url": "https://vagimeli.github.io/troubleshoot/index/#requests-to-opensearch-dashboards-fail-with-request-must-contain-a-osd-xsrf-header",
    "relUrl": "/troubleshoot/index/#requests-to-opensearch-dashboards-fail-with-request-must-contain-a-osd-xsrf-header"
  },"2421": {
    "doc": "Common issues",
    "title": "Multi-tenancy issues in OpenSearch Dashboards",
    "content": "If you’re testing multiple users in OpenSearch Dashboards and encounter unexpected changes in tenant, use Google Chrome in an Incognito window or Firefox in a Private window. ",
    "url": "https://vagimeli.github.io/troubleshoot/index/#multi-tenancy-issues-in-opensearch-dashboards",
    "relUrl": "/troubleshoot/index/#multi-tenancy-issues-in-opensearch-dashboards"
  },"2422": {
    "doc": "Common issues",
    "title": "Expired certificates",
    "content": "If your certificates have expired, you might receive the following error or something similar: . ERROR org.opensearch.security.ssl.transport.SecuritySSLNettyTransport - Exception during establishing a SSL connection: javax.net.ssl.SSLHandshakeException: PKIX path validation failed: java.security.cert.CertPathValidatorException: validity check failed Caused by: java.security.cert.CertificateExpiredException: NotAfter: Thu Sep 16 11:27:55 PDT 2021 . To check the expiration date for a certificate, run this command: . openssl x509 -enddate -noout -in &lt;certificate&gt; . ",
    "url": "https://vagimeli.github.io/troubleshoot/index/#expired-certificates",
    "relUrl": "/troubleshoot/index/#expired-certificates"
  },"2423": {
    "doc": "Common issues",
    "title": "Encryption at rest",
    "content": "The operating system for each OpenSearch node handles encryption of data at rest. To enable encryption at rest in most Linux distributions, use the cryptsetup command: . cryptsetup luksFormat --key-file &lt;key&gt; &lt;partition&gt; . For full documentation about the command, see cryptsetup(8) — Linux manual page. ",
    "url": "https://vagimeli.github.io/troubleshoot/index/#encryption-at-rest",
    "relUrl": "/troubleshoot/index/#encryption-at-rest"
  },"2424": {
    "doc": "Common issues",
    "title": "Can’t update by script when FLS, DLS, or field masking is active",
    "content": "The Security plugin blocks the update by script operation (POST &lt;index&gt;/_update/&lt;id&gt;) when field-level security, document-level security, or field masking are active. You can still update documents using the standard index operation (PUT &lt;index&gt;/_doc/&lt;id&gt;). ",
    "url": "https://vagimeli.github.io/troubleshoot/index/#cant-update-by-script-when-fls-dls-or-field-masking-is-active",
    "relUrl": "/troubleshoot/index/#cant-update-by-script-when-fls-dls-or-field-masking-is-active"
  },"2425": {
    "doc": "Common issues",
    "title": "Illegal reflective access operation in logs",
    "content": "This is a known issue with Performance Analyzer that shouldn’t affect functionality. ",
    "url": "https://vagimeli.github.io/troubleshoot/index/#illegal-reflective-access-operation-in-logs",
    "relUrl": "/troubleshoot/index/#illegal-reflective-access-operation-in-logs"
  },"2426": {
    "doc": "Troubleshoot OpenID Connect",
    "title": "OpenID Connect troubleshooting",
    "content": "This page includes troubleshooting steps for using OpenID Connect with the Security plugin. . | Set log level to debug | “Failed when trying to obtain the endpoints from your IdP” | “ValidationError: child ‘opensearch_security’ fails” | “Authentication failed. Please provide a new token.” . | Leftover cookies or cached credentials | Wrong client secret | “Failed to get subject from JWT claims” | “Failed to get roles from JWT claims with roles_key” | . | . ",
    "url": "https://vagimeli.github.io/troubleshoot/openid-connect/#openid-connect-troubleshooting",
    "relUrl": "/troubleshoot/openid-connect/#openid-connect-troubleshooting"
  },"2427": {
    "doc": "Troubleshoot OpenID Connect",
    "title": "Set log level to debug",
    "content": "To help troubleshoot OpenID Connect, set the log level to debug on OpenSearch. Add the following lines in config/log4j2.properties and restart the node: . logger.securityjwt.name = com.amazon.dlic.auth.http.jwt logger.securityjwt.level = trace . This setting prints a lot of helpful information to your log file. If this information isn’t sufficient, you can also set the log level to trace. ",
    "url": "https://vagimeli.github.io/troubleshoot/openid-connect/#set-log-level-to-debug",
    "relUrl": "/troubleshoot/openid-connect/#set-log-level-to-debug"
  },"2428": {
    "doc": "Troubleshoot OpenID Connect",
    "title": "“Failed when trying to obtain the endpoints from your IdP”",
    "content": "This error indicates that the Security plugin can’t reach the metadata endpoint of your IdP. In opensearch_dashboards.yml, check the following setting: . plugins.security.openid.connect_url: \"http://keycloak.example.com:8080/auth/realms/master/.well-known/openid-configuration\" . If this error occurs on OpenSearch, check the following setting in config.yml: . openid_auth_domain: enabled: true order: 1 http_authenticator: type: \"openid\" ... config: openid_connect_url: http://keycloak.examplesss.com:8080/auth/realms/master/.well-known/openid-configuration ... ",
    "url": "https://vagimeli.github.io/troubleshoot/openid-connect/#failed-when-trying-to-obtain-the-endpoints-from-your-idp",
    "relUrl": "/troubleshoot/openid-connect/#failed-when-trying-to-obtain-the-endpoints-from-your-idp"
  },"2429": {
    "doc": "Troubleshoot OpenID Connect",
    "title": "“ValidationError: child ‘opensearch_security’ fails”",
    "content": "This indicates that one or more of the OpenSearch Dashboards configuration settings are missing. Check opensearch_dashboards.yml and make sure you have set the following minimal configuration: . plugins.security.openid.connect_url: \"...\" plugins.security.openid.client_id: \"...\" plugins.security.openid.client_secret: \"...\" . ",
    "url": "https://vagimeli.github.io/troubleshoot/openid-connect/#validationerror-child-opensearch_security-fails",
    "relUrl": "/troubleshoot/openid-connect/#validationerror-child-opensearch_security-fails"
  },"2430": {
    "doc": "Troubleshoot OpenID Connect",
    "title": "“Authentication failed. Please provide a new token.”",
    "content": "This error has several potential root causes. Leftover cookies or cached credentials . Please delete all cached browser data, or try again in a private browser window. Wrong client secret . To trade the access token for an identity token, most IdPs require you to provide a client secret. Check if the client secret in opensearch_dashboards.yml matches the client secret of your IdP configuration: . plugins.security.openid.client_secret: \"...\" . “Failed to get subject from JWT claims” . This error is logged on OpenSearch and means that the username could not be extracted from the ID token. Make sure the following setting matches the claims in the JWT your IdP issues: . openid_auth_domain: enabled: true order: 1 http_authenticator: type: \"openid\" ... config: subject_key: &lt;subject key&gt; ... “Failed to get roles from JWT claims with roles_key” . This error indicates that the roles key you configured in config.yml does not exist in the JWT issued by your IdP. Make sure the following setting matches the claims in the JWT your IdP issues: . openid_auth_domain: enabled: true order: 1 http_authenticator: type: \"openid\" ... config: roles_key: &lt;roles key&gt; ... ",
    "url": "https://vagimeli.github.io/troubleshoot/openid-connect/#authentication-failed-please-provide-a-new-token",
    "relUrl": "/troubleshoot/openid-connect/#authentication-failed-please-provide-a-new-token"
  },"2431": {
    "doc": "Troubleshoot OpenID Connect",
    "title": "Troubleshoot OpenID Connect",
    "content": " ",
    "url": "https://vagimeli.github.io/troubleshoot/openid-connect/",
    "relUrl": "/troubleshoot/openid-connect/"
  },"2432": {
    "doc": "Troubleshoot SAML",
    "title": "SAML troubleshooting",
    "content": "This page includes troubleshooting steps for using SAML for OpenSearch Dashboards authentication. . | Check sp.entity_id | Check the SAML assertion consumer service URL | Sign all documents | Role settings | Inspect the SAML response | Check role mapping | Inspect the JWT token | . ",
    "url": "https://vagimeli.github.io/troubleshoot/saml/#saml-troubleshooting",
    "relUrl": "/troubleshoot/saml/#saml-troubleshooting"
  },"2433": {
    "doc": "Troubleshoot SAML",
    "title": "Check sp.entity_id",
    "content": "Most identity providers (IdPs) allow you to configure multiple authentication methods for different applications. For example, in Okta, these clients are called “Applications.” In Keycloak, they are called “Clients.” Each one has its own entity ID. Make sure to configure sp.entity_id to match those settings: . saml: ... http_authenticator: type: 'saml' challenge: true config: ... sp: entity_id: opensearch-dashboards-saml . ",
    "url": "https://vagimeli.github.io/troubleshoot/saml/#check-spentity_id",
    "relUrl": "/troubleshoot/saml/#check-spentity_id"
  },"2434": {
    "doc": "Troubleshoot SAML",
    "title": "Check the SAML assertion consumer service URL",
    "content": "After a successful login, your IdP sends a SAML response using HTTP POST to OpenSearch Dashboards’s “assertion consumer service URL” (ACS). The endpoint the OpenSearch Dashboards Security plugin provides is: . /_opendistro/_security/saml/acs . Make sure that you have configured this endpoint correctly in your IdP. Some IdPs also require you to add all endpoints to the allow list that they send requests to. Ensure that the ACS endpoint is listed. OpenSearch Dashboards also requires you to add this endpoint to the allow list. Make sure you have the following entry in opensearch_dashboards.yml: . server.xsrf.allowlist: [/_opendistro/_security/saml/acs] . ",
    "url": "https://vagimeli.github.io/troubleshoot/saml/#check-the-saml-assertion-consumer-service-url",
    "relUrl": "/troubleshoot/saml/#check-the-saml-assertion-consumer-service-url"
  },"2435": {
    "doc": "Troubleshoot SAML",
    "title": "Sign all documents",
    "content": "Some IdPs do not sign the SAML documents by default. Make sure the IdP signs all documents. Keycloak . ",
    "url": "https://vagimeli.github.io/troubleshoot/saml/#sign-all-documents",
    "relUrl": "/troubleshoot/saml/#sign-all-documents"
  },"2436": {
    "doc": "Troubleshoot SAML",
    "title": "Role settings",
    "content": "Including user roles in the SAML response is dependent on your IdP. For example, in Keycloak, this setting is in the Mappers section of your client. In Okta, you have to set group attribute statements. Make sure this is configured correctly and that the roles_key in the SAML configuration matches the role name in the SAML response: . saml: ... http_authenticator: type: 'saml' challenge: true config: ... roles_key: Role . ",
    "url": "https://vagimeli.github.io/troubleshoot/saml/#role-settings",
    "relUrl": "/troubleshoot/saml/#role-settings"
  },"2437": {
    "doc": "Troubleshoot SAML",
    "title": "Inspect the SAML response",
    "content": "If you are not sure what the SAML response of your IdP contains and where it places the username and roles, you can enable debug mode in the log4j2.properties: . logger.token.name = com.amazon.dlic.auth.http.saml.Token logger.token.level = debug . This setting prints the SAML response to the OpenSearch log file so that you can inspect and debug it. Setting this logger to debug generates many statements, so we don’t recommend using it in production. Another way of inspecting the SAML response is to monitor network traffic while logging in to OpenSearch Dashboards. The IdP uses HTTP POST requests to send Base64-encoded SAML responses to: . /_opendistro/_security/saml/acs . Inspect the payload of this POST request, and use a tool like base64decode.org to decode it. ",
    "url": "https://vagimeli.github.io/troubleshoot/saml/#inspect-the-saml-response",
    "relUrl": "/troubleshoot/saml/#inspect-the-saml-response"
  },"2438": {
    "doc": "Troubleshoot SAML",
    "title": "Check role mapping",
    "content": "The Security plugin uses a standard role mapping to map a user or backend role to one or more Security roles. For username, the Security plugin uses the NameID attribute of the SAML response by default. For some IdPs, this attribute does not contain the expected username, but some internal user ID. Check the content of the SAML response to locate the element you want to use as username, and configure it by setting the subject_key: . saml: ... http_authenticator: type: 'saml' challenge: true config: ... subject_key: preferred_username . For checking that the correct backend roles are contained in the SAML response, inspect the contents, and set the correct attribute name: . saml: ... http_authenticator: type: 'saml' challenge: true config: ... roles_key: Role . ",
    "url": "https://vagimeli.github.io/troubleshoot/saml/#check-role-mapping",
    "relUrl": "/troubleshoot/saml/#check-role-mapping"
  },"2439": {
    "doc": "Troubleshoot SAML",
    "title": "Inspect the JWT token",
    "content": "The Security plugin trades the SAML response for a more lightweight JSON web token. The username and backend roles in the JWT are ultimately mapped to roles in the Security plugin. If there is a problem with the mapping, you can enable the token debug mode using the same setting as Inspect the SAML response. This setting prints the JWT to the OpenSearch log file so that you can inspect and debug it using a tool like JWT.io. ",
    "url": "https://vagimeli.github.io/troubleshoot/saml/#inspect-the-jwt-token",
    "relUrl": "/troubleshoot/saml/#inspect-the-jwt-token"
  },"2440": {
    "doc": "Troubleshoot SAML",
    "title": "Troubleshoot SAML",
    "content": " ",
    "url": "https://vagimeli.github.io/troubleshoot/saml/",
    "relUrl": "/troubleshoot/saml/"
  },"2441": {
    "doc": "Troubleshoot securityadmin.sh",
    "title": "securityadmin.sh Troubleshooting",
    "content": "This page includes troubleshooting steps for securityadmin.sh. The script can be found at /plugins/opensearch-security/tools/securityadmin.sh. For more information about using this tool, see Applying changes to configuration files. . | Cluster not reachable . | Check hostname | Check the port | . | None of the configured nodes are available . | Check cluster name | Check hostname verification | Check cluster state | Check the security index name | . | “ERR: DN is not an admin user” | Use the diagnose option | . ",
    "url": "https://vagimeli.github.io/troubleshoot/security-admin/#securityadminsh-troubleshooting",
    "relUrl": "/troubleshoot/security-admin/#securityadminsh-troubleshooting"
  },"2442": {
    "doc": "Troubleshoot securityadmin.sh",
    "title": "Cluster not reachable",
    "content": "If securityadmin.sh can’t reach the cluster, it outputs: . OpenSearch Security Admin v6 Will connect to localhost:9300 ERR: Seems there is no opensearch running on localhost:9300 - Will exit . Check hostname . By default, securityadmin.sh uses localhost. If your cluster runs on any other host, specify the hostname using the -h option. Check the port . Check that you are running securityadmin.sh against the transport port, not the HTTP port. By default, securityadmin.sh uses 9300. If your cluster runs on a different port, use the -p option to specify the port number. ",
    "url": "https://vagimeli.github.io/troubleshoot/security-admin/#cluster-not-reachable",
    "relUrl": "/troubleshoot/security-admin/#cluster-not-reachable"
  },"2443": {
    "doc": "Troubleshoot securityadmin.sh",
    "title": "None of the configured nodes are available",
    "content": "If securityadmin.sh can reach the cluster, but can’t update the configuration, it outputs this error: . Contacting opensearch cluster 'opensearch' and wait for YELLOW clusterstate ... Cannot retrieve cluster state due to: None of the configured nodes are available: [{#transport#-1}{mr2NlX3XQ3WvtVG0Dv5eHw}{localhost}{127.0.0.1:9300}]. This is not an error, will keep on trying ... | Try running securityadmin.sh with -icl and -nhnv. If this works, check your cluster name as well as the hostnames in your SSL certificates. If this does not work, try running securityadmin.sh with --diagnose and see diagnose trace log file. | Add --accept-red-cluster to allow securityadmin.sh to operate on a red cluster. | . Check cluster name . By default, securityadmin.sh uses opensearch as the cluster name. If your cluster has a different name, you can either ignore the name completely using the -icl option or specify the name using the -cn option. Check hostname verification . By default, securityadmin.sh verifies that the hostname in your node’s certificate matches the node’s actual hostname. If this is not the case (e.g. if you’re using the demo certificates), you can disable hostname verification by adding the -nhnv option. Check cluster state . By default, securityadmin.sh only executes if the cluster state is at least yellow. If your cluster state is red, you can still execute securityadmin.sh, but you need to add the -arc option. Check the security index name . By default, the Security plugin uses .opendistro_security as the name of the configuration index. If you configured a different index name in opensearch.yml, specify it using the -i option. ",
    "url": "https://vagimeli.github.io/troubleshoot/security-admin/#none-of-the-configured-nodes-are-available",
    "relUrl": "/troubleshoot/security-admin/#none-of-the-configured-nodes-are-available"
  },"2444": {
    "doc": "Troubleshoot securityadmin.sh",
    "title": "“ERR: DN is not an admin user”",
    "content": "If the TLS certificate used to start securityadmin.sh isn’t an admin certificate, the script outputs: . Connected as CN=node-0.example.com,OU=SSL,O=Test,L=Test,C=DE ERR: CN=node-0.example.com,OU=SSL,O=Test,L=Test,C=DE is not an admin user . You must use an admin certificate when executing the script. To learn more, see Configuring admin certificates. ",
    "url": "https://vagimeli.github.io/troubleshoot/security-admin/#err-dn-is-not-an-admin-user",
    "relUrl": "/troubleshoot/security-admin/#err-dn-is-not-an-admin-user"
  },"2445": {
    "doc": "Troubleshoot securityadmin.sh",
    "title": "Use the diagnose option",
    "content": "For more information on why securityadmin.sh is not executing, add the --diagnose option: ./securityadmin.sh -diagnose -cd ../../../config/opensearch-security/ -cacert ... -cert ... -key ... -keypass ... The script prints the location of the generated diagnostic file. ",
    "url": "https://vagimeli.github.io/troubleshoot/security-admin/#use-the-diagnose-option",
    "relUrl": "/troubleshoot/security-admin/#use-the-diagnose-option"
  },"2446": {
    "doc": "Troubleshoot securityadmin.sh",
    "title": "Troubleshoot securityadmin.sh",
    "content": " ",
    "url": "https://vagimeli.github.io/troubleshoot/security-admin/",
    "relUrl": "/troubleshoot/security-admin/"
  },"2447": {
    "doc": "Troubleshoot TLS",
    "title": "TLS troubleshooting",
    "content": "This page includes troubleshooting steps for configuring TLS certificates with the Security plugin. . | Validate YAML | View contents of PEM certificates . | Check for special characters and whitespace in DNs | Check certificate IP addresses | Validate certificate chain | Check the configured alias | . | View contents of your keystore and truststore | Check SAN hostnames and IP addresses | Check OID for node certificates | Check EKU field for node certificates | TLS versions | Supported ciphers | . ",
    "url": "https://vagimeli.github.io/troubleshoot/tls/#tls-troubleshooting",
    "relUrl": "/troubleshoot/tls/#tls-troubleshooting"
  },"2448": {
    "doc": "Troubleshoot TLS",
    "title": "Validate YAML",
    "content": "opensearch.yml and the files in config/opensearch-security/ are in the YAML format. A linter like YAML Validator can help verify that you don’t have any formatting errors. ",
    "url": "https://vagimeli.github.io/troubleshoot/tls/#validate-yaml",
    "relUrl": "/troubleshoot/tls/#validate-yaml"
  },"2449": {
    "doc": "Troubleshoot TLS",
    "title": "View contents of PEM certificates",
    "content": "You can use OpenSSL to display the content of each PEM certificate: . openssl x509 -subject -nameopt RFC2253 -noout -in node1.pem . Then ensure that the value matches the one in opensearch.yml. For more complete information on a certificate: . openssl x509 -in node1.pem -text -noout . Check for special characters and whitespace in DNs . The Security plugin uses the string representation of Distinguished Names (RFC1779) when validating node certificates. If parts of your DN contain special characters (e.g. a comma), make sure you escape it in your configuration: . plugins.security.nodes_dn: - 'CN=node-0.example.com,OU=SSL,O=My\\, Test,L=Test,C=DE' . You can have whitespace within a field, but not between fields. Bad configuration . plugins.security.nodes_dn: - 'CN=node-0.example.com, OU=SSL,O=My\\, Test, L=Test, C=DE' . Good configuration . plugins.security.nodes_dn: - 'CN=node-0.example.com,OU=SSL,O=My\\, Test,L=Test,C=DE' . Check certificate IP addresses . Sometimes the IP address in your certificate is not the one communicating with the cluster. This problem can occur if your node has multiple interfaces or is running on a dual stack network (IPv6 and IPv4). If this problem occurs, you might see the following in the node’s OpenSearch log: . SSL Problem Received fatal alert: certificate_unknown javax.net.ssl.SSLException: Received fatal alert: certificate_unknown . You might also see the following message in your cluster’s master log when the new node tries to join the cluster: . Caused by: java.security.cert.CertificateException: No subject alternative names matching IP address 10.0.0.42 found . Check the IP address in the certificate: . IPAddress: 2001:db8:0:1:1.2.3.4 . In this example, the node tries to join the cluster with the IPv4 address of 10.0.0.42, but the certificate contains the IPv6 address of 2001:db8:0:1:1.2.3.4. Validate certificate chain . TLS certificates are organized in a certificate chain. You can check with keytool that the certificate chain is correct by inspecting the owner and the issuer of each certificate. If you used the demo installation script that ships with the Security plugin, the chain looks like: . Node certificate . Owner: CN=node-0.example.com, OU=SSL, O=Test, L=Test, C=DE Issuer: CN=Example Com Inc. Signing CA, OU=Example Com Inc. Signing CA, O=Example Com Inc., DC=example, DC=com . Signing certificate . Owner: CN=Example Com Inc. Signing CA, OU=Example Com Inc. Signing CA, O=Example Com Inc., DC=example, DC=com Issuer: CN=Example Com Inc. Root CA, OU=Example Com Inc. Root CA, O=Example Com Inc., DC=example, DC=com . Root certificate . Owner: CN=Example Com Inc. Root CA, OU=Example Com Inc. Root CA, O=Example Com Inc., DC=example, DC=com Issuer: CN=Example Com Inc. Root CA, OU=Example Com Inc. Root CA, O=Example Com Inc., DC=example, DC=com . From the entries, you can see that the root certificate signed the intermediate certificate, which signed the node certificate. The root certificate signed itself, hence the name “self-signed certificate.” If you’re using separate keystore and truststore files, your root CA can most likely in the truststore. Generally, the keystore contains client or node certificate and all intermediate certificates, and the truststore contains the root certificate. Check the configured alias . If you have multiple entries in the keystore and you are using aliases to refer to them, make sure that the configured alias in opensearch.yml matches the one in the keystore. If there is only one entry in the keystore, you do not need to configure an alias. ",
    "url": "https://vagimeli.github.io/troubleshoot/tls/#view-contents-of-pem-certificates",
    "relUrl": "/troubleshoot/tls/#view-contents-of-pem-certificates"
  },"2450": {
    "doc": "Troubleshoot TLS",
    "title": "View contents of your keystore and truststore",
    "content": "In order to view information about the certificates stored in your keystore or truststore, use the keytool command like: . keytool -list -v -keystore keystore.jks . keytool prompts for the password of the keystore and lists all entries. For example, you can use this output to check for the correctness of the SAN and EKU settings. ",
    "url": "https://vagimeli.github.io/troubleshoot/tls/#view-contents-of-your-keystore-and-truststore",
    "relUrl": "/troubleshoot/tls/#view-contents-of-your-keystore-and-truststore"
  },"2451": {
    "doc": "Troubleshoot TLS",
    "title": "Check SAN hostnames and IP addresses",
    "content": "The valid hostnames and IP addresses of a TLS certificates are stored as SAN entries. Check that the hostname and IP entries in the SAN section are correct, especially when you use hostname verification: . Certificate[1]: Owner: CN=node-0.example.com, OU=SSL, O=Test, L=Test, C=DE ... Extensions: ... #5: ObjectId: 2.5.29.17 Criticality=false SubjectAlternativeName [ DNSName: node-0.example.com DNSName: localhost IPAddress: 127.0.0.1 ... ] . ",
    "url": "https://vagimeli.github.io/troubleshoot/tls/#check-san-hostnames-and-ip-addresses",
    "relUrl": "/troubleshoot/tls/#check-san-hostnames-and-ip-addresses"
  },"2452": {
    "doc": "Troubleshoot TLS",
    "title": "Check OID for node certificates",
    "content": "If you are using OIDs to denote valid node certificates, check that the SAN extension for your node certificate contains the correct OIDName: . Certificate[1]: Owner: CN=node-0.example.com, OU=SSL, O=Test, L=Test, C=DE ... Extensions: ... #5: ObjectId: 2.5.29.17 Criticality=false SubjectAlternativeName [ ... OIDName: 1.2.3.4.5.5 ] . ",
    "url": "https://vagimeli.github.io/troubleshoot/tls/#check-oid-for-node-certificates",
    "relUrl": "/troubleshoot/tls/#check-oid-for-node-certificates"
  },"2453": {
    "doc": "Troubleshoot TLS",
    "title": "Check EKU field for node certificates",
    "content": "Node certificates need to have both serverAuth and clientAuth set in the extended key usage field: . #3: ObjectId: 2.5.29.37 Criticality=false ExtendedKeyUsages [ serverAuth clientAuth ] . ",
    "url": "https://vagimeli.github.io/troubleshoot/tls/#check-eku-field-for-node-certificates",
    "relUrl": "/troubleshoot/tls/#check-eku-field-for-node-certificates"
  },"2454": {
    "doc": "Troubleshoot TLS",
    "title": "TLS versions",
    "content": "The Security plugin disables TLS version 1.0 by default; it is outdated, insecure, and vulnerable. If you need to use TLSv1 and accept the risks, you can enable it in opensearch.yml: . plugins.security.ssl.http.enabled_protocols: - \"TLSv1\" - \"TLSv1.1\" - \"TLSv1.2\" . ",
    "url": "https://vagimeli.github.io/troubleshoot/tls/#tls-versions",
    "relUrl": "/troubleshoot/tls/#tls-versions"
  },"2455": {
    "doc": "Troubleshoot TLS",
    "title": "Supported ciphers",
    "content": "TLS relies on the server and client negotiating a common cipher suite. Depending on your system, the available ciphers will vary. They depend on the JDK or OpenSSL version you’re using, and whether or not the JCE Unlimited Strength Jurisdiction Policy Files are installed. For legal reasons, the JDK does not include strong ciphers like AES256. In order to use strong ciphers you need to download and install the Java Cryptography Extension (JCE) Unlimited Strength Jurisdiction Policy Files. If you don’t have them installed, you might see an error message on startup: . [INFO ] AES-256 not supported, max key length for AES is 128 bit. That is not an issue, it just limits possible encryption strength. To enable AES 256 install 'Java Cryptography Extension (JCE) Unlimited Strength Jurisdiction Policy Files' . The Security plugin still works and falls back to weaker cipher suites. The plugin also prints out all available cipher suites during startup: . [INFO ] sslTransportClientProvider: JDK with ciphers [TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256, TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256, TLS_DHE_RSA_WITH_AES_128_CBC_SHA256, TLS_DHE_DSS_WITH_AES_128_CBC_SHA256, ...] . ",
    "url": "https://vagimeli.github.io/troubleshoot/tls/#supported-ciphers",
    "relUrl": "/troubleshoot/tls/#supported-ciphers"
  },"2456": {
    "doc": "Troubleshoot TLS",
    "title": "Troubleshoot TLS",
    "content": " ",
    "url": "https://vagimeli.github.io/troubleshoot/tls/",
    "relUrl": "/troubleshoot/tls/"
  }
}
